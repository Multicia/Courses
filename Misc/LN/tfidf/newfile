0: scikitlearn user guide release 0.11 scikitlearn developers june contents user guide 
1:  
2:  
3:  
4:  
5:  
6:  
7: installing scikitlearn tutorials bottom scikitlearn supervised learning 1.1 1.2 1.3 1.4 unsupervised learning 1.5 model selection 1.6 dataset transformations 1.7 dataset loading utilities 1.8 
8: reference 
9:  
10:  
11:  
12:  
13:  
14:  
15:  
16:  
17:  
18:  
19:  
20: example gallery examples 
21: 2.1 
22:  
23:  
24:  
25:  
26:  
27:  
28:  
29:  
30:  
31:  
32:  
33:  
34:  
35: development 
36:  
37:  
38:  
39:  
40:  
41:  
42: contributing 
43: 3.1 3.2 optimize speed 3.3 utilities developers 3.4 developers tips debugging 3.5 support 3.6 0.11 3.7 0.10 3.8 3.9 0.9 3.10 0.8 3.11 0.7 3.12 0.6 3.13 0.5 3.14 0.4 3.15 presentations tutorials scikitlearn 
44:  
45:  
46:  
47:  
48:  
49:  
50:  
51:  
52:  
53:  
54:  
55: bibliography python module index python module index index scikitlearn user guide release 0.11 scikitlearn python module integrating classic machine learning algorithms tightlyknit sci entic python world numpy scipy matplotlib aims provide simple efcient solutions learning problems accessible everybody reusable various contexts machinelearning versatile tool science engineering 
56: license open source commercially usable bsd license clause documentation scikitlearn version 0.11. versions printable format see documentation resources 
57: contents scikitlearn user guide release 0.11 contents chapter one user guide 1.1 installing scikitlearn different ways get scikitlearn installed install version scikitlearn provided operating system distribution quickest option operating systems distribute scikitlearn 
58: install ofcial release best approach users want stable version number arent concerned running slightly older version scikitlearn 
59: install latest development version best users want latestandgreatest features arent afraid running brandnew code 
60: note wish contribute project recommended install latest development version 
61: 1.1.1 installing ofcial release installing source installing source requires installed python 2.6 numpy 1.3 scipy 0.7 setuptools python development headers working compiler debianbased systems get executing root privileges sudo aptget install pythondev pythonnumpy pythonnumpydev pythonsetuptools pythonnumpydev pythonscipy libatlasdev order build documentation run example code contains documentation need note matplotlib sudo aptget install pythonmatplotlib note ubuntu lts 10.04 package libatlasdev called libatlasheaders easy install usually fastest way install latest stable release pip easy_install install update command scikitlearn user guide release 0.11 pip install scikitlearn easy_install scikitlearn easy_install note might need root privileges run commands 
62: source package download package http pypi.python.orgpypiscikitlearn unpack sources archive packages uses distutils default way installing python modules install command python setup.py install windows installer download windows installer downloads projects web page note must also installed packages numpy setuptools package also expected work python 2.6.5.5 
63: installing windows 64bit http www.lfd.uci.edugohlkepythonlibs scikitlearn note numpy scipy matplotlib easiest option also download url 
64: require compatible version download 64bit version install binaries scikit building windows build scikitlearn windows need compiler addition numpy scipy setuptools least mingw port gcc windows microsoft visual work box force use particular compiler write named setup.cfg source directory content build_ext compilermy_compiler build compilermy_compiler my_compiler one mingw32 msvc appropriate compiler set assuming python path see python faq windows details installation done executing command python setup.py install build precompiled package like ones distributed downloads section command execute python setup.py bdist_wininst doclogosscikitlearnlogo.bmp create installable binary directory dist 
65: chapter user guide scikitlearn user guide release 0.11 1.1.2 third party distributions scikitlearn thirdparty distributions providing versions scikitlearn integrated packagemanagement systems make installation upgrading much easier users since integration includes ability automat ically install dependencies numpy scipy scikitlearn requires following list linux distributions provide version scikitlearn debian derivatives ubuntu debian package named pythonsklearn formerly pythonscikitslearn installed using following commands root privileges aptget install pythonsklearn additionally backport builds recent release scikitlearn existing releases debian ubuntu available neurodebian repository 
66: python python distributes scikitlearn additional plugin found additional plugins page 
67: enthought python distribution enthought python distribution already ships recent version 
68: macports macports package named py26sklearn py27sklearn depending version python installed typing following command sudo port install py26scikitslearn sudo port install py27scikitslearn depending version python want use 
69: netbsd scikitlearn available via pkgsrcwip http pkgsrc.sewippyscikit_learn 1.1.3 bleeding edge see section retrieving latest code get development version 
70: 1.1 
71: installing scikitlearn scikitlearn user guide release 0.11 1.1.4 testing testing requires nose library installation package tested executing outside source directory nosetests sklearn exe give lot output warnings eventually nish text similar ran tests 27.920s skip2 otherwise please consider posting issue bug tracker mailing list 
72: note alternative testing method reason recommended method failing please try alternate method python import sklearn sklearn.test method might display doctest failures nosetests issues 
73: scikitlearn also tested without package installed must compile sources inplace source directory python setup.py build_ext inplace test run using nosetests nosetests sklearn automated commands make make test 1.2 tutorials bottom scikitlearn quick start section introduce machine learning vocabulary use throughout scikitlearn give simple learning example 
74: 1.2.1 introduction machine learning scikitlearn section contents section introduce machine learning vocabulary use throughout scikitlearn give simple learning example 
75: chapter user guide scikitlearn user guide release 0.11 machine learning problem setting general learning problem considers set samples data try predict properties unknown data sample single number instance multidimensional entry aka multivariate data said several attributes features separate learning problems large categories supervised learning data comes additional attributes want predict click scikitlearn supervised learning page .this problem either classication samples belong two classes want learn already labeled data predict class unlabeled data example classication problem would digit recognition example aim assign input vector one nite number discrete categories 
76: regression desired output consists one continuous variables task called regression example regression problem would prediction length salmon function age weight 
77: unsupervised learning training data consists set input vectors without corresponding target values goal problems may discover groups similar examples within data called clustering determine distribution data within input space known density estima tion project data highdimensional space two thee dimensions purpose visualization click scikitlearn unsupervised learning page 
78: training set testing set machine learning learning properties data set applying new data common practice machine learning evaluate algorithm split data hand two sets one call training set learn data properties one call testing set test properties 
79: loading example dataset scikitlearn comes standard datasets instance iris digits datasets classication boston house prices dataset regression sklearn import datasets iris datasets.load_iris digits datasets.load_digits dataset dictionarylike object holds data metadata data data stored .data member n_samples n_features array case supervised problem explanatory variables stored .target member details different datasets found dedicated section instance case digits dataset digits.data gives access features used classify digits samples 
80: print digits.data ... ... ... ... ... ... ... 
81:  
82:  
83:  
84:  
85:  
86:  
87: 1.2. tutorials bottom scikitlearn scikitlearn user guide release 0.11 digits.target gives ground truth digit dataset number corresponding digit image trying learn digits.target array ... shape data arrays data always array n_samples n_features although original data may different shape case digits original sample image shape accessed using digits.images array 13. 15. 12. 11. 14. 13. 15. 10. 10. 13. 10. 15. 11. 12. 12. simple example dataset illustrates starting original problem one shape data consumption scikitlearn 
88: learning predicting case digits dataset task predict value handwritten digit image given samples possible classes estimator able predict labels corresponding new data scikitlearn estimator plain python class implements methods predict example estimator class sklearn.svm.svc implements support vector classication con structor estimator takes arguments parameters model time consider estimator black box sklearn import svm clf svm.svc gamma0.001 c100 choosing parameters model example set value gamma manually possible automatically good values parameters using tools grid search cross validation 
89: call estimator instance clf classier must tted model must learn model done passing training set fit method training set let use images dataset apart last one clf.fit digits.data digits.target svc c100.0 cache_size200 class_weightnone coef00.0 degree3 gamma0.001 kernelrbf probabilityfalse shrinkingtrue tol0.001 verbosefalse predict new values particular ask classier digit last image digits dataset used train classier chapter user guide clf.predict digits.data array scikitlearn user guide release 0.11 corresponding image following images poor resolution agree classier complete example classication problem available example run study recognizing handwritten digits 
90: see challenging task model persistence possible save model scikit using pythons builtin persistence model namely pickle sklearn import svm sklearn import datasets clf svm.svc iris datasets.load_iris iris.data iris.target clf.fit svc c1.0 cache_size200 class_weightnone coef00.0 degree3 gamma0.25 kernelrbf probabilityfalse shrinkingtrue tol0.001 verbosefalse import pickle pickle.dumps clf clf2 pickle.loads clf2.predict array specic case scikit may interesting use joblibs replacement pickle joblib.dump joblib.load efcient big data pickle disk string sklearn.externals import joblib joblib.dump clf filename.pkl statisticallearning tutorial tutorial covers models tools available dataprocessing scikit learn learn data 
91: 1.2.2 tutorial statisticallearning scientic data processing 1.2. tutorials bottom scikitlearn scikitlearn user guide release 0.11 statistical learning machine learning technique growing importance size datasets experimental sciences facing rapidly growing problems tackles range building prediction function linking different observations classifying observations learning structure unlabeled dataset tutorial explore statistical learning use machine learning techniques goal statistical inference drawing conclusions data hand sklearn python module integrating classic machine learning algorithms tightlyknit world scien tic python packages numpy scipy matplotlib 
92: warning crossversion compatibility use scikitlearn release 0.9 import path changed scikits.learn sklearn import try sklearn import something except importerror scikits.learn import something statistical learning setting estimator object scikitlearn datasets scikitlearn deals learning information one datasets represented arrays understood list multidimensional observations say rst axis arrays samples axis second features axis 
93: simple example shipped scikit iris dataset sklearn import datasets iris datasets.load_iris data iris.data data.shape made observations irises described features sepal petal length width detailed iris.descr 
94: data intially n_samples n_features shape needs preprocessed used scikit 
95: chapter user guide example reshaping data digits dataset scikitlearn user guide release 0.11 digits dataset made 8x8 images handwritten digits digits datasets.load_digits digits.images.shape import pylab pl.imshow digits.images cmappl.cm.gray_r matplotlib.image.axesimage object ... use dataset scikit transform 8x8 image feature vector length data digits.images.reshape digits.images.shape estimators objects fitting data core object scikitlearn estimator object estimator objects expose method takes dataset array estimator.fit data estimator parameters parameters estimator set instanciated modifying corresponding attribute estimator estimator param11 param22 estimator.param1 estimated parameters data tted estimator parameters estimated data hand estimated parameters attributes estimator object ending underscore estimator.estimated_param_ supervised learning predicting output variable highdimensional observations problem solved supervised learning supervised learning consists learning link two datasets observed data external variable trying predict usually called target labels often array length n_samples supervised estimators scikitlearn implement method model predict method given unlabeled observations returns predicted labels 
96: 1.2. tutorials bottom scikitlearn scikitlearn user guide release 0.11 vocabulary classication regression prediction task classify observations set nite labels words name objects observed task said classication task opposite goal predict continous target variable said regression task scikitlearn classication tasks vector integers note see introduction machine learning scikitlearn tutorial quick runthrough basic machine learning vocabulary used within scikitlearn 
97: nearest neighbor curse dimensionality classifying irises different types irises setosa versicolour virginica petal sepal length width iris dataset classication task consisting identifying import numpy sklearn import datasets iris datasets.load_iris iris_x iris.data iris_y iris.target np.unique iris_y array knearest neighbors classier simplest possible classier nearest neighbor given new observation x_test training set i.e data used train estimator observation closest feature vector please see nearest neighbors section online scikitlearn documentation information type classier training set testing set experimenting learning algorithm important test prediction estimator data used estimator would evaluating performance estimator new data datasets often split train test data 
98: chapter user guide scikitlearn user guide release 0.11 knn nearest neighbors classication example split iris data train test data random permutation split data randomly np.random.seed indices np.random.permutation len iris_x iris_x_train iris_x indices iris_y_train iris_y indices iris_x_test iris_x indices iris_y_test iris_y indices create fit nearestneighbor classifier sklearn.neighbors import kneighborsclassifier knn kneighborsclassifier knn.fit iris_x_train iris_y_train kneighborsclassifier algorithmauto leaf_size30 n_neighbors5 warn_on_equidistanttrue weightsuniform knn.predict iris_x_test array iris_y_test array curse dimensionality estimator effective need distance neighboring points less value depends problem one dimension requires average points context knn example data described one feature values ranging training observations new data thus away therefore nearest neighbor decision rule efcient soon small compared scale betweenclass feature variations number features require 1dp points lets say require points one dimension 10p points required dimensions pave space becomes large number training points required good estimator grows exponentially example point single number bytes effective knn estimator paltry p20 mensions would require training data current estimated size entire internet exabytes called curse dimensionality core problem machine learning addresses 
99: 1.2. tutorials bottom scikitlearn scikitlearn user guide release 0.11 linear model regression sparsity diabetes dataset diabetes dataset consists physiological variables age sex weight blood pressure measure patients indication disease progression one year diabetes datasets.load_diabetes diabetes_x_train diabetes.data diabetes_x_test diabetes.data diabetes_y_train diabetes.target diabetes_y_test diabetes.target task hand predict disease progression physiological variables 
100: linear regression linearregression simplest form linear model data set adjust ing set parameters order make sum squared residuals model small possilbe 
101: linear models data target variable coefcients observation noise sklearn import linear_model regr linear_model.linearregression regr.fit diabetes_x_train diabetes_y_train linearregression copy_xtrue fit_intercepttrue normalizefalse print regr.coef_ 0.30349955 237.63931533 102.84845219 492.81458798 510.53060544 184.60648906 327.73698041 814.13170937 743.51961675 76.09517222 mean square error np.mean regr.predict diabetes_x_test diabetes_y_test 2004.56760268.. 
102: explained variance score perfect prediction means linear relationship regr.score diabetes_x_test diabetes_y_test 0.5850753022690.. 
103: chapter user guide shrinkage data points per dimension noise observations induces high variance scikitlearn user guide release 0.11 np.c_ test np.c_ regr linear_model.linearregression import pylab pl.figure np.random.seed range ... ... ... 
104: this_x .1np.random.normal size regr.fit this_x pl.plot test regr.predict test pl.scatter this_x solution two randomly chosen set observations likely uncorrelated 
105: highdimensional statistical learning shrink regression coefcients zero called ridge regression regr linear_model.ridge alpha.1 pl.figure np.random.seed range ... ... ... 
106: this_x .1np.random.normal size regr.fit this_x pl.plot test regr.predict test pl.scatter this_x 1.2. tutorials bottom scikitlearn scikitlearn user guide release 0.11 example biasvariance tradeoff larger ridge alpha parameter higher bias lower variance choose alpha minimize left error time using diabetes dataset rather synthetic data alphas np.logspace print regr.set_params alphaalpha ... ... 0.5851110683883 ... 0.5852073015444 ... 0.5854677540698 ... 0.5855512036503 ... 0.5830717085554 ... 0.57058999437 ... .fit diabetes_x_train diabetes_y_train .score diabetes_x_test diabetes_y_test alpha alphas note capturing tted parameters noise prevents model generalize new data called overtting bias introduced ridge regression called regularization 
107: sparsity fitting features note representation full diabetes dataset would involve dimensions feature dimensions one target variable hard develop intuition representation may useful keep mind would fairly empty space 
108: see although feature strong coefcient full model conveys little information considered feature improve conditioning problem mitigate curse dimensionality would interesting select informative features set noninformative ones like feature ridge regression decrease contribution set zero another penalization approach called lasso least absolute shrinkage selection operator set coefcients zero methods called sparse method sparsity seen application occams razor prefer simpler models 
109: chapter user guide scikitlearn user guide release 0.11 .fit diabetes_x_train diabetes_y_train .score diabetes_x_test diabetes_y_test regr linear_model.lasso scores regr.set_params alphaalpha ... ... ... best_alpha alphas scores.index max scores regr.alpha best_alpha regr.fit diabetes_x_train diabetes_y_train lasso alpha0.025118864315095794 copy_xtrue fit_intercepttrue alpha alphas max_iter1000 normalizefalse positivefalse precomputeauto tol0.0001 warm_startfalse print regr.coef_ 
110: 187.19554705 212.43764548 69.38229038 517.19478111 508.66011217 313.77959962 160.8303982 
111: 71.84239008 different algorithms problem different algorithms used solve mathematical problem instance lasso object scikitlearn solves lasso regression using coordinate decent method efcient large datasets however scikitlearn also provides lassolars object using lars efcient problems weight vector estimated sparse problems observations 
112: classication classication labeling iris task linear regression right approach give much weight data far decision frontier linear approach sigmoid function logistic function sigmoid offset exp offset logistic linear_model.logisticregression c1e5 logistic.fit iris_x_train iris_y_train logisticregression c100000.0 class_weightnone dualfalse fit_intercepttrue intercept_scaling1 penaltyl2 tol0.0001 1.2. tutorials bottom scikitlearn scikitlearn user guide release 0.11 known logisticregression 
113: multiclass classication several classes predict option often used oneversusall classiers use voting heuristic nal decision 
114: shrinkage sparsity logistic regression parameter controls amount regularization logisticregression object large value results less regularization penaltyl2 gives shrinkage i.e nonsparse coefcients penaltyl1 gives sparsity 
115: exercise try classifying digits dataset nearest neighbors linear model leave last test prediction performance observations 
116: sklearn import datasets neighbors linear_model digits datasets.load_digits x_digits digits.data y_digits digits.target solution ....auto_examplesexercisesplot_digits_classification_exercise.py support vector machines svms linear svms support vector machines belong discrimant model family try combination samples build plane maximizing margin two classes regularization set parameter small value means margin calculated using many observations around separating line regularization large value means margin calculated observations close separating line less regularization 
117: chapter user guide unregularized svm regularized svm default scikitlearn user guide release 0.11 regression classication svc support vector classication 
118: svms used regression svr support vector sklearn import svm svc svm.svc kernellinear svc.fit iris_x_train iris_y_train svc c1.0 cache_size200 class_weightnone coef00.0 degree3 gamma0.0 kernellinear probabilityfalse shrinkingtrue tol0.001 verbosefalse warning normalizing data many estimators including svms datasets unit standard deviation feature important get good prediction 
119: using kernels classes always linearly separable feature space solution build decision function linear may instance polynomial done using kernel trick seen creating decision energy positioning kernels observations 1.2. tutorials bottom scikitlearn scikitlearn user guide release 0.11 linear kernel polynomial kernel svc svm.svc kernellinear svc svm.svc kernelpoly ... degree polynomial degree degree3 rbf kernel radial basis function svc svm.svc kernelrbf gamma inverse size radial kernel interactive example see svm gui download svm_gui.py add data points classes right left button model change parameters data 
120: chapter user guide scikitlearn user guide release 0.11 exercise try classifying classes iris dataset svms rst features leave class test prediction performance observations warning classes ordered leave last would testing one class hint use decision_function method grid get intuitions 
121: iris datasets.load_iris iris.data iris.target solution ....auto_examplesexercisesplot_iris_exercise.py model selection choosing estimators parameters score crossvalidated scores seen every estimator exposes score method judge quality prediction new data bigger better sklearn import datasets svm digits datasets.load_digits x_digits digits.data y_digits digits.target svc svm.svc kernellinear svc.fit x_digits y_digits .score x_digits y_digits 0.97999999999999998 get better measure prediction accuracy use proxy goodness model successively split data folds use training testing import numpy x_folds np.array_split x_digits y_folds np.array_split y_digits scores list range ... ... ... ... ... ... ... ... print scores 0.93489148580968284 0.95659432387312182 0.93989983305509184 use list copy order pop later x_train list x_folds x_test x_train np.concatenate x_train y_train list y_folds y_test y_train np.concatenate y_train scores.append svc.fit x_train y_train .score x_test y_test x_train.pop y_train.pop called kfold cross validation crossvalidation generators code split data train test sets tedious write sklearn exposes crossvalidation generators generate list indices purpose 1.2. tutorials bottom scikitlearn scikitlearn user guide release 0.11 sklearn import cross_validation k_fold cross_validation.kfold indicestrue train_indices test_indices k_fold ... train test train test train test print train test train_indices test_indices crossvalidation implemented easily kfold cross_validation.kfold len x_digits svc.fit x_digits train y_digits train .score x_digits test y_digits test ... 0.93489148580968284 0.95659432387312182 0.93989983305509184 train test kfold compute score method estimator sklearn exposes helper function cross_validation.cross_val_score svc x_digits y_digits cvkfold n_jobs1 array 0.93489149 0.93989983 0.95659432 n_jobs1 means computation dispatched cpus computer 
122: crossvalidation generators kfold stratifiedkfold split folds train test leftout make sure classes even accross folds leaveoneout leave one observation leaveonelabelout labels takes label array group observations chapter user guide scikitlearn user guide release 0.11 exercise digits dataset plot crossvalidation score svc estimator rbf kernel function parameter use logarithmic grid points 
123: sklearn import cross_validation datasets svm digits datasets.load_digits digits.data digits.target svc svm.svc c_s np.logspace scores list scores_std list solution ....auto_examplesexercisesplot_cv_digits.py gridsearch crossvalidated estimators gridsearch sklearn provides object given data computes score estimator parameter grid chooses parameters maximize crossvalidation score object takes estimator construction exposes estimator api sklearn.grid_search import gridsearchcv gammas np.logspace clf gridsearchcv estimatorsvc param_griddict gammagammas ... clf.fit x_digits y_digits gridsearchcv cvnone ... clf.best_score_ 0.988991985997974 clf.best_estimator_.gamma 9.9999999999999995e07 n_jobs1 prediction performance test set good train set clf.score x_digits y_digits 0.94228356336260977 default gridsearchcv uses 3fold crossvalidation however detects classier passed rather regressor uses stratied 3fold 
124: nested crossvalidation cross_validation.cross_val_score clf x_digits y_digits array 0.97996661 two crossvalidation loops performed parallel one gridsearchcv estimator set gamma one cross_val_score measure prediction performance estimator resulting scores unbiased estimates prediction score new data 
125: 0.98330551 0.98163606 warning nest objects parallel computing n_jobs different 
126: 1.2. tutorials bottom scikitlearn scikitlearn user guide release 0.11 crossvalidated estimators crossvalidation set parameter done efciently algorithmby algorithm basis certain estimators sklearn exposes crossvalidation evaluating estimator per formance estimators set parameter automatically crossvalidation sklearn import linear_model datasets lasso linear_model.lassocv diabetes datasets.load_diabetes x_diabetes diabetes.data y_diabetes diabetes.target lasso.fit x_diabetes y_diabetes lassocv alphasarray 2.14804 2.00327 ... 0.0023 0.00215 copy_xtrue cvnone eps0.001 fit_intercepttrue max_iter1000 n_alphas100 normalizefalse precomputeauto tol0.0001 verbosefalse estimator chose automatically lambda lasso.alpha 0.01318.. 
127: estimators called similarly counterparts appended name 
128: exercise diabetes dataset optimal regularization parameter alpha bonus much trust selection alpha import numpy import pylab sklearn import cross_validation datasets linear_model diabetes datasets.load_diabetes diabetes.data diabetes.target lasso linear_model.lasso alphas np.logspace solution ....auto_examplesexercisesplot_cv_diabetes.py unsupervised learning seeking representations data clustering grouping observations together problem solved clustering given iris dataset knew types iris access taxonomist label could try clustering task split observations wellseparated group called clusters 
129: chapter user guide kmeans clustering note exists lot different clustering criteria associated algorithms sim scikitlearn user guide release 0.11 plest clustering algorithm kmeans 
130: sklearn import cluster datasets iris datasets.load_iris x_iris iris.data y_iris iris.target k_means cluster.kmeans k_means.fit x_iris kmeans copy_xtrue initkmeans max_iter300 ... print k_means.labels_ print y_iris warning absolutely guarantee recovering ground truth first choosing right number clusters hard second algorithm sensitive initialization fall local minima although sklearn package play many tricks mitigate issue 
131: bad initialization dont overinterpret clustering results clusters ground truth 1.2. tutorials bottom scikitlearn scikitlearn user guide release 0.11 application example vector quantization clustering general kmeans particular seen way choosing small number examplars compress information problem sometimes known vector quantization instance used posterize image lena sp.lena scipy import misc lena misc.lena import scipy try ... ... except attributeerror ... ... lena.reshape need n_sample n_feature array k_means cluster.kmeans n_init1 k_means.fit kmeans copy_xtrue initkmeans ... values k_means.cluster_centers_.squeeze labels k_means.labels_ lena_compressed np.choose labels values lena_compressed.shape lena.shape raw image kmeans quantization equal bins image histogram hierarchical agglomerative clustering ward hierarchical clustering method type cluster analysis aims build hierarchy clusters general various approaches technique either agglomerative bottomup approaches divisive topdown approaches 
132: estimating large number clusters topdown approaches statisticaly illposed slow due starting observations one cluster splits recursively agglomerative hierarchicalclustering bottomup approach successively merges observations together particularly useful clusters interest made observations ward clustering minimizes criterion similar kmeans bottomup approach number clusters large much computationally efcient kmeans 
133: connectivityconstrained clustering ward clustering possible specify samples clus tered together giving connectivity graph graphs scikit represented adjacency matrix ten sparse matrix used useful instance retrieve connect regions clustering image chapter user guide scikitlearn user guide release 0.11 generate data lena sp.misc.lena downsample image factor lena lena lena lena lena np.reshape lena define structure data pixels connected neighbors connectivity grid_to_graph lena.shape compute clustering print compute structured hierarchical clustering ... time.time n_clusters number regions ward ward n_clustersn_clusters connectivityconnectivity .fit label np.reshape ward.labels_ lena.shape print elaspsed time time.time print number pixels label.size print number clusters np.unique label .size feature agglomeration seen sparsity could used mitigate curse dimensionality i.e insufcience observations compared number features another approach merge together similar features feature agglomeration approach implementing clustering feature direction words clustering transposed data 
134: digits datasets.load_digits images digits.images np.reshape images len images connectivity grid_to_graph images .shape agglo cluster.wardagglomeration connectivityconnectivity ... agglo.fit wardagglomeration connectivity ... x_reduced agglo.transform n_clusters32 x_approx agglo.inverse_transform x_reduced images_approx np.reshape x_approx images.shape 1.2. tutorials bottom scikitlearn scikitlearn user guide release 0.11 transform inverse_transform methods estimators expose transform method instance reduce dimensionality dataset 
135: decompositions signal components loadings components loadings multivariate data problem trying solve rewrite different observation basis want learn loadings set components different criteria exist choose components principal component analysis pca principal component analysis pca selects successive components explain maximum variance signal 
136: point cloud spanned observations one direction one univariate features almost exactly computed using pca nds directions data used transform data pca reduce dimensionality data projecting principal subspace 
137: create signal useful dimensions np.random.normal size100 np.random.normal size100 np.c_ sklearn import decomposition pca decomposition.pca pca.fit pca copytrue n_componentsnone whitenfalse print pca.explained_variance_ 2.18565811e00 1.19346747e00 8.43026679e32 see first components useful pca.n_components x_reduced pca.fit_transform x_reduced.shape chapter user guide independent component analysis ica independent component analysis ica selects components able recover non distribution loadings carries maximum amount independent information 
138: scikitlearn user guide release 0.11 gaussian independent signals generate sample data time np.linspace np.sin time np.sign np.sin time np.c_ 0.2 np.random.normal sizes.shape s.std axis0 mix data np.array 0.5 mixing matrix np.dot a.t generate observations standardize data signal sinusoidal signal signal square signal add noise compute ica ica decomposition.fastica ica.fit .transform get estimated sources ica.get_mixing_matrix get estimated mixing matrix np.allclose np.dot a_.t true 1.2. tutorials bottom scikitlearn scikitlearn user guide release 0.11 putting together pipelining seen estimators transform data estimators predict variables create combined estimators import pylab sklearn import linear_model decomposition datasets cross_validation logistic linear_model.logisticregression pca decomposition.pca sklearn.pipeline import pipeline pipe pipeline steps pca pca logistic logistic digits datasets.load_digits x_digits digits.data y_digits digits.target plot pca spectrum pca.fit x_digits pl.figure figsize pl.clf pl.axes pl.plot pca.explained_variance_ linewidth2 pl.axis tight pl.xlabel n_components pl.ylabel explained_variance_ prediction sklearn.grid_search import gridsearchcv n_components np.logspace parameters pipelines set using separated parameter names estimator gridsearchcv pipe dict pca__n_componentsn_components logistic__ccs chapter user guide scikitlearn user guide release 0.11 estimator.fit x_digits y_digits pl.axvline estimator.best_estimator_.named_steps pca .n_components linestyle labeln_components chosen pl.legend propdict size12 face recognition eigenfaces dataset used example preprocessed excerpt labeled faces wild aka lfw http viswww.cs.umass.edulfwlfwfunneled.tgz 233mb faces recognition example using eigenfaces svms dataset used example preprocessed excerpt labeled faces wild aka lfw_ http viswww.cs.umass.edulfwlfwfunneled.tgz 233mb _lfw http viswww.cs.umass.edulfw expected results top represented people dataset precision recall f1score support gerhard_schroeder donald_rumsfeld tony_blair colin_powell george_w_bush avg total 0.91 0.84 0.65 0.78 0.93 0.86 0.75 0.82 0.82 0.88 0.86 0.84 0.82 0.83 0.73 0.83 0.90 0.85 print __doc__ time import time import logging import pylab sklearn.cross_validation import train_test_split sklearn.datasets import fetch_lfw_people sklearn.grid_search import gridsearchcv sklearn.metrics import classification_report sklearn.metrics import confusion_matrix sklearn.decomposition import randomizedpca sklearn.svm import svc display progress logs stdout logging.basicconfig levellogging.info format asctime message 1.2. tutorials bottom scikitlearn scikitlearn user guide release 0.11 download data already disk load numpy arrays lfw_people fetch_lfw_people min_faces_per_person70 resize0.4 introspect images arrays find shapes plotting n_samples lfw_people.images.shape fot machine learning use data directly relative pixel positions info ignored model lfw_people.data n_features x.shape label predict person lfw_people.target target_names lfw_people.target_names n_classes target_names.shape print total dataset size print n_samples n_samples print n_features n_features print n_classes n_classes split training set test set using stratified fold split training testing set x_train x_test y_train y_test train_test_split test_fraction0.25 compute pca eigenfaces face dataset treated unlabeled dataset unsupervised feature extraction dimensionality reduction n_components print extracting top eigenfaces faces n_components x_train.shape time pca randomizedpca n_componentsn_components whitentrue .fit x_train print done 0.3fs time eigenfaces pca.components_.reshape n_components print projecting input data eigenfaces orthonormal basis time x_train_pca pca.transform x_train x_test_pca pca.transform x_test print done 0.3fs time train svm classification model print fitting classifier training set time param_grid chapter user guide scikitlearn user guide release 0.11 1e3 5e3 1e4 5e4 1e5 gamma 0.0001 0.0005 0.001 0.005 0.01 0.1 clf gridsearchcv svc kernelrbf class_weightauto param_grid clf clf.fit x_train_pca y_train print done 0.3fs time print best estimator found grid search print clf.best_estimator_ quantitative evaluation model quality test set print predicting people names testing set time y_pred clf.predict x_test_pca print done 0.3fs time print classification_report y_test y_pred target_namestarget_names print confusion_matrix y_test y_pred labelsrange n_classes qualitative evaluation predictions using matplotlib def plot_gallery images titles n_row3 n_col4 helper function plot gallery portraits pl.figure figsize 1.8 n_col 2.4 n_row pl.subplots_adjust bottom0 left.01 right.99 top.90 hspace.35 range n_row n_col pl.subplot n_row n_col pl.imshow images .reshape cmappl.cm.gray pl.title titles size12 pl.xticks pl.yticks plot result prediction portion test set def title y_pred y_test target_names pred_name target_names y_pred .rsplit true_name target_names y_test .rsplit return predicted sntrue pred_name true_name prediction_titles title y_pred y_test target_names range y_pred.shape plot_gallery x_test prediction_titles plot gallery significative eigenfaces eigenface_titles eigenface range eigenfaces.shape plot_gallery eigenfaces eigenface_titles pl.show 1.2. tutorials bottom scikitlearn scikitlearn user guide release 0.11 prediction expected results top represented people dataset eigenfaces precision recall f1score support gerhard_schroeder donald_rumsfeld tony_blair colin_powell george_w_bush avg total 0.91 0.84 0.65 0.78 0.93 0.86 0.75 0.82 0.82 0.88 0.86 0.84 0.82 0.83 0.73 0.83 0.90 0.85 open problem stock market structure predict variation stock prices google visualizing stock market structure finding help project mailing list encounter bug scikitlearn something needs clarication docstring online documentation please feel free ask mailing list communities machine learning practictioners metaoptimizeqa forum machine learning natural language processing data analytics discussions similar stackoverow developers http metaoptimize.comqa good starting point discussion good freely available textbooks machine learning quora.com quora topic machine learning related questions also features interesting discussions http quora.commachinelearning look best questions section good resources learning machine learning 
139: note videos videos tutorials also found videos section 
140: chapter user guide scikitlearn user guide release 0.11 1.3 supervised learning 1.3.1 generalized linear models following set methods intended regression target value expected linear combi nation input variables mathematical notion predicted value 
141: w1x1 ... wpxp across module designate vector ... coef_ intercept_ perform classication generalized linear models see logisitic regression 
142: ordinary least squares linearregression linear model coefcients ... minimize residual sum squares observed responses dataset responses predicted linear approximation mathemati cally solves problem form min linearregression take method arrays store coefcients linear model coef_ member sklearn import linear_model clf linear_model.linearregression clf.fit linearregression copy_xtrue fit_intercepttrue normalizefalse clf.coef_ array 0.5 0.5 1.3. supervised learning scikitlearn user guide release 0.11 however coefcient estimates ordinary least squares rely independence model terms terms correlated columns design matrix approximate linear dependence design matrix becomes close singular result leastsquares estimate becomes highly sensitive random errors observed response producing large variance situation multicollinearity arise example data collected without experimental design 
143: examples linear regression example ordinary least squares complexity method computes least squares solution using singular value decomposition matrix size method cost np2 assuming 
144: ridge regression ridge regression addresses problems ordinary least squares imposing penalty size coefcients ridge coefcients minimize penalized residual sum squares complexity parameter controls amount shrinkage larger value greater amount shrinkage thus coefcients become robust collinearity 
145: min linear models ridge take method arrays store coefcients linear model coef_ member sklearn import linear_model clf linear_model.ridge alpha clf.fit ridge alpha0.5 copy_xtrue fit_intercepttrue normalizefalse tol0.001 clf.coef_ array 0.34545455 clf.intercept_ 0.13636.. 
146: 0.34545455 chapter user guide scikitlearn user guide release 0.11 examples plot ridge coefcients function regularization classication text documents using sparse features ridge complexity method order complexity ordinary least squares 
147: setting regularization parameter generalized crossvalidation ridgecv implements ridge regression builtin crossvalidation alpha parameter object works way gridsearchcv except defaults generalized crossvalidation gcv efcient form leaveoneout crossvalidation sklearn import linear_model clf linear_model.ridgecv alphas 0.1 1.0 10.0 clf.fit ridgecv alphas 0.1 1.0 10.0 cvnone fit_intercepttrue loss_funcnone normalizefalse score_funcnone clf.best_alpha 0.1 references notes regularized least squares rifkin lippert technical report course slides 
148: lasso lasso linear model estimates sparse coefcients useful contexts due tendency prefer solutions fewer parameter values effectively reducing number variables upon given solution dependent reason lasso variants fundamental eld compressed sensing certain conditions recover exact set nonzero weights see compressive sensing tomography reconstruction prior lasso mathematically consists linear model trained cid96 prior regularizer objective function minimize min 2nsamples lasso estimate thus solves minimization leastsquares penalty added constant cid96 1norm parameter vector implementation class lasso uses coordinate descent algorithm coefcients see least angle regression another implementation clf linear_model.lasso alpha 0.1 clf.fit lasso alpha0.1 copy_xtrue fit_intercepttrue max_iter1000 1.3. supervised learning scikitlearn user guide release 0.11 normalizefalse positivefalse precomputeauto tol0.0001 warm_startfalse clf.predict array 0.8 also useful lowerlevel tasks function lasso_path computes coefcients along full path possible values 
149: examples lasso elastic net sparse signals compressive sensing tomography reconstruction prior lasso note feature selection lasso lasso regression yields sparse models thus used perform feature selection detailed l1based feature selection 
150: setting regularization parameter alpha parameter control degree sparsity coefcients estimated 
151: using crossvalidation scikitlearn exposes objects set lasso alpha parameter crossvalidation lassocv lassolarscv lassolarscv based least angle regression algorithm explained highdimensional datasets many collinear regressors lassocv often preferrable lassolarscv advantage exploring relevant values alpha parameter number samples small compared number observations often faster lassocv 
152: informationcriteria based model selection alternatively estimator lassolarsic proposes use akaike information criterion aic bayes information criterion bic computationally cheaper ternative optimal value alpha regularization path computed instead times using kfold crossvalidation however criteria needs proper estimation degrees freedom solution derived large samples asymptotic results assume model correct i.e data chapter user guide actually generated model also tend break problem badly conditioned features samples 
153: scikitlearn user guide release 0.11 examples lasso model selection crossvalidation aic bic elastic net elasticnet linear model trained prior regularizer objective function minimize case min 2nsamples class elasticnetcv used set parameters alpha rho crossvalidation 
154: 1.3. supervised learning scikitlearn user guide release 0.11 examples lasso elastic net sparse signals lasso elastic net least angle regression leastangle regression lars regression algorithm highdimensional data developed bradley efron trevor hastie iain johnstone robert tibshirani advantages lars numerically efcient contexts i.e. number dimensions signicantly greater number points computationally fast forward selection order complexity ordinary least squares 
155: produces full piecewise linear solution path useful crossvalidation similar attempts tune model 
156: two variables almost equally correlated response coefcients increase proximately rate algorithm thus behaves intuition would expect also stable 
157: easily modied produce solutions estimators like lasso 
158: disadvantages lars method include lars based upon iterative retting residuals would appear especially sensitive effects noise problem discussed detail weisberg discussion section efron annals statistics article 
159: lars model used using estimator lars lowlevel implementation lars_path 
160: lars lasso lassolars lasso model implemented using lars algorithm unlike implementation based coordinate_descent yields exact solution piecewise linear function norm coefcients 
161: chapter user guide scikitlearn user guide release 0.11 sklearn import linear_model clf linear_model.lassolars alpha.1 clf.fit lassolars alpha0.1 copy_xtrue eps ... fit_intercepttrue max_iter500 normalizetrue precomputeauto verbosefalse clf.coef_ array 0.717157 ... 
162: examples lasso path using lars lars algorithm provides full path coefcients along regularization parameter almost free thus common operation consist retrieving path function lars_path mathematical formulation algorithm similar forward stepwise regression instead including variables step estimated parameters increased direction equiangular ones correlations residual instead giving vector result lars solution consists curve denoting solution value norm parameter vector full coefents path stored array coef_path_ size n_features max_features1 rst column always zero 
163: references original algorithm detailed paper least angle regression hastie 
164: orthogonal matching pursuit omp orthogonalmatchingpursuit orthogonal_mp implements omp algorithm approximating linear model constraints imposed number nonzero coefcients pseudonorm forward feature selection method like least angle regression orthogonal matching pursuit approximate optimum solution vector xed number nonzero elements arg miny subject nnonzerocoef alternatively orthogonal matching pursuit target specic error instead specic number nonzero coef cients expressed arg min0 subject tol omp based greedy algorithm includes step atom highly correlated current residual similar simpler matching pursuit method better iteration residual recomputed using orthogonal projection space previously chosen dictionary elements 
165: examples orthogonal matching pursuit 1.3. supervised learning scikitlearn user guide release 0.11 references http www.cs.technion.ac.ilronrubinpublicationsksvdompv2.pdf matching pursuits timefrequency dictionaries mallat zhang bayesian regression bayesian regression techniques used include regularization parameters estimation procedure regularization parameter set hard sense tuned data hand done introducing uninformative priors hyper parameters model cid96 regularization used ridge regression equivalent nding maximum apostiori solution gaussian prior parameters precision instead setting lambda manually possible treat random variable estimated data obtain fully probabilistic model output assumed gaussian distributed around yxw alpha treated random variable estimated data advantages bayesian regression adapts data hand used include regularization parameters estimation procedure 
166: disadvantages bayesian regression include inference model time consuming 
167: references good introduction bayesian methods given bishop pattern recognition machine learning original algorithm detailed book bayesian learning neural networks radford neal bayesian ridge regression bayesianridge estimates probabilistic model regression problem described prior parameter given spherical gaussian 1ip priors choosen gamma distributions conjugate prior precision gaussian resulting model called bayesian ridge regression similar classical ridge parameters estimated jointly model remaining hyperparameters parameters gamma priors usually choosen noninformative parameters estimated maximizing marginal log likelihood default 1.e6 bayesian ridge regression used regression chapter user guide scikitlearn user guide release 0.11 sklearn import linear_model clf linear_model.bayesianridge clf.fit bayesianridge alpha_11e06 alpha_21e06 compute_scorefalse copy_xtrue fit_intercepttrue lambda_11e06 lambda_21e06 n_iter300 normalizefalse tol0.001 verbosefalse tted model used predict new values clf.predict array 0.50000013 weights model access clf.coef_ array 0.49999993 0.49999993 due bayesian framework weights found slightly different ones found ordinary least squares however bayesian ridge regression robust illposed problem 
168: examples bayesian ridge regression references details found article bayesian interpolation mackay david 
169: automatic relevance determination ard ardregression similar bayesian ridge regression lead sparser weights ardregression poses different prior dropping assuption gaussian spherical 
170: david wipf srikantan nagarajan new view automatic relevance determination 
171: 1.3. supervised learning scikitlearn user guide release 0.11 instead distribution assumed axisparallel elliptical gaussian distribution means weight drawn gaussian distribution centered zero precision diag ... constrast bayesian ridge regression coordinate standard deviation prior choosen gamma distribution given hyperparameters 
172: examples automatic relevance determination regression ard references logisitic regression task hand choose class sample belongs given nite hopefuly small set choices learning problem classication rather regression linear models used decision best use called logistic regression doesnt try minimize sum square residuals regression rather hit miss cost logisticregression class used penalized logistic regression penalization yields sparse predicting weights penalization sklearn.svm.l1_min_c allows calculate lower bound order get non null feature weights zero model 
173: examples penalty sparsity logistic regression path logistic regression chapter user guide scikitlearn user guide release 0.11 note feature selection sparse logistic regression logistic regression penalty yields sparse models thus used perform feature selection detailed l1based feature selection 
174: stochastic gradient descent sgd stochastic gradient descent simple yet efcient approach linear models particulary useful number samples number features large classes sgdclassifier sgdregressor provide functionality linear models classication regression using different convex loss functions different penalties 
175: references stochastic gradient descent perceptron perceptron another simple algorithm suitable large scale learning default require learning rate regularized penalized updates model mistakes 
176: last characteristic implies perceptron slightly faster train sgd hinge loss resulting models sparser 
177: 1.3.2 support vector machines support vector machines svms set supervised learning methods used classication regression outliers detection advantages support vector machines effective high dimensional spaces still effective cases number dimensions greater number samples uses subset training points decision function called support vectors also memory efcient versatile different kernel functions specied decision function common kernels provided also possible specify custom kernels disadvantages support vector machines include number features much greater number samples method likely give poor perfor mances 
178: svms directly provide probability estimates calculated using vefold crossvalidation thus performance suffer 
179: 1.3. supervised learning scikitlearn user guide release 0.11 support vector machines scikitlearn support dens numpy.ndarray convertible numpy.asarray sparse scipy.sparse sample vectors input however use svm make pre dictions sparse data must data optimal performance use cordered numpy.ndarray dense scipy.sparse.csr_matrix sparse dtypefloat64 previous versions scikitlearn sparse input support existed sklearn.svm.sparse module duplicated sklearn.svm interface module still exists backward compatibility deprecated removed scikitlearn 0.12 
180: classication svc nusvc linearsvc classes capable performing multiclass classication dataset 
181: svc nusvc similar methods accept slightly different sets parameters different mathematical formulations see section mathematical formulation hand linearsvc another implementation support vector classication case linear kernel note linearsvc accept keyword kernel assumed linear also lacks members svc nusvc like support_ classiers svc nusvc linearsvc take input two arrays array size n_samples n_features holding training samples array integer values size n_samples holding class labels training samples sklearn import svm chapter user guide scikitlearn user guide release 0.11 clf svm.svc clf.fit svc c1.0 cache_size200 class_weightnone coef00.0 degree3 gamma0.5 kernelrbf probabilityfalse shrinkingtrue tol0.001 verbosefalse tted model used predict new values clf.predict array svms decision function depends subset training data called support vectors properties support vectors found members support_vectors_ support_ n_support get support vectors clf.support_vectors_ array get indices support vectors clf.support_ array ... get number support vectors class clf.n_support_ array ... multiclass classication svc nusvc implement oneagainstone approach knerr al. multi class classication n_class number classes n_class n_class classiers constructed one trains data two classes clf svm.svc clf.fit svc c1.0 cache_size200 class_weightnone coef00.0 degree3 gamma1.0 kernelrbf probabilityfalse shrinkingtrue tol0.001 verbosefalse dec clf.decision_function dec.shape classes hand linearsvc implements onevstherest multiclass strategy thus training n_class models two classes one model trained lin_clf svm.linearsvc lin_clf.fit linearsvc c1.0 class_weightnone dualtrue fit_intercepttrue intercept_scaling1 lossl2 multi_classovr penaltyl2 tol0.0001 verbose0 dec lin_clf.decision_function dec.shape see mathematical formulation complete description decision function 
182: 1.3. supervised learning scikitlearn user guide release 0.11 note linearsvc also implements alternative multiclass strategy socalled multiclass svm formu lated crammer singer using option multi_classcrammer_singer method consistent true onevsrest classication practice onvsrest classication usually preferred since results mostly similar runtime signicantly less onevsrest linearsvc attributes coef_ intercept_ shape n_class n_features n_class respectively row coefcients corresponds one n_class many onevsrest classiers simliar interecepts order one class case onevsone svc layout attributes little involved case linear kernel layout coef_ intercept_ similar one described linearsvc described except shape coef_ n_class n_class corresponding many binary clas siers order classes ... shape dual_coef_ n_class1 n_sv somewhat hard grasp layout columns corre spond support vectors involved n_class n_class onevsone classiers support vectors used n_class classiers n_class entries row correspond dual coefcients classiers might made clear example consider three class problem class support vectors support vectors coefcient support vector class two dual coefcients lets call dual_coef_ looks like respectively support vector classier classes 
183: coefcients svs class coefcients svs class coefcients svs class unbalanced problems problems desired give importance certain classes certain individual samples keywords class_weight sample_weight used svc nusvc implement keyword class_weight method dictionary form class_label value value oating point number sets parameter class class_label value svc nusvc svr nusvr oneclasssvm implement also weights individual samples method fit keyword sample_weight 
184: examples plot different svm classiers iris dataset svm maximum margin separating hyperplane svm separating hyperplane unbalanced classes svmanova svm univariate feature selection nonlinear svm svm weighted samples chapter user guide scikitlearn user guide release 0.11 1.3. supervised learning scikitlearn user guide release 0.11 regression method support vector classication extended solve regression problems method called support vector regression model produced support vector classication described depends subset training data cost function building model care training points lie beyond margin analogously model produced support vector regression depends subset training data cost function building model ignores training data close model prediction two avors support vector regression svr nusvr classication classes method take argument vectors case expected oating point values instead integer values sklearn import svm 0.5 2.5 clf svm.svr clf.fit svr c1.0 cache_size200 coef00.0 degree3 epsilon0.1 gamma0.5 kernelrbf probabilityfalse shrinkingtrue tol0.001 verbosefalse clf.predict array 1.5 examples support vector regression svr using linear nonlinear kernels density estimation novelty detection oneclass svm used novelty detection given set samples detect soft boundary set classify new points belonging set class implements called oneclasssvm case type unsupervised learning method take input array class labels see section novelty outlier detection details usage 
185: examples oneclass svm nonlinear kernel rbf species distribution modeling complexity support vector machines powerful tools compute storage requirements increase rapidly number training vectors core svm quadratic programming problem separating support vectors rest training data solver used libsvmbased implementation scales eatures samples depending efciently libsvm cache used practice dataset dependent data sparse eatures replaced average number non zero features sample vector 
186: samples eatures chapter user guide scikitlearn user guide release 0.11 also note linear case algorithm used linearsvc liblinear implementation much efcient libsvmbased svc counterpart scale almost linearly millions samples andor features 
187: tips practical use avoiding data copy svc svr nusvc nusvr data passed certain methods cordered contiguous double precision copied calling underlying implementation check whether give numpy array ccontiguous inspecting ags attribute linearsvc logisticregression input passed numpy array copied converted liblinear internal sparse data representation double precision oats int32 indices nonzero components want largescale linear classier without copying dense numpy ccontiguous double precision array input suggest use sgdclassier class instead objective function congured almost linearsvc model 
188: kernel cache size svc svr nusvc nusvr size kernel cache strong impact run times larger problems enough ram available recommended set cache_size higher value default 
189: setting constrast scaling libsvm liblinear parameter sklearn.svm per sample penalty commonly good values often large i.e seldom 
190: support vector machine algorithms scale invariant highly recommended scale data example scale attribute input vector standardize mean variance note scaling must applied test vector obtain meaningful results see section preprocessing data details scaling normalization 
191: parameter nusvconeclasssvmnusvr approximates fraction training errors support vec tors 
192: svc data classication unbalanced e.g many positive negative set class_weightauto andor try different penalty parameters 
193: 1.3. supervised learning scikitlearn user guide release 0.11 underlying linearsvc implementation uses random number generator select features tting model thus uncommon slightly different results input data happens try smaller tol parameter 
194: using penalization provided linearsvc lossl2 penaltyl1 dualfalse yields sparse solution i.e subset feature weights different zero contribute decision function increasing yields complex model feature selected value yields null model weights equal zero calculated using l1_min_c 
195: kernel functions kernel function following 
196: linear cid48 polynomial cid48 specied keyword degree coef0 rbf exp cid48 specied keyword gamma sigmoid tanh specied coef0 
197: different kernels specied keyword kernel initialization linear_svc svm.svc kernellinear linear_svc.kernel linear rbf_svc svm.svc kernelrbf rbf_svc.kernel rbf custom kernels dene kernels either giving kernel python function precomputing gram matrix classiers custom kernels behave way classiers except field support_vectors_ empty indices support vectors stored support_ reference copy rst argument method stored future reference array changes use predict unexpected results 
198: using python functions kernels also use dened kernels passing function keyword kernel constructor kernel must take arguments two matrices return third matrix following code denes linear kernel creates classier instance use kernel import numpy sklearn import svm def my_kernel ... ... clf svm.svc kernelmy_kernel return np.dot y.t examples svm custom kernel 
199: chapter user guide using gram matrix set kernelprecomputed pass gram matrix instead method moment kernel values training vectors test vectors must provided 
200: scikitlearn user guide release 0.11 import numpy sklearn import svm np.array clf svm.svc kernelprecomputed linear kernel computation gram np.dot x.t clf.fit gram svc c1.0 cache_size200 class_weightnone coef00.0 degree3 gamma0.0 kernelprecomputed probabilityfalse shrinkingtrue tol0.001 verbosefalse predict training examples clf.predict gram array mathematical formulation support vector machine constructs hyperplane set hyperplanes high innite dimensional space used classication regression tasks intuitively good separation achieved hyperplane largest distance nearest training data points class socalled functional margin since general larger margin lower generalization error classier 
201: 1.3. supervised learning scikitlearn user guide release 0.11 svc given training vectors ... two classes vector svc solves following primal problem min cid88 dual subject ... min subject ... vector ones upper bound positive semidenite matrix qij kernel training vectors mapped higher maybe innite dimensional space function decision function cid88 sgn yiik note svm models derived libsvm liblinear use regularization parameter estimators use alpha relation nsamples alpha 
202: parameters accessed members dual_coef_ holds product yii support_vectors_ holds support vectors intercept_ holds independent term references automatic capacity tuning large vcdimension classiers guyon boser vapnik advances neural information processing supportvector networks cortes vapnik machine leaming nusvc introduce new parameter controls number support vectors training errors parameter upper bound fraction training errors lower bound fraction support vectors shown nusvc formulation reparametrization csvc therefore mathematically equivalent 
203: chapter user guide scikitlearn user guide release 0.11 implementation details internally use libsvm liblinear handle computations libraries wrapped using cython 
204: references description implementation details algorithms used please refer libsvm library support vector machines liblinear library large linear classication 1.3.3 stochastic gradient descent stochastic gradient descent sgd simple yet efcient approach discriminative learning linear clas siers convex loss functions linear support vector machines logistic regression even though sgd around machine learning community long time received considerable amount attention recently context largescale learning sgd successfully applied largescale sparse machine learning problems often encountered text classication natural language processing given data sparse classiers module easily scale problems training examples features advantages stochastic gradient descent efciency ease implementation lots opportunities code tuning 
205: disadvantages stochastic gradient descent include sgd requires number hyperparameters regularization parameter number iterations sgd sensitive feature scaling 
206: classication warning make sure permute shufe training data tting model use shufetrue shufe iterations 
207: class sgdclassifier implements plain stochastic gradient descent learning routine supports different loss functions penalties classication classiers sgd tted two arrays array size n_samples n_features holding training samples array size n_samples holding target values class labels training samples sklearn.linear_model import sgdclassifier clf sgdclassifier loss hinge penalty clf.fit sgdclassifier alpha0.0001 class_weightnone eta00.0 fit_intercepttrue learning_rateoptimal losshinge n_iter5 n_jobs1 penaltyl2 power_t0.5 rho0.85 seed0 shufflefalse verbose0 warm_startfalse tted model used predict new values 1.3. supervised learning scikitlearn user guide release 0.11 clf.predict array sgd linear model training data member coef_ holds model parameters clf.coef_ array 9.90090187 9.90090187 member intercept_ holds intercept aka offset bias clf.intercept_ array 9.990 ... whether model use intercept i.e biased hyperplane controlled parameter t_intercept get signed distance hyperplane use decision_function clf.decision_function array 29.61357756 concrete loss function set via loss parameter sgdclassifier supports following loss functions losshinge softmargin linear support vector machine lossmodied_huber smoothed hinge loss losslog logistic regression rst two loss functions lazy update model parameters example violates margin con straint makes training efcient log loss hand provides probability estimates case binary classication losslog get probability estimate ycx using predict_proba largest class label chapter user guide scikitlearn user guide release 0.11 clf sgdclassifier loss log .fit clf.predict_proba array 0.99999949 concrete penalty set via penalty parameter sgd supports following penalties penaltyl2 norm penalty coef_ penaltyl1 norm penalty coef_ penaltyelasticnet convex combination rho rho 
208: default setting penaltyl2 penalty leads sparse solutions driving coefcients zero elastic net solves deciencies penalty presence highly correlated attributes parameter rho specied user sgdclassifier supports multiclass classication combining multiple binary classiers one versus ova scheme classes binary classier learned discriminates classes testing time compute condence score i.e signed distances hyperplane classier choose class highest condence figure illustrates ova approach iris dataset dashed lines represent three ova classiers background colors show decision surface induced three classiers 
209: case multiclass classication coef_ twodimensionaly array shape n_classes n_features tercept_ one dimensional array shape n_classes ith row coef_ holds weight vector ova classier ith class classes indexed ascending order see attribute classes sgdclassifier supports weighted classes weighted instances via parameters class_weight sample_weight see examples doc string sgdclassifier.fit information 
210: 1.3. supervised learning scikitlearn user guide release 0.11 examples sgd maximum margin separating hyperplane plot multiclass sgd iris dataset sgd separating hyperplane weighted classes sgd weighted samples regression class sgdregressor implements plain stochastic gradient descent learning routine supports different loss functions penalties linear regression models sgdregressor well suited regression prob lems large number training samples 10.000 problems recommend ridge lasso elasticnet 
211: concrete loss function set via loss parameter sgdregressor supports following loss functions losssquared_loss ordinary least squares losshuber huber loss robust regression 
212: huber loss function epsilon insensitive loss function robust regression width insensitive region specied via parameter epsilon 
213: examples ordinary least squares sgd stochastic gradient descent sparse data chapter user guide scikitlearn user guide release 0.11 note sparse implementation produces slightly different results dense implementation due shrunk learning rate intercept 
214: builtin support sparse data given matrix format supported scipy.sparse maximum efciency however use csr matrix format dened scipy.sparse.csr_matrix 
215: examples classication text documents using sparse features complexity major advantage sgd efciency basically linear number training examples matrix size training cost knp number iterations epochs average number nonzero attributes per sample recent theoretical results however show runtime get desired optimization accuracy increase training set size increases 
216: tips practical use stochastic gradient descent sensitive feature scaling highly recommended scale data example scale attribute input vector standardize mean variance note scaling must applied test vector obtain meaningful results easily done using scaler sklearn.preprocessing import scaler scaler scaler scaler.fit x_train dont cheat fit training data x_train scaler.transform x_train x_test scaler.transform x_test apply transformation test data attributes intrinsic scale e.g word frequencies indicator features scaling needed 
217: finding reasonable regularization term best done using gridsearchcv usually range 10.0 np.arange 
218: empirically found sgd converges observing approx training samples thus reasonable rst guess number iterations n_iter np.ceil size training set 
219: apply sgd features extracted using pca found often wise scale feature values constant average norm training data equals one 
220: references efcient backprop lecun bottou orr mller neural networks tricks trade 
221: mathematical formulation given set training examples goal learn linear scoring function model parameters intercept order make predictions 1.3. supervised learning scikitlearn user guide release 0.11 simply look sign common choice model parameters minimizing regularized training error given cid88 loss function measures model mis regularization term aka penalty penalizes model complexity nonnegative hyperparameter different choices entail different classiers hinge softmargin support vector machines log logistic regression leastsquares ridge regression 
222: loss functions regarded upper bound misclassication error zeroone loss shown figure 
223: popular choices regularization term include cid80 norm cid80 cid80 leads sparse solutions 
224: norm cid80 elastic net convex combination 
225: figure shows contours different regularization terms parameter space 
226: sgd stochastic gradient descent optimization method unconstrained optimization problems contrast batch gradient descent sgd approximates true gradient considering single training example time 
227: chapter user guide scikitlearn user guide release 0.11 class sgdclassifier implements rstorder sgd learning routine algorithm iterates training examples example updates model parameters according update rule given learning rate controls stepsize parameter space intercept updated similarly without regularization learning rate either constant gradually decaying classication default learning rate schedule learning_rateoptimal given time step total n_samples epochs time steps determined based heuristic proposed lon bottou expected initial updates comparable expected size weights assuming norm training samples approx see tradeoffs large scale machine learning lon bottou details regression default learning rate schedule inverse scaling learning_rateinvscaling given eta0 tpower_t eta0 power_t hyperparameters choosen user via eta0 power_t resp constant learning rate use learning_rateconstant use eta0 specify learning rate model parameters accessed members coef_ intercept_ member coef_ holds weights member intercept_ holds 1.3. supervised learning scikitlearn user guide release 0.11 references solving large scale linear prediction problems using stochastic gradient descent algorithms zhang regularization variable selection via elastic net zou hastie journal royal statis proceedings icml 
228: tical society series 
229: implementation details implementation sgd inuenced stochastic gradient svm lon bottou similar svmsgd weight vector represented product scalar vector allows efcient weight update case regularization case sparse feature vectors intercept updated smaller learning rate multiplied 0.01 account fact updated frequently training examples picked sequentially learning rate lowered observed example adopted learning rate schedule shalevshwartz 2007. multiclass classication one versus approach used use truncated gradient algorithm proposed tsuruoka regularization elastic net code written cython 
230: references stochastic gradient descent bottou website 2010. tradeoffs large scale machine learning bottou website 2011. pegasos primal estimated subgradient solver svm shalevshwartz singer srebro proceedings icml 
231: stochastic gradient descent training l1regularized loglinear models cumulative penalty 
232: tsuruoka tsujii ananiadou proceedings afnlpacl 
233: 1.3.4 nearest neighbors sklearn.neighbors provides functionality unsupervised supervised neighborsbased learning methods unsupervised nearest neighbors foundation many learning methods notably manifold learning spectral clustering supervised neighborsbased learning comes two avors classication data discrete labels regression data continuous labels principle behind nearest neighbor methods predened number training samples closest distance new point predict label number samples userdened constant knearest neighbor learning vary based local density points radiusbased neighbor learning distance general metric measure standard euclidean distance common choice neighborsbased meth ods known nongeneralizing machine learning methods since simply remember training data possibly transformed fast indexing structure ball tree tree. despite simplicity nearest neighbors successful large number classication regression prob lems including handwritten digits satellite image scenes often successful classication situations decision boundary irregular classes sklearn.neighbors handle either numpy arrays scipy.sparse matrices input arbitrary minkowski metrics supported searches 
234: unsupervised nearest neighbors nearestneighbors implements unsupervised nearest neighbors learning 
235: acts uniform interface chapter user guide scikitlearn user guide release 0.11 three different nearest neighbors algorithms balltree scipy.spatial.ckdtree bruteforce algo rithm based routines sklearn.metrics.pairwise choice neighbors search algorithm con trolled keyword algorithm must one auto ball_tree kd_tree brute default value auto passed algorithm attempts determine best approach training data discussion strengths weaknesses option see nearest neighbor algorithms 
236: nearest neighbors classication neighborsbased classication type instancebased learning nongeneralizing learning attempt construct general internal model simply stores instances training data classication computed simple majority vote nearest neighbors point query point assigned data class representatives within nearest neighbors point scikitlearn implements two different nearest neighbors classiers kneighborsclassifier implements learn ing based nearest neighbors query point integer value specied user radiusneighborsclassifier implements learning based number neighbors within xed radius training point oatingpoint value specied user kneighbors classication kneighborsclassifier commonly used two techniques optimal choice value highly datadependent general larger suppresses effects noise makes classication boundaries less distinct radiusneighborsclassifier better choice user species xed radius points sparser neighborhoods use fewer nearest neighbors classication highdimensional parameter spaces method becomes less effective due socalled curse dimensionality basic nearest neighbors classication uses uniform weights value assigned query point computed simple majority vote nearest neighbors circumstances better weight neighbors nearer neighbors contribute accomplished weights keyword default value weights uniform assigns uniform weights neighbor weights distance assigns weights proportional inverse distance query point alternatively userdened function distance supplied used compute weights 
237: cases classication radiusbased uniformly sampled neighbors data 1.3. supervised learning scikitlearn user guide release 0.11 examples nearest neighbors classication example classication using nearest neighbors 
238: nearest neighbors regression neighborsbased regression used cases data labels continuous rather discrete variables label assigned query point computed based mean labels nearest neighbors scikitlearn implements two different neighbors regressors kneighborsregressor implements learning based nearest neighbors query point integer value specied user radiusneighborsregressor implements learning based neighbors within xed radius query point oatingpoint value specied user basic nearest neighbors regression uses uniform weights point local neighborhood contributes uniformly classication query point circumstances advantageous weight points nearby points contribute regression faraway points accomplished weights keyword default value weights uniform assigns equal weights points weights distance assigns weights proportional inverse distance query point alternatively userdened function distance supplied used compute weights 
239: examples nearest neighbors regression example regression using nearest neighbors 
240: nearest neighbor algorithms brute force fast computation nearest neighbors active area research machine learning naive neighbor search implementation involves bruteforce computation distances pairs points dataset samples dimensions approach scales efcient bruteforce neighbors searches competetive small data samples however number samples grows bruteforce proach quickly becomes infeasible classes within sklearn.neighbors bruteforce neighbors searches chapter user guide scikitlearn user guide release 0.11 specied using keyword algorithm brute computed using routines available sklearn.metrics.pairwise 
241: tree address computational inefciencies bruteforce approach variety treebased data structures invented general structures attempt reduce required number distance calculations efciently encoding aggregate distance information sample basic idea point distant point point close point know points distant without explicitly calculate distance way computational cost nearest neighbors search reduced log better signicant improvement bruteforce large early approach taking advantage aggregate information tree data structure short dimensional tree generalizes twodimensional quadtrees 3dimensional octtrees arbitrary number dimensions tree tree structure recursively partitions parameter space along data axes deviding nested orthotopic regions data points led construction tree fast partitioning performed along data axes ddimensional distances need computed constructed nearest neighbor query point determined log distance computations though tree approach fast lowdimensional neighbors searches becomes inefcient grows large one manifestation socalled curse dimensionality scikitlearn tree neighbors searches specied using keyword algorithm kd_tree computed using class scipy.spatial.ckdtree 
242: references multidimensional binary search trees used associative searching bentley j.l. communications acm 1.3. supervised learning scikitlearn user guide release 0.11 ball tree address inefciencies trees higher dimensions ball tree data structure developed trees partition data along cartesian axes ball trees partition data series nesting hyperspheres makes tree construction costly tree results data structure allows efcient neighbors searches even high dimensions ball tree recursively divides data nodes dened centroid radius point node lies within hypersphere dened number candidate points neighbor search reduced use triangle inequality setup single distance calculation test point centroid sufcient determine lower upper bound distance points within node spherical geometry ball tree nodes performance degrade high dimensions scikitlearn balltreebased neigh bors searches specied using keyword algorithm ball_tree computed using class sklearn.neighbors.balltree alternatively user work balltree class directly 
243: references five balltree construction algorithms omohundro s.m. international computer science institute tech nical report choice nearest neighbors algorithm optimal algorithm given dataset complicated choice depends number factors number samples i.e n_samples dimensionality i.e n_features 
244: brute force query time grows ball tree query time grows approximately log tree query time changes way difcult precisely characterise small less cost approximately log tree query efcient larger cost increases nearly overhead due tree structure lead queries slower brute force 
245: small data sets less log comparable brute force algorithms efcient treebased approach ckdtree balltree address providing leaf size parameter controls number samples query switches bruteforce allows algorithms approach efciency bruteforce computation small 
246: data structure intrinsic dimensionality data andor sparsity data intrinsic dimensionality refers dimension manifold data lies linearly nonlinearly embedded parameter space sparsity refers degree data lls parameter space distinguished concept used sparse matrices data matrix may zero entries structure still sparse sense 
247: brute force query time unchanged data structure ball tree tree query times greatly inuenced data structure general sparser data smaller intrinsic dimensionality leads faster query times tree internal representation aligned parameter axes generally show much improvement ball tree arbitrarily structured data 
248: chapter user guide scikitlearn user guide release 0.11 datasets used machine learning tend structured wellsuited treebased queries 
249: number neighbors requested query point 
250: brute force query time largely unaffected value ball tree tree query time become slower increases due two effects rst larger leads necessity search larger portion parameter space second using requires internal queueing results tree traversed 
251: becomes large compared ability prune branches treebased query reduced situation brute force queries efcient 
252: number query points ball tree tree require construction phase cost construction becomes negligible amortized many queries small number queries performed however construction make signicant fraction total cost query points required brute force better treebased method 
253: currently algorithm auto selects ball_tree brute otherwise choice based assumption number query points least order number training points leaf_size close default value 
254: effect leaf_size noted small sample sizes brute force search efcient treebased query fact accounted ball tree tree internally switching brute force searches within leaf nodes level switch specied parameter leaf_size parameter choice many effects construction time larger leaf_size leads faster tree construction time fewer nodes need created query time large small leaf_size lead suboptimal query cost leaf_size approaching overhead involved traversing nodes signicantly slow query times leaf_size approach ing size training set queries become essentially brute force good compromise leaf_size default value parameter 
255: memory leaf_size increases memory required store tree structure decreases especially important case ball tree stores ddimensional centroid node required storage space balltree approximately leaf_size times size training set 
256: leaf_size referenced brute force queries 
257: nearest centroid classier nearestcentroid classier simple algorithm represents class centroid members effect makes similar label updating phase sklearn.kmeans algorithm also parameters choose making good baseline classier however suffer nonconvex classes well classes drastically different variances equal variance dimensions assumed see linear discriminant analysis sklearn.lda.lda quadratic discriminant analysis sklearn.qda.qda complex methods make assumption usage default nearestcentroid simple sklearn.neighbors.nearest_centroid import nearestcentroid import numpy np.array np.array clf nearestcentroid clf.fit nearestcentroid metriceuclidean shrink_thresholdnone 1.3. supervised learning scikitlearn user guide release 0.11 print clf.predict 0.8 nearest shrunken centroid nearestcentroid classier shrink_threshold parameter implements nearest shrunken cen troid classier effect value feature centroid divided withinclass variance feature feature values reduced shrink_threshold notably particular feature value crosses zero set zero effect removes feature affecting classication useful example removing noisy features example using small shrink threshold increases accuracy model 0.81 0.82 
258: examples nearest centroid classication example classication using nearest centroid different shrink thresholds 
259: chapter user guide scikitlearn user guide release 0.11 1.3.5 gaussian processes gaussian processes machine learning gpml generic supervised learning method primarily designed solve regression problems also extended probabilistic classication present implementation postprocessing regression exercise advantages gaussian processes machine learning prediction interpolates observations least regular correlation models prediction probabilistic gaussian one compute empirical condence intervals excee dence probabilities might used ret online tting adaptive tting prediction region interest 
260: versatile different linear regression models correlation models specied common models provided also possible specify custom models provided stationary 
261: disadvantages gaussian processes machine learning include sparse uses whole samplesfeatures information perform prediction loses efciency high dimensional spaces namely number features exceeds dozens might indeed give poor performance loses computational efciency 
262: classication postprocessing meaning one rst need solve regression problem providing complete scalar oat precision output experiment one attempt model 
263: thanks gaussian property prediction given varied applications e.g global optimization probabilistic classication 
264: examples introductory regression example say want surrogate function sin function evaluated onto design experi ments dene gaussianprocess model whose regression correlation models might specied using additional kwargs ask model tted data depending number parameters provided instanciation tting procedure may recourse maximum likelihood estimation parameters alternatively uses given parameters 
265: return np.sin import numpy sklearn import gaussian_process def ... np.atleast_2d .ravel np.atleast_2d np.linspace gaussian_process.gaussianprocess theta01e2 thetal1e4 thetau1e1 gp.fit gaussianprocess beta0none corr function squared_exponential ... normalizetrue nuggetarray 2.22 ... optimizerfmin_cobyla random_start1 random_state ... regr function constant ... storage_modefull theta0array 0.01 thetalarray 0.0001 thetauarray 0.1 verbosefalse y_pred sigma2_pred gp.predict eval_msetrue 1.3. supervised learning scikitlearn user guide release 0.11 chapter user guide fitting noisy data scikitlearn user guide release 0.11 data includes noise gaussian process model used specifying variance noise point gaussianprocess takes parameter nugget added diagonal correlation matrix training points general type tikhonov regularization special case squared exponential correlation function normalization equivalent specifying fractional variance input cid20 cid21 nuggeti nugget corr properly set gaussian processes used robustly recover underlying function noisy data examples gaussian processes classication example exploiting probabilistic output 1.3. supervised learning scikitlearn user guide release 0.11 mathematical formulation initial assumption suppose one wants model output computer experiment say mathematical function rnfeatures cid55 gpml starts assumption function conditional sample path gaussian process additionally assumed read follows linear regression model zeromean gaussian process fully stationary covari ance function cid48 cid48 variance correlation function solely depends absolute relative distance sample possibly featurewise stationarity assumption basic formulation note gpml nothing extension basic least squares linear regression problem except additionaly assume spatial coherence correlation samples dictated correlation function indeed ordinary least squares assumes correlation model cid48 one cid48 zero otherwise dirac correlation model sometimes referred nugget correlation model kriging literature 
266: best linear unbiased prediction blup derive best linear unbiased prediction sample path conditioned observations xy1 ... ynsamples xnsamples derived given properties linear linear combination observations unbiased best mean squared error sense arg min optimal weight vector solution following equality constrained optimization problem arg min s.t chapter user guide scikitlearn user guide release 0.11 rewriting constrained optimization problem form lagrangian looking rst order optimality conditions satised one ends closed form expression sought predictor see references complete proof end blup shown gaussian random variate mean variance introduced correlation matrix whose terms dened wrt autocorrelation function builtin parameters ... vector crosscorrelations point prediction made points doe ... regression matrix vandermonde matrix polynomial basis generalized least square regression weights ... ... vectors important notice probabilistic response gaussian process predictor fully analytic mostly relies basic linear algebra operations precisely mean prediction sum two simple linear combinations dot products variance requires two matrix inversions correlation matrix decomposed using cholesky decomposition algorithm 
267: empirical best linear unbiased predictor eblup autocorrelation regression models assumed given practice however never known advance one make motivated empirical choices models correlation models provided choices made one estimate remaining unknown parameters involved blup one uses set provided observations conjunction inference technique present implemen tation based daces matlab toolbox uses maximum likelihood estimation technique see dace manual references complete equations maximum likelihood estimation problem turned global optimization problem onto autocorrelation parameters present implementation global optimization solved means fmin_cobyla optimization function scipy.optimize case anisotropy however provide implementation welchs componentwise optimization algorithm see references comprehensive description theoretical aspects gaussian processes machine learning please refer references 1.3. supervised learning scikitlearn user guide release 0.11 references dace matlab kriging toolbox lophaven nielsen sondergaard screening predicting computer experiments welch buck sacks wynn mitchell morris technometrics gaussian processes machine learning rasmussen cki williams mit press diet trich design analysis computer experiments santner williams notz springer correlation models common correlation models matches famous svms kernels mostly built equivalent sumptions must fulll mercers conditions additionaly remain stationary note however choice correlation model made agreement known properties original experiment observations come instance original experiment known innitely differentiable smooth one use squared exponential correlation model 
268: one rather use exponential correlation model note also exists correlation model takes degree derivability input matern correlation model implemented todo 
269: detailed discussion selection appropriate correlation models see book rasmussen williams references 
270: regression models common linear regression models involve zero constant rst secondorder polynomials one may specify form python function takes features input returns vector containing values functional set constraint number functions must exceed number available observations underlying regression problem underdetermined 
271: implementation details present implementation based translation dace matlab toolbox 
272: references dace matlab kriging toolbox lophaven nielsen sondergaard w.j welch r.j. buck sacks h.p wynn t.j. mitchell m.d morris screening predicting computer experiments technometrics 
273: 1.3.6 partial least squares partial least squares pls models useful linear relations two multivariate datasets pls arguments method arrays pls nds fundamental relations two matrices latent variable approach modeling covariance structures two spaces pls model try multidimensional direction chapter user guide scikitlearn user guide release 0.11 space explains maximum multidimensional variance direction space plsregression particularly suited matrix predictors variables observations multicollinearity among values contrast standard regression fail cases classes included module plsregression plscanonical cca plssvd reference wegelin survey partial least squares pls methods emphasis twoblock case examples pls partial least squares 1.3.7 naive bayes naive bayes methods set supervised learning algorithms based applying bayes theorem naive assumption independence every pair features given class variable dependent feature vector bayes theorem states following relationship using naive independence assumption xiy xi1 xi1 xiy 1.3. supervised learning scikitlearn user guide release 0.11 relationship simplied cid81 since constant given input use following classication rule cid89 arg max cid89 use maximum posteriori map estimation estimate former relative frequency class training set different naive bayes classiers differ mainly assumptions make regarding distribution spite apparently oversimplied assumptions naive bayes classiers worked quite well many real world situations famously document classication spam ltering requires small amount training data estimate necessary parameters theoretical reasons naive bayes works well types data see references naive bayes learners classiers extremely fast compared sophisticated methods decoupling class conditional feature distributions means distribution independently estimated one dimensional distribution turn helps alleviate problems stemming curse dimensionality side although naive bayes known decent classier known bad estimator probability outputs predict_proba taken seriously 
274: references zhang optimality naive bayes proc flairs 
275: gaussian naive bayes gaussiannb implements gaussian naive bayes algorithm classication likelihood features assumed gaussian cid113 cid18 cid19 exp parameters estimated using maximum likelihood sklearn import datasets iris datasets.load_iris sklearn.naive_bayes import gaussiannb gnb gaussiannb y_pred gnb.fit iris.data iris.target .predict iris.data print number mislabeled points iris.target y_pred .sum number mislabeled points chapter user guide scikitlearn user guide release 0.11 multinomial naive bayes multinomialnb implements naive bayes algorithm multinomially distributed data one two classic naive bayes variants used text classication data typically represented word vector counts although tfidf vectors also known work well practice distribution parametrized vectors class number features text classication size vocabulary probability feature appearing sample belonging class parameters estimated smoothed version maximum likelihood i.e relative frequency counting nyi nyi cid80 cid80 number times feature appears sample class training set nyi total count features class 
276: smoothing priors accounts features present learning samples prevents zero probabilities computations setting called laplace smoothing called lidstone smoothing 
277: bernoulli naive bayes bernoullinb implements naive bayes training classication algorithms data distributed cording multivariate bernoulli distributions i.e. may multiple features one assumed binaryvalued bernoulli boolean variable therefore class requires samples represented binaryvalued feature vectors handed kind data bernoullinb instance may binarizes input depending binarize parameter decision rule bernoulli naive bayes based differs multinomial nbs rule explicitly penalizes nonoccurrence feature indicator class multinomial variant would simply ignore nonoccurring feature case text classication word occurrence vectors rather word count vectors may used train use classier bernoullinb might perform better datasets especially shorter documents advisable evaluate models time permits 
278: references c.d manning raghavan schtze introduction information retrieval cambridge university press 
279: mccallum nigam comparison event models naive bayes text classication 
280: proc aaaiicml98 workshop learning text categorization 
281: metsis androutsopoulos paliouras spam ltering naive bayes naive bayes 3rd conf email antispam ceas 
282: 1.3.8 decision trees decision trees dts nonparametric supervised learning method used classication regression goal create model predicts value target variable learning simple decision rules inferred data features instance example decision trees learn data approximate sine curve set ifthenelse decision rules deeper tree complex decision rules tter model 
283: 1.3. supervised learning scikitlearn user guide release 0.11 advantages decision trees simple understand interpret trees visualised requires little data preparation techniques often require data normalisation dummy variables need created blank values removed note however module support missing values 
284: cost using tree i.e. predicting data logarithmic number data points used train tree able handle numerical categorical data techniques usually specialised analysing datasets one type variable see algorithms information 
285: uses white box model given situation observable model explanation condition easily explained boolean logic constrast black box model e.g. articial neural network results may difcult interpret 
286: possible validate model using statistical tests makes possible account reliability model 
287: performs well even assumptions somewhat violated true model data generated 
288: disadvantages decision trees include decisiontree learners create overcomplex trees generalise data well called overt ting mechanisms pruning currently supported setting minimum number samples required leaf node setting maximum depth tree necessary avoid problem 
289: decision trees unstable small variations data might result completely different tree generated problem mitigated using decision trees within ensemble 
290: problem learning optimal decision tree known npcomplete several aspects optimality even simple concepts consequently practical decisiontree learning algorithms based heuristic algorithms greedy algorithm locally optimal decisions made node algorithms guarantee return globally optimal decision tree mitigated training multiple trees ensemble learner features samples randomly sampled replacement 
291: chapter user guide scikitlearn user guide release 0.11 concepts hard learn decision trees express easily xor parity multiplexer problems 
292: decision tree learners create biased trees classes dominate therefore recommended balance dataset prior tting decision tree 
293: classication decisiontreeclassifier class capable performing multiclass classication dataset classiers decisiontreeclassifier take input two arrays array size n_samples n_features holding training samples array integer values size n_samples holding class bels training samples sklearn import tree clf tree.decisiontreeclassifier clf clf.fit tted model used predict new values clf.predict array decisiontreeclassifier capable binary labels classication multiclass labels ... classication using iris dataset construct tree follows sklearn.datasets import load_iris sklearn import tree iris load_iris clf tree.decisiontreeclassifier clf clf.fit iris.data iris.target trained export tree graphviz format using export_graphviz exporter example export tree trained entire iris dataset stringio import stringio stringio tree.export_graphviz clf out_fileout tted model used predict new values clf.predict iris.data array examples plot decision surface decision tree iris dataset regression decision trees also applied regression problems using decisiontreeregressor class 
294: 1.3. supervised learning scikitlearn user guide release 0.11 chapter user guide petal length 2.45000004768error 0.666666686535samples 150value error 0.0samples 50value petal width 1.75error 0.5samples 100value petal length 4.94999980927error 0.168038412929samples 54value petal length 4.85000038147error 0.0425330810249samples 46value petal width 1.65000009537error 0.040798611939samples 48value petal width 1.54999995232error 0.444444447756samples 6value sepal length 5.94999980927error 0.444444447756samples 3value error 0.0samples 43value error 0.0samples 47value error 0.0samples 1value error 0.0samples 3value sepal length 6.94999980927error 0.444444447756samples 3value error 0.0samples 2value error 0.0samples 1value error 0.0samples 1value error 0.0samples 2value scikitlearn user guide release 0.11 classication setting method take argument arrays case expected oating point values instead integer values sklearn import tree 0.5 2.5 clf tree.decisiontreeregressor clf clf.fit clf.predict array 0.5 examples decision tree regression complexity general run time cost construct balanced binary tree nsamplesnf eatureslog nsamples query time log nsamples although tree construction algorithm attempts generate balanced trees always balanced assuming subtrees remain approximately balanced cost node consists searching eatures feature offers largest reduction entropy cost eaturesnsampleslog nsamples node leading total cost entire trees summing cost node eaturesn2 scikitlearn offers efcient implementation construction decision trees naive implementation would recompute class label histograms classication means regression new split point along given feature presorting feature relevant samples retaining running label count reduce complexity node eatureslog nsamples results total cost eaturesnsampleslog nsamples 
295: sampleslog nsamples 
296: 1.3. supervised learning scikitlearn user guide release 0.11 implementation also offers parameter min_density control optimization heuristic sample mask used mask data points inactive given node avoids copying data important large datasets training trees within ensemble density dened ratio active data samples total samples given node minimum density parameter species level fancy indexing therefore data copied sample mask reset min_density fancy indexing always used data partitioning tree building phase case size memory proportion input data required node depth approximated using geometric series size 1rn ratio samples used node best case analysis shows lowest memory requirement innitely deep tree partition divides data half worst case analysis shows memory requirement increase practise usually requires times setting min_density always use sample mask select subset samples node results little additional memory allocated making appropriate massive datasets within ensemble learners default value min_density 0.1 empirically leads fast training many problems typically high values min_density lead excessive reallocation slowing algorithm signicantly 
297: tips practical use decision trees tend overt data large number features getting right ratio samples number features important since tree samples high dimensional space likely overt 
298: consider performing dimensionality reduction pca ica feature selection beforehand give tree better chance nding features discriminative 
299: visualise tree training using export function use max_depth3 initial tree depth get feel tree tting data increase depth 
300: remember number samples required populate tree doubles additional level tree grows use max_depth control size tree prevent overtting 
301: use min_samples_split min_samples_leaf control number samples leaf node small number usually mean tree overt whereas large number prevent tree learning data try min_samples_leaf5 initial value main difference two min_samples_leaf guarantees minimum number samples leaf min_samples_split create arbitrary small leaves though min_samples_split common literature 
302: balance dataset training prevent tree creating tree biased toward classes dominant 
303: decision trees use fortran ordered np.float32 arrays internally training data format copy dataset made 
304: tree algorithms id3 c4.5 c5.0 cart various decision tree algorithms differ one implemented scikitlearn id3 iterative dichotomiser developed ross quinlan algorithm creates multiway tree nding node i.e greedy manner categorical feature yield largest information gain categorical targets trees grown maximum size pruning step usually applied improve ability tree generalise unseen data c4.5 successor id3 removed restriction features must categorical dynamically dening discrete attribute based numerical variables partitions continuous attribute value discrete set intervals c4.5 converts trained trees i.e output id3 algorithm sets ifthen rules accuracy rule evaluated determine order applied pruning done removing rules precondition accuracy rule improves without 
305: chapter user guide scikitlearn user guide release 0.11 c5.0 quinlans latest version release proprietary license uses less memory builds smaller rulesets c4.5 accurate cart classication regression trees similar c4.5 differs supports numerical target variables regression compute rule sets cart constructs binary trees using feature threshold yield largest information gain node scikitlearn uses optimised version cart algorithm 
306: mathematical formulation given training vectors ... label vector decision tree recursively partitions space samples labels grouped together let data node represented candidate split consisting feature threshold partition data qlef qright subsets qlef qright qlef impurity computed using impurity function choice depends task solved classication regression nlef qlef nright qright select parameters minimises impurity argming recurse subsets qlef qright maximum allowable depth reached min_samples 
307: classication criteria target classication outcome taking values ... node representing region observations let cid88 xirm pmk 1nm proportion class observations node common measures impurity gini crossentropy misclassication pmk pmk cid88 cid88 pmklog pmk max pmk 1.3. supervised learning scikitlearn user guide release 0.11 regression criteria target continuous value node representing region observations common criterion minimise mean squared error cid88 cid88 inm inm references http en.wikipedia.orgwikidecision_tree_learning http en.wikipedia.orgwikipredictive_analytics breiman friedman olshen stone classication regression trees wadsworth belmont 
308: j.r. quinlan programs machine learning morgan kaufmann 1993. hastie tibshirani friedman elements statistical learning springer 
309: 1.3.9 ensemble methods goal ensemble methods combine predictions several models built given learning algorithm order improve generalizability robustness single model two families ensemble methods usually distinguished averaging methods driving principle build several models independently average predictions average combined model usually better single model variance reduced examples bagging methods forests randomized trees.. 
310: contrast boosting methods models built sequentially one tries reduce bias combined model motivation combine several weak models produce powerful ensemble examples adaboost least squares boosting gradient tree boosting 
311: forests randomized trees sklearn.ensemble module includes two averaging algorithms based randomized decision trees ran domforest algorithm extratrees method algorithms perturbandcombine techniques b1998 specically designed trees means diverse set classiers created introducing randomness classier construction prediction ensemble given averaged prediction individual classiers classiers forest classiers tted two arrays array size n_samples n_features holding training samples array size n_samples holding target values class labels training samples sklearn.ensemble import randomforestclassifier clf randomforestclassifier n_estimators10 clf clf.fit chapter user guide scikitlearn user guide release 0.11 random forests random forests see randomforestclassifier randomforestregressor classes tree ensemble built sample drawn replacement i.e. bootstrap sample training set addition splitting node construction tree split chosen longer best split among features instead split picked best split among random subset features result randomness bias forest usually slightly increases respect bias single nonrandom tree due averaging variance also decreases usually compensating increase bias hence yielding overall better model contrast original publication b2001 scikitlearn implementation combines classiers averaging probabilistic prediction instead letting classier vote single class 
312: extremely randomized trees extremely randomized trees see extratreesclassifier extratreesregressor classes random ness goes one step way splits computed random forests random subset candidate features used instead looking discriminative thresholds thresholds drawn random candi date feature best randomlygenerated thresholds picked splitting rule usually allows reduce variance model bit expense slightly greater increase bias sklearn.cross_validation import cross_val_score sklearn.datasets import make_blobs sklearn.ensemble import randomforestclassifier sklearn.ensemble import extratreesclassifier sklearn.tree import decisiontreeclassifier make_blobs n_samples10000 n_features10 centers100 
313: random_state0 random_state0 clf decisiontreeclassifier max_depthnone min_samples_split1 ... scores cross_val_score clf scores.mean 0.978.. 
314: min_samples_split1 random_state0 clf randomforestclassifier n_estimators10 max_depthnone ... scores cross_val_score clf scores.mean 0.999.. 
315: min_samples_split1 random_state0 clf extratreesclassifier n_estimators10 max_depthnone ... scores cross_val_score clf scores.mean 0.999 true parameters main parameters adjust using methods n_estimators max_features former number trees forest larger better also longer take compute addition note results stop getting signicantly better beyond critical number trees latter size random subsets features consider splitting node lower greater reduction variance also greater increase bias empiricial good default values max_featuresn_features regression 1.3. supervised learning scikitlearn user guide release 0.11 problems max_featuressqrt n_features classication tasks n_features number features data best results also usually reached setting max_depthnone combination min_samples_split1 i.e. fully developping trees bear mind though values usually optimal best parameter values always cross validated addition note bootstrap samples used default random forests bootstraptrue default strategy use original dataset building extratrees bootstrapfalse training large datasets runtime memory requirements important might also benecial adjust min_density parameter controls heuristic speeding computations tree see complexity trees details 
316: parallelization finally module also features parallel construction trees parallel computation predictions n_jobs parameter n_jobsk computations partitioned jobs run cores machine n_jobs1 cores available machine used note interprocess communication overhead speedup might linear i.e. using jobs unfortunately times fast signicant speedup still achieved though building large number trees building single tree requires fair amount time e.g. large datasets 
317: examples plot decision surfaces ensembles trees iris dataset pixel importances parallel forest trees references chapter user guide scikitlearn user guide release 0.11 gradient tree boosting gradient tree boosting gradient boosted regression trees gbrt generalization boosting arbitrary differentiable loss functions gbrt accurate effective offtheshelf procedure used regression classication problems gradient tree boosting models used variety areas including web search ranking ecology advantages gbrt natural handling data mixed type heterogeneous features predictive power robustness outliers input space via robust loss functions disadvantages gbrt scalability due sequential nature boosting hardly parallelized 
318: module sklearn.ensemble provides methods classication regression via gradient boosted regression trees 
319: classication gradientboostingclassifier supports binary multiclass classication via deviance loss func tion lossdeviance following example shows gradient boosting classier decision stumps weak learners sklearn.datasets import make_hastie_10_2 sklearn.ensemble import gradientboostingclassifier make_hastie_10_2 random_state0 x_train x_test y_train y_test clf gradientboostingclassifier n_estimators100 learn_rate1.0 ... clf.score x_test y_test 0.913.. 
320: max_depth1 random_state0 .fit x_train y_train number weak learners i.e regression trees controlled parameter n_estimators maximum depth tree controlled via max_depth learn_rate hyperparameter range 0.0 1.0 controls overtting via shrinkage 
321: regression gradientboostingregressor supports number different loss functions regression spec ied via argument loss currently supported least squares lossls least absolute deviation losslad robust w.r.t outliers see f2001 detailed information 
322: import numpy sklearn.metrics import mean_squared_error sklearn.datasets import make_friedman1 sklearn.ensemble import gradientboostingregressor make_friedman1 n_samples1200 random_state0 noise1.0 x_train x_test y_train y_test 1.3. supervised learning scikitlearn user guide release 0.11 clf gradientboostingregressor n_estimators100 learn_rate1.0 ... max_depth1 random_state0 lossls .fit x_train y_train mean_squared_error y_test clf.predict x_test 6.90.. 
323: gure shows results applying gradientboostingregressor least squares loss base learners boston houseprice dataset see sklearn.datasets.load_boston plot left shows train test error iteration plots like often used early stopping plot right shows feature importances optained via feature_importance property 
324: mathematical formulation gbrt considers additive models following form cid88 mhm basis functions usually called weak learners context boosting gradient tree boosting uses decision trees xed size weak learners decision trees number abilities make valuable boosting namely ability handle data mixed type ability model complex functions similar boosting algorithms gbrt builds additive model forward stagewise fashion fm1 mhm stage decision tree choosen minimizes loss function given current model fm1 fm1 fm1 arg min cid88 fm1 chapter user guide scikitlearn user guide release 0.11 initial model problem specic leastsquares regression one usually chooses mean target values 
325: note initial model also specied via init argument passed object implement fit predict 
326: gradient boosting attempts solve minimization problem numerically via steepest descent steepest descent direction negative gradient loss function evaluated current model fm1 calculated differentiable loss function fm1 cid88 fm1 step length choosen using line search cid88 arg min fm1 fm1 fm1 algorithms regression classication differ concrete loss function used 
327: loss functions following loss functions supported specied using parameter loss regression least squares natural choice regression due superior computational properties initial model given mean target values 
328: least absolute deviation lad robust loss function regression initial model given median target values 
329: classication binomial deviance deviance negative binomial loglikelihood loss function binary classi cation provides probability estimates initial model given log oddsratio 
330: multinomial deviance deviance negative multinomial loglikelihood loss function multi class classication n_classes mutually exclusive classes provides probability estimates initial model given prior probability class iteration n_classes regression trees constructed makes gbrt rather inefcient data sets large number classes 
331: regularization shrinkage f2001 proposed simple regularization strategy scales contribution weak learner factor parameter also called learning rate scales step length gradient descent procedure set via learn_rate parameter 
332: fm1 mhm 1.3. supervised learning scikitlearn user guide release 0.11 parameter learn_rate strongly interacts parameter n_estimators number weak learners smaller values learn_rate require larger numbers weak learners maintain constant training error empirical evidence suggests small values learn_rate favor better test error htf2009 recommend set learning rate small constant e.g learn_rate 0.1 choose n_estimators early stopping detailed discussion interaction learn_rate n_estimators see r2007 
333: subsampling f1999 proposed stochastic gradient boosting combines gradient boosting bootstrap averaging bagging iteration base classier trained fraction subsample available training data subsample drawn without replacement typical value subsample 0.5. gure illustrates effect shrinkage subsampling goodnessoft model clearly see shrinkage outperforms noshrinkage subsampling shrinkage increase accuracy model subsampling without shrinkage hand poorly 
334: examples gradient boosting regression gradient boosting regularization references 1.3.10 multiclass multilabel algorithms module implements multiclass multilabel learning algorithms chapter user guide scikitlearn user guide release 0.11 onevstherest onevsall onevsone error correcting output codes multiclass classication means classication two classes multilabel classication different task classier used predict set target labels instance i.e. set target classes assumed disjoint ordinary binary multiclass classication also called anyof classication estimators provided module metaestimators require base estimator provided constructor example possible use estimators turn binary classier regressor multiclass classier also possible use estimators multiclass estimators hope accuracy runtime performance improves 
335: note dont need use estimators unless want experiment different multiclass strategies classiers scikitlearn support multiclass classication outofthebox summary classiers supported scikitlearn grouped strategy used 
336: inherently multiclass naive bayes sklearn.lda.lda decision trees random forests onevsone sklearn.svm.svc onevsall sklearn.svm.linearsvc sklearn.linear_model.logisticregression sklearn.linear_model.sgdclassifier sklearn.linear_model.ridgeclassifier 
337: note moment evaluation metrics implemented multilabel learnings 
338: onevstherest strategy also known onevsall implemented onevsrestclassifier strategy consists tting one classier per class classier class tted classes addition computational efciency n_classes classiers needed one advantage approach interpretability since class represented one one classier possible gain knowledge class inspecting corresponding classier commonly used strategy fair default choice example sklearn import datasets sklearn.multiclass import onevsrestclassifier sklearn.svm import linearsvc iris datasets.load_iris iris.data iris.target onevsrestclassifier linearsvc .fit .predict array multilabel learning ovr onevsrestclassifier also supports multilabel classication use feature feed classier list tuples containing target labels like example 
339: 1.3. supervised learning scikitlearn user guide release 0.11 examples multilabel classication onevsone onevsoneclassifier constructs one classier per pair classes prediction time class received votes selected since requires n_classes n_classes classiers method usually slower onevstherest due n_classes2 complexity however method may advantageous algorithms kernel algorithms dont scale well n_samples individual learning problem involves small subset data whereas onevstherest complete dataset used n_classes times example sklearn import datasets sklearn.multiclass import onevsoneclassifier sklearn.svm import linearsvc iris datasets.load_iris iris.data iris.target onevsoneclassifier linearsvc .fit .predict array chapter user guide scikitlearn user guide release 0.11 errorcorrecting outputcodes outputcode based strategies fairly different onevstherest onevsone strategies class represented euclidean space dimension another way put class represented binary code array matrix keeps track locationcode class called code book code size dimensionality aforementioned space intuitively class represented code unique possible good code book designed optimize classication accuracy implementation simply use randomlygenerated code book advocated although elaborate methods may added future tting time one binary classier per bit code book tted prediction time classiers used project new points class space class closest points chosen outputcodeclassifier code_size attribute allows user control number classiers used percentage total number classes number require fewer classiers onevstherest theory log2 n_classes n_classes sufcient represent class unambiguously however practice may lead good accuracy since log2 n_classes much smaller n_classes number greater require classiers onevstherest case classiers theory correct mistakes made classiers hence name errorcorrecting practice however may happen classier mistakes typically correlated errorcorrecting output codes similar effect bagging example sklearn import datasets sklearn.multiclass import outputcodeclassifier sklearn.svm import linearsvc iris datasets.load_iris iris.data iris.target outputcodeclassifier linearsvc code_size2 random_state0 .fit .predict array references 1.3.11 feature selection classes sklearn.feature_selection module used feature selectiondimensionality duction sample sets either improve estimators accuracy scores boost performance high dimensional datasets 
340: univariate feature selection univariate feature selection works selecting best features based univariate statistical tests seen preprocessing step estimator scikitlearn exposes feature selection routines objects implement error coding method picts james hastie journal computational graphical statistics 
341: 1.3. supervised learning scikitlearn user guide release 0.11 transform method selecting kbest features selectkbest setting percentile features keep selectpercentile using common univariate statistical tests feature false positive rate selectfpr false discovery rate selectfdr family wise error selectfwe 
342: objects take input scoring function returns univariate pvalues regression f_regression classication chi2 f_classif feature selection sparse data use sparse data i.e data represented sparse matrices chi2 deal data without making dense 
343: warning beware use regression scoring function classication problem get useless results 
344: examples univariate feature selection recursive feature elimination given external estimator assigns weights features e.g. coefcients linear model recursive feature elimination rfe select features recursively considering smaller smaller sets features first estimator trained initial set features weights assigned one features whose absolute weights smallest pruned current set features procedure recursively repeated pruned set desired number features select eventually reached 
345: examples recursive feature elimination recursive feature elimination example showing relevance pixels digit classication task 
346: recursive feature elimination crossvalidation recursive feature elimination example auto matic tuning number features selected crossvalidation 
347: l1based feature selection selecting nonzero coefcients linear models penalized norm sparse solutions many estimated coefcients zero goal reduce dimensionality data use another classier expose transform method lect nonzero coefcient particular sparse estimators useful purpose linear_model.lasso regression linear_model.logisticregression svm.linearsvc classication chapter user guide scikitlearn user guide release 0.11 sklearn.svm import linearsvc sklearn.datasets import load_iris iris load_iris iris.data iris.target x.shape x_new linearsvc c0.01 penalty dualfalse .fit_transform x_new.shape svms logisticregression parameter controls sparsity smaller fewer features selected lasso higher alpha parameter fewer features selected 
348: examples classication text documents using sparse features comparison different algorithms document classication including l1based feature selection 
349: l1recovery compressive sensing good choice alpha lasso fully recover exact set nonzero variables using observations provided certain specic conditions met paraticular number samples sufciently large models perform random sufciently large depends number nonzero coefcients logarithm number features amount noise smallest absolute value nonzero coefcients structure design matrix addition design matrix must display certain specic properties correlated general rule select alpha parameter recovery nonzero coefcients set crossvalidation lassocv lassolarscv though may lead underpenalized models including small number nonrelevant variables detrimental prediction score bic lassolarsic tends opposite set high values alpha reference richard baraniuk compressive sensing ieee signal processing magazine july http dsp.rice.edulescsbaraniukcslecture07.pdf randomized sparse models limitation l1based sparse models faced group correlated features select one mitigate problem possible use randomization techniques reestimating sparse model many times perturbing design matrix subsampling data counting many times given regressor selected randomizedlasso implements lasso randomizedlogisticregression uses logistic regression suitable classication tasks get full path stability scores use lasso_stability_path note randomized sparse models powerful standard statistics detecting nonzero features ground truth model sparse words small fraction features non zero 
350: regression settings using strategy examples sparse recovery feature selection sparse linear models example comparing different feature selection approaches discussing situation approach favored 
351: 1.3. supervised learning scikitlearn user guide release 0.11 references meinshausen buhlmann stability selection journal royal statistical society http arxiv.orgpdf0809.2932 bach modelconsistent sparse estimation bootstrap http hal.inria.frhal00354771 treebased feature selection treebased estimators see sklearn.tree module forest trees sklearn.ensemble module used compute feature importances turn used discard irrelevant features sklearn.ensemble import extratreesclassifier sklearn.datasets import load_iris iris load_iris iris.data iris.target x.shape clf extratreesclassifier compute_importancestrue random_state0 x_new clf.fit .transform x_new.shape examples feature importances forests trees example synthetic data showing recovery actually meaningful features 
352: pixel importances parallel forest trees example face recognition data 
353: chapter user guide scikitlearn user guide release 0.11 1.3.12 semisupervised semisupervised learning situation training data samples labeled semisupervised estimators sklean.semi_supervised able make use addition unlabeled data capture better shape underlying data distribution generalize better new samples algorithms perform well small amount labeled points large amount unlabeled points 
354: unlabeled entries important assign identier unlabeled points along labeled data training model method identier implementation uses integer value 
355: label propagation label propagation denote variations semisupervised graph inference algorithms features available model used classication regression tasks kernel methods project data alternate dimensional spaces scikitlearn provides two label propagation models labelpropagation labelspreading work constructing similarity graph items input dataset 
356: figure 1.1 illustration labelpropagation structure unlabeled observations consistent class structure thus class label propagated unlabeled observations training set 
357: labelpropagation labelspreading differ modications similarity matrix graph clamping effect label distributions clamping allows algorithm change weight true ground labeled data degree labelpropagation algorithm performs hard clamping input labels means clamping factor relaxed say 0.8 means always retain percent original label distribution algorithm gets change condence distribution within percent labelpropagation uses raw similarity matrix constructed data modications contrast labelspreading minimizes loss function regularization properties often robust noise algorithm iterates modied version original graph normalizes edge weights computing normalized graph laplacian matrix procedure also used spectral clustering label propagation models two builtin kernel methods choice kernel effects scalability performance algorithms following available rbf exp specied keyword gamma 
358: 1.3. supervised learning scikitlearn user guide release 0.11 knn cid48 specied keyword n_neighbors 
359: rbf kernel produce fully connected graph represented memory dense matrix matrix may large combined cost performing full matrix multiplication calculation iteration algorithm lead prohibitively long running times hand knn kernel produce much memory friendly sparse matrix drastically reduce running times 
360: examples decision boundary label propagation versus svm iris dataset label propagation learning complex structure decision boundary label propagation versus svm iris dataset label propagation digits active learning references yoshua bengio olivier delalleau nicolas roux semisupervised learning olivier delalleau yoshua bengio nicolas roux efcient nonparametric function induction semi supervised learning aistat http research.microsoft.comenuspeoplenicolaslefcient_ssl.pdf 1.3.13 linear quadratic discriminant analysis linear discriminant analysis lda.lda quadratic discriminant analysis qda.qda two classic classi ers names suggest linear quadratic decision surface respectively classiers attractive closed form solutions easily computed inherently multiclass proven work well practice also parameters tune algorithms 
361: chapter user guide scikitlearn user guide release 0.11 plot shows decision boundaries lda qda bottom row demonstrates lda learn linear boundaries qda learn quadratic boundaries therefore exible 
362: examples linear quadratic discriminant analysis condence ellipsoid comparison lda qda syn thetic data 
363: references dimensionality reduction using lda lda.lda used perform supervised dimensionality reduction projecting input data subspace con sisting discriminant directions implemented lda.lda.transform desired dimension ality set using n_components constructor parameter parameter inuence lda.lda.fit lda.lda.predict 
364: mathematical idea methods work modeling class conditional distribution data class predictions obtained using bayes rule cid88 cid48 cid48 cid48 linear quadratic discriminant analysis modeled gaussian distribution case lda gaussians class assumed share covariance matrix leads linear decision surface seen comparing logprobability rations log case qda assumptions covariance matrices gaussians leading quadratic decision surface 
365: 1.4 unsupervised learning 1.4.1 gaussian mixture models sklearn.mixture package enables one learn gaussian mixture models diagonal spherical tied full covariance matrices supported sample estimate data facilities help determine appropriate number components also provided 
366: gaussian mixture model probabilistic model assumes data points generated mixture nite number gaussian distributions unknown parameters one think mixture models generalizing kmeans clustering incorporate information covariance structure data well centers latent gaussians scikitlearn implements different classes estimate gaussian mixture models correspond different esti mation strategies detailed 
367: 1.4. unsupervised learning scikitlearn user guide release 0.11 figure 1.2 twocomponent gaussian mixture model data points equiprobability surfaces model 
368: gmm classier gmm object implements expectationmaximization algorithm tting mixtureofgaussian models also draw condence ellipsoids multivariate models compute bayesian information criterion assess number clusters data gmm.fit method provided learns gaussian mixture model train data given test data assign sample class gaussian mostly probably belong using gmm.predict method gmm comes different options constrain covariance difference classes estimated spherical diagonal tied full covariance 
369: examples see gmm classication example using gmm classier iris dataset see density estimation mixture gaussians example plotting density estimation 
370: pros cons class gmm expectationmaximization inference pros cons speed fastest algorithm learning mixture models agnostic algorithm maximizes likelihood bias means towards zero bias cluster sizes specic structures might might apply 
371: singularities one insufciently many points per mixture estimating covariance matrices becomes difcult algorithm known diverge solutions innite likelihood unless one regularizes covariances articially 
372: number components algorithm always use components access needing held data information theoretical criteria decide many components use absence external cues 
373: chapter user guide scikitlearn user guide release 0.11 selecting number components classical gmm bic criterion used select number components gmm efcient way theory recovers true number components asymptotic regime i.e much data available note using dpgmm avoids specication number components gaussian mixture model 
374: examples see gaussian mixture model selection example model selection performed classical gmm 
375: estimation algorithm expectationmaximization main difculty learning gaussian mixture models unlabeled data one usually doesnt know points came latent component one access information gets easy separate gaussian distribution set points expectationmaximization wellfundamented statistical algorithm get around problem iterative process first one assumes random components randomly centered data points learned kmeans even normally distributed around origin computes point probability generated component model one tweaks parameters maximize likelihood data given assignments repeating process guaranteed always converge local optimum 
376: 1.4. unsupervised learning scikitlearn user guide release 0.11 vbgmm classier variational gaussian mixtures vbgmm object implements variant gaussian mixture model variational inference algorithms api identical gmm essentially middleground gmm dpgmm properties dirichlet process 
377: pros cons class vbgmm variational inference regularization due incorporation prior information variational solutions less pathological special cases expectationmaximization solutions one use full covariance matrices high dimensions cases components might centered around single point without risking divergence 
378: pros cons bias regularize model one add biases variational algorithm bias means towards origin part prior information adds ghost point origin every mixture component bias covariances spherical also depending concentration parameter bias cluster structure either towards uniformity towards richget richer scenario 
379: hyperparameters algorithm needs extra hyperparameter might need experimental tuning via crossvalidation 
380: estimation algorithm variational inference variational inference extension expectationmaximization maximizes lower bound model evidence including priors instead data likelihood principle behind variational methods expectation maximization iterative algorithms alternate nding probabilities point generated mixture tting mixtures assigned points variational methods add regular ization integrating information prior distributions avoids singularities often found expectation maximization solutions introduces subtle biases model inference often notably slower usually much render usage unpractical 
381: chapter user guide scikitlearn user guide release 0.11 due bayesian nature variational algorithm needs hyperparameters expectationmaximization important concentration parameter alpha specifying high value alpha leads often uniformlysized mixture components specifying small values lead mixture components getting almost points mixture components centered remaining points 
382: dpgmm classier innite gaussian mixtures dpgmm object implements variant gaussian mixture model variable bounded number components using dirichlet process api identical gmm class doesnt require user choose number components expense extra computational time user needs specify loose upper bound number concentration parameter 
383: examples compare gaussian mixture models xed number components dpgmm models left gmm tted components dataset composed clusters see dpgmm able limit components whereas gmm data many components note little observations dpgmm take conservative stand one component right tting dataset welldepicted mixture gaussian adjusting alpha parameter dpgmm controls number components used data 
384: examples see gaussian mixture model ellipsoids example plotting condence ellipsoids gmm dpgmm 
385: gaussian mixture model sine curve shows using gmm dpgmm sine wave pros cons class dpgmm diriclet process mixture model pros less sensitivity number parameters unlike nite models almost always use components much hence produce wildly different solutions different numbers components dirichlet process solution wont change much changes parameters leading stability less tuning 
386: need specify number components upper bound number needs pro vided note however dpmm formal model selection procedure thus provides guarantee result 
387: 1.4. unsupervised learning scikitlearn user guide release 0.11 cons speed extra parametrization necessary variational inference structure dirichlet process make inference slower although much 
388: bias variational techniques many implicit biases dirichlet process inference algorithms whenever mismatch biases data might possible better models using nite mixture 
389: dirichlet process describe variational inference algorithms dirichlet process mixtures dirichlet process prior probability distribution clusterings innite unbounded number partitions variational techniques let incorporate prior structure gaussian mixture models almost penalty inference time comparing nite gaussian mixture model important question dirichlet process use innite unbounded number clusters still consistent full explanation doesnt manual one think chinese restaurant process analogy help understanding chinese restaurant process generative story dirichlet process imagine chinese restaurant innite number tables rst empty rst customer day arrives sits rst table every following customer either sit occupied table probability proportional number customers table sit entirely new table probability proportional concentration parameter alpha nite number customers sat easy see nitely many innite tables ever used higher value alpha total tables used dirichlet process clustering unbounded number mixture components assuming asymmetrical prior structure assignments points components concentrated property known richgetricher full tables chinese restaurant process tend get fuller simulation progresses variational inference techniques dirichlet process still work nite approximation innite mixture model instead specify priori many components one wants use one species concen tration parameter upper bound number mixture components upper bound assuming higher true number components affects algorithmic complexity actual number components used 
390: derivation see full derivation algorithm 
391: variational gaussian mixture models api identical gmm class main difference offers access precision matrices well covariance matrices inference algorithm one following paper variational inference dirichlet process mixtures david blei michael jordan bayesian analysis paper presents parts inference algorithm concerned structure dirichlet pro cess detail mixture modeling part complex even reason present full derivation inference algorithm update lowerbound equations youre interested learning derive similar algorithms youre interested changingdebugging implementation scikit document complexity implementation linear number mixture components data points regards dimensionality linear using spherical diag quadraticcubic using tied full spherical diag n_states n_points dimension tied full n_states n_points dimension2 chapter user guide scikitlearn user guide release 0.11 n_states dimension3 necessary invert covarianceprecision matrices compute determinant hence cubic term implementation expected scale least well mixture gaussians 
392: update rules inference full mathematical derivation variational bayes update rules gaussian mixture models given main parameters model dened class 1..k class proportion mean parameters covariance parameters characterized variational wishart density ishart degrees freedom scale matrix depending covariance parameterization positive scalar positive vector symmetric positive denite matrix 
393: spherical model model variational distribution well use beta ormal gamma sbp ormal beta ormal gamma discrete bound variational bound log cid80 cid80 cid80 cid80 cid80 log log log log log log log log log bound log beta log beta log log log log log bound cid82 log cid82 log log log log 2k2 log bound ill use inverse scale parametrization gamma distribution 
394: log log log log 1.4. unsupervised learning scikitlearn user guide release 0.11 bound cid80 cid16 cid16 cid80 log log cid17 jk1 log cid17 bound recall need bound log cid80 cid16 log log 2bk cid17 log simplicity ill later call term inside parenthesis log xizi updates updating cid80 cid80 cid80 
395: updating updates essentially weighted expectations regularized prior see taking gradient bound w.r.t setting zero gradient update cid88 cid80 cid80 updating odd reason doesnt really work derive updates using gradients lower bound terms involving cid48 function show hard isolate however use formula log cid54 log const terms involving get folded constant get two terms prior probability gives log gamma distribution log cid88 cid88 log cid88 cid80 
396: verify normalizing previous term updating log log xizi cid88 
397: chapter user guide scikitlearn user guide release 0.11 diagonal model model tha variational distribution well use beta ormal gamma sbp ormal beta ormal gamma discrete lower bound changes lower bound previous model distributions lot bound bound safelly ommited bound main difference precision matrix scales norm extra term computing expectation log cid80 cid80 cid16 cid80 log cid80 log cid17 log updates updates chance weight new change folded xizi term variables update cid32 cid33 cid32 cid88 cid88 cid33 updates well something similar spheric model main difference controls one dimension bound log log hence cid88 cid88 cid88 cid88 1.4. unsupervised learning scikitlearn user guide release 0.11 tied model model tha variational distribution well use beta ormal ishart sbp ormal beta ormal ishart discrete lower bound two changes lowerbound bound log log cid80 log cid80 cid1 log log cid1 cid0 cid80 cid0 a1d cid16 cid0 cid80 cid0 a1d log d1d log a1d atr cid17 atr log log cid1 log log cid1 bound log cid80 updates last setting changes trivial update update update update cid32 cid88 cid33 cid32 cid88 cid33 update distribution far complicated even going try going gradient way 
398: log log log cid19 nontrivially seeing quadratic form middle expressed trace something reduces cid19 log log log hence bit squinting looks like wishart parameters cid32 cid88 cid88 cid33 chapter user guide cid18 cid18 cid88 cid88 cid88 cid88 cid32 akbk akbk cid88 cid33 cid32 cid88 cid33 cid88 scikitlearn user guide release 0.11 full model model variational distribution well use beta ormal ishart sbp ormal beta ormal ishart discrete lower bound changes lower bound comparison previous one priors different precision matrices correct indices bound 
399: updates changes updates update uses proper sigma updates dont sum cid32 cid32 cid88 cid33 cid88 cid33 1.4.2 manifold learning look bare necessities simple bare necessities forget worries strife mean bare necessities old mother natures recipes bring bare necessities life baloos song jungle book manifold learning approach nonlinear dimensionality reduction algorithms task based idea dimensionality many data sets articially high 
400: 1.4. unsupervised learning scikitlearn user guide release 0.11 introduction highdimensional datasets difcult visualize data two three dimensions plotted show inherent structure data equivalent highdimensional plots much less intuitive aid visualization structure dataset dimension must reduced way simplest way accomplish dimensionality reduction taking random projection data though allows degree visualization data structure randomness choice leaves much desired random projection likely interesting structure within data lost 
401: chapter user guide scikitlearn user guide release 0.11 address concern number supervised unsupervised linear dimensionality reduction frameworks designed principal component analysis pca independent component analysis linear discriminant analysis others algorithms dene specic rubrics choose interesting linear projection data methods powerful often miss important nonlinear structure data 
402: manifold learning thought attempt generalize linear frameworks like pca sensitive non linear structure data though supervised variants exist typical manifold learning problem unsupervised learns highdimensional structure data data without use predetermined classications 
403: 1.4. unsupervised learning scikitlearn user guide release 0.11 examples curve dataset 
404: see manifold learning handwritten digits locally linear embedding isomap ... example dimensionality reduction handwritten digits 
405: see comparison manifold learning methods example dimensionality reduction toy manifold learning implementations available sklearn summarized isomap one earliest approaches manifold learning isomap algorithm short isometric mapping isomap viewed extension multidimensional scaling mds kernel pca isomap seeks lowerdimensional embedding maintains geodesic distances points isomap performed object isomap 
406: complexity isomap algorithm comprises three stages nearest neighbor search isomap uses sklearn.neighbors.balltree efcient neighbor search 
407: cost approximately log log nearest neighbors points dimensions 
408: shortestpath graph search efcient known algorithms dijkstras algorithm approximately log floydwarshall algorithm algorithm selected user path_method keyword isomap unspecied code attempts choose best algorithm input data 
409: partial eigenvalue decomposition embedding encoded eigenvectors corresponding largest eigenvalues isomap kernel dense solver cost approximately cost often improved using arpack solver eigensolver specied user path_method keyword isomap unspecied code attempts choose best algorithm input data 
410: overall complexity isomap log log log 
411: number training data points chapter user guide scikitlearn user guide release 0.11 input dimension number nearest neighbors output dimension references global geometric framework nonlinear dimensionality reduction tenenbaum j.b. silva langford j.c. science locally linear embedding locally linear embedding lle seeks lowerdimensional projection data preserves distances within local neighborhoods thought series local principal component analyses globally compared best nonlinear embedding locally linear embedding performed function locally_linear_embedding objectoriented counterpart locallylinearembedding 
412: complexity standard lle algorithm comprises three stages nearest neighbors search see discussion isomap weight matrix construction construction lle weight matrix involves solution linear equation local neighborhoods partial eigenvalue decomposition see discussion isomap 
413: overall complexity standard lle log log 
414: number training data points input dimension number nearest neighbors output dimension 1.4. unsupervised learning scikitlearn user guide release 0.11 references nonlinear dimensionality reduction locally linear embedding roweis saul science modied locally linear embedding one wellknown issue lle regularization problem number neighbors greater number input dimensions matrix dening local neighborhood rankdecient address standard lle applies arbitrary regularization parameter chosen relative trace local weight matrix though shown formally solution coverges desired embedding guarantee optimal solution found problem manifests embeddings distort underlying geometry manifold one method address regularization problem use multiple weight vectors neighborhood essence modied locally linear embedding mlle mlle performed function locally_linear_embedding objectoriented counterpart locallylinearembedding key word method modified requires n_neighbors n_components 
415: complexity mlle algorithm comprises three stages nearest neighbors search standard lle weight matrix construction approximately rst term exactly equivalent standard lle second term constructing weight matrix multiple weights practice added cost constructing mlle weight matrix relatively small compared cost steps 
416: partial eigenvalue decomposition standard lle overall complexity mlle log log 
417: number training data points input dimension chapter user guide scikitlearn user guide release 0.11 number nearest neighbors output dimension references mlle modied locally linear embedding using multiple weights zhang wang 
418: hessian eigenmapping hessian eigenmapping also known hessianbased lle hlle another method solving regularization problem lle revolves around hessianbased quadratic form neighborhood used recover locally linear structure though implementations note poor scaling data size sklearn imple ments algorithmic improvements make cost comparable lle variants small output dimension hlle performed function locally_linear_embedding objectoriented counter part locallylinearembedding keyword method hessian requires n_neighbors n_components n_components 
419: complexity hlle algorithm comprises three stages nearest neighbors search standard lle weight matrix construction approximately rst term reects similar cost standard lle second term comes decomposition local hessian estimator 
420: partial eigenvalue decomposition standard lle overall complexity standard hlle log log 
421: number training data points input dimension number nearest neighbors output dimension 1.4. unsupervised learning scikitlearn user guide release 0.11 references hessian eigenmaps locally linear embedding techniques highdimensional data donoho grimes proc natl acad sci usa local tangent space alignment though technically variant lle local tangent space alignment ltsa algorithmically similar enough lle put category rather focusing preserving neighborhood distances lle ltsa seeks characterize local geometry neighborhood via tangent space performs global optimization align local tangent spaces learn embedding ltsa performed function locally_linear_embedding objectoriented counterpart locallylinearembedding key word method ltsa 
422: complexity ltsa algorithm comprises three stages nearest neighbors search standard lle weight matrix construction approximately k2d rst term reects similar cost standard lle 
423: partial eigenvalue decomposition standard lle overall complexity standard ltsa log log k2d 
424: number training data points input dimension number nearest neighbors output dimension chapter user guide scikitlearn user guide release 0.11 references principal manifolds nonlinear dimensionality reduction via tangent space alignment zhang zha journal shanghai univ tips practical use make sure scale used features manifold learning methods based nearest neighbor search algorithm may perform poorly otherwise see scaler convenient ways scaling het erogeneous data 
425: reconstruction error computed routine used choose optimal output dimension ddimensional manifold embedded ddimensional parameter space reconstruction error decrease n_components increased n_components 
426: note noisy data shortcircuit manifold essence acting bridge parts manifold would otherwise wellseparated manifold learning noisy andor incomplete data active area research 
427: certain input congurations lead singular weight matrices example two points dataset identical data split disjointed groups case methodarpack fail null space easiest way address use methoddense work singular matrix though may slow depending number input points alternatively one attempt understand source singularity due disjoint sets increasing n_neighbors may help due identical points dataset removing points may help 
428: 1.4.3 clustering clustering unlabeled data performed module sklearn.cluster clustering algorithm comes two variants class implements method learn clusters train data function given train data returns array integer labels corresponding different clusters class labels training data found labels_ attribute 
429: input data one important thing note algorithms implemented module take different kinds trix input one hand meanshift kmeans take data matrices shape n_samples n_features obtained classes sklearn.feature_extraction module hand affinitypropagation spectralclustering take similarity matrices shape n_samples n_samples obtained functions sklearn.metrics.pairwise module words meanshift kmeans work points vector space whereas affinitypropagation spectralclustering work arbitrary objects long simi larity measure exists objects 
430: 1.4. unsupervised learning scikitlearn user guide release 0.11 figure 1.3 comparison clustering algorithms scikitlearn overview clustering methods method name kmeans afnity propaga tion mean shift spectral clustering hierar chical clustering dbscan gaussian mixtures parame ters number clusters damping sample preference bandwidth number clusters scalability usecase large n_samples medium n_clusters minibatch code scalable n_samples generalpurpose even cluster size geometry many clusters many clusters uneven cluster size nonat geometry scalable n_samples medium n_samples small n_clusters many clusters uneven cluster size nonat geometry clusters even cluster size nonat geometry number clusters large n_samples n_clusters many clusters possibly connectivity constraints geometry metric used distances points graph distance e.g nearestneighbor graph distances points graph distance e.g nearestneighbor graph distances points neighbor hood size many large n_samples medium n_clusters nonat geometry uneven cluster sizes distances nearest points scalable flat geometry good density estimation mahalanobis distances centers nonat geometry clustering useful clusters specic shape i.e nonat manifold standard euclidean distance right metric case arises two top rows gure gaussian mixture models useful clustering described another chapter documentation dedicated mixture models kmeans seen special case gaussian mixture model equal covariance per component 
431: kmeans kmeans algorithm clusters data trying separate samples groups equal variance minimizing criterion known inertia groups algorithm requires number cluster specied scales well chapter user guide scikitlearn user guide release 0.11 large number samples however results may dependent initialisation result computation often done several times different initialisation centroids kmeans often referred lloyds algorithm initialization kmeans consists looping two major steps first voronoi diagram points calculated using current centroids segment voronoi diagram becomes separate cluster secondly centroids updated mean segment algorithm repeats stopping criteria fullled usually implementation algorithm stops relative increment results iterations less given tolerance value parameter given allow kmeans run parallel called n_jobs giving parameter positive value uses many processors default1 value uses processors using one less parallelization generally speeds computation cost memory case multiple copies centroids need stored one job kmeans used vector quantization achieved using transform method trained model kmeans 
432: examples demo kmeans clustering handwritten digits data clustering handwritten digits mini batch kmeans minibatchkmeans variant kmeans algorithm using minibatches random subset dataset compute centroids althought minibatchkmeans converge faster kmeans version quality results measured inertia sum distance points nearest centroid good kmeans algorithm 
433: examples demo means clustering algorithm comparison kmeans minibatchkmeans clustering text documents using kmeans document clustering using sparse minibatchkmeans 1.4. unsupervised learning scikitlearn user guide release 0.11 references web scale kmeans clustering sculley proceedings 19th international conference world wide web afnity propagation affinitypropagation clusters data diffusion similarity matrix algorithm automatically sets numbers cluster difculties scaling thousands samples 
434: examples demo afnity propagation clustering algorithm afnity propagation synthetic datasets visualizing stock market structure afnity propagation financial time series groups classes 
435: companies mean shift meanshift clusters data estimating blobs smooth density points matrix algorithm automati cally sets numbers cluster difculties scaling thousands samples utility function estimate_bandwidth used guess optimal bandwidth meanshift data 
436: examples demo meanshift clustering algorithm mean shift clustering synthetic datasets classes 
437: spectral clustering spectralclustering lowdimension embedding afnity matrix samples followed kmeans low dimensional space especially efcient afnity matrix sparse pyamg module chapter user guide scikitlearn user guide release 0.11 installed spectralclustering requires number clusters specied works well small number clusters advised using many clusters two clusters solves convex relaxation normalised cuts problem similarity graph cutting graph two weight edges cut small compared weights edges inside cluster criteria especially interesting working images graph vertices pixels edges similarity graph function gradient image 
438: warning shapeless isotropic data data really shapeless i.e generated random distribution clusters spectral clustering problem illconditioned different choices almost equivalent spectral clustering solver chooses arbitrary one putting rst sample alone one bin 
439: examples spectral clustering image segmentation segmenting objects noisy background using spectral clustering 
440: segmenting picture lena regions spectral clustering split image lena regions 
441: 1.4. unsupervised learning scikitlearn user guide release 0.11 references tutorial spectral clustering ulrike von luxburg normalized cuts image segmentation jianbo shi jitendra malik random walks view spectral segmentation marina meila jianbo shi spectral clustering analysis algorithm andrew michael jordan yair weiss hierarchical clustering hierarchical clustering general family clustering algorithms build nested clusters merging succes sively hierarchy clusters represented tree dendrogram root tree unique cluster gathers samples leaves clusters one sample see wikipedia page details ward object performs hierarchical clustering based ward algorithm varianceminimizing proach step minimizes sum squared differences within clusters inertia criterion algorithm scale large number samples used jointly connectivity matrix computationally expensive connectivity constraints added samples considers step possible merges 
442: adding connectivity constraints interesting aspect ward object connectivity constraints added algorithm adjacent clusters merged together connectivity matrix denes sample neighboring samples following given structure data instance swissroll example connectivity constraints forbid merging points adjacent swiss roll thus avoid forming clusters extend across overlapping folds roll 
443: connectivity constraints imposed via connectivity matrix scipy sparse matrix elements intersection row column indices dataset connected trix constructed apriori information instance whish cluster web pages merg ing pages link pointing one another also learned data instance using sklearn.neighbors.kneighbors_graph restrict merging nearest neighbors swiss roll exam ple using sklearn.feature_extraction.image.grid_to_graph enable merging neigh boring pixels image lena example 
444: chapter user guide scikitlearn user guide release 0.11 examples demo structured ward hierarchical clustering lena image ward clustering split image lena regions 
445: hierarchical clustering structured unstructured ward example ward algorithm swissroll comparison structured approaches versus unstructured approaches 
446: feature agglomeration vs. univariate selection example dimensionality reduction feature glomeration based ward hierarchical clustering 
447: dbscan dbscan algorithm clusters data nding core points many neighbours within given radius core point found cluster expanded adding neighbours current cluster recusively checking core points formally point considered core point min_points points similarity greater given threshold eps shown gure color indicates cluster membership large circles indicate core points found algorithm moreover algorithm detect outliers indicated black points outliers dened points belong current cluster enough close neighbours start new cluster 
448: examples demo dbscan clustering algorithm clustering synthetic data dbscan references densitybased algorithm discovering clusters large spatial databases noise ester kriegel sander proceedings 2nd international conference knowledge discovery data mining portland aaai press clustering performance evaluation evaluating performance clustering algorithm trivial counting number errors precision recall supervised classication algorithm particular evaluation metric take absolute 1.4. unsupervised learning scikitlearn user guide release 0.11 values cluster labels account rather clustering dene separations data similar ground truth set classes satisfying assumption members belong class similar members different classes according similarity metric 
449: inertia presentation usage todo factorize inertia computation kmeans write advantages need ground truth knowledge real classes 
450: drawbacks inertia makes assumption clusters convex isotropic always case especially clusters manifolds weird shapes instance inertia useless metrics evaluate clustering algorithm tries identify nested circles plane 
451: inertia normalized metrics know lower values better bounded zero one potential solution would adjust inertia random clustering assuming number ground truth classes known 
452: adjusted rand index presentation usage given knowledge ground truth class assignments labels_true clus tering algorithm assignments samples labels_pred adjusted rand index function mea sures similarity two assignements ignoring permutations chance normalization sklearn import metrics labels_true labels_pred metrics.adjusted_rand_score labels_true labels_pred 0.24.. 
453: one permute predicted labels rename get score labels_pred metrics.adjusted_rand_score labels_true labels_pred 0.24.. 
454: furthermore adjusted_rand_score symmetric swapping argument change score thus used consensus measure metrics.adjusted_rand_score labels_pred labels_true 0.24.. 
455: perfect labeling scored 1.0 labels_pred labels_true metrics.adjusted_rand_score labels_true labels_pred 1.0 bad e.g independent labelings negative close 0.0 scores chapter user guide scikitlearn user guide release 0.11 labels_true labels_pred metrics.adjusted_rand_score labels_true labels_pred 0.12.. 
456: advantages random uniform label assignements ari score close 0.0 value n_clusters n_samples case raw rand index vmeasure instance 
457: bounded range negative values bad independent labelings similar clusterings positve ari 1.0 perfect match score 
458: assumption made cluster structure used compare clustering algorithms means assumes isotropic blob shapes results spectral clustering algorithms cluster folded shapes 
459: drawbacks contrary inertia ari requires knowlege ground truth classes almost never available practice requires manual assignment human annotators supervised learning setting however ari also useful purely unsupervised setting building block consensus index used clustering model selection todo 
460: examples adjustment chance clustering performance evaluation analysis impact dataset size value clustering measures random assignements 
461: mathematical formulation ground truth class assignement clustering let dene number pairs elements set set number pairs elements different sets different sets raw unadjusted rand index given nsamples total number possible pairs dataset without ordering 
462: nsamples however score guarantee random label assignements get value close zero esp number clusters order magnitude number samples counter effect discount expected random labelings dening adjusted rand index follows ari expected_ri max expected_ri 1.4. unsupervised learning scikitlearn user guide release 0.11 references comparing partitions hubert arabie journal classication wikipedia entry adjusted rand index adjusted mutual information presentation usage given knowledge ground truth class assignments labels_true clus tering algorithm assignments samples labels_pred adjusted mutual information function measures agreement two assignements ignoring permutations chance normalization sklearn import metrics labels_true labels_pred metrics.adjusted_mutual_info_score labels_true labels_pred 0.22504.. 
463: one permute predicted labels rename get score labels_pred metrics.adjusted_mutual_info_score labels_true labels_pred 0.22504.. 
464: furthermore adjusted_mutual_info_score symmetric swapping argument change score thus used consensus measure metrics.adjusted_mutual_info_score labels_pred labels_true 0.22504.. 
465: perfect labeling scored 1.0 labels_pred labels_true metrics.adjusted_mutual_info_score labels_true labels_pred 1.0 bad e.g independent labelings nonpositive scores labels_true labels_pred metrics.adjusted_mutual_info_score labels_true labels_pred 0.10526.. 
466: advantages random uniform label assignements ami score close 0.0 value n_clusters n_samples case raw mutual information vmeasure instance 
467: bounded range values close zero indicate two label assignments largely independent values close one indicate signicant agreement values exactly indicate purely independent label assignments ami exactly indicates two label assignments equal without permutation 
468: assumption made cluster structure used compare clustering algorithms means assumes isotropic blob shapes results spectral clustering algorithms cluster folded shapes 
469: chapter user guide scikitlearn user guide release 0.11 drawbacks contrary inertia ami requires knowlege ground truth classes almost never available practice requires manual assignment human annotators supervised learning setting however ami also useful purely unsupervised setting building block consensus index used clustering model selection 
470: examples adjustment chance clustering performance evaluation analysis impact dataset size value clustering measures random assignements example also includes adjusted rand index 
471: mathematical formulation assume two label assignments data classes classes entropy either amount uncertaintly array calculated log cid48 log cid48 cid88 cid88 cid88 cid88 number instances class likewise number instances class nonadjusted mutual information calculated log cid48 number instances label also label value mutual information adjusted cfor chance tend increase number different labels clusters increases regardless actual amount mutual information label assignments expected value mutual information calculated using following equation vinh epps bailey equation number instances label number instances label 
472: cid88 cid88 min cid88 nij aibjn nij log n.nij aibj nij nij nij nij using expected value adjusted mutual information calculated using similar form adjusted rand index expected_m max expected_m 1.4. unsupervised learning scikitlearn user guide release 0.11 references vinh epps bailey 26th annual information theoretic measures clusterings comparison icml 
473: international conference machine learning proceedings doi10.11451553374.1553511 isbn 
474: information theoretic measures correction vinh epps comparison http jmlr.csail.mit.edupapersvolume11vinh10avinh10a.pdf properties normalization bailey variants 
475: wikipedia entry adjusted mutual information clusterings jmlr chance homogeneity completeness vmeasure presentation usage given knowledge ground truth class assignments samples possible dene intuitive metric using conditional entropy analysis particular rosenberg hirschberg dene following two desirable objectives cluster assign ment homogeneity cluster contains members single class completeness members given class assigned cluster 
476: turn concept scores homogeneity_score completeness_score bounded 0.0 1.0 higher better sklearn import metrics labels_true labels_pred metrics.homogeneity_score labels_true labels_pred 0.66.. 
477: metrics.completeness_score labels_true labels_pred 0.42.. 
478: harmonic mean called vmeasure computed v_measure_score metrics.v_measure_score labels_true labels_pred 0.51.. 
479: three metrics computed using homogeneity_completeness_v_measure follows metrics.homogeneity_completeness_v_measure labels_true labels_pred ... 0.66 ... 0.42 ... 0.51 ... following clustering assignment slighlty better since homogeneous complete labels_pred metrics.homogeneity_completeness_v_measure labels_true labels_pred ... 1.0 0.68 ... 0.81 ... note v_measure_score symmetric used evaluate agreement two independent assigne ments dataset case completeness_score homogeneity_score bound relationship chapter user guide scikitlearn user guide release 0.11 homogeneity_score completeness_score advantages bounded scores 0.0 bad 1.0 perfect score intuitive interpretation clustering bad vmeasure qualitatively analyzed terms homogeneity completeness better feel kind mistakes done assigmenent 
480: assumption made cluster structure used compare clustering algorithms means assumes isotropic blob shapes results spectral clustering algorithms cluster folded shapes 
481: drawbacks previously introduced metrics normalized w.r.t random labeling means depending number samples clusters ground truth classes completely random labeling always yield values homogeneity completeness hence vmeasure particular random labeling wont yield zero scores especially number clusters large problem safely ignored number samples thousand number clusters less 10. smaller sample sizes larger number clusters safer use adjusted index adjusted rand index ari 
482: 1.4. unsupervised learning scikitlearn user guide release 0.11 metrics require knowlege ground truth classes almost never available practice requires manual assignment human annotators supervised learning setting 
483: examples adjustment chance clustering performance evaluation analysis impact dataset size value clustering measures random assignements 
484: mathematical formulation homogeneity completeness scores formally given conditional entropy classes given cluster assignments given log cid88 cid88 cid88 entropy classes given log total number samples number samples respectively belonging class cluster nally number samples class assigned cluster conditional entropy clusters given class entropy clusters dened sym metric manner rosenberg hirschberg dene vmeasure harmonic mean homogeneity completeness references vmeasure conditional entropybased external cluster evaluation measure andrew rosenberg julia hirschberg silhouette coefcient chapter user guide scikitlearn user guide release 0.11 presentation usage ground truth labels known evaluation must performed using model self silhouette coefcient sklearn.metrics.silhouette_score example evaluation higher silhouette coefcient score relates model better dened clusters silhouette coefcient dened sample composed two scores mean distance sample points class mean distance sample points next nearest cluster 
485: silhoeutte coefcient single sample given max silhouette coefcient set samples given mean silhouette coefcient sample 
486: sklearn import metrics sklearn.metrics import pairwise_distances sklearn import datasets dataset datasets.load_iris dataset.data dataset.target normal usage silhouette coefcient applied results cluster analysis 
487: import numpy sklearn.cluster import kmeans kmeans_model kmeans random_state1 .fit labels kmeans_model.labels_ metrics.silhouette_score labels metriceuclidean ... 0.55.. 
488: references peter rousseeuw silhouettes graphical aid interpretation validation cluster analysis computational applied mathematics 5365. doi10.101603770427 
489: advantages score bounded incorrect clustering highly dense clustering scores around zero indicate overlapping clusters 
490: score higher clusters dense well separated relates standard concept cluster 
491: drawbacks silhouette coefcient generally higher convex clusters concepts clusters density based clusters like obtained dbscan 
492: 1.4. unsupervised learning scikitlearn user guide release 0.11 1.4.4 decomposing signals components matrix factorization problems principal component analysis pca exact pca probabilistic interpretation pca used decompose multivariate dataset set successive orthogonal components explain maximum amount variance scikitlearn pca implemented transformer object learns components method used new data project components optional parameter whitentrue parameter make possible project data onto singular space scaling component unit variance often useful models downstream make strong assumptions isotropy signal example case support vector machines rbf kernel kmeans clustering algorithm however case inverse transform longer exact since information lost forward transforming addition probabilisticpca object provides probabilistic interpretation pca give like lihood data based amount variance explains implements score method used crossvalidation example iris dataset comprised features projected dimensions explain variance examples comparison lda pca projection iris dataset chapter user guide scikitlearn user guide release 0.11 approximate pca often interested projecting data onto lower dimensional space preserves variance dropping singular vector components associated lower singular values instance face recognition work 64x64 gray level pixel pictures dimensionality data slow train rbf support vector machine wide data furthermore know intrinsic dimensionality data much lower since faces pictures look alike samples lie manifold much lower dimension say around instance pca algorithm used linearly transform data reducing dimensionality preserve explained variance time class randomizedpca useful case since going drop singular vectors much efcient limit computation approximated estimate singular vectors keep actually perform transform instance following shows sample portraits centered around 0.0 olivetti dataset right hand side rst singular vectors reshaped portraits since require top singular vectors dataset size nsamples eatures computation time less randomizedpca hence used drop replacement pca minor exception need give size lower dimensional space n_components mandatory input parameter note nmax max nsamples eatures nmin min nsamples eatures time complexity 1.4. unsupervised learning scikitlearn user guide release 0.11 max ncomponents instead max nmin exact method implemented pca randomizedpca memory footprint randomizedpca also proportional nmax ncomponents instead nmax nmin exact method furthermore randomizedpca able work scipy.sparse matrices input make suitable reducing dimensionality features extracted text documents instance note implementation inverse_transform randomizedpca exact inverse transform transform even whitenfalse default 
493: examples faces recognition example using eigenfaces svms faces dataset decompositions references finding structure randomness stochastic algorithms constructing approximate matrix decom positions halko al. kernel pca kernelpca extension pca achieves nonlinear dimensionality reduction use kernels many applications including denoising compression structured prediction kernel dependency estimation kernelpca supports transform inverse_transform 
494: chapter user guide scikitlearn user guide release 0.11 examples kernel pca sparse principal components analysis sparsepca minibatchsparsepca sparsepca variant pca goal extracting set sparse components best reconstruct data mini batch sparse pca minibatchsparsepca variant sparsepca faster less accurate increased speed reached iterating small chunks set features given number iterations principal component analysis pca disadvantage components extracted method exclu sively dense expressions i.e nonzero coefcients expressed linear combinations original variables make interpretation difcult many cases real underlying components naturally imagined sparse vectors example face recognition components might naturally map parts faces sparse principal components yields parsimonious interpretable representation clearly emphasizing original features contribute differences samples following example illustrates components extracted using sparse pca olivetti faces dataset seen regularization term induces many zeros furthermore natural structure data causes nonzero coefcients vertically adjacent model enforce mathematically component vector r4096 notion vertical adjacency except humanfriendly visualization 64x64 pixel images fact components shown appear local effect inherent structure data makes local patterns minimize reconstruction error exist sparsityinducing norms take account adjacency different kinds structure see see jen09 review methods details use sparse pca see examples section 
495: 1.4. unsupervised learning scikitlearn user guide release 0.11 note many different formulations sparse pca problem one implemented based mrl09 optimization problem solved pca problem dictionary learning cid96 penalty components arg min subject touk2 ncomponents sparsity inducing cid96 norm also prevents learning components noise training samples available degree penalization thus sparsity adjusted hyperparameter alpha small values lead gently regularized factorization larger values shrink many coefcients zero 
496: note spirit online algorithm class minibatchsparsepca implement partial_t algorithm online along features direction samples direction 
497: examples faces dataset decompositions references dictionary learning sparse coding precomputed dictionary sparsecoder object estimator used transform signals sparse linear combination atoms xed precomputed dictionary discrete wavelet basis object therefore implement method transformation amounts sparse coding problem nding representation data linear combination dictionary atoms possible variations dictionary learning implement following transform methods controllable via transform_method initialization parameter chapter user guide scikitlearn user guide release 0.11 orthogonal matching pursuit orthogonal matching pursuit omp leastangle regression least angle regression lasso computed leastangle regression lasso using coordinate descent lasso thresholding thresholding fast yield accurate reconstructions shown useful literature classication tasks image reconstruction tasks orthogonal matching pursuit yields accurate unbiased reconstruction dictionary learning objects offer via split_code parameter possibility separate positive negative values results sparse coding useful dictionary learning used extracting features used supervised learning allows learning algorithm assign different weights negative loadings particular atom corresponding positive loading split code single sample length n_atoms constructed using following rule first regular code length n_atoms computed rst n_atoms entries split_code lled positive part regular code vector second half split code lled negative part code vector positive sign therefore split_code nonnegative 
498: examples sparse coding precomputed dictionary generic dictionary learning dictionary learning dictionarylearning matrix factorization problem amounts nding usually overcomplete dictionary perform good sparsely encoding tted data representing data sparse combinations atoms overcomplete dictionary suggested way mammal primary visual cortex works consequently dictionary learning applied image patches shown give good results image processing tasks image completion inpainting denoising well supervised recognition tasks dictionary learning optimization problem solved alternatively updating sparse code solution multiple lasso problems considering dictionary xed updating dictionary best sparse code 
499: arg min subject tovk2 natoms 1.4. unsupervised learning scikitlearn user guide release 0.11 using procedure dictionary transform simply sparse coding step shares implementation dictionary learning objects see sparse coding precomputed dictionary following image shows dictionary learned 4x4 pixel image patches extracted part image lena looks like 
500: chapter user guide scikitlearn user guide release 0.11 examples image denoising using dictionary learning references online dictionary learning sparse coding mairal bach ponce sapiro minibatch dictionary learning minibatchdictionarylearning implements faster less accurate version dictionary learning algo rithm better suited large datasets default minibatchdictionarylearning divides data minibatches optimizes online manner cycling minibatches specied number iterations however moment implement stopping condition estimator also implements partial_t updates dictionary iterating minibatch used online learning data readily available start data memory 
501: independent component analysis ica independent component analysis separates multivariate signal additive subcomponents maximally inde pendent implemented scikitlearn using fast ica algorithm classically used separate mixed signals problem known blind source separation example ica also used yet another non linear decomposition nds components sparsity 1.4. unsupervised learning scikitlearn user guide release 0.11 examples blind source separation using fastica fastica point clouds faces dataset decompositions nonnegative matrix factorization nmf nnmf nmf alternative approach decomposition assumes data components nonnegative nmf plugged instead pca variants cases data matrix contain negative values unlike pca representation vector obtained additive fashion superimposing components without substracting additive models efcient representing images text observed hoyer carefully constrained nmf produce partsbased representation dataset resulting interpretable models following example displays sparse components found nmf images olivetti faces dataset comparison pca eigenfaces 
502: chapter user guide scikitlearn user guide release 0.11 init attribute determines initialization method applied great impact performance method nmf implements method nonnegative double singular value decomposition nndsvd based two svd processes one approximating data matrix approximating positive sections resulting partial svd factors utilizing algebraic property unit rank matrices basic nndsvd algorithm better sparse factorization variants nndsvda zeros set equal mean elements data nndsvdar zeros set random perturbations less mean data divided recommended dense case nmf also initialized random nonnegative matrices passing integer seed randomstate init nmf sparseness enforced setting attribute sparseness data components sparse components lead localized features sparse data leads efcient representation data 
503: examples faces dataset decompositions topics extraction nonnegative matrix factorization 1.4. unsupervised learning scikitlearn user guide release 0.11 references learning parts objects nonnegative matrix factorization lee seung nonnegative matrix factorization sparseness constraints hoyer projected gradient methods nonnegative matrix factorization c.j lin svd based initialization head start nonnegative matrix factorization boutsidis gallopoulos 1.4.5 covariance estimation many statistical problems require point estimation populations covariance matrix seen estimation data set scatter plot shape time estimation done sample whose properties size structure homogeneity large inuence estimations quality sklearn.covariance package aims providing tools affording accurate estimation populations covariance matrix various settings assume observations independent identically distributed i.i.d 
504: empirical covariance covariance matrix data set known well approximated classical maximum likelihood estimator empirical covariance provided number observations large enough compared number features variables describing observations precisely maximum likelihood estimator sample unbiased estimator corresponding population covariance matrix empirical covariance matrix sample computed using empirical_covariance func tion data sample empiricalcovariance.fit method careful depending whether data centered sult different one may want use assume_centered parameter accurately 
505: package tting empiricalcovariance object examples see ledoitwolf empiricalcovariance object data 
506: covariance simple estimation example shrunk covariance basic shrinkage despite unbiased estimator covariance matrix maximum likelihood estimator good esti mator eigenvalues covariance matrix precision matrix obtained inversion accurate sometimes even occurs empirical covariance matrix inverted numerical reasons avoid inversion problem transformation empirical covariance matrix introduced shrinkage consists reducing ratio smallest largest eigenvalue empirical covariance matrix done simply shifting every eigenvalue according given offset equivalent nding l2penalized maximum likelihood estimator covariance matrix reducing highest eigenvalue increasing smallest help convex transformation shrunk latter approach implemented scikitlearn convex transformation userdened shrinkage coefcient directly applied precomputed covari ance shrunk_covariance method also shrunk estimator covariance tted data chapter user guide scikitlearn user guide release 0.11 shrunkcovariance object shrunkcovariance.fit method depending whether data centered result different one may want use assume_centered parameter accurately 
507: examples see ledoitwolf covariance simple estimation example shrunkcovariance object data 
508: ledoitwolf shrinkage paper ledoit wolf propose formula compute optimal shrinkage coefcient minimizes mean squared error estimated real covariance matrix terms frobenius norm ledoitwolf estimator covariance matrix computed sample ledoit_wolf function sklearn.covariance package otherwise obtained tting ledoitwolf object sample ledoit wolf wellconditioned estimator largedimensional covariance matrices jour nal multivariate analysis volume issue february pages 
509: examples see ledoitwolf covariance simple estimation example ledoitwolf object data visualizing performances ledoitwolf estimator terms likelihood 
510: oracle approximating shrinkage assumption data gaussian distributed chen derived formula aimed choosing shrinkage coefcient yields smaller mean squared error one given ledoit wolfs formula resulting estimator known oracle shrinkage approximating estimator covariance 
511: 1.4. unsupervised learning scikitlearn user guide release 0.11 oas estimator covariance matrix computed sample oas function sklearn.covariance package otherwise obtained tting oas object sample mula used implement oas correspond one given article taken matlab program available authors webpage https tbayes.eecs.umich.eduyiluncovestimation chen al. shrinkage algorithms mmse covariance estimation ieee trans sign proc. volume issue october 
512: examples see ledoitwolf covariance simple estimation example oas object data see ledoitwolf oas estimation visualize mean squared error difference ledoitwolf oas estimator covariance 
513: sparse inverse covariance matrix inverse covariance matrix often called precision matrix proportional partial correlation matrix gives partial independence relationship words two features independent conditionally others corresponding coefcient precision matrix zero makes sense estimate sparse precision matrix learning independence relations data estimation covariance matrix better conditioned known covariance selection smallsamples situation n_samples order magnitude n_features smaller sparse inverse covariance estimators tend work better shrunk covariance estimators however opposite situation correlated data numerically unstable addition unlike shrinkage estimators sparse estimators able recover offdiagonal structure higher alpha graphlasso estimator uses penalty enforce sparsity precision matrix parameter sparse precision matrix corresponding graphlassocv object uses crossvalidation automatically set alpha parameter 
514: chapter user guide scikitlearn user guide release 0.11 figure 1.4 comparison maximum likelihood shrinkage sparse estimates covariance precision matrix small samples settings 
515: note structure recovery recovering graphical structure correlations data challenging thing interested recovery keep mind recovery easier correlation matrix covariance matrix standardize observations running graphlasso underlying graph nodes much connections average node algorithm miss connections 
516: number observations large compared number edges underlying graph recover 
517: even favorable recovery conditions alpha parameter chosen crossvalidation e.g using graphlassocv object lead selecting many edges however relevant edges heavier weights irrelevant ones 
518: mathematical formulation following argmink cid0 trsk logdetk cid107 cid107 cid1 precision matrix estimated sample covariance matrix cid107 cid107 sum absolute values offdiagonal coefcients algorithm employed solve problem glasso algorithm friedman biostatistics paper algorithm glasso package 
519: examples sparse inverse covariance estimation example synthetic data showing recovery structure comparing covariance estimators 
520: visualizing stock market structure example real stock market data nding symbols linked 
521: 1.4. unsupervised learning scikitlearn user guide release 0.11 references friedman sparse inverse covariance estimation graphical lasso biostatistics robust covariance estimation real data set often subjects measurement recording errors regular uncommon observations may also appear variety reason every observation uncommon called outlier empirical covari ance estimator shrunk covariance estimators presented sensitive presence outlying observations data therefore one use robust covariance estimators estimate covariance real data sets alternatively robust covariance estimators used perform outlier detection discarddownweight observations according processing data sklearn.covariance package implements robust estimator covariance minimum covariance determinant 
522: minimum covariance determinant minimum covariance determinant estimator robust estimator data sets covariance introduced p.j.rousseuw idea given proportion good observations outliers com pute empirical covariance matrix empirical covariance matrix rescaled compensate performed selection observations consistency step computed minimum covariance determinant estimator one give weights observations according mahalanobis distance leading reweighted estimate covariance matrix data set reweighting step rousseuw van driessen developed fastmcd algorithm order compute minimum covariance determinant algorithm used scikitlearn tting mcd object data fastmcd algorithm also computes robust estimate data set location time raw estimates accessed raw_location_ raw_covariance_ attributes mincovdet robust covariance estimator object rousseeuw least median squares regression 
523: 10. stat ass 
524: fast algorithm minimum covariance determinant estimator american statistical associa tion american society quality technometrics 
525: examples see robust empirical covariance estimate example mincovdet object data see estimate remains accurate despite presence outliers 
526: see robust covariance estimation mahalanobis distances relevance visualize difference tween empiricalcovariance mincovdet covariance estimators terms mahalanobis dis tance get better estimate precision matrix 
527: chapter user guide inuence outliers location covariance estimates separating inliers outliers using mahalonis distance scikitlearn user guide release 0.11 1.4.6 novelty outlier detection many applications require able decide whether new observation belongs distribution exiting observations inlier considered different outlier often ability used clean real data sets two important distinction must made novelty detection training data polluted outliers interested detecting anoma lies new observations 
528: outlier detection training data contains outliers need central mode training data ignoring deviant observations 
529: scikitlearn project provides set machine learning tools used novelty outliers detection strategy implemented objects learning unsupervised way data estimor.fit x_train new observations sorted inliers outliers predict method estimator.predict x_test inliers labeled outliers labeled 
530: novelty detection consider data set observations distribution described features consider add one observation data set new observation different others doubt regular i.e come distribution contrary similar distinguish original observations question adressed novelty detection tools methods general learn rough close frontier delimiting contour initial observations distribution plotted embedding pdimensional space observations lay within frontierdelimited subspace considered coming population initial observations otherwise lay outside frontier say abnormal given condence assessment oneclass svm introduced purpose implemented support vector machines module svm.oneclasssvm object requires choice kernel scalar parameter dene frontier rbf kernel usually chosen although exist exact formula algorithm set bandwith parameter default scikitlearn implementation parameter also known margin oneclass svm corresponds probability nding new regular observation outside frontier 
531: examples see oneclass svm nonlinear kernel rbf vizualizing frontier learned around data svm.oneclasssvm object 
532: 1.4. unsupervised learning scikitlearn user guide release 0.11 outlier detection outlier detection similar novelty detection sense goal separate core regular observations polutting ones called outliers yet case outlier detection dont clean data set representing population regular observations used train tool 
533: fitting elliptic envelop one common way performing outlier detection assume regular data come known distribution e.g data gaussian distributed assumption generaly try dene shape data dene outlying observations observations stand far enough shape scikitlearn provides object covariance.ellipticenvelope robust covariance estimate data thus ellipse central data points ignoring points outside central mode instance assuming inlier data gaussian distributed estimate inlier location covariance robust way i.e whithout inuenced outliers mahalanobis distances obtained estimate used derive measure outlyingness strategy illustrated 
534: examples see robust covariance estimation mahalanobis distances relevance illustration dif ference using standard covariance.empiricalcovariance robust estimate covariance.mincovdet location covariance assess degree outlyingness servation 
535: references chapter user guide scikitlearn user guide release 0.11 oneclass svm versus elliptic envelop strictlyspeaking oneclass svm outlierdetection method noveltydetection method training set contaminated outliers may said outlier detection highdimension without assumptions distribution inlying data challenging oneclass svm gives useful results situations examples illustrate performance covariance.ellipticenvelope degrades data less less unimodal svm.oneclasssvm works better data multiple modes 
536: table 1.1 comparing oneclass svm approach elliptic envelopp inlier mode wellcentered elliptic svm.oneclasssvm able benet rotational symmetry inlier population addition bit outliers present training set opposite decision rule based tting covariance.ellipticenvelope learns ellipse well inlier distribution 
537: inlier distribution becomes bimodal covariance.ellipticenvelope well inliers however see svm.oneclasssvm tends overt model inliers interprets region chance outliers clustered inliers 
538: inlier distribution strongly non gaussian svm.oneclasssvm able recover reasonable approximation whereas covariance.ellipticenvelope completely fails 
539: 1.4. unsupervised learning scikitlearn user guide release 0.11 examples see outlier detection several methods 
540: svm.oneclasssvm tuned perform like outlier detection method covariancebased outlier detection covariance.mincovdet 
541: comparison 1.4.7 hidden markov models sklearn.hmm implements algorithms hidden markov model hmm hmm generative probabilistic model sequence observable variable generated sequence internal hidden state hidden states observed directly transition hidden states aussumed rst order markov chain specied start probability vector transition probability matrix emission probability observable distribution parameters conditioned current hidden state index e.g multinomial gaussian thus hmm completely determined three fundamental problems hmm given model parameters observed data estimate optimal sequence hidden states given model parameters observed data calculate likelihood data given observed data estimate model parameters 
542: rst second problem solved dynamic programing algorithms known viterbi algorithm forwardbackward algorithm respectively last one solved expectationmaximization iterative algorithm known baumwelch algorithm see ref listed detailed information 
543: references rabiner89 tutorial hidden markov models selected applications speech recognition lawrence rabiner using hmm classes module include multinomalhmm gaussianhmm gmmhmm implement hmm emis sion probability multimomial distribution gaussian distribution mixture gaussian distributions 
544: building hmm generating samples build hmm instance passing parameters described constructor generate samples hmm calling sample import numpy sklearn import hmm startprob np.array 0.6 0.3 0.1 transmat np.array 0.7 0.2 0.1 0.3 0.5 0.2 0.3 0.3 0.4 means np.array 0.0 0.0 3.0 3.0 5.0 10.0 covars np.tile np.identity model hmm.gaussianhmm full startprob transmat model.means_ means chapter user guide model.covars_ covars model.sample scikitlearn user guide release 0.11 examples demonstration sampling hmm training hmm parameters infering hidden states train hmm calling method input list sequence observed value note since emalgorithm gradient based optimization method generally stuck local optimal try run various initialization select highest scored model score model calculated score method infered optimal hidden states obtained calling predict method predict method specied decoder algorithm currently viterbi algorithm viterbi maximum posteriori estimation map supported time input single sequence observed values model2 hmm.gaussianhmm full model2.fit gaussianhmm algorithmviterbi covariance_typefull covars_prior0.01 covars_weight1 means_priornone means_weight0 n_components3 random_statenone startprobnone startprob_prior1.0 transmatnone transmat_prior1.0 model.predict examples gaussian hmm stock data 1.4. unsupervised learning scikitlearn user guide release 0.11 implementing hmms emission probabilities want implement emission probability e.g poisson make hmm class inheriting _basehmm override necessary methods __init__ _compute_log_likelihood _set _get addiitional parameters _initialize_sufcient_statistics _accumulate_sufcient_statistics _do_mstep 
545: 1.5 model selection 1.5.1 crossvalidation evaluating estimator performance learning parameters prediction function testing data methodological mistake model would repeat labels samples seen would perfect score would fail predict anything useful yetunseen data avoid overtting dene two different sets training set x_train y_train used learning parameters predictive model testing set x_test y_test used evaluating tted predictive model scikitlearn random split quickly computed train_test_split helper function let load iris data set linear support vector machine model import numpy sklearn import cross_validation sklearn import datasets sklearn import svm iris datasets.load_iris iris.data.shape iris.target.shape quickly sample training set holding data testing evaluating classier x_train x_test y_train y_test cross_validation.train_test_split 
546: iris.data iris.target test_size0.4 random_state0 x_train.shape y_train.shape x_test.shape y_test.shape clf svm.svc kernellinear .fit x_train y_train clf.score x_test y_test 0.96.. 
547: however dening two sets drastically reduce number samples used learning model results depend particular random choice pair train test sets solution split whole data several consecutive times different train set test set return averaged value prediction scores obtained different sets procedure called crossvalidation approach computationally expensive waste much data case xing arbitrary test set major advantage problem inverse inference number samples small 
548: chapter user guide scikitlearn user guide release 0.11 computing crossvalidated metrics simplest way use perform crossvalidation call cross_val_score helper function estimator dataset following example demonstrates estimate accuracy linear kernel support vector machine iris dataset splitting data tting model computing score consecutive times different splits time clf svm.svc kernellinear scores cross_validation.cross_val_score ... ... scores array 
549: clf iris.data iris.target cv5 0.9 ... 0.96 ... 0.96 ... ... 
550: mean score standard deviation score estimate hence given print accuracy 0.2f 0.2f scores.mean scores.std accuracy 0.97 0.02 default score computed iteration score method estimator possible change passing custom scoring function e.g metrics module sklearn import metrics cross_validation.cross_val_score clf iris.data iris.target cv5 ... ... array 
551: score_funcmetrics.f1_score 0.89 ... 0.96 ... 0.96 ... ... 
552: case iris dataset samples balanced across target classes hence accuracy f1score almost equal argument integer cross_val_score uses kfold stratifiedkfold strategies default depending absence presence target array also possible use othe cross validation strategies passing cross validation iterator instead instance n_samples iris.data.shape cross_validation.shufflesplit n_samples n_iterations3 
553: test_size0.3 random_state0 cross_validation.cross_val_score clf iris.data iris.target cvcv ... array 0.97 ... 0.97 ... 
554: available cross validation iterators introduced following 
555: examples receiver operating characteristic roc cross validation recursive feature elimination crossvalidation parameter estimation using grid search nested crossvalidation sample pipeline text feature extraction evaluation 1.5. model selection scikitlearn user guide release 0.11 cross validation iterators following sections list utilities generate boolean masks indices used generate dataset splits according different cross validation strategies 
556: boolean mask integer indices cross validators support generating boolean masks integer indices select samples given fold data matrix sparse integer indices work expected integer indexing hence default behavior since version 0.10 explicitly pass indicesfalse constructor object supported use boolean mask method instead 
557: kfold kfold divides samples math groups samples called folds equivalent leave one strategy equal sizes possible prediction function learned using folds fold left used test example 2fold import numpy sklearn.cross_validation import kfold np.array np.array kfold len indicesfalse print sklearn.cross_validation.kfold train test ... false false true print train test true true false false false false true true true false false true true fold constituted two arrays rst one related training set second one test set thus one create trainingtest sets using x_train x_test y_train y_test train test train test scipy.sparse matrices train test need integer indices obtained setting parameter indices true creating crossvalidation procedure np.array np.array kfold len indicestrue train test ... print train test chapter user guide scikitlearn user guide release 0.11 stratied kfold stratifiedkfold variation kfold returns stratied folds i.e creates folds preserving percentage target class complete set example stratied 2fold sklearn.cross_validation import stratifiedkfold ... ... ... ... ... ... skf stratifiedkfold print skf sklearn.cross_validation.stratifiedkfold labels train test skf ... print train test leaveoneout loo leaveoneout loo simple crossvalidation learning set created taking samples except one test set sample left thus samples different learning sets different tests set crossvalidation procedure waste much data one sample removed learning set sklearn.cross_validation import leaveoneout np.array np.array loo leaveoneout len print loo sklearn.cross_validation.leaveoneout print train test train test loo ... leavepout lpo leavepout similar leaveoneout creates possible trainingtest sets removing samples complete set example leave2out sklearn.cross_validation import leavepout 1.5. model selection scikitlearn user guide release 0.11 lpo leavepout len print lpo sklearn.cross_validation.leavepout print train test train test lpo ... leaveonelabelout lolo leaveonelabelout lolo crossvalidation scheme holds samples according thirdparty provided label label information used encode arbitrary domain specic stratications samples integers training set thus constituted samples except ones related specic label example cases multiple experiments lolo used create crossvalidation based different experiments create training set using samples experiments except one sklearn.cross_validation import leaveonelabelout labels lolo leaveonelabelout labels print lolo sklearn.cross_validation.leaveonelabelout labels train test lolo ... print train test another common application use time information instance labels could year collection samples thus allow crossvalidation timebased splits 
558: leaveplabelout leaveplabelout similar leaveonelabelout removes samples related labels trainingtest set example leave2label sklearn.cross_validation import leaveplabelout labels lplo leaveplabelout labels chapter user guide scikitlearn user guide release 0.11 print lplo sklearn.cross_validation.leaveplabelout labels print train test train test lplo ... random permutations crossvalidation a.k.a shufe split shufflesplit shufflesplit iterator generate user dened number independent train test dataset splits samples rst shufed splitted pair train test sets possible control randomness reproducibility results explicitly seeding random_state pseudo random number generator usage example random_state0 cross_validation.shufflesplit n_iterations3 test_size0.25 ... len print shufflesplit n_iterations3 test_size0.25 indicestrue ... print train_index test_index train_index test_index ... ... shufflesplit thus good alternative kfold cross validation allows ner control number iterations proportion samples side train test split 
559: see also stratifiedshufflesplit variation shufesplit returns stratied splits i.e creates splits preserving percentage target class complete set 
560: bootstrapping crossvalidation bootstrap bootstrapping general statistics technique iterates computation estimator resampled dataset bootstrap iterator generate user dened number independent train test dataset splits samples drawn replacement side split furthermore possible control size train test subset make union smaller total dataset large 
561: note contrary crossvalidation strategies bootstrapping allow samples occur several times splits 
562: 1.5. model selection scikitlearn user guide release 0.11 cross_validation.bootstrap random_state0 len print bootstrap n_bootstraps3 train_size5 test_size4 random_state0 print train_index test_index train_index test_index ... ... cross validation model selection cross validation iterators also used directly perform model selection using grid search optimal hyperparameters model topic next section grid search setting estimator parameters 
563: 1.5.2 grid search setting estimator parameters grid search used optimize parameters model e.g kernel gamma support vector classier alpha lasso etc using internal crossvalidation evaluating estimator performance scheme 
564: gridsearchcv main class implementing hyperparameters grid search scikitlearn grid_search.gridsearchcv class passed base model instance example sklearn.svm.svc along grid potential hyperparameter values gamma 0.001 0.0001 kernel rbf kernel linear grid_search.gridsearchcv instance implements usual estimator api tting dataset possible combinations hyperparameter values evaluated best combinations retained 
565: model selection development evaluation model selection gridsearchcv seen way use labeled data train hyper parameters grid evaluating resulting model important heldout samples seen grid search process recommended split data development set fed gridsearchcv instance evaluation set compute performance metrics done using cross_validation.train_test_split utility function 
566: examples see parameter estimation using grid search nested crossvalidation example grid search com putation digits dataset 
567: chapter user guide scikitlearn user guide release 0.11 see sample pipeline text feature extraction evaluation example grid search coupling parame ters text documents feature extractor ngram count vectorizer tfidf transformer classier linear svm trained sgd either elastic net penalty using pipeline.pipeline instance 
568: note computations run parallel supports using keyword n_jobs1 see function signature details 
569: alternatives brute force grid search model specic crossvalidation models data range value parameter almost efciently tting estimator single value parameter feature leveraged perform efcient crossvalidation used model selection parameter common parameter amenable strategy parameter encoding strength regularizer case say compute regularization path estimator list models linear_model.ridgecv alphas ... ridge regression builtin crossvalidation linear_model.ridgeclassifiercv alphas ... ridge classier builtin crossvalidation crossvalidated least angle regression model linear_model.larscv t_intercept ... linear_model.lassolarscv t_intercept ... crossvalidated lasso using lars algorithm lasso linear model iterative tting along regularization path linear_model.lassocv eps n_alphas ... linear_model.elasticnetcv rho eps ... elastic net model iterative tting along regularization path sklearn.linear_model.ridgecv class sklearn.linear_model.ridgecv alphasarray 0.1 
570: t_intercepttrue malizefalse score_funcnone loss_funcnone cvnone gcv_modenone ridge regression builtin crossvalidation default performs generalized crossvalidation form efcient leaveoneout cross validation 
571: parameters alphas numpy array shape n_alpha array alpha values try small positive values alpha improve conditioning problem reduce variance estimates alpha corresponds linear models logisticregression linearsvc 
572: t_intercept boolean whether calculate intercept model set false intercept used calculations e.g data expected already centered 
573: normalize boolean optional true regressors normalized score_func callable optional function takes arguments compares order evaluate performance prediction big good none passed score estimator maximized 1.5. model selection scikitlearn user guide release 0.11 loss_func callable optional function takes arguments compares order evaluate performance prediction small good none passed score estimator maximized crossvalidation generator optional none generalized crossvalidation efcient leaveoneout used 
574: see also ridgeridge regression ridgeclassifierridge classier ridgecvridge regression builtin cross validation attributes coef_ gcv_mode methods shape n_features array n_classes n_features none auto svd eigen tional weight vector 
575: flag indicating strategy use performing generalized crossvalidation options auto use svd n_samples n_features otherwise use eigen svd force computation via singular value decomposition eigen force computation via eigendecomposition auto mode default intended pick cheaper tion two depending upon shape training data 
576: decision_function decision function linear model fit sample_weight get_params deep predict score set_params params fit ridge regression model get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
577: __init__ alphasarray 0.1 
578: t_intercepttrue normalizefalse score_funcnone loss_funcnone cvnone gcv_modenone decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
579: fit sample_weight1.0 fit ridge regression model chapter user guide scikitlearn user guide release 0.11 parameters arraylike shape n_samples n_features training data arraylike shape n_samples n_samples n_responses target values sample_weight oat arraylike shape n_samples sample weight returns self returns self 
580: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
581: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
582: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
583: parameters arraylike shape n_samples n_features training set 
584: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.linear_model.ridgeclassiercv class sklearn.linear_model.ridgeclassifiercv alphasarray 0.1 t_intercepttrue normalizefalse score_funcnone loss_funcnone cvnone class_weightnone 
585: ridge classier builtin crossvalidation default performs generalized crossvalidation form efcient leaveoneout cross validation currently n_features n_samples case handled efciently 
586: 1.5. model selection scikitlearn user guide release 0.11 parameters alphas numpy array shape n_alpha array alpha values try small positive values alpha improve conditioning problem reduce variance estimates alpha corresponds linear models logisticregression linearsvc 
587: t_intercept boolean whether calculate intercept model set false intercept used calculations e.g data expected already centered 
588: normalize boolean optional true regressors normalized score_func callable optional function takes arguments compares order evaluate performance prediction big good none passed score estimator maximized loss_func callable optional function takes arguments compares order evaluate performance prediction small good none passed score estimator maximized crossvalidation generator optional none generalized crossvalidation efcient leaveoneout used 
589: class_weight dict optional weights associated classes form class_label weight given classes supposed weight one 
590: see also ridgeridge regression ridgeclassifierridge classier ridgecvridge regression builtin cross validation notes multiclass classication n_class classiers trained oneversusall approach concretely implemented taking advantage multivariate response support ridge 
591: methods decision_function fit sample_weight class_weight get_params deep predict score set_params params fit ridge classier get parameters estimator predict target values according tted model returns coefcient determination prediction set parameters estimator 
592: __init__ alphasarray 0.1 
593: t_intercepttrue normalizefalse score_funcnone loss_funcnone cvnone class_weightnone chapter user guide scikitlearn user guide release 0.11 fit sample_weight1.0 class_weightnone fit ridge classier 
594: parameters arraylike shape n_samples n_features training vectors n_samples number samples n_features num ber features 
595: arraylike shape n_samples target values 
596: sample_weight oat numpy array shape n_samples sample weight class_weight dict optional weights associated classes form class_label weight given classes supposed weight one 
597: returns self object returns self get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
598: predict predict target values according tted model 
599: parameters arraylike shape n_samples n_features returns array shape n_samples score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
600: parameters arraylike shape n_samples n_features training set 
601: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self 1.5. model selection scikitlearn user guide release 0.11 sklearn.linear_model.larscv class sklearn.linear_model.larscv t_intercepttrue verbosefalse max_iter500 normal izetrue precomputeauto cvnone max_n_alphas1000 n_jobs1 eps2.2204460492503131e16 copy_xtrue crossvalidated least angle regression model parameters t_intercept boolean whether calculate intercept model set false intercept used calculations e.g data expected already centered 
602: verbose boolean integer optional sets verbosity amount normalize boolean optional true regressors normalized copy_x boolean optional default true true copied else may overwritten 
603: precompute true false auto arraylike whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
604: max_iter integer optional maximum number iterations perform 
605: crossvalidation generator optional see sklearn.cross_validation module none passed default 5fold strategy max_n_alphas integer optional maximum number points path used compute residuals cross validation n_jobs integer optional number cpus use cross validation use cpus eps oat optional machineprecision regularization computation cholesky diagonal fac tors increase illconditioned systems 
606: see also lars_path lassolars lassolarscv attributes coef_ intercept_ coef_path array shape n_features n_alpha array shape n_features oat parameter vector fomulation formula independent term decision function varying values coefcients along path chapter user guide scikitlearn user guide release 0.11 methods decision_function decision function linear model fit get_params deep predict score set_params params fit model using training data get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
607: __init__ t_intercepttrue verbosefalse max_iter500 normalizetrue precomputeauto cvnone max_n_alphas1000 n_jobs1 eps2.2204460492503131e16 copy_xtrue decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
608: fit fit model using training data 
609: parameters arraylike shape n_samples n_features training data 
610: arraylike shape n_samples target values returns self object returns instance self 
611: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
612: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
613: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
614: parameters arraylike shape n_samples n_features training set 
615: 1.5. model selection scikitlearn user guide release 0.11 arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.linear_model.lassolarscv class sklearn.linear_model.lassolarscv t_intercepttrue verbosefalse max_iter500 precomputeauto n_jobs1 crossvalidated lasso using lars algorithm optimization objective lasso normalizetrue cvnone eps2.2204460492503131e16 copy_xtrue max_n_alphas1000 n_samples xw2_2 alpha w_1 parameters t_intercept boolean whether calculate intercept model set false intercept used calculations e.g data expected already centered 
616: verbose boolean integer optional sets verbosity amount normalize boolean optional true regressors normalized precompute true false auto arraylike whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
617: max_iter integer optional maximum number iterations perform 
618: crossvalidation generator optional see sklearn.cross_validation module none passed default 5fold strategy max_n_alphas integer optional maximum number points path used compute residuals cross validation n_jobs integer optional number cpus use cross validation use cpus eps oat optional machineprecision regularization computation cholesky diagonal fac tors increase illconditioned systems 
619: copy_x boolean optional default true chapter user guide scikitlearn user guide release 0.11 true copied else may overwritten 
620: see also lars_path lassolars larscv lassocv notes object solves problem lassocv object however unlike lassocv relevent alphas values general property stable however fragile heavily multicollinear datasets efcient lassocv small number features selected compared total number instance samples compared number features 
621: attributes coef_ intercept_ coef_path array shape n_features n_alpha alphas_ array shape n_alpha cv_alphas array shape n_cv_alphas cv_mse_path_ array shape n_folds n_cv_alphas methods array shape n_features oat parameter vector fomulation formula independent term decision function varying values coefcients along path different values alpha along path values alpha along path different folds mean square error leftout fold along path alpha values given cv_alphas decision_function decision function linear model fit get_params deep predict score set_params params fit model using training data get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
622: __init__ t_intercepttrue verbosefalse max_iter500 normalizetrue precomputeauto cvnone max_n_alphas1000 n_jobs1 eps2.2204460492503131e16 copy_xtrue decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
623: fit fit model using training data 
624: 1.5. model selection scikitlearn user guide release 0.11 parameters arraylike shape n_samples n_features training data 
625: arraylike shape n_samples target values returns self object returns instance self 
626: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
627: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
628: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
629: parameters arraylike shape n_samples n_features training set 
630: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.linear_model.lassocv class sklearn.linear_model.lassocv eps0.001 n_alphas100 alphasnone t_intercepttrue max_iter1000 precomputeauto normalizefalse tol0.0001 copy_xtrue cvnone verbosefalse lasso linear model iterative tting along regularization path best model selected crossvalidation optimization objective lasso n_samples xw2_2 alpha w_1 chapter user guide scikitlearn user guide release 0.11 parameters eps oat optional length path eps1e3 means alpha_min alpha_max 1e3 
631: n_alphas int optional number alphas along regularization path alphas numpy array optional list alphas compute models none alphas set automatically precompute true false auto arraylike whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
632: max_iter int optional maximum number iterations tol oat optional tolerance optimization updates smaller tol optimization code checks dual gap optimality continues smaller tol 
633: integer crossvalidation generator optional integer passed number fold default specic crossvalidation jects passed see sklearn.cross_validation module list possible objects verbose bool integer amount verbosity see also lars_path lasso_path lassolars lasso lassolarscv notes see exampleslinear_modellasso_path_with_crossvalidation.py example avoid unnecessary memory duplication argument method directly passed fortran contiguous numpy array 
634: attributes alpha_ oat coef_ intercept_ mse_path_ array shape n_alphas n_folds methods array shape n_features oat amount penalization choosen cross validation parameter vector fomulation formula independent term decision function mean square error test set fold varying alpha 1.5. model selection scikitlearn user guide release 0.11 decision_function fit get_params deep path eps n_alphas alphas ... compute lasso path coordinate descent predict score set_params params predict using linear model returns coefcient determination prediction set parameters estimator 
635: decision function linear model fit linear model coordinate descent along decreasing alphas get parameters estimator __init__ eps0.001 n_alphas100 alphasnone t_intercepttrue normalizefalse precom puteauto max_iter1000 tol0.0001 copy_xtrue cvnone verbosefalse decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
636: fit fit linear model coordinate descent along decreasing alphas using crossvalidation parameters numpy array shape n_samples n_features training data pass directly fortran contiguous data avoid unnecessary memory duplication numpy array shape n_samples target values get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators precomputeauto xynone eps0.001 t_intercepttrue normalizefalse copy_xtrue verbosefalse params static path n_alphas100 alphasnone compute lasso path coordinate descent optimization objective lasso n_samples xw2_2 alpha w_1 parameters numpy array shape n_samples n_features training data pass directly fortran contiguous data avoid unnecessary memory duplication numpy array shape n_samples target values eps oat optional length path eps1e3 means alpha_min alpha_max 1e3 n_alphas int optional chapter user guide scikitlearn user guide release 0.11 number alphas along regularization path alphas numpy array optional list alphas compute models none alphas set automatically precompute true false auto arraylike whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
637: arraylike optional np.dot x.t precomputed useful gram matrix precomputed 
638: t_intercept bool fit intercept normalize boolean optional true regressors normalized copy_x boolean optional default true true copied else may overwritten 
639: verbose bool integer amount verbosity params kwargs keyword arguments passed lasso objects returns models list models along regularization path see also lars_path sklearn.decomposition.sparse_encode lasso lassolars lassocv lassolarscv notes see exampleslinear_modelplot_lasso_coordinate_descent_path.py example avoid unnecessary memory duplication argument method directly passed fortran contiguous numpy array 
640: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
641: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
642: 1.5. model selection scikitlearn user guide release 0.11 parameters arraylike shape n_samples n_features training set 
643: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.linear_model.elasticnetcv class sklearn.linear_model.elasticnetcv rho0.5 eps0.001 n_alphas100 alphasnone precom t_intercepttrue puteauto max_iter1000 tol0.0001 cvnone copy_xtrue verbose0 n_jobs1 normalizefalse elastic net model iterative tting along regularization path best model selected crossvalidation 
644: parameters rho oat optional oat passed elasticnet scaling penalties rho penalty penalty rho penalty rho penalty combination parameter list case different values tested crossvalidation one giving best prediction score used note good choice list values rho often put values close i.e lasso less close i.e ridge .95 .99 eps oat optional length path eps1e3 means alpha_min alpha_max 1e3 
645: n_alphas int optional number alphas along regularization path alphas numpy array optional list alphas compute models none alphas set automatically precompute true false auto arraylike whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
646: max_iter int optional maximum number iterations tol oat optional tolerance optimization updates smaller tol optimization code checks dual gap optimality continues smaller tol 
647: integer crossvalidation generator optional integer passed number fold default specic crossvalidation jects passed see sklearn.cross_validation module list possible objects chapter user guide scikitlearn user guide release 0.11 verbose bool integer amount verbosity n_jobs integer optional number cpus use cross validation use cpus note used multiple values rho given 
648: see also enet_path elasticnet notes see exampleslinear_modellasso_path_with_crossvalidation.py example avoid unnecessary memory duplication argument method directly passed fortran contiguous numpy array parameter rho corresponds alpha glmnet package alpha corresponds lambda param eter glmnet specically optimization objective n_samples xw2_2 alpha rho w_1 0.5 alpha rho w2_2 interested controlling penalty separately keep mind equivalent alpha rho attributes alpha_ oat rho_ oat coef_ intercept_ mse_path_ array shape n_rho n_alpha n_folds methods array shape n_features oat amount penalization choosen cross validation compromise penalization choosen cross validation parameter vector fomulation formula independent term decision function mean square error test set fold varying rho alpha decision_function fit get_params deep path rho eps n_alphas alphas ... compute elasticnet path coordinate descent predict score decision function linear model fit linear model coordinate descent along decreasing alphas get parameters estimator predict using linear model returns coefcient determination prediction 
649: 1.5. model selection continued next page scikitlearn user guide release 0.11 set_params params table 1.8 continued previous page set parameters estimator 
650: __init__ rho0.5 eps0.001 n_alphas100 alphasnone t_intercepttrue normalizefalse tol0.0001 cvnone copy_xtrue verbose0 precomputeauto max_iter1000 n_jobs1 decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
651: fit fit linear model coordinate descent along decreasing alphas using crossvalidation parameters numpy array shape n_samples n_features training data pass directly fortran contiguous data avoid unnecessary memory duplication numpy array shape n_samples target values get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
652: static path rho0.5 eps0.001 n_alphas100 alphasnone precomputeauto xynone t_intercepttrue normalizefalse copy_xtrue verbosefalse params compute elasticnet path coordinate descent elastic net optimization function n_samples xw2_2 alpha rho w_1 0.5 alpha rho w2_2 parameters numpy array shape n_samples n_features training data pass directly fortran contiguous data avoid unnecessary memory duplication numpy array shape n_samples target values rho oat optional oat passed elasticnet scaling penalties rho1 corresponds lasso eps oat length path eps1e3 means alpha_min alpha_max 1e3 n_alphas int optional chapter user guide scikitlearn user guide release 0.11 number alphas along regularization path alphas numpy array optional list alphas compute models none alphas set automatically precompute true false auto arraylike whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
653: arraylike optional np.dot x.t precomputed useful gram matrix precomputed 
654: t_intercept bool fit intercept normalize boolean optional true regressors normalized copy_x boolean optional default true true copied else may overwritten 
655: verbose bool integer amount verbosity params kwargs keyword arguments passed lasso objects returns models list models along regularization path see also elasticnet elasticnetcv notes see examplesplot_lasso_coordinate_descent_path.py example 
656: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
657: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
658: parameters arraylike shape n_samples n_features training set 
659: 1.5. model selection scikitlearn user guide release 0.11 arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self information criterion models offer informationtheoretic closedform formula optimal estimate regularization parameter computing single regularization path instead several using crossvalidation list models benetting aikike information criterion aic bayesian information crite rion bic automated model selection linear_model.lassolarsic criterion ... lasso model lars using bic aic model selection sklearn.linear_model.lassolarsic class sklearn.linear_model.lassolarsic criterionaic lasso model lars using bic aic model selection optimization objective lasso verbosefalse normalizetrue precomputeauto max_iter500 eps2.2204460492503131e16 copy_xtrue t_intercepttrue n_samples xw2_2 alpha w_1 aic akaike information criterion bic bayes information criterion criteria useful select value regularization parameter making tradeoff goodness complexity model good model explain well data simple 
660: parameters criterion bic aic type criterion use 
661: t_intercept boolean whether calculate intercept model set false intercept used calculations e.g data expected already centered 
662: verbose boolean integer optional sets verbosity amount normalize boolean optional true regressors normalized copy_x boolean optional default true true copied else may overwritten 
663: precompute true false auto arraylike whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
664: chapter user guide scikitlearn user guide release 0.11 max_iter integer optional maximum number iterations perform used early stopping 
665: eps oat optional machineprecision regularization computation cholesky diagonal fac tors increase illconditioned systems unlike tol parameter iterative optimizationbased algorithms parameter control tolerance optimization 
666: see also lars_path lassolars lassolarscv notes estimation number degrees freedom given degrees freedom lasso hui zou trevor hastie robert tibshirani ann statist volume number 21732192. http en.wikipedia.orgwikiakaike_information_criterion http en.wikipedia.orgwikibayesian_information_criterion examples sklearn import linear_model clf linear_model.lassolarsic criterionbic clf.fit 1.1111 1.1111 ... lassolarsic copy_xtrue criterionbic eps ... fit_intercepttrue max_iter500 normalizetrue precomputeauto verbosefalse print clf.coef_ 
667: 1.11 ... attributes coef_ intercept_ alpha_ array shape n_features oat oat parameter vector fomulation formula independent term decision function alpha parameter chosen information criterion methods decision_function decision function linear model fit copy_x get_params deep predict score set_params params fit model using training data get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
668: 1.5. model selection scikitlearn user guide release 0.11 __init__ criterionaic t_intercepttrue verbosefalse normalizetrue precomputeauto max_iter500 eps2.2204460492503131e16 copy_xtrue decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
669: fit copy_xtrue fit model using training data 
670: parameters arraylike shape n_samples n_features training data 
671: arraylike shape n_samples target values returns self object returns instance self 
672: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
673: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
674: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
675: parameters arraylike shape n_samples n_features training set 
676: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self chapter user guide scikitlearn user guide release 0.11 bag estimates using ensemble methods base upon bagging i.e generating new training sets using sampling replacement part training set remains unused classier ensemble different part training set left left portion used estimate generalization error without rely separate validation set estimate comes free addictional data needed used model selection currently implemented following classes ensemble.randomforestclassifier ... ensemble.randomforestregressor ... ensemble.extratreesclassifier ... ensemble.extratreesregressor n_estimators ... ensemble.gradientboostingclassifier loss ... gradient boosting classication ensemble.gradientboostingregressor loss ... random forest classier random forest regressor extratrees classier extratrees regressor 
677: gradient boosting regression 
678: sklearn.ensemble.randomforestclassier class sklearn.ensemble.randomforestclassifier n_estimators10 criteriongini max_depthnone min_samples_split1 min_samples_leaf1 min_density0.1 max_featuresauto bootstraptrue com pute_importancesfalse oob_scorefalse n_jobs1 random_statenone verbose0 random forest classier random forest meta estimator number classical decision trees various subsamples dataset use averaging improve predictive accuracy control overtting 
679: parameters n_estimators integer optional default10 number trees forest 
680: criterion string optional defaultgini function measure quality split supported criteria gini gini impurity entropy information gain note parameter treespecic 
681: max_depth integer none optional defaultnone none nodes expanded leaves maximum depth tree pure leaves contain less min_samples_split samples note parameter treespecic 
682: min_samples_split integer optional default1 minimum number samples required split internal node note parame ter treespecic 
683: min_samples_leaf integer optional default1 minimum number samples newly created leaves split discarded split one leaves would contain less min_samples_leaf samples note parameter treespecic 
684: min_density oat optional default0.1 parameter controls tradeoff optimization heuristic controls minimum density sample_mask i.e fraction samples mask density falls 1.5. model selection scikitlearn user guide release 0.11 threshold mask recomputed input data packed results min_density equals one partitions always represented data copying copies original data otherwise partitions represented bit masks aka sample masks note parameter treespecic 
685: max_features int string none optional defaultauto number features consider looking best split max_featuressqrt n_features classication tasks auto max_featuresn_features regression problems sqrt max_featuressqrt n_features log2 max_featureslog2 n_features none max_featuresn_features 
686: note parameter treespecic 
687: bootstrap boolean optional defaulttrue whether bootstrap samples used building trees 
688: compute_importances boolean optional defaulttrue whether computed feature_importances_ attribute calling 
689: importances feature stored oob_score bool whether use outofbag samples estimate generalization error 
690: n_jobs integer optional default1 number jobs run parallel number jobs set number cores 
691: random_state int randomstate instance none optional defaultnone int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used np.random 
692: verbose int optional default0 controlls verbosity tree building process 
693: see also decisiontreeclassifier extratreesclassifier references r59 chapter user guide attributes fea ture_importances_ oob_score_ array shape n_features oat oob_decision_function_array shape n_samples n_classes methods scikitlearn user guide release 0.11 feature importances higher important feature score training dataset obtained using outofbag estimate decision function computed outofbag estimate training set 
694: fit fit_transform get_params deep predict predict_log_proba predict_proba score set_params params transform threshold reduce important features 
695: build forest trees training set fit data transform get parameters estimator predict class predict class logprobabilities predict class probabilities returns mean accuracy given test data labels set parameters estimator 
696: __init__ n_estimators10 criteriongini max_depthnone min_samples_leaf1 min_density0.1 max_featuresauto bootstraptrue pute_importancesfalse oob_scorefalse n_jobs1 random_statenone verbose0 min_samples_split1 com fit build forest trees training set 
697: parameters arraylike shape n_samples n_features training input samples 
698: arraylike shape n_samples target values integers correspond classes classication real numbers regression 
699: returns self object returns self 
700: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
701: parameters numpy array shape n_samples n_features training set 
702: numpy array shape n_samples target values 
703: returns x_new numpy array shape n_samples n_features_new transformed array 
704: 1.5. model selection scikitlearn user guide release 0.11 notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
705: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
706: predict predict class predicted class input sample computed majority prediction trees forest 
707: parameters arraylike shape n_samples n_features input samples 
708: returns array shape n_samples predicted classes 
709: predict_log_proba predict class logprobabilities predicted class logprobabilities input sample computed mean predicted class log probabilities trees forest 
710: parameters arraylike shape n_samples n_features input samples 
711: returns array shape n_samples class logprobabilities input samples classes ordered arithmetical order predict_proba predict class probabilities predicted class probabilities input sample computed mean predicted class probabilities trees forest 
712: parameters arraylike shape n_samples n_features input samples 
713: returns array shape n_samples class probabilities input samples classes ordered arithmetical order 
714: score returns mean accuracy given test data labels 
715: parameters arraylike shape n_samples n_features training set 
716: arraylike shape n_samples labels 
717: returns oat chapter user guide scikitlearn user guide release 0.11 set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform thresholdnone reduce important features 
718: parameters array scipy sparse matrix shape n_samples n_features input samples 
719: threshold string oat none optional defaultnone threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor e.g. 1.25mean may also used none available object attribute threshold used otherwise mean used default 
720: returns x_r array shape n_samples n_selected_features input samples selected features 
721: sklearn.ensemble.randomforestregressor class sklearn.ensemble.randomforestregressor n_estimators10 criterionmse min_samples_split1 max_depthnone min_samples_leaf1 min_density0.1 max_featuresauto bootstraptrue com pute_importancesfalse oob_scorefalse n_jobs1 random_statenone verbose0 random forest regressor random forest meta estimator number classical decision trees various subsamples dataset use averaging improve predictive accuracy control overtting 
722: parameters n_estimators integer optional default10 number trees forest 
723: criterion string optional defaultmse function measure quality split supported criterion mse mean squared error note parameter treespecic 
724: max_depth integer none optional defaultnone maximum depth tree none nodes expanded leaves pure leaves contain less min_samples_split samples note parameter treespecic 
725: min_samples_split integer optional default1 minimum number samples required split internal node note parame ter treespecic 
726: min_samples_leaf integer optional default1 1.5. model selection scikitlearn user guide release 0.11 minimum number samples newly created leaves split discarded split one leaves would contain less min_samples_leaf samples note parameter treespecic 
727: min_density oat optional default0.1 parameter controls tradeoff optimization heuristic controls minimum density sample_mask i.e fraction samples mask density falls threshold mask recomputed input data packed results min_density equals one partitions always represented data copying copies original data otherwise partitions represented bit masks aka sample masks note parameter treespecic 
728: max_features int string none optional defaultauto number features consider looking best split max_featuressqrt n_features classication tasks auto max_featuresn_features regression problems sqrt max_featuressqrt n_features log2 max_featureslog2 n_features none max_featuresn_features 
729: note parameter treespecic 
730: bootstrap boolean optional defaulttrue whether bootstrap samples used building trees 
731: compute_importances boolean optional defaulttrue whether computed feature_importances_ attribute calling 
732: importances feature stored oob_score bool whether use outofbag samples estimate generalization error 
733: n_jobs integer optional default1 number jobs run parallel number jobs set number cores 
734: random_state int randomstate instance none optional defaultnone int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used np.random 
735: verbose int optional default0 controlls verbosity tree building process 
736: see also decisiontreeregressor extratreesregressor references r60 chapter user guide scikitlearn user guide release 0.11 attributes fea ture_importances_ oob_score_ array shape n_features oat oob_prediction_ array shape n_samples feature mportances higher important feature score training dataset obtained using outofbag estimate prediction computed outofbag estimate training set 
737: methods fit fit_transform get_params deep predict score set_params params transform threshold reduce important features 
738: build forest trees training set fit data transform get parameters estimator predict regression target returns coefcient determination prediction set parameters estimator 
739: __init__ n_estimators10 criterionmse max_depthnone min_samples_leaf1 min_density0.1 max_featuresauto bootstraptrue pute_importancesfalse oob_scorefalse n_jobs1 random_statenone verbose0 min_samples_split1 com fit build forest trees training set 
740: parameters arraylike shape n_samples n_features training input samples 
741: arraylike shape n_samples target values integers correspond classes classication real numbers regression 
742: returns self object returns self 
743: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
744: parameters numpy array shape n_samples n_features training set 
745: numpy array shape n_samples target values 
746: returns x_new numpy array shape n_samples n_features_new transformed array 
747: 1.5. model selection scikitlearn user guide release 0.11 notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
748: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
749: predict predict regression target predicted regression target input sample computed mean predicted regression targets trees forest 
750: parameters arraylike shape n_samples n_features input samples 
751: returns array shape n_samples predicted values 
752: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
753: parameters arraylike shape n_samples n_features training set 
754: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform thresholdnone reduce important features 
755: parameters array scipy sparse matrix shape n_samples n_features input samples 
756: threshold string oat none optional defaultnone threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor e.g. 1.25mean may also used none available object attribute threshold used otherwise mean used default 
757: chapter user guide scikitlearn user guide release 0.11 returns x_r array shape n_samples n_selected_features input samples selected features 
758: sklearn.ensemble.extratreesclassier class sklearn.ensemble.extratreesclassifier n_estimators10 max_depthnone min_samples_leaf1 max_featuresauto bootstrapfalse pute_importancesfalse n_jobs1 random_statenone verbose0 criteriongini min_samples_split1 min_density0.1 com oob_scorefalse extratrees classier class implements meta estimator number randomized decision trees a.k.a extratrees various subsamples dataset use averaging improve predictive accuracy control overtting 
759: parameters n_estimators integer optional default10 number trees forest 
760: criterion string optional defaultgini function measure quality split supported criteria gini gini impurity entropy information gain note parameter treespecic 
761: max_depth integer none optional defaultnone maximum depth tree none nodes expanded leaves pure leaves contain less min_samples_split samples note parameter treespecic 
762: min_samples_split integer optional default1 minimum number samples required split internal node note parame ter treespecic 
763: min_samples_leaf integer optional default1 minimum number samples newly created leaves split discarded split one leaves would contain less min_samples_leaf samples note parameter treespecic 
764: min_density oat optional default0.1 parameter controls tradeoff optimization heuristic controls minimum density sample_mask i.e fraction samples mask density falls threshold mask recomputed input data packed results min_density equals one partitions always represented data copying copies original data otherwise partitions represented bit masks aka sample masks note parameter treespecic 
765: max_features int string none optional defaultauto number features consider looking best split 
766: max_featuressqrt n_features classication tasks auto max_featuresn_features regression problems sqrt max_featuressqrt n_features log2 max_featureslog2 n_features none max_featuresn_features 
767: 1.5. model selection scikitlearn user guide release 0.11 note parameter treespecic 
768: bootstrap boolean optional defaultfalse whether bootstrap samples used building trees 
769: compute_importances boolean optional defaulttrue whether computed feature_importances_ attribute calling 
770: importances feature stored oob_score bool whether use outofbag samples estimate generalization error 
771: n_jobs integer optional default1 number jobs run parallel number jobs set number cores 
772: random_state int randomstate instance none optional defaultnone int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used np.random 
773: verbose int optional default0 controlls verbosity tree building process 
774: see also sklearn.tree.extratreeclassifierbase classier ensemble randomforestclassifierensemble classier based trees optimal splits 
775: references r57 attributes fea ture_importances_ oob_score_ array shape n_features oat oob_decision_function_array shape n_samples n_classes feature mportances higher important feature score training dataset obtained using outofbag estimate decision function computed outofbag estimate training set 
776: methods fit fit_transform get_params deep predict predict_log_proba build forest trees training set fit data transform get parameters estimator predict class predict class logprobabilities 
777: continued next page chapter user guide scikitlearn user guide release 0.11 table 1.14 continued previous page predict_proba score set_params params transform threshold reduce important features 
778: predict class probabilities returns mean accuracy given test data labels set parameters estimator 
779: __init__ n_estimators10 criteriongini max_depthnone min_samples_leaf1 min_density0.1 max_featuresauto bootstrapfalse pute_importancesfalse oob_scorefalse n_jobs1 random_statenone verbose0 min_samples_split1 com fit build forest trees training set 
780: parameters arraylike shape n_samples n_features training input samples 
781: arraylike shape n_samples target values integers correspond classes classication real numbers regression 
782: returns self object returns self 
783: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
784: parameters numpy array shape n_samples n_features training set 
785: numpy array shape n_samples target values 
786: returns x_new numpy array shape n_samples n_features_new transformed array 
787: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
788: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
789: predict predict class predicted class input sample computed majority prediction trees forest 
790: parameters arraylike shape n_samples n_features 1.5. model selection scikitlearn user guide release 0.11 input samples 
791: returns array shape n_samples predicted classes 
792: predict_log_proba predict class logprobabilities predicted class logprobabilities input sample computed mean predicted class log probabilities trees forest 
793: parameters arraylike shape n_samples n_features input samples 
794: returns array shape n_samples class logprobabilities input samples classes ordered arithmetical order predict_proba predict class probabilities predicted class probabilities input sample computed mean predicted class probabilities trees forest 
795: parameters arraylike shape n_samples n_features input samples 
796: returns array shape n_samples class probabilities input samples classes ordered arithmetical order 
797: score returns mean accuracy given test data labels 
798: parameters arraylike shape n_samples n_features training set 
799: arraylike shape n_samples labels 
800: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform thresholdnone reduce important features 
801: parameters array scipy sparse matrix shape n_samples n_features input samples 
802: threshold string oat none optional defaultnone chapter user guide scikitlearn user guide release 0.11 threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor e.g. 1.25mean may also used none available object attribute threshold used otherwise mean used default 
803: returns x_r array shape n_samples n_selected_features input samples selected features 
804: sklearn.ensemble.extratreesregressor class sklearn.ensemble.extratreesregressor n_estimators10 max_depthnone min_samples_leaf1 max_featuresauto pute_importancesfalse n_jobs1 random_statenone verbose0 criterionmse min_samples_split1 min_density0.1 com oob_scorefalse bootstrapfalse extratrees regressor class implements meta estimator number randomized decision trees a.k.a extratrees various subsamples dataset use averaging improve predictive accuracy control overtting 
805: parameters n_estimators integer optional default10 number trees forest 
806: criterion string optional defaultmse function measure quality split supported criterion mse mean squared error note parameter treespecic 
807: max_depth integer none optional defaultnone none nodes expanded leaves maximum depth tree pure leaves contain less min_samples_split samples note parameter treespecic 
808: min_samples_split integer optional default1 minimum number samples required split internal node note parame ter treespecic 
809: min_samples_leaf integer optional default1 minimum number samples newly created leaves split discarded split one leaves would contain less min_samples_leaf samples note parameter treespecic 
810: min_density oat optional default0.1 parameter controls tradeoff optimization heuristic controls minimum density sample_mask i.e fraction samples mask density falls threshold mask recomputed input data packed results min_density equals one partitions always represented data copying copies original data otherwise partitions represented bit masks aka sample masks note parameter treespecic 
811: max_features int string none optional defaultauto number features consider looking best split 1.5. model selection scikitlearn user guide release 0.11 max_featuressqrt n_features classication tasks auto max_featuresn_features regression problems sqrt max_featuressqrt n_features log2 max_featureslog2 n_features none max_featuresn_features 
812: note parameter treespecic 
813: bootstrap boolean optional defaultfalse whether bootstrap samples used building trees note parameter tree specic 
814: compute_importances boolean optional defaulttrue whether computed feature_importances_ attribute calling 
815: importances feature stored oob_score bool whether use outofbag samples estimate generalization error 
816: n_jobs integer optional default1 number jobs run parallel number jobs set number cores 
817: random_state int randomstate instance none optional defaultnone int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used np.random 
818: verbose int optional default0 controlls verbosity tree building process 
819: see also sklearn.tree.extratreeregressorbase estimator ensemble randomforestregressorensemble regressor using trees optimal splits 
820: references r58 attributes fea ture_importances_ oob_score_ array shape n_features oat oob_prediction_ array shape n_samples feature mportances higher important feature score training dataset obtained using outofbag estimate prediction computed outofbag estimate training set 
821: chapter user guide scikitlearn user guide release 0.11 methods fit fit_transform get_params deep predict score set_params params transform threshold reduce important features 
822: build forest trees training set fit data transform get parameters estimator predict regression target returns coefcient determination prediction set parameters estimator 
823: __init__ n_estimators10 criterionmse max_depthnone min_samples_leaf1 min_density0.1 max_featuresauto bootstrapfalse pute_importancesfalse oob_scorefalse n_jobs1 random_statenone verbose0 min_samples_split1 com fit build forest trees training set 
824: parameters arraylike shape n_samples n_features training input samples 
825: arraylike shape n_samples target values integers correspond classes classication real numbers regression 
826: returns self object returns self 
827: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
828: parameters numpy array shape n_samples n_features training set 
829: numpy array shape n_samples target values 
830: returns x_new numpy array shape n_samples n_features_new transformed array 
831: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
832: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
833: 1.5. model selection scikitlearn user guide release 0.11 predict predict regression target predicted regression target input sample computed mean predicted regression targets trees forest 
834: parameters arraylike shape n_samples n_features input samples 
835: returns array shape n_samples predicted values 
836: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
837: parameters arraylike shape n_samples n_features training set 
838: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform thresholdnone reduce important features 
839: parameters array scipy sparse matrix shape n_samples n_features input samples 
840: threshold string oat none optional defaultnone threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor e.g. 1.25mean may also used none available object attribute threshold used otherwise mean used default 
841: returns x_r array shape n_samples n_selected_features input samples selected features 
842: sklearn.ensemble.gradientboostingclassier class sklearn.ensemble.gradientboostingclassifier lossdeviance learn_rate0.1 subsam n_estimators100 min_samples_split1 ple1.0 min_samples_leaf1 max_depth3 initnone random_statenone gradient boosting classication 
843: chapter user guide scikitlearn user guide release 0.11 builds additive model forward stagewise fashion allows optimization arbitrary differen tiable loss functions stage n_classes_ regression trees negative gradient binomial multinomial deviance loss function binary classication special case single regression tree induced 
844: parameters loss deviance optional defaultdeviance loss function optimized deviance refers deviance logistic regression classication probabilistic outputs refers least squares regression 
845: learn_rate oat optional default0.1 learning rate shrinks contribution tree learn_rate tradeoff learn_rate n_estimators 
846: n_estimators int default100 number boosting stages perform gradient boosting fairly robust tting large number usually results better performance 
847: max_depth integer optional default3 maximum depth individual regression estimators maximum depth limits number nodes tree tune parameter best performance best value depends interaction input variables 
848: min_samples_split integer optional default1 minimum number samples required split internal node 
849: min_samples_leaf integer optional default1 minimum number samples required leaf node 
850: subsample oat optional default1.0 fraction samples used tting individual base learners smaller 1.0 results stochastic gradient boosting subsample interacts parameter n_estimators 
851: see also sklearn.tree.decisiontreeclassifier randomforestclassifier references friedman greedy function approximation gradient boosting machine annals statistics vol 
852: 10.friedman stochastic gradient boosting hastie tibshirani friedman elements statistical learning springer 
853: examples samples labels sklearn.ensemble import gradientboostingclassifier gradientboostingclassifier .fit samples labels print gb.predict 0.5 1.5. model selection scikitlearn user guide release 0.11 methods fit fit_stage x_argsorted y_pred ... get_params deep predict predict_proba score set_params params staged_decision_function fit gradient boosting model fit another stage n_classes_ trees boosting model get parameters estimator predict class predict class probabilities returns mean accuracy given test data labels set parameters estimator compute decision function 
854: __init__ lossdeviance learn_rate0.1 n_estimators100 subsample1.0 min_samples_split1 min_samples_leaf1 max_depth3 initnone random_statenone fit fit gradient boosting model 
855: parameters arraylike shape n_samples n_features training vectors n_samples number samples n_features num ber features use fortranstyle avoid memory copies 
856: arraylike shape n_samples target values integers classication real numbers regression classication labels must correspond classes ... n_classes_1 returns self object returns self 
857: fit_stage x_argsorted y_pred sample_mask fit another stage n_classes_ trees boosting model 
858: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
859: predict predict class 
860: parameters arraylike shape n_samples n_features input samples 
861: returns array shape n_samples predicted classes 
862: predict_proba predict class probabilities 
863: parameters arraylike shape n_samples n_features input samples 
864: returns array shape n_samples class probabilities input samples classes ordered arithmetical order 
865: chapter user guide scikitlearn user guide release 0.11 score returns mean accuracy given test data labels 
866: parameters arraylike shape n_samples n_features training set 
867: arraylike shape n_samples labels 
868: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self staged_decision_function compute decision function method allows monitoring i.e determine error testing set stage 
869: parameters arraylike shape n_samples n_features input samples 
870: returns array shape n_samples n_classes decision function input samples classes ordered arithmetical order regression binary classication special cases n_classes 
871: sklearn.ensemble.gradientboostingregressor class sklearn.ensemble.gradientboostingregressor lossls n_estimators100 ple1.0 min_samples_leaf1 initnone random_statenone learn_rate0.1 subsam min_samples_split1 max_depth3 gradient boosting regression builds additive model forward stagewise fashion allows optimization arbitrary differ entiable loss functions stage regression tree negative gradient given loss function 
872: parameters loss lad optional defaultls loss function optimized refers least squares regression lad least absolute deviation highly robust loss function soley based order information input variables 
873: learn_rate oat optional default0.1 learning rate shrinks contribution tree learn_rate tradeoff learn_rate n_estimators 
874: n_estimators int default100 number boosting stages perform gradient boosting fairly robust tting large number usually results better performance 
875: max_depth integer optional default3 1.5. model selection scikitlearn user guide release 0.11 maximum depth individual regression estimators maximum depth limits number nodes tree tune parameter best performance best value depends interaction input variables 
876: min_samples_split integer optional default1 minimum number samples required split internal node 
877: min_samples_leaf integer optional default1 minimum number samples required leaf node 
878: subsample oat optional default1.0 fraction samples used tting individual base learners smaller 1.0 results stochastic gradient boosting subsample interacts parameter n_estimators 
879: see also sklearn.tree.decisiontreeregressor randomforestregressor references friedman greedy function approximation gradient boosting machine annals statistics vol 
880: 10.friedman stochastic gradient boosting hastie tibshirani friedman elements statistical learning springer 
881: examples samples labels sklearn.ensemble import gradientboostingregressor gradientboostingregressor .fit samples labels print gb.predict 1.32806997e05 attributes fea ture_importances_ array shape n_features oob_score_ array shape n_estimators train_score_ array shape n_estimators methods feature importances higher important feature 
882: score training dataset obtained using outofbag estimate ith score oob_score_ deviance loss model iteration outofbag sample ith score train_score_ deviance loss model iteration inbag sample subsample deviance training data 
883: chapter user guide scikitlearn user guide release 0.11 fit fit_stage x_argsorted y_pred ... get_params deep predict score set_params params staged_decision_function staged_predict fit gradient boosting model fit another stage n_classes_ trees boosting model get parameters estimator predict regression target returns coefcient determination prediction set parameters estimator compute decision function predict regression target stage 
884: __init__ lossls min_samples_leaf1 max_depth3 initnone random_statenone learn_rate0.1 n_estimators100 subsample1.0 min_samples_split1 fit fit gradient boosting model 
885: parameters arraylike shape n_samples n_features training vectors n_samples number samples n_features num ber features use fortranstyle avoid memory copies 
886: arraylike shape n_samples target values integers classication real numbers regression classication labels must correspond classes ... n_classes_1 returns self object returns self 
887: fit_stage x_argsorted y_pred sample_mask fit another stage n_classes_ trees boosting model 
888: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
889: predict predict regression target 
890: parameters arraylike shape n_samples n_features input samples 
891: returns array shape n_samples predicted values 
892: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
893: parameters arraylike shape n_samples n_features training set 
894: arraylike shape n_samples 1.5. model selection scikitlearn user guide release 0.11 returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self staged_decision_function compute decision function method allows monitoring i.e determine error testing set stage 
895: parameters arraylike shape n_samples n_features input samples 
896: returns array shape n_samples n_classes decision function input samples classes ordered arithmetical order regression binary classication special cases n_classes 
897: staged_predict predict regression target stage method allows monitoring i.e determine error testing set stage 
898: parameters arraylike shape n_samples n_features input samples 
899: returns array shape n_samples predicted value input samples 
900: 1.5.3 pipeline chaining estimators pipeline used chain multiple estimators one useful often xed sequence steps processing data example feature selection normalization classication pipeline serves two purposes convenience call fit predict data whole sequence estimators joint parameter selection grid search parameters estimators pipeline 
901: estimators usable within pipeline except last one need transform function otherwise dataset passed estimator 
902: usage pipeline build using list key value pairs key string containing name want give step value estimator object sklearn.pipeline import pipeline sklearn.svm import svc sklearn.decomposition import pca estimators reduce_dim pca svm svc clf pipeline estimators chapter user guide scikitlearn user guide release 0.11 clf pipeline steps reduce_dim pca copytrue n_componentsnone whitenfalse svm svc c1.0 cache_size200 class_weightnone coef00.0 degree3 gamma0.0 kernelrbf probabilityfalse shrinkingtrue tol0.001 verbosefalse estimators pipeline stored list steps attribute clf.steps reduce_dim pca copytrue n_componentsnone whitenfalse dict named_steps clf.named_steps reduce_dim pca copytrue n_componentsnone whitenfalse parameters estimators pipeline accessed using estimator parameter syntax clf.set_params svm__c10 normalize_whitespace pipeline steps reduce_dim pca copytrue n_componentsnone whitenfalse svm svc c10 cache_size200 class_weightnone coef00.0 degree3 gamma0.0 kernelrbf probabilityfalse shrinkingtrue tol0.001 verbosefalse particularly important grid searches sklearn.grid_search import gridsearchcv params dict reduce_dim__n_components ... grid_search gridsearchcv clf param_gridparams svm__c 0.1 examples pipeline anova svm sample pipeline text feature extraction evaluation pipelining chaining pca logistic regression explicit feature map approximation rbf kernels svmanova svm univariate feature selection notes calling fit pipeline calling fit estimator turn transform input pass next step pipeline methods last estimator pipline i.e last estimator classier pipeline used classier last estimator transformer pipeline 
903: 1.6 dataset transformations 1.6.1 preprocessing data sklearn.preprocessing package provides several common utility functions transformer classes change raw feature vectors representation suitable downstream estimators 
904: 1.6. dataset transformations scikitlearn user guide release 0.11 standardization mean removal variance scaling standardization datasets common requirement many machine learning estimators implemented scikit might behave badly individual feature less look like standard normally distributed data gaussian zero mean unit variance practice often ignore shape distribution transform data center removing mean value feature scale dividing nonconstant features standard deviation instance many elements used objective function learning algorithm rbf kernel support vector machines regularizers linear models assume features centered around zero variance order feature variance orders magnitude larger others might dominate objective function make estimator unable learn features correctly expected function scale provides quick easy way perform operation single arraylike dataset sklearn import preprocessing ... ... x_scaled preprocessing.scale x_scaled array 
905: ... 1.22 ... 1.33 ... ... 0.26 ... 1.22 ... 1.06 ... 1.22 ... 1.22 ... scaled data zero mean unit variance x_scaled.mean axis0 array x_scaled.std axis0 array preprocessing module provides utility class scaler implements transformer api compute mean standard deviation training set able later reapply transformation testing set class hence suitable use early steps sklearn.pipeline.pipeline scaler preprocessing.scaler .fit scaler scaler copytrue with_meantrue with_stdtrue scaler.mean_ array ... scaler.std_ array 0.81 ... ... 0.33 ... 0.81 ... 1.24 ... scaler.transform array 
906: ... 1.22 ... 1.22 ... 1.22 ... 1.33 ... ... 0.26 ... 1.22 ... 1.06 ... scaler instance used new data transform way training set scaler.transform array 2.44 ... 1.22 ... 0.26 ... chapter user guide scikitlearn user guide release 0.11 possible disable either centering scaling either passing with_meanfalse with_stdfalse constructor scaler 
907: references discussion importance centering scaling data available faq normal izestandardizerescale data scaling whitening sometimes enough center scale features independently since downstream model make assumption linear independence features sklearn.decomposition.randomizedpca whitentrue correlation across features 
908: remove linear sklearn.decomposition.pca address issue use sparse input scale scaler accept scipy.sparse matrices input with_meanfalse explicitly passed constructor otherwise valueerror raised silently centering would break sparsity would often crash execution allocating excessive amounts memory unintentionally centered data expected small enough explicitly convert input array using toarray method sparse matrices instead data converted compressed sparse rows representation see scipy.sparse.csr_matrix avoid unnecessary memory copies recommended choose csr representation upstream 
909: sparse input scaling target variables regression scale scaler work outofthebox arrays useful scaling target response variables used regression 
910: normalization normalization process scaling individual samples unit norm process useful plan use quadratic form dotproduct kernel quantify similarity pair samples assumption base vector space model often used text classication clustering contexts function normalize provides quick easy way perform operation single arraylike dataset either using norms ... ... x_normalized preprocessing.normalize norml2 x_normalized array 0.40 ... 0.40 ... ... 
911: ... ... 0.81 ... ... 0.70 ... 0.70 ... 1.6. dataset transformations scikitlearn user guide release 0.11 preprocessing module provides utility class normalizer implements operation using transformer api even though fit method useless case class stateless operation treats samples independently class hence suitable use early steps sklearn.pipeline.pipeline normalizer preprocessing.normalizer .fit fit nothing normalizer normalizer copytrue norml2 normalizer instance used sample vectors transformer normalizer.transform array 0.40 ... 0.40 ... ... 0.81 ... ... 0.70 ... 0.70 ... 
912: ... ... normalizer.transform array 0.70 ... 
913: 0.70 ... ... sparse input normalize normalizer accept dense arraylike sparse matrices scipy.sparse input data converted compressed sparse rows representation see scipy.sparse.csr_matrix fed efcient cython routines avoid unnecessary memory copies recommended choose csr representation upstream 
914: sparse input binarization feature binarization feature binarization process thresholding numerical features get boolean values useful downsteam probabilistic estimators make assumption input data distributed according multivariate bernoulli distribution instance case common class restricted boltzmann machines yet implemented scikit also commmon among text processing community use binary feature values probably simplify probabilistic reasoning even normalized counts a.k.a term frequencies tfidf valued features often perform slightly better practice used early stages sklearn.pipeline.pipeline fit method nothing sample treated independently others utility class binarizer meant normalizer ... ... binarizer preprocessing.binarizer .fit fit nothing binarizer binarizer copytrue threshold0.0 binarizer.transform array chapter user guide scikitlearn user guide release 0.11 possible adjust threshold binarizer binarizer preprocessing.binarizer threshold1.1 binarizer.transform array scaler normalizer classes preprocessing module provides companion function binarize used transformer api necessary 
915: sparse input binarize binarizer accept dense arraylike sparse matrices scipy.sparse input data converted compressed sparse rows representation see scipy.sparse.csr_matrix avoid unnecessary memory copies recommended choose csr representation upstream 
916: sparse input 1.6.2 feature extraction sklearn.feature_extraction module used extract features format supported machine learning algorithms datasets consisting formats text image 
917: note feature extraction different feature selection former consists transforming arbitrary data text images numerical features usable machine learning later machine learning technique applied features 
918: loading features dicts class dictvectorizer used convert feature arrays represented lists standard python dict objects numpyscipy representation used scikitlearn estimators particularly fast process pythons dict advantages convenient use sparse absent features need stored storing feature names addition values dictvectorizer implements called oneofk onehot coding categorical aka nominal discrete features categorical features attributevalue pairs value restricted list discrete possibilities without ordering e.g topic identiers types objects tags names ... following city categorical attribute temperature traditional numerical feature measurements ... ... ... ... city dubai temperature city london temperature city san fransisco temperature sklearn.feature_extraction import dictvectorizer vec dictvectorizer vec.fit_transform measurements .toarray 1.6. dataset transformations scikitlearn user guide release 0.11 array vec.get_feature_names citydubai citylondon citysan fransisco temperature dictvectorizer also useful representation transformation training sequence classiers natural lan guage processing models typically work extracting feature windows around particular word interest example suppose rst algorithm extracts part speech pos tags want use complementary tags training sequence classier e.g chunker following dict could window feature extracted around word sat sentence cat sat mat pos_window ... ... ... ... ... ... ... ... ... ... word2 pos2 word1 cat pos1 word1 pos1 real application one would extract many dictionaries description vectorized sparse twodimensional matrix suitable feeding classier maybe piped text.tfidftransformer normalization vec dictvectorizer pos_vectorized vec.fit_transform pos_window pos_vectorized 1x6 sparse matrix type type numpy.float64 stored elements coordinate format pos_vectorized.toarray array vec.get_feature_names pos1pp pos1nn pos2dt word1on word1cat word2the imagine one extracts context around individual word corpus documents resulting matrix wide many onehotfeatures valued zero time make resulting data structure able memory dictvectorizer class uses scipy.sparse matrix default instead numpy.ndarray 
919: text feature extraction bag words representation text analysis major application eld machine learning algorithms however raw data sequence symbols fed directly algorithms expect numerical feature vectors xed size rather raw text documents variable length order address scikitlearn provides utilities common ways extract numerical features text content namely tokenizing strings giving integer possible token instance using whitespaces punctuation token separators 
920: chapter user guide scikitlearn user guide release 0.11 counting occurrences tokens document normalizing weighting diminishing importance tokens occur majority samples docu ments 
921: scheme features samples dened follows individual token occurrence frequency normalized treated feature vector token frequencies given document considered multivariate sample 
922: corpus documents thus represented matrix one row per document one column per token e.g word occurring corpus call vectorization general process turning collection text documents numerical feature vectors specic stragegy tokenization counting normalization called bag words bag ngrams repre sentation documents described word occurrences completely ignoring relative position information words document combined tfidf normalization bag words encoding also known vector space model 
923: sparsity documents typically use subset words used corpus resulting matrix many feature values zeros typically instance collection short text documents emails use vocabulary size order unique words total document use unique words individually order able store matrix memory also speed algebraic operations matrix vector imple mentations typically use sparse representation implementations available scipy.sparse package 
924: common vectorizer usage countvectorizer implements tokenization occurrence counting single class sklearn.feature_extraction.text import countvectorizer model many parameters however default values quite reasonable please see reference documen tation details vectorizer countvectorizer vectorizer countvectorizer analyzerword binaryfalse charsetutf8 charset_errorstrict dtype type long inputcontent lowercasetrue max_df1.0 max_featuresnone max_n1 min_n1 preprocessornone stop_wordsnone strip_accentsnone token_patternubwwb tokenizernone vocabularynone lets use tokenize count word occurrences minimalistic corpus text documents corpus ... ... ... ... ... vectorizer.fit_transform corpus first document. second second document. third one. first document 1.6. dataset transformations scikitlearn user guide release 0.11 4x9 sparse matrix type type numpy.int64 stored elements coordinate format default conguration tokenizes string extracting words least letters specic function step requested explicitly analyze vectorizer.build_analyzer analyze text document analyze uthis uis utext udocument uto uanalyze term found analyzer assigned unique integer index corresponding column resulting matrix interpretation columns retrieved follows vectorizer.get_feature_names uand udocument ufirst uis uone usecond uthe uthird uthis x.toarray array ... converse mapping feature name column index stored vocabulary_ attribute vectorizer vectorizer.vocabulary_.get document hence words seen training corpus completely ignored future calls transform method vectorizer.transform something completely new .toarray ... array ... note previous corpus rst last documents exactly words hence encoded equal vectors particular lose information last document interogative form preserve local ordering information extract 2grams words addition 1grams word themselvs bigram_vectorizer countvectorizer min_n1 max_n2 ... analyze bigram_vectorizer.build_analyzer analyze bigrams cool ubi ugrams uare ucool ubi grams ugrams uare cool token_patternurbwb vocabulary extracted vectorizer hence much bigger resolve ambiguities encoded local positioning patterns x_2 bigram_vectorizer.fit_transform corpus .toarray x_2 ... array ... particular interogative form present last document chapter user guide scikitlearn user guide release 0.11 feature_index bigram_vectorizer.vocabulary_.get uis x_2 feature_index array ... tfidf normalization large text corpus words present e.g english hence carrying little meaningul information actual contents document feed direct count data directly classier frequent terms would shadow frequencies rarer yet interesting terms order reweight count features oating point values suitable usage classier common use tfidf transform means termfrequency tfidf means termfrequency times inverse documentfrequency orginally term weighting scheme developed information retrieval ranking function search engines results also found good use document classication clustering normalization implemented text.tfidftransformer class sklearn.feature_extraction.text import tfidftransformer transformer tfidftransformer transformer tfidftransformer norml2 smooth_idftrue sublinear_tffalse use_idftrue please see reference documentation details parameters lets take example following counts rst term present time hence interesting two features less time hence probably representative content documents counts ... ... ... ... ... ... tfidf transformer.fit_transform counts tfidf 6x3 sparse matrix type type numpy.float64 stored elements compressed sparse row format tfidf.toarray array 0.85 ... ... ... ... 0.55 ... 0.63 ... ... ... ... ... 0.83 ... ... 0.52 ... ... ... ... ... 0.77 ... row normalized unit euclidean norm weights feature computed fit method call stored model attribute transformer.idf_ array ... 2.25 ... 1.84 ... tfidf often used text features also another class called tfidfvectorizer combines option countvectorizer tfidftransformer single model 1.6. dataset transformations scikitlearn user guide release 0.11 sklearn.feature_extraction.text import tfidfvectorizer vectorizer tfidfvectorizer vectorizer.fit_transform corpus ... 4x9 sparse matrix type type numpy.float64 stored elements compressed sparse row format tfidf normalization often useful might cases binary occurrence markers might offer better features achieved using binary parameter countvectorizer particular estimators bernoulli naive bayes explicitly model discrete boolean random variables also short text likely noisy tfidf values binary occurrence info stable usual way best adjust feature extraction parameters use crossvalidated grid search instance pipelining feature extractor classier sample pipeline text feature extraction evaluation applications examples bag words representation quite simplistic surprisingly useful practice particular supervised setting successfully combined fast scalable linear models train document classicers instance classication text documents using sparse features unsupervised setting used group similar documents together applying clustering algorithms kmeans clustering text documents using kmeans finally possible discover main topics corpus relaxing hard assignement constraint clustering instance using nonnegative matrix factorization nmf nnmf topics extraction nonnegative matrix factorization limitations bag words representation local positioning information preserved extracting ngrams instead individual words bag words bag ngrams destroy inner structure document hence meaning carried internal structure order address wider task natural language understanding local structure sentences paragraphs thus taken account many models thus casted structured output problems currently outside scope scikitlearn 
925: customizing vectorizer classes possible customize behavior passing callable parameters vectorizer def my_tokenizer ... return s.split ... vectorizer countvectorizer tokenizermy_tokenizer vectorizer.build_analyzer ... punctuation usome ... upunctuation chapter user guide scikitlearn user guide release 0.11 particular name preprocessor callable takes string input return another string removing html tags converting lower case instance tokenizer callable takes string input output sequence feature occurrences a.k.a 
926: tokens 
927: analyzer callable wraps calls preprocessor tokenizer perform ltering ngrams extractions tokens 
928: make preprocessor tokenizer analyzers aware model parameters possible derive class override build_preprocessor build_tokenizer build_analyzer factory method instead customizing vectorizer useful handle asian languages use explicit word separator whitespace instance 
929: image feature extraction patch extraction extract_patches_2d function extracts patches image stored twodimensional array threedimensional color information along third axis rebuilding image patches use reconstruct_from_patches_2d example let use generate 4x4 pixel picture color channels e.g rgb format import numpy sklearn.feature_extraction import image one_image np.arange .reshape one_image array channel fake rgb picture random_state0 patches image.extract_patches_2d one_image max_patches2 ... patches.shape patches array patches image.extract_patches_2d one_image patches.shape patches array let try reconstruct original image patches averaging overlapping areas reconstructed image.reconstruct_from_patches_2d patches np.testing.assert_array_equal one_image reconstructed 1.6. dataset transformations scikitlearn user guide release 0.11 patchextractor class works way extract_patches_2d supports multiple images input implemented estimator used pipelines see five_images np.arange .reshape patches image.patchextractor .transform five_images patches.shape connectivity graph image several estimators scikitlearn use connectivity information features samples instance ward clustering hierarchical clustering cluster together neighboring pixels image thus forming contiguous patches purpose estimators use connectivity matrix giving samples connected function img_to_graph returns matrix image similarly grid_to_graph build connectivity matrix images given shape image matrices used impose connectivity estimators use connectivity information ward clustering hierarchical clustering also build precomputed kernels similarity matrices 
930: note examples demo structured ward hierarchical clustering lena image spectral clustering image segmentation feature agglomeration vs. univariate selection 1.6.3 kernel approximation submodule contains functions approximate feature mappings correspond certain kernels used example support vector machines see support vector machines following feature functions perform nonlinear transformations input serve basis linear classication algorithms advantage using approximate explicit feature maps compared kernel trick makes use feature maps implicitly explicit mappings better suited online learning signicantly reduce cost learning large datasets standard kernelized svms scale well large datasets using approximate kernel map possible use much efcient linear svms particularly combination kernel map approximations sgdclassifier make nonlinear learning large datasets possible 
931: chapter user guide scikitlearn user guide release 0.11 since much empirical work using approximate embeddings advisable compare results exact kernel methods possible 
932: radial basis function kernel rbfsampler constructs approximate mapping radial basis function kernel mapping relies monte carlo approximation kernel values fit function performs monte carlo sampling whereas transform method performs mapping data inherent randomness process results may vary different calls fit function fit function takes two arguments n_components target dimensionality feature transform gamma parameter rbfkernel higher n_components result better approximation kernel yield results similar produced kernel svm note tting feature function actually depend data given fit function dimensionality data used details method found rr2007 
933: figure 1.5 comparing exact rbf kernel left approximation right examples explicit feature map approximation rbf kernels additive chi squared kernel chi squared kernel kernel histograms often used computer vision chi squared kernel given cid88 2xiyi since kernel additive possible treat components separately embedding makes possible sample fourier transform regular intervals instead approximating using monte carlo sampling class additivechi2sampler implements component wise deterministic sampling component sampled times yielding 2n1 dimensions per input dimension multiple two stems real complex part fourier transform literature usually choosen transforming dataset size n_samples n_features case approximate feature map provided additivechi2sampler combined approximate feature map provided rbfsampler yield approximate feature map exponentiated chi squared kernel see vz2010 details vvz2010 combination rbfsampler 
934: 1.6. dataset transformations scikitlearn user guide release 0.11 skewed chi squared kernel skewed chi squared kernel given cid89 properties similar exponentiated chi squared kernel often used computer vision allows simple monte carlo approximation feature map usage skewedchi2sampler usage described rbfsampler difference free parameter called motivation mapping mathematical details see ls2010 
935: mathematical details kernel methods like support vector machines kernelized pca rely property reproducing kernel hilbert spaces positive denite kernel function called mercer kernel guaranteed exists mapping hilbert space denotes inner product hilbert space algorithm linear support vector machine pca relies scalar product data points one may use value corresponds applying algorithm mapped data points advantage using mapping never calculated explicitly allowing arbitrary large features even innite one drawback kernel methods might necessary store many kernel values optimiza tion kernelized classier applied new data needs computed make predictions possibly many different training set classes submodule allow approximate embedding thereby working explicitly representa tions obviates need apply kernel store training examples 
936: references 1.7 dataset loading utilities sklearn.datasets package embeds small toy datasets introduced getting started section evaluate impact scale dataset n_samples n_features controlling statistical properties data typically correlation informativeness features also possible generate synthetic data package also features helpers fetch larger datasets commonly used machine learning community benchmark algorithm data comes real world 
937: 1.7.1 general dataset api three distinct kinds dataset interfaces different types datasets simplest one interface sample images described sample images section 
938: chapter user guide scikitlearn user guide release 0.11 dataset generation functions svmlight loader share simplistic interface returning tuple con sisting n_samples n_features numpy array array length n_samples containing targets toy datasets well real world datasets datasets fetched mldata.org sophisti cated structure functions return bunch dictionary accessible dict.key syntax datasets least two keys data containg array shape n_samples n_features except 20newsgroups target numpy array length n_features containing targets datasets also contain description descr contain feature_names target_names see dataset descriptions details 
939: 1.7.2 toy datasets scikitlearn comes small standard datasets require download external website 
940: load return boston houseprices dataset regression load_boston load return iris dataset classication load_iris load_diabetes load return diabetes dataset regression load_digits n_class load return digits dataset classication load_linnerud load return linnerud dataset multivariate regression 
941: datasets useful quickly illustrate behavior various algorithms implemented scikit however often small representative real world machine learning tasks 
942: 1.7.3 sample images scikit also embed couple sample jpeg images published creative commons license authors image useful test algorithms pipeline data 
943: load_sample_images load_sample_image image_name load numpy array single sample image load sample images image manipulation 
944: warning default coding images based uint8 dtype spare memory often machine learning algorithms work best input converted oating point representation rst also plan use pylab.imshow dont forget scale range done following example 
945: examples color quantization using kmeans 1.7. dataset loading utilities scikitlearn user guide release 0.11 1.7.4 sample generators addition scikitlearn includes various random sample generators used build artical datasets controled size complexity 
946: generate random nclass classication problem 
947: make_classification n_samples n_features ... make_multilabel_classification n_samples ... generate random multilabel classication problem make_regression n_samples n_features ... make_blobs n_samples n_features centers ... make_friedman1 n_samples n_features ... make_friedman2 n_samples noise random_state make_friedman3 n_samples noise random_state make_hastie_10_2 n_samples random_state make_low_rank_matrix n_samples ... make_sparse_coded_signal n_samples ... ... make_sparse_uncorrelated n_samples ... make_spd_matrix n_dim random_state make_swiss_roll n_samples noise random_state make_s_curve n_samples noise random_state make_sparse_spd_matrix dim alpha ... generate random regression problem generate isotropic gaussian blobs clustering generate friedman regression problem generate friedman regression problem generate friedman regression problem generates data binary classication used generate mostly low rank matrix bellshaped singular values generate signal sparse combination dictionary elements generate random regression problem sparse uncorrelated design generate random symmetric positivedenite matrix generate swiss roll dataset generate curve dataset generate sparse symetric denite positive matrix 
948: 1.7.5 datasets svmlight libsvm format scikitlearn includes utility functions loading datasets svmlight libsvm format format line takes form label featureid featurevalue featureid featurevalue ... format especially suitable sparse datasets module scipy sparse csr matrices used numpy arrays used may load dataset like follows sklearn.datasets import load_svmlight_file x_train y_train load_svmlight_file pathtotrain_dataset.txt 
949: may also load two datasets x_train y_train x_test y_test load_svmlight_files ... 
950: pathtotrain_dataset.txt pathtotest_dataset.txt chapter user guide scikitlearn user guide release 0.11 case x_train x_test guaranteed number features another way achieve result number features x_test y_test load_svmlight_file ... 
951: pathtotest_dataset.txt n_featuresx_train.shape related links public datasets svmlight libsvm format http www.csie.ntu.edu.twcjlinlibsvmtoolsdatasets faster apicompatible implementation https github.commblondelsvmlightloader 1.7.6 olivetti faces dataset dataset contains set face images taken april april laboratories cambridge website describing original dataset defunct archived copies accessed internet archives wayback machine described original website ten different images distinct subjects subjects images taken different times varying lighting facial expressions open closed eyes smiling smiling facial details glasses glasses images taken dark homogeneous background subjects upright frontal position tolerance side movement 
952: image quantized grey levels stored unsigned 8bit integers loader convert oating point values interval easier work many algorithms target database integer indicating identity person pictured however examples per class relatively small dataset interesting unsupervised semisupervised perspective original dataset consisted version available consists 64x64 images using images please give credit laboratories cambridge 
953: 1.7.7 newsgroups text dataset newsgroups dataset comprises around newsgroups posts topics splitted two subsets one training development one testing performance evaluation split train test set based upon messages posted specic date rst one sklearn.datasets.fetch_20newsgroups module contains returns sklearn.feature_extraction.text.vectorizer custom parameters extract feature vectors second one sklearn.datasets.fetch_20newsgroups_vectorized returns readytouse features i.e. necessary use feature extractor 
954: two loaders raw text extractors fed list text feature les usage sklearn.datasets.fetch_20newsgroups function data fetching caching functions downloads data archive original newsgroups website extracts archive contents scikit_learn_data20news_home folder calls sklearn.datasets.load_file either training testing set folder 1.7. dataset loading utilities scikitlearn user guide release 0.11 sklearn.datasets import fetch_20newsgroups newsgroups_train fetch_20newsgroups subsettrain pprint import pprint pprint list newsgroups_train.target_names alt.atheism comp.graphics comp.os.mswindows.misc comp.sys.ibm.pc.hardware comp.sys.mac.hardware comp.windows.x misc.forsale rec.autos rec.motorcycles rec.sport.baseball rec.sport.hockey sci.crypt sci.electronics sci.med sci.space soc.religion.christian talk.politics.guns talk.politics.mideast talk.politics.misc talk.religion.misc real data lies filenames target attributes target attribute integer index category newsgroups_train.filenames.shape newsgroups_train.target.shape newsgroups_train.target array possible load subselection categories passing list categories load fetch_20newsgroups function cats alt.atheism sci.space newsgroups_train fetch_20newsgroups subsettrain categoriescats list newsgroups_train.target_names alt.atheism sci.space newsgroups_train.filenames.shape newsgroups_train.target.shape newsgroups_train.target array order feed predictive clustering models text data one rst need turn text vec tors numerical values suitable statistical analysis achieved utilities sklearn.feature_extraction.text demonstrated following example extract tfidf vectors unigram tokens sklearn.feature_extraction.text import vectorizer documents open .read newsgroups_train.filenames vectorizer vectorizer vectors vectorizer.fit_transform documents chapter user guide scikitlearn user guide release 0.11 vectors.shape extracted tfidf vectors sparse average non zero components sample dimensional space less non zero features vectors.nnz vectors.shape sklearn.datasets.fetch_20newsgroups_vectorized function returns readytouse tdf features instead names 
955: examples sample pipeline text feature extraction evaluation classication text documents using sparse features 1.7.8 downloading datasets mldata.org repository mldata.org public repository machine learning data supported pascal network sklearn.datasets package able directly download data sets repository using function fetch_mldata dataname example download mnist digit recognition database sklearn.datasets import fetch_mldata mnist fetch_mldata mnist original data_homecustom_data_home mnist database contains total examples handwritten digits size 28x28 pixels labeled mnist.data.shape mnist.target.shape np.unique mnist.target array rst download dataset cached locally path specied data_home keyword argument defaults scikit_learn_data os.listdir os.path.join custom_data_home mldata mnistoriginal.mat data sets mldata.org adhere strict naming formatting convention fetch_mldata able make sense common cases allows tailor defaults individual datasets data arrays mldata.org often shaped n_features n_samples posite scikitlearn convention fetch_mldata transposes matrix default transpose_data keyword controls behavior iris fetch_mldata iris data_homecustom_data_home iris.data.shape iris fetch_mldata iris transpose_datafalse 
956: data_homecustom_data_home 1.7. dataset loading utilities scikitlearn user guide release 0.11 iris.data.shape datasets multiple columns fetch_mldata tries identify target data columns rename target data done looking arrays named label data dataset failing choosing rst array target second data behavior changed target_name data_name keywords setting specic name index number name order columns datasets found mldata.org tab data iris2 fetch_mldata datasetsuci iris target_name1 data_name0 ... iris3 fetch_mldata datasetsuci iris target_nameclass 
957: data_namedouble0 data_homecustom_data_home data_homecustom_data_home 1.7.9 labeled faces wild face recognition dataset dataset collection jpeg pictures famous people collected internet details available ofcial website http viswww.cs.umass.edulfw picture centered single face typical task called face verication given pair two pictures binary classier must predict whether two images person alternative task face recognition face identication given picture face unknown person identify name person referring gallery previously seen pictures identied persons face verication face recognition tasks typically performed output model trained perform face detection popular model face detection called violajones implemented opencv library lfw faces extracted face detector various online websites 
958: usage scikitlearn provides two loaders automatically download cache parse metadata les decode jpeg convert interesting slices memmaped numpy arrays dataset size rst load typically takes couple minutes fully decode relevant part jpeg les numpy arrays dataset loaded following times loading times less 200ms using memmaped version memoized disk scikit_learn_datalfw_home folder using joblib rst loader used face identication task multiclass classication task hence supervised learning sklearn.datasets import fetch_lfw_people lfw_people fetch_lfw_people min_faces_per_person70 resize0.4 print name name lfw_people.target_names ... ... ariel sharon colin powell donald rumsfeld george bush gerhard schroeder hugo chavez tony blair default slice rectangular shape around face removing background chapter user guide scikitlearn user guide release 0.11 lfw_people.data.dtype dtype float32 lfw_people.data.shape lfw_people.images.shape faces assigned single person target array lfw_people.target.shape list lfw_people.target second loader typically used face verication task sample pair two picture belonging person sklearn.datasets import fetch_lfw_pairs lfw_pairs_train fetch_lfw_pairs subsettrain list lfw_pairs_train.target_names different persons person lfw_pairs_train.pairs.shape lfw_pairs_train.data.shape lfw_pairs_train.target.shape fetch_lfw_people fetch_lfw_pairs function possible get additional dimension rgb color channels passing colortrue case shape fetch_lfw_pairs datasets subdived subsets development train set development test set evaluation 10_folds set meant compute performance metrics using 10folds cross validation scheme 
959: references labeled faces wild database studying face recognition unconstrained environments gary huang manu ramesh tamara berg erik learnedmiller university massachusetts amherst technical report october 
960: examples faces recognition example using eigenfaces svms 1.7. dataset loading utilities scikitlearn user guide release 0.11 1.8 reference class function reference scikitlearn please refer full user guide details class function raw specications may enough give full guidelines uses 
961: list modules sklearn.cluster clustering classes functions sklearn.covariance covariance estimators sklearn.cross_validation cross validation sklearn.datasets datasets sklearn.decomposition matrix decomposition sklearn.ensemble ensemble methods sklearn.feature_extraction feature extraction loaders samples generator images text sklearn.feature_selection feature selection sklearn.gaussian_process gaussian processes sklearn.grid_search grid search sklearn.hmm hidden markov models sklearn.kernel_approximation kernel approximation sklearn.semi_supervised semisupervised learning sklearn.lda linear discriminant analysis sklearn.linear_model generalized linear models sklearn.manifold manifold learning sklearn.metrics metrics dense data sparse data classication metrics regression metrics clustering metrics pairwise metrics sklearn.mixture gaussian mixture models sklearn.multiclass multiclass multilabel classication multiclass multilabel classication strategies sklearn.naive_bayes naive bayes sklearn.neighbors nearest neighbors sklearn.pls partial least squares sklearn.pipeline pipeline sklearn.preprocessing preprocessing normalization sklearn.qda quadratic discriminant analysis sklearn.svm support vector machines estimators lowlevel methods sklearn.tree decision trees sklearn.utils utilities chapter user guide scikitlearn user guide release 0.11 1.8.1 sklearn.cluster clustering sklearn.cluster module gathers popular unsupervised clustering algorithms user guide see clustering section details 
962: classes cluster.affinitypropagation damping ... cluster.dbscan eps min_samples metric ... cluster.kmeans init n_init max_iter ... cluster.minibatchkmeans init max_iter ... minibatch kmeans clustering cluster.meanshift bandwidth seeds ... cluster.spectralclustering mode ... cluster.ward n_clusters memory ... meanshift clustering apply kmeans projection normalized laplacian ward hierarchical clustering constructs tree cuts 
963: perform afnity propagation clustering data perform dbscan clustering vector array distance matrix kmeans clustering sklearn.cluster.afnitypropagation class sklearn.cluster.affinitypropagation damping0.5 max_iter200 convit30 copytrue perform afnity propagation clustering data parameters damping oat optional damping factor max_iter int optional maximum number iterations convit int optional number iterations change number estimated clusters stops convergence 
964: copy boolean optional make copy input data true default 
965: notes see examplesplot_afnity_propagation.py example algorithmic complexity afnity propagation quadratic number points 
966: references brendan frey delbert dueck clustering passing messages data points science feb. attributes cluster_centers_indices_ labels_ array n_clusters array n_samples indices cluster centers labels point 1.8. reference scikitlearn user guide release 0.11 methods fit get_params deep set_params params compute afnity propagation clustering get parameters estimator set parameters estimator 
967: __init__ damping0.5 max_iter200 convit30 copytrue fit pnone compute afnity propagation clustering 
968: parameters array n_points n_points matrix similarities points array n_points oat optional preferences point points larger values preferences likely chosen exemplars number exemplars clusters inuenced input preferences value preferences passed arguments set median input similarities 
969: damping oat optional damping factor copy boolean optional copy false afnity matrix modied inplace algorithm memory efciency get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.cluster.dbscan class sklearn.cluster.dbscan eps0.5 min_samples5 metriceuclidean random_statenone perform dbscan clustering vector array distance matrix dbscan densitybased spatial clustering applications noise finds core samples high density expands clusters good data contains clusters similar density 
970: parameters eps oat optional maximum distance two samples considered neighborhood 
971: chapter user guide scikitlearn user guide release 0.11 min_samples int optional number samples neighborhood point considered core point 
972: metric string callable metric use calculating distance instances feature array metric string callable must one options allowed met rics.pairwise.calculate_distance metric parameter metric precomputed assumed distance matrix must square 
973: random_state numpy.randomstate optional generator used initialize centers defaults numpy.random 
974: notes see examplesplot_dbscan.py example 
975: references ester kriegel sander densitybased algorithm discovering clusters large spatial databases noise proceedings 2nd international conference knowledge discovery data mining portland aaai press attributes array shape n_core_samples array shape n_core_samples n_features array shape n_samples indices core samples 
976: copy core sample found training 
977: cluster labels point dataset given noisy samples given label 
978: core_sample_indices_ components_ labels_ methods fit params get_params deep set_params params perform dbscan clustering vector array distance matrix get parameters estimator set parameters estimator 
979: __init__ eps0.5 min_samples5 metriceuclidean random_statenone fit params perform dbscan clustering vector array distance matrix 
980: parameters array n_samples n_samples n_samples n_features array distances samples feature array array treated feature array unless metric given precomputed 
981: params dict overwrite keywords __init__ 
982: 1.8. reference scikitlearn user guide release 0.11 get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.cluster.kmeans class sklearn.cluster.kmeans initkmeans n_init10 max_iter300 verbose0 tol0.0001 random_statenone kmeans clustering precompute_distancestrue copy_xtrue n_jobs1 parameters int optional default number clusters form well number centroids generate 
983: max_iter int maximum number iterations kmeans algorithm single run 
984: n_init int optional default number time kmeans algorithm run different centroid seeds nal results best output n_init consecutive runs terms inertia 
985: init kmeans random ndarray method initialization defaults kmeans kmeans selects initial cluster centers kmean clustering smart way speed convergence see section notes k_init details random choose observations rows random data initial centroids init array used seed centroids precompute_distances boolean precompute distances faster takes memory 
986: tol oat optional default 1e4 relative tolerance w.r.t inertia declare convergence n_jobs int number jobs use computation works breaking pairwise matrix n_jobs even slices computing parallel cpus used given parallel computing code used useful debuging n_jobs n_cpus n_jobs used thus n_jobs cpus one used 
987: chapter user guide scikitlearn user guide release 0.11 random_state integer numpy.randomstate optional generator used initialize centers defaults global numpy random number generator 
988: integer given xes seed 
989: see also minibatchkmeansalternative online implementation incremental updates centers positions using minibatches large scale learning say n_samples 10k minibatchkmeans probably much faster default batch implementation 
990: notes kmeans problem solved using lloyds algorithm average complexity given number samples number iteration worst case complexity given k2p n_samples n_features arthur vassilvitskii slow kmeans method socg2006 practice kmeans algorithm fast one fastest clustering algorithms available falls local minima thats useful restart several times 
991: attributes cluster_centers_ array n_clusters n_features labels_ inertia_ oat methods coordinates cluster centers labels point value inertia criterion associated chosen partition 
992: fit fit_predict get_params deep predict score set_params params transform compute kmeans compute cluster centers predict cluster index sample get parameters estimator predict closest cluster sample belongs opposite value kmeans objective set parameters estimator transform data clusterdistance space __init__ initkmeans n_init10 pute_distancestrue verbose0 random_statenone copy_xtrue n_jobs1 max_iter300 tol0.0001 precom fit ynone compute kmeans fit_predict compute cluster centers predict cluster index sample convenience method equivalent calling followed predict 
993: get_params deeptrue get parameters estimator 1.8. reference scikitlearn user guide release 0.11 parameters deep boolean optional true return parameters estimator contained subobjects estimators 
994: predict predict closest cluster sample belongs vector quantization literature cluster_centers_ called code book value returned predict index closest code code book 
995: parameters arraylike sparse matrix shape n_samples n_features new data predict 
996: returns array shape n_samples index closest center sample belongs 
997: score opposite value kmeans objective 
998: parameters arraylike sparse matrix shape n_samples n_features new data 
999: returns score oat opposite value kmeans objective 
1000: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform ynone transform data clusterdistance space new space dimension distance cluster centers note even sparse array returned transform typically dense 
1001: parameters arraylike sparse matrix shape n_samples n_features new data transform 
1002: returns x_new array shape n_samples transformed new space 
1003: sklearn.cluster.minibatchkmeans class sklearn.cluster.minibatchkmeans initkmeans max_iter100 batch_size100 verbose0 compute_labelstrue random_statenone tol0.0 max_no_improvement10 init_sizenone n_init3 chunk_sizenone minibatch kmeans clustering parameters int optional default number clusters form well number centroids generate 
1004: chapter user guide scikitlearn user guide release 0.11 max_iter int optional maximum number iterations complete dataset stopping independently early stopping criterion heuristics 
1005: max_no_improvement int optional control early stopping based consecutive number mini batches yield improvement smoothed inertia disable convergence detection based inertia set max_no_improvement none 
1006: tol oat optional control early stopping based relative center changes measured smoothed variancenormalized mean center squared position changes early stopping heuristics closer one used batch variant algorithms induces slight computational memory overhead inertia heuristic disable convergence detection based normalized center change set tol 0.0 default 
1007: batch_size int optional default size mini batches 
1008: init_size int optional default batch_size number samples randomly sample speeding initialization sometimes expense accurracy algorithm initialized running batch kmeans random subset data needs larger 
1009: init kmeans random ndarray method initialization defaults kmeans kmeans selects initial cluster centers kmean clustering smart way speed convergence see section notes k_init details random choose observations rows random data initial centroids init array used seed centroids compute_labels boolean compute label assignements inertia complete dataset minibatch optimization converged 
1010: random_state integer numpy.randomstate optional generator used initialize centers defaults global numpy random number generator 
1011: integer given xes seed 
1012: notes see http www.eecs.tufts.edudsculleypapersfastkmeans.pdf 1.8. reference scikitlearn user guide release 0.11 attributes cluster_centers_ array n_clusters n_features labels_ inertia_ oat methods coordinates cluster centers labels point compute_labels set true value inertia criterion associated chosen partition compute_labels set true inertia dened sum square distances samples nearest neighbor 
1013: fit fit_predict get_params deep partial_fit predict score set_params params transform compute centroids chunking minibatches compute cluster centers predict cluster index sample get parameters estimator update means estimate single minibatch predict closest cluster sample belongs opposite value kmeans objective set parameters estimator transform data clusterdistance space __init__ initkmeans max_iter100 batch_size100 verbose0 compute_labelstrue n_init3 tol0.0 max_no_improvement10 random_statenone chunk_sizenone init_sizenone fit ynone compute centroids chunking minibatches 
1014: parameters arraylike shape n_samples n_features coordinates data points cluster fit_predict compute cluster centers predict cluster index sample convenience method equivalent calling followed predict 
1015: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1016: partial_fit ynone update means estimate single minibatch 
1017: parameters arraylike shape n_samples n_features coordinates data points cluster 
1018: predict predict closest cluster sample belongs vector quantization literature cluster_centers_ called code book value returned predict index closest code code book 
1019: chapter user guide scikitlearn user guide release 0.11 parameters arraylike sparse matrix shape n_samples n_features new data predict 
1020: returns array shape n_samples index closest center sample belongs 
1021: score opposite value kmeans objective 
1022: parameters arraylike sparse matrix shape n_samples n_features new data 
1023: returns score oat opposite value kmeans objective 
1024: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform ynone transform data clusterdistance space new space dimension distance cluster centers note even sparse array returned transform typically dense 
1025: parameters arraylike sparse matrix shape n_samples n_features new data transform 
1026: returns x_new array shape n_samples transformed new space 
1027: sklearn.cluster.meanshift class sklearn.cluster.meanshift bandwidthnone seedsnone bin_seedingfalse clus meanshift clustering ter_alltrue parameters bandwidth oat optional bandwith used rbf kernel set bandwidth estimated see cluster ing.estimate_bandwidth seeds array n_samples n_features optional set seeds used initialize kernels seeds calculated cluster ing.get_bin_seeds bandwidth grid size default values parame ters 
1028: cluster_all boolean default true true points clustered even orphans within kernel orphans assigned nearest kernel false orphans given cluster label 
1029: 1.8. reference scikitlearn user guide release 0.11 notes scalability implementation uses kernel ball tree look members kernel complexity tnlog lower dimensions number samples number points higher dimensions complexity tend towards tn2 scalability boosted using fewer seeds examply using higher value min_bin_freq get_bin_seeds function note estimate_bandwidth function much less scalable mean shift algorithm bottleneck used 
1030: references dorin comaniciu peter meer mean shift robust approach toward feature space analysis ieee trans actions pattern analysis machine intelligence 2002. 
1031: attributes cluster_centers_ labels_ methods array n_clusters n_features coordinates cluster centers labels point fit get_params deep set_params params compute meanshift get parameters estimator set parameters estimator 
1032: __init__ bandwidthnone seedsnone bin_seedingfalse cluster_alltrue fit compute meanshift parameters array n_samples n_features input points get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self chapter user guide scikitlearn user guide release 0.11 sklearn.cluster.spectralclustering class sklearn.cluster.spectralclustering modenone random_statenone n_init10 apply kmeans projection normalized laplacian practice spectral clustering useful structure individual clusters highly nonconvex generally measure center spread cluster suitable description complete cluster instance clusters nested circles plan afnity adjacency matrix graph method used normalized graph cuts 
1033: parameters integer optional dimension projection subspace 
1034: mode none arpack amg eigenvalue decomposition strategy use amg requires pyamg installed faster large sparse problems may also lead instabilities random_state int seed randomstate instance none default pseudo random number generator used initialization lobpcg eigen vec tors decomposition mode amg kmeans initialization 
1035: n_init int optional default number time kmeans algorithm run different centroid seeds nal results best output n_init consecutive runs terms inertia 
1036: references cuts image normalized http citeseer.ist.psu.eduviewdocsummary doi10.1.1.160.2324 http citeseerx.ist.psu.eduviewdocsummary doi10.1.1.165.9323 segmentation clustering spectral tutorial jianbo shi jitendra malik ulrike von luxburg attributes labels_ labels point methods fit get_params deep set_params params compute spectral clustering afnity matrix get parameters estimator set parameters estimator 
1037: __init__ modenone random_statenone n_init10 fit compute spectral clustering afnity matrix parameters arraylike sparse matrix shape n_samples n_samples afnity matrix describing pairwise similarity data also 1.8. reference scikitlearn user guide release 0.11 jacency matrix graph embed must symmetric entries must positive zero zero means elements nothing common whereas high values mean elements strongly similar 
1038: notes afnity matrix distance matrix means identical elements high values means dissimilar elements transformed similarity matrix well suited algorithm applying gaussian heat kernel np.exp delta another alternative take symmetric version nearest neighbors connectivity matrix points pyamg package installed used greatly speeds computation 
1039: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.cluster.ward class sklearn.cluster.ward n_clusters2 memorymemory cachedirnone connectivitynone ward hierarchical clustering constructs tree cuts 
1040: copytrue n_componentsnone parameters n_clusters int ndarray number clusters 
1041: connectivity sparse matrix 
1042: connectivity matrix denes sample neigbhoring samples following given structure data default none i.e hiearchical clustering algorithm unstructured 
1043: memory instance joblib.memory string used cache output computation tree default caching done string given path caching directory 
1044: copy bool copy connectivity matrix work inplace 
1045: n_components int optional chapter user guide scikitlearn user guide release 0.11 number connected components graph dened connectivity matrix set estimated 
1046: attributes children_ labels_ n_leaves_ arraylike shape n_nodes array n_points int list children nodes leaves tree appear cluster labels point number leaves hiearchical tree 
1047: methods fit get_params deep set_params params fit hierarchical clustering data get parameters estimator set parameters estimator 
1048: __init__ n_clusters2 n_componentsnone memorymemory cachedirnone connectivitynone copytrue fit fit hierarchical clustering data parameters arraylike shape n_samples n_features samples a.k.a observations 
1049: returns self get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self functions cluster.estimate_bandwidth quantile ... cluster.k_means init ... cluster.ward_tree connectivity ... cluster.affinity_propagation convit ... cluster.dbscan eps min_samples ... cluster.mean_shift bandwidth seeds ... cluster.spectral_clustering afnity ... estimate bandwith use meanshift algorithm kmeans clustering algorithm ward clustering based feature matrix perform afnity propagation clustering data perform dbscan clustering vector array distance matrix perform meanshift clustering data using kernel apply kmeans projection normalized laplacian 1.8. reference scikitlearn user guide release 0.11 sklearn.cluster.estimate_bandwidth sklearn.cluster.estimate_bandwidth quantile0.3 n_samplesnone random_state0 estimate bandwith use meanshift algorithm parameters array n_samples n_features input points quantile oat default 0.3 0.5 means median pairwise distances used n_samples int number samples use none samples used 
1050: random_state int randomstate pseudo number generator state used random sampling 
1051: returns bandwidth oat bandwidth parameter sklearn.cluster.k_means sklearn.cluster.k_means initkmeans precompute_distancestrue tol0.0001 n_init10 random_statenone kmeans clustering algorithm 
1052: max_iter300 copy_xtrue n_jobs1 verbosefalse parameters arraylike oats shape n_samples n_features observations cluster 
1053: int number clusters form well number centroids generate 
1054: max_iter int optional default maximum number iterations kmeans algorithm run 
1055: n_init int optional default number time kmeans algorithm run different centroid seeds nal results best output n_init consecutive runs terms inertia 
1056: init kmeans random ndarray callable optional method initialization default kmeans kmeans selects initial cluster centers kmean clustering smart way speed convergence see section notes k_init details random generate centroids gaussian mean variance estimated data ndarray passed shape gives initial centers callable passed take arguments random state return initialization tol oat optional chapter user guide scikitlearn user guide release 0.11 relative increment results declaring convergence 
1057: verbose boolean optional verbosity mode random_state integer numpy.randomstate optional generator used initialize centers defaults global numpy random number generator 
1058: integer given xes seed 
1059: copy_x boolean optional precomputing distances numerically accurate center data rst copy_x true original data modied false original data modied put back function returns small numerical differences may introduced subtracting adding data mean 
1060: n_jobs int number jobs use computation works breaking pairwise matrix n_jobs even slices computing parallel cpus used given parallel computing code used useful debuging n_jobs n_cpus n_jobs used thus n_jobs cpus one used 
1061: returns centroid oat ndarray shape n_features centroids found last iteration kmeans label integer ndarray shape n_samples label code index centroid ith observation closest 
1062: inertia oat nal value inertia criterion sum squared distances closest centroid observations training set 
1063: sklearn.cluster.ward_tree sklearn.cluster.ward_tree connectivitynone n_componentsnone copytrue ward clustering based feature matrix inertia matrix uses heapqbased representation structured version takes account topological structure samples 
1064: parameters array shape n_samples n_features feature matrix representing n_samples samples clustered connectivity sparse matrix 
1065: connectivity matrix denes sample neigbhoring samples following given structure data matrix assumed symmetric upper trian gular half used default none i.e ward algorithm unstructured 
1066: n_components int optional number connected components none number connected components estimated connectivity matrix 
1067: copy bool optional 1.8. reference scikitlearn user guide release 0.11 make copy connectivity work inplace connectivity lil type copy case 
1068: returns children list pairs lenght n_nodes list children nodes leaves tree empty list children 
1069: n_components sparse matrix 
1070: number connected components graph 
1071: n_leaves int number leaves tree sklearn.cluster.afnity_propagation sklearn.cluster.affinity_propagation pnone convit30 max_iter200 damping0.5 perform afnity propagation clustering data copytrue verbosefalse parameters array n_points n_points matrix similarities points array n_points oat optional preferences point points larger values preferences likely chosen exemplars number exemplars clusters inuenced input preferences value preferences passed arguments set median input similarities resulting moderate number clusters smaller amount clusters set minimum value similarities 
1072: damping oat optional damping factor copy boolean optional copy false afnity matrix modied inplace algorithm memory efciency verbose boolean optional verbosity level returns cluster_centers_indices array n_clusters index clusters centers labels array n_points cluster labels point notes see examplesplot_afnity_propagation.py example 
1073: references brendan frey delbert dueck clustering passing messages data points science feb. chapter user guide scikitlearn user guide release 0.11 sklearn.cluster.dbscan sklearn.cluster.dbscan eps0.5 min_samples5 metriceuclidean random_statenone perform dbscan clustering vector array distance matrix 
1074: parameters array n_samples n_samples n_samples n_features array distances samples feature array array treated feature array unless metric given precomputed 
1075: eps oat optional maximum distance two samples considered neighborhood 
1076: min_samples int optional number samples neighborhood point considered core point 
1077: metric string callable metric use calculating distance instances feature array metric string callable must one options allowed met rics.pairwise.calculate_distance metric parameter metric precomputed assumed distance matrix must square 
1078: random_state numpy.randomstate optional generator used initialize centers defaults numpy.random 
1079: returns core_samples array n_core_samples indices core samples 
1080: labels array n_samples cluster labels point noisy samples given label 
1081: notes see examplesplot_dbscan.py example 
1082: references ester kriegel sander densitybased algorithm discovering clusters large spatial databases noise proceedings 2nd international conference knowledge discovery data mining portland aaai press sklearn.cluster.mean_shift sklearn.cluster.mean_shift bandwidthnone seedsnone bin_seedingfalse clus perform meanshift clustering data using kernel seed using binning technique scalability 
1083: ter_alltrue max_iterations300 parameters array n_samples n_features input points 1.8. reference scikitlearn user guide release 0.11 bandwidth oat optional kernel bandwidth bandwidth dened set using heuristic given median pairwise distances seeds array n_seeds n_features point used initial kernel locations bin_seeding boolean true initial kernel locations locations points rather location discretized version points points binned onto grid whose coarseness corresponds bandwidth setting option true speed algorithm fewer seeds initialized default value false ignored seeds argument none min_bin_freq int optional speed algorithm accept bins least min_bin_freq points seeds dened set 
1084: returns cluster_centers array n_clusters n_features coordinates cluster centers labels array n_samples cluster labels point notes see examplesplot_meanshift.py example 
1085: sklearn.cluster.spectral_clustering sklearn.cluster.spectral_clustering afnity n_componentsnone modenone ran dom_statenone n_init10 apply kmeans projection normalized laplacian practice spectral clustering useful structure individual clusters highly nonconvex generally measure center spread cluster suitable description complete cluster instance clusters nested circles plan afnity adjacency matrix graph method used normalized graph cuts 
1086: parameters afnity arraylike sparse matrix shape n_samples n_samples afnity matrix describing relationship samples embed must metric possible examples adjacency matrix graph heat kernel pairwise distance matrix samples symmetic knearest neighbours connectivity matrix samples 
1087: integer optional number clusters extract 
1088: chapter user guide scikitlearn user guide release 0.11 n_components integer optional default number eigen vectors use spectral embedding mode none arpack amg eigenvalue decomposition strategy use amg requires pyamg installed faster large sparse problems may also lead instabilities random_state int seed randomstate instance none default pseudo random number generator used initialization lobpcg eigen vec tors decomposition mode amg kmeans initialization 
1089: n_init int optional default number time kmeans algorithm run different centroid seeds nal results best output n_init consecutive runs terms inertia 
1090: returns labels array integers shape n_samples labels clusters 
1091: centers array integers shape indices cluster centers notes graph contain one connect component elsewhere results make little sense algorithm solves normalized cut normalized spectral clustering 
1092: references cuts image normalized http citeseer.ist.psu.eduviewdocsummary doi10.1.1.160.2324 http citeseerx.ist.psu.eduviewdocsummary doi10.1.1.165.9323 segmentation clustering spectral tutorial jianbo shi jitendra malik ulrike von luxburg 1.8.2 sklearn.covariance covariance estimators sklearn.covariance module includes methods algorithms robustly estimate covariance fea tures given set points precision matrix dened inverse covariance also estimated covariance estimation closely related theory gaussian graphical models user guide see covariance estimation section details 
1093: covariance.empiricalcovariance ... covariance.ellipticenvelope ... covariance.graphlasso alpha mode tol ... covariance.graphlassocv alphas ... covariance.ledoitwolf store_precision ... covariance.mincovdet store_precision ... covariance.oas store_precision ... covariance.shrunkcovariance ... maximum likelihood covariance estimator object detecting outliers gaussian distributed dataset sparse inverse covariance estimation l1penalized estimator sparse inverse covariance crossvalidated choice penality ledoitwolf estimator minimum covariance determinant mcd robust estimator covariance oracle approximating shrinkage estimator covariance estimator shrinkage 1.8. reference scikitlearn user guide release 0.11 sklearn.covariance.empiricalcovariance class sklearn.covariance.empiricalcovariance store_precisiontrue sume_centeredfalse maximum likelihood covariance estimator parameters store_precision bool species estimated precision stored attributes covari ance_ preci sion_ ndarray shape n_features n_features ndarray shape n_features n_features estimated covariance matrix estimated pseudoinverse matrix stored store_precision true methods error_norm comp_cov norm scaling squared computes mean squared error two covariance estimators fit get_params deep mahalanobis observations score x_test assume_centered set_params params fits maximum likelihood estimator covariance model get parameters estimator computes mahalanobis distances given observations computes loglikelihood gaussian data set self.covariance_ estimator covariance matrix set parameters estimator 
1094: __init__ store_precisiontrue assume_centeredfalse parameters store_precision bool specify estimated precision stored assume_centered boolean true data centered computation useful working data whose mean almost exactly zero false data centered computa tion 
1095: error_norm comp_cov normfrobenius scalingtrue squaredtrue computes mean squared error two covariance estimators sense frobenius norm parameters comp_cov arraylike shape n_features n_features covariance compare 
1096: norm str type norm used compute error available error types frobenius fault sqrt at.a spectral sqrt max eigenvalues at.a error comp_cov self.covariance_ 
1097: scaling bool true default squared error norm divided n_features false squared error norm rescaled 
1098: chapter user guide scikitlearn user guide release 0.11 squared bool whether compute squared error norm error norm true default squared error norm returned false error norm returned 
1099: returns mean squared error sense frobenius norm self comp_cov covariance estimators fit fits maximum likelihood estimator covariance model according given training data param eters 
1100: parameters arraylike shape n_samples n_features training data n_samples number samples n_features number features 
1101: returns self object returns self get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1102: mahalanobis observations computes mahalanobis distances given observations provided observations assumed centered one may want center using location estimate rst 
1103: parameters observations arraylike shape n_observations n_features observations mahalanobis distances compute 
1104: returns mahalanobis_distance array shape n_observations mahalanobis distances observations 
1105: score x_test assume_centeredfalse computes loglikelihood gaussian data set self.covariance_ estimator covariance matrix 
1106: parameters x_test arraylike shape n_samples n_features test data compute likelihood n_samples number sam ples n_features number features 
1107: returns res oat likelihood data set self.covariance_ estimator covariance matrix 
1108: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self 1.8. reference scikitlearn user guide release 0.11 sklearn.covariance.ellipticenvelope class sklearn.covariance.ellipticenvelope store_precisiontrue assume_centeredfalse object detecting outliers gaussian distributed dataset 
1109: support_fractionnone contamination0.1 parameters store_precision bool specify estimated precision stored assume_centered boolean true support robust location covariance estimates computed covariance estimate recomputed without centering data useful work data whose mean signicantly equal zero exactly zero false robust location covariance directly computed fastmcd algorithm without additional treatment 
1110: support_fraction oat support_fraction proportion points included support raw mcd estimate default none implies minimum value support_fraction used within algorithm n_sample n_features contamination oat contamination 0.5 amount contamination data set i.e proportion outliers data set 
1111: see also empiricalcovariance mincovdet notes outlier detection covariance estimation may break perform well highdimensional settings particular one always take care work n_samples n_features 
1112: references attributes contamination oat contamination 0.5 location_ arraylike shape n_features covariance_ arraylike shape n_features n_features precision_ arraylike shape n_features n_features support_ arraylike shape n_samples amount contamination data set i.e proportion outliers data set estimated robust location estimated robust covariance matrix estimated pseudo inverse matrix stored store_precision true mask observations used compute robust estimates location shape 
1113: methods chapter user guide scikitlearn user guide release 0.11 apply correction raw minimum covariance determinant estimates compute decision function given observations 
1114: correct_covariance data decision_function raw_mahalanobis error_norm comp_cov norm scaling squared computes mean squared error two covariance estimators fit get_params deep mahalanobis observations predict reweight_covariance data score set_params params get parameters estimator computes mahalanobis distances given observations outlyingness observations according tted model reweight raw minimum covariance determinant estimates returns mean accuracy given test data labels set parameters estimator 
1115: __init__ store_precisiontrue tion0.1 correct_covariance data assume_centeredfalse support_fractionnone contamina apply correction raw minimum covariance determinant estimates correction using empirical correction factor suggested rousseeuw van driessen rouseeuw1984 
1116: parameters data arraylike shape n_samples n_features data matrix features samples data set must one used compute raw estimates 
1117: returns covariance_corrected arraylike shape n_features n_features corrected robust covariance estimate 
1118: decision_function raw_mahalanobisfalse compute decision function given observations 
1119: parameters arraylike shape n_samples n_features raw_mahalanobis bool whether consider raw mahalanobis distances decision function must false default compatibility others outlier detection tools 
1120: returns decision arraylike shape n_samples values decision function observations equal mahalanobis distances raw_mahalanobis true default raw_mahalanobistrue equal cubic root shifted mahalanobis distances case threshold outlier ensures compatibility outlier detection tools oneclass svm 
1121: error_norm comp_cov normfrobenius scalingtrue squaredtrue computes mean squared error two covariance estimators sense frobenius norm parameters comp_cov arraylike shape n_features n_features covariance compare 
1122: norm str type norm used compute error available error types frobenius fault sqrt at.a spectral sqrt max eigenvalues at.a error comp_cov self.covariance_ 
1123: 1.8. reference scikitlearn user guide release 0.11 scaling bool true default squared error norm divided n_features false squared error norm rescaled 
1124: squared bool whether compute squared error norm error norm true default squared error norm returned false error norm returned 
1125: returns mean squared error sense frobenius norm self comp_cov covariance estimators fit get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1126: mahalanobis observations computes mahalanobis distances given observations provided observations assumed centered one may want center using location estimate rst 
1127: parameters observations arraylike shape n_observations n_features observations mahalanobis distances compute 
1128: returns mahalanobis_distance array shape n_observations mahalanobis distances observations 
1129: predict outlyingness observations according tted model 
1130: parameters arraylike shape n_samples n_features returns is_outliers array shape n_samples dtype bool observations tells whether considered outlier accord ing tted model 
1131: threshold oat values less outlying points decision function 
1132: reweight_covariance data reweight raw minimum covariance determinant estimates reweight observations using rousseeuws method equivalent deleting outlying observations data set computing location covariance estimates rouseeuw1984 parameters data arraylike shape n_samples n_features data matrix features samples data set must one used compute raw estimates 
1133: returns location_reweighted arraylike shape n_features reweighted robust location estimate 
1134: covariance_reweighted arraylike shape n_features n_features chapter user guide scikitlearn user guide release 0.11 reweighted robust covariance estimate 
1135: support_reweighted arraylike type boolean shape n_samples mask observations used compute reweighted robust loca tion covariance estimates 
1136: score returns mean accuracy given test data labels 
1137: parameters arraylike shape n_samples n_features training set 
1138: arraylike shape n_samples labels 
1139: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.covariance.graphlasso class sklearn.covariance.graphlasso alpha0.01 modecd tol0.0001 max_iter100 ver sparse inverse covariance estimation l1penalized estimator 
1140: bosefalse parameters alpha positive oat optional regularization parameter higher alpha regularization sparser inverse covariance cov_init array n_features n_features optional initial guess covariance mode lars lasso solver use coordinate descent lars use lars sparse derlying graphs elsewhere prefer numerically stable 
1141: tol positive oat optional tolerance declare convergence dual gap goes value iterations stopped max_iter integer optional maximum number iterations verbose boolean optional verbose true objective function dual gap plotted iteration see also graph_lasso graphlassocv 1.8. reference scikitlearn user guide release 0.11 attributes covariance_ precision_ arraylike shape n_features n_features arraylike shape n_features n_features estimated covariance matrix estimated pseudo inverse matrix 
1142: methods error_norm comp_cov norm scaling squared computes mean squared error two covariance estimators fit get_params deep mahalanobis observations score x_test assume_centered set_params params get parameters estimator computes mahalanobis distances given observations computes loglikelihood gaussian data set self.covariance_ estimator covariance matrix set parameters estimator 
1143: __init__ alpha0.01 modecd tol0.0001 max_iter100 verbosefalse error_norm comp_cov normfrobenius scalingtrue squaredtrue computes mean squared error two covariance estimators sense frobenius norm parameters comp_cov arraylike shape n_features n_features covariance compare 
1144: norm str type norm used compute error available error types frobenius fault sqrt at.a spectral sqrt max eigenvalues at.a error comp_cov self.covariance_ 
1145: scaling bool true default squared error norm divided n_features false squared error norm rescaled 
1146: squared bool whether compute squared error norm error norm true default squared error norm returned false error norm returned 
1147: returns mean squared error sense frobenius norm self comp_cov covariance estimators get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1148: mahalanobis observations computes mahalanobis distances given observations provided observations assumed centered one may want center using location estimate rst 
1149: parameters observations arraylike shape n_observations n_features chapter user guide scikitlearn user guide release 0.11 observations mahalanobis distances compute 
1150: returns mahalanobis_distance array shape n_observations mahalanobis distances observations 
1151: score x_test assume_centeredfalse computes loglikelihood gaussian data set self.covariance_ estimator covariance matrix 
1152: parameters x_test arraylike shape n_samples n_features test data compute likelihood n_samples number sam ples n_features number features 
1153: returns res oat likelihood data set self.covariance_ estimator covariance matrix 
1154: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.covariance.graphlassocv class sklearn.covariance.graphlassocv alphas4 n_renements4 sparse inverse covariance crossvalidated choice penality max_iter100 modecd n_jobs1 verbosefalse cvnone tol0.0001 parameters alphas integer list positive oat optional integer given xes number points grids alpha used list given gives grid used see notes class docstring details 
1155: n_renements strictly positive integer number time grid rened used explicit values alphas passed 
1156: crossvalidation generator optional see sklearn.cross_validation module none passed default 3fold strategy tol positive oat optional tolerance declare convergence dual gap goes value iterations stopped max_iter integer optional maximum number iterations mode lars lasso solver use coordinate descent lars use lars sparse derlying graphs elsewhere prefer numerically stable 
1157: n_jobs int optional 1.8. reference scikitlearn user guide release 0.11 number jobs run parallel default verbose boolean optional verbose true objective function dual gap print iteration see also graph_lasso graphlasso notes search optimal alpha done iteratively rened grid rst crossvalidated scores grid computed new rened grid center around maximum ... one challenges face solvers fail converge wellconditioned estimate corresponding values alpha come missing values optimum may close missing values 
1158: attributes covariance_ precision_ alpha_ oat cv_alphas_ list oat cv_scores array n_alphas n_folds methods arraylike shape n_features n_features arraylike shape n_features n_features estimated covariance matrix estimated precision matrix inverse covariance penalization parameter selected penalization parameters explored loglikelihood score leftout data across folds 
1159: error_norm comp_cov norm scaling squared computes mean squared error two covariance estimators fit get_params deep mahalanobis observations score x_test assume_centered set_params params get parameters estimator computes mahalanobis distances given observations computes loglikelihood gaussian data set self.covariance_ estimator covariance matrix set parameters estimator 
1160: __init__ alphas4 n_renements4 cvnone tol0.0001 max_iter100 modecd n_jobs1 verbosefalse error_norm comp_cov normfrobenius scalingtrue squaredtrue computes mean squared error two covariance estimators sense frobenius norm parameters comp_cov arraylike shape n_features n_features covariance compare 
1161: norm str type norm used compute error available error types frobenius fault sqrt at.a spectral sqrt max eigenvalues at.a error comp_cov self.covariance_ 
1162: chapter user guide scikitlearn user guide release 0.11 scaling bool true default squared error norm divided n_features false squared error norm rescaled 
1163: squared bool whether compute squared error norm error norm true default squared error norm returned false error norm returned 
1164: returns mean squared error sense frobenius norm self comp_cov covariance estimators get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1165: mahalanobis observations computes mahalanobis distances given observations provided observations assumed centered one may want center using location estimate rst 
1166: parameters observations arraylike shape n_observations n_features observations mahalanobis distances compute 
1167: returns mahalanobis_distance array shape n_observations mahalanobis distances observations 
1168: score x_test assume_centeredfalse computes loglikelihood gaussian data set self.covariance_ estimator covariance matrix 
1169: parameters x_test arraylike shape n_samples n_features test data compute likelihood n_samples number sam ples n_features number features 
1170: returns res oat likelihood data set self.covariance_ estimator covariance matrix 
1171: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.covariance.ledoitwolf class sklearn.covariance.ledoitwolf store_precisiontrue assume_centeredfalse ledoitwolf estimator 1.8. reference scikitlearn user guide release 0.11 ledoitwolf particular form shrinkage shrinkage coefcient computed using o.ledoit m.wolfs formula described wellconditioned estimator largedimensional covariance matrices ledoit wolf journal multivariate analysis volume issue february pages 
1172: parameters store_precision bool specify estimated precision stored notes regularised covariance shrinkage cov shrinkagemunp.identity n_features trace cov n_features shinkage given ledoit wolf formula see references references wellconditioned estimator largedimensional covariance matrices ledoit wolf journal mul tivariate analysis volume issue february pages 
1173: attributes covariance_ precision_ shrinkage_ oat shrinkage methods arraylike shape n_features n_features arraylike shape n_features n_features estimated covariance matrix estimated pseudo inverse matrix stored store_precision true coefcient convex combination used computation shrunk estimate 
1174: error_norm comp_cov norm scaling squared computes mean squared error two covariance estimators fit assume_centered get_params deep mahalanobis observations score x_test assume_centered set_params params fits ledoitwolf shrunk covariance model get parameters estimator computes mahalanobis distances given observations computes loglikelihood gaussian data set self.covariance_ estimator covariance matrix set parameters estimator 
1175: __init__ store_precisiontrue assume_centeredfalse parameters store_precision bool specify estimated precision stored assume_centered boolean true data centered computation useful working data whose mean almost exactly zero false data centered computa tion 
1176: error_norm comp_cov normfrobenius scalingtrue squaredtrue chapter user guide scikitlearn user guide release 0.11 computes mean squared error two covariance estimators sense frobenius norm parameters comp_cov arraylike shape n_features n_features covariance compare 
1177: norm str type norm used compute error available error types frobenius fault sqrt at.a spectral sqrt max eigenvalues at.a error comp_cov self.covariance_ 
1178: scaling bool true default squared error norm divided n_features false squared error norm rescaled 
1179: squared bool whether compute squared error norm error norm true default squared error norm returned false error norm returned 
1180: returns mean squared error sense frobenius norm self comp_cov covariance estimators fit assume_centeredfalse fits ledoitwolf shrunk covariance model according given training data parameters 
1181: parameters arraylike shape n_samples n_features training data n_samples number samples n_features number features 
1182: assume_centered boolean true data centered computation usefull work data whose mean signicantly equal zero exactly zero false data centered computation 
1183: returns self object returns self get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1184: mahalanobis observations computes mahalanobis distances given observations provided observations assumed centered one may want center using location estimate rst 
1185: parameters observations arraylike shape n_observations n_features observations mahalanobis distances compute 
1186: returns mahalanobis_distance array shape n_observations mahalanobis distances observations 
1187: 1.8. reference scikitlearn user guide release 0.11 score x_test assume_centeredfalse computes loglikelihood gaussian data set self.covariance_ estimator covariance matrix 
1188: parameters x_test arraylike shape n_samples n_features test data compute likelihood n_samples number sam ples n_features number features 
1189: returns res oat likelihood data set self.covariance_ estimator covariance matrix 
1190: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.covariance.mincovdet class sklearn.covariance.mincovdet store_precisiontrue assume_centeredfalse sup minimum covariance determinant mcd robust estimator covariance port_fractionnone random_statenone parameters store_precision bool specify estimated precision stored assume_centered boolean true support robust location covariance estimates computed covariance estimate recomputed without centering data useful work data whose mean signicantly equal zero exactly zero false robust location covariance directly computed fastmcd algorithm without additional treatment 
1191: support_fraction oat support_fraction proportion points included support raw mcd estimate default none implies minimum value support_fraction used within algorithm n_sample n_features random_state integer numpy.randomstate optional random generator used integer given xes seed defaults global numpy random number generator 
1192: references rouseeuw1984 rouseeuw1999 butler1993 chapter user guide scikitlearn user guide release 0.11 raw robust estimated location correction reweighting raw robust estimated covariance correction reweighting mask observations used compute raw robust estimates location shape correction reweighting estimated robust location estimated robust covariance matrix estimated pseudo inverse matrix stored store_precision true mask observations used compute robust estimates location shape 
1193: attributes raw_location_ arraylike shape n_features raw_covariance_ arraylike shape n_features n_features raw_support_ arraylike shape n_samples location_ arraylike shape n_features covariance_ arraylike shape n_features n_features precision_ arraylike shape n_features n_features support_ arraylike shape n_samples methods correct_covariance data apply correction raw minimum covariance determinant estimates error_norm comp_cov norm scaling squared computes mean squared error two covariance estimators fits minimum covariance determinant fastmcd algorithm fit get parameters estimator get_params deep mahalanobis observations computes mahalanobis distances given observations reweight raw minimum covariance determinant estimates reweight_covariance data computes loglikelihood gaussian data set self.covariance_ estimator covariance matrix score x_test assume_centered set_params params set parameters estimator 
1194: __init__ store_precisiontrue dom_statenone correct_covariance data assume_centeredfalse support_fractionnone ran apply correction raw minimum covariance determinant estimates correction using empirical correction factor suggested rousseeuw van driessen rouseeuw1984 
1195: parameters data arraylike shape n_samples n_features data matrix features samples data set must one used compute raw estimates 
1196: returns covariance_corrected arraylike shape n_features n_features corrected robust covariance estimate 
1197: error_norm comp_cov normfrobenius scalingtrue squaredtrue computes mean squared error two covariance estimators sense frobenius norm parameters comp_cov arraylike shape n_features n_features covariance compare 
1198: norm str 1.8. reference scikitlearn user guide release 0.11 type norm used compute error available error types frobenius fault sqrt at.a spectral sqrt max eigenvalues at.a error comp_cov self.covariance_ 
1199: scaling bool true default squared error norm divided n_features false squared error norm rescaled 
1200: squared bool whether compute squared error norm error norm true default squared error norm returned false error norm returned 
1201: returns mean squared error sense frobenius norm self comp_cov covariance estimators fit fits minimum covariance determinant fastmcd algorithm 
1202: parameters arraylike shape n_samples n_features training data n_samples number samples n_features number features 
1203: returns self object returns self get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1204: mahalanobis observations computes mahalanobis distances given observations provided observations assumed centered one may want center using location estimate rst 
1205: parameters observations arraylike shape n_observations n_features observations mahalanobis distances compute 
1206: returns mahalanobis_distance array shape n_observations mahalanobis distances observations 
1207: reweight_covariance data reweight raw minimum covariance determinant estimates reweight observations using rousseeuws method equivalent deleting outlying observations data set computing location covariance estimates rouseeuw1984 parameters data arraylike shape n_samples n_features data matrix features samples data set must one used compute raw estimates 
1208: returns location_reweighted arraylike shape n_features reweighted robust location estimate 
1209: chapter user guide scikitlearn user guide release 0.11 covariance_reweighted arraylike shape n_features n_features reweighted robust covariance estimate 
1210: support_reweighted arraylike type boolean shape n_samples mask observations used compute reweighted robust loca tion covariance estimates 
1211: score x_test assume_centeredfalse computes loglikelihood gaussian data set self.covariance_ estimator covariance matrix 
1212: parameters x_test arraylike shape n_samples n_features test data compute likelihood n_samples number sam ples n_features number features 
1213: returns res oat likelihood data set self.covariance_ estimator covariance matrix 
1214: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.covariance.oas class sklearn.covariance.oas store_precisiontrue assume_centeredfalse oracle approximating shrinkage estimator oas particular form shrinkage described shrinkage algorithms mmse covariance estimation chen al. ieee trans sign proc. volume issue october 2010. formula used correspond one given article taken matlab programm available authors webpage https tbayes.eecs.umich.eduyiluncovestimation 
1215: parameters store_precision bool specify estimated precision stored notes regularised covariance shrinkage cov shrinkagemunp.identity n_features trace cov n_features shinkage given oas formula see references references shrinkage algorithms mmse covariance estimation chen al. ieee trans sign proc. volume issue october 
1216: 1.8. reference scikitlearn user guide release 0.11 attributes covariance_ precision_ shrinkage_ oat shrinkage methods arraylike shape n_features n_features arraylike shape n_features n_features estimated covariance matrix estimated pseudo inverse matrix stored store_precision true coefcient convex combination used computation shrunk estimate 
1217: error_norm comp_cov norm scaling squared computes mean squared error two covariance estimators fit assume_centered get_params deep mahalanobis observations score x_test assume_centered set_params params fits oracle approximating shrinkage covariance model get parameters estimator computes mahalanobis distances given observations computes loglikelihood gaussian data set self.covariance_ estimator covariance matrix set parameters estimator 
1218: __init__ store_precisiontrue assume_centeredfalse parameters store_precision bool specify estimated precision stored assume_centered boolean true data centered computation useful working data whose mean almost exactly zero false data centered computa tion 
1219: error_norm comp_cov normfrobenius scalingtrue squaredtrue computes mean squared error two covariance estimators sense frobenius norm parameters comp_cov arraylike shape n_features n_features covariance compare 
1220: norm str type norm used compute error available error types frobenius fault sqrt at.a spectral sqrt max eigenvalues at.a error comp_cov self.covariance_ 
1221: scaling bool true default squared error norm divided n_features false squared error norm rescaled 
1222: squared bool whether compute squared error norm error norm true default squared error norm returned false error norm returned 
1223: returns mean squared error sense frobenius norm self comp_cov covariance estimators chapter user guide scikitlearn user guide release 0.11 fit assume_centeredfalse fits oracle approximating shrinkage covariance model according given training data rameters 
1224: parameters arraylike shape n_samples n_features training data n_samples number samples n_features number features 
1225: assume_centered boolean true data centered computation usefull work data whose mean signicantly equal zero exactly zero false data centered computation 
1226: returns self object returns self get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1227: mahalanobis observations computes mahalanobis distances given observations provided observations assumed centered one may want center using location estimate rst 
1228: parameters observations arraylike shape n_observations n_features observations mahalanobis distances compute 
1229: returns mahalanobis_distance array shape n_observations mahalanobis distances observations 
1230: score x_test assume_centeredfalse computes loglikelihood gaussian data set self.covariance_ estimator covariance matrix 
1231: parameters x_test arraylike shape n_samples n_features test data compute likelihood n_samples number sam ples n_features number features 
1232: returns res oat likelihood data set self.covariance_ estimator covariance matrix 
1233: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self 1.8. reference scikitlearn user guide release 0.11 sklearn.covariance.shrunkcovariance class sklearn.covariance.shrunkcovariance store_precisiontrue shrinkage0.1 covariance estimator shrinkage parameters store_precision bool specify estimated precision stored shrinkage oat shrinkage coefcient convex combination used computation shrunk estimate 
1234: notes regularized covariance given shrinkage cov shrinkagemunp.identity n_features trace cov n_features attributes covariance_ precision_ shrinkage oat shrinkage methods arraylike shape n_features n_features arraylike shape n_features n_features estimated covariance matrix estimated pseudo inverse matrix stored store_precision true coefcient convex combination used computation shrunk estimate 
1235: error_norm comp_cov norm scaling squared computes mean squared error two covariance estimators fit assume_centered get_params deep mahalanobis observations score x_test assume_centered set_params params fits shrunk covariance model according given training data parameters get parameters estimator computes mahalanobis distances given observations computes loglikelihood gaussian data set self.covariance_ estimator covariance matrix set parameters estimator 
1236: __init__ store_precisiontrue shrinkage0.1 error_norm comp_cov normfrobenius scalingtrue squaredtrue computes mean squared error two covariance estimators sense frobenius norm parameters comp_cov arraylike shape n_features n_features covariance compare 
1237: norm str type norm used compute error available error types frobenius fault sqrt at.a spectral sqrt max eigenvalues at.a error chapter user guide scikitlearn user guide release 0.11 comp_cov self.covariance_ 
1238: scaling bool true default squared error norm divided n_features false squared error norm rescaled 
1239: squared bool whether compute squared error norm error norm true default squared error norm returned false error norm returned 
1240: returns mean squared error sense frobenius norm self comp_cov covariance estimators fit assume_centeredfalse fits shrunk covariance model according given training data parameters 
1241: parameters arraylike shape n_samples n_features training data n_samples number samples n_features number features 
1242: assume_centered boolean true data centered computation usefull work data whose mean signicantly equal zero exactly zero false data centered computation 
1243: returns self object returns self get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1244: mahalanobis observations computes mahalanobis distances given observations provided observations assumed centered one may want center using location estimate rst 
1245: parameters observations arraylike shape n_observations n_features observations mahalanobis distances compute 
1246: returns mahalanobis_distance array shape n_observations mahalanobis distances observations 
1247: score x_test assume_centeredfalse computes loglikelihood gaussian data set self.covariance_ estimator covariance matrix 
1248: parameters x_test arraylike shape n_samples n_features test data compute likelihood n_samples number sam ples n_features number features 
1249: returns res oat 1.8. reference scikitlearn user guide release 0.11 likelihood data set self.covariance_ estimator covariance matrix 
1250: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self computes maximum likelihood covariance estimator covariance.empirical_covariance ... covariance.ledoit_wolf assume_centered estimates shrunk ledoitwolf covariance matrix covariance.shrunk_covariance emp_cov ... calculates covariance matrix shrunk diagonal covariance.oas assume_centered covariance.graph_lasso emp_cov alpha ... estimate covariance oracle approximating shrinkage algorithm l1penalized covariance estimator sklearn.covariance.empirical_covariance sklearn.covariance.empirical_covariance assume_centeredfalse computes maximum likelihood covariance estimator parameters ndarray shape n_samples n_features data compute covariance estimate assume_centered boolean true data centered computation useful working data whose mean almost exactly zero false data centered computa tion 
1251: returns covariance ndarray shape n_features n_features empirical covariance maximum likelihood estimator sklearn.covariance.ledoit_wolf sklearn.covariance.ledoit_wolf assume_centeredfalse estimates shrunk ledoitwolf covariance matrix 
1252: parameters arraylike shape n_samples n_features data compute covariance estimate assume_centered boolean true data centered computation usefull work data whose mean signicantly equal zero exactly zero false data centered computation 
1253: returns shrunk_cov arraylike shape n_features n_features shrunk covariance shrinkage oat coefcient convex combination used computation shrunk estimate 
1254: chapter user guide scikitlearn user guide release 0.11 notes regularised shrunk covariance shrinkage cov shrinkage np.identity n_features trace cov n_features sklearn.covariance.shrunk_covariance sklearn.covariance.shrunk_covariance emp_cov shrinkage0.1 calculates covariance matrix shrunk diagonal parameters emp_cov arraylike shape n_features n_features covariance matrix shrunk shrinkage oat shrinkage coefcient convex combination used computation shrunk estimate 
1255: returns shrunk_cov arraylike shrunk covariance notes regularized shrunk covariance given shrinkage cov shrinkagemunp.identity n_features trace cov n_features sklearn.covariance.oas sklearn.covariance.oas assume_centeredfalse estimate covariance oracle approximating shrinkage algorithm 
1256: parameters arraylike shape n_samples n_features data compute covariance estimate assume_centered boolean true data centered computation usefull work data whose mean signicantly equal zero exactly zero false data centered computation 
1257: returns shrunk_cov arraylike shape n_features n_features shrunk covariance shrinkage oat coefcient convex combination used computation shrunk estimate 
1258: 1.8. reference scikitlearn user guide release 0.11 notes regularised shrunk covariance shrinkage cov shrinkage np.identity n_features trace cov n_features sklearn.covariance.graph_lasso sklearn.covariance.graph_lasso emp_cov alpha max_iter100 eps2.2204460492503131e16 cov_initnone modecd verbosefalse tol0.0001 return_costsfalse l1penalized covariance estimator parameters emp_cov ndarray shape n_features n_features empirical covariance compute covariance estimate alpha positive oat regularization parameter higher alpha regularization sparser inverse covariance cov_init array n_features n_features optional initial guess covariance mode lars lasso solver use coordinate descent lars use lars sparse derlying graphs elsewhere prefer numerically stable 
1259: tol positive oat optional tolerance declare convergence dual gap goes value iterations stopped max_iter integer optional maximum number iterations verbose boolean optional verbose true objective function dual gap printed iteration return_costs boolean optional return_costs true objective function dual gap iteration returned eps oat optional machineprecision regularization computation cholesky diagonal fac tors increase illconditioned systems 
1260: returns covariance ndarray shape n_features n_features estimated covariance matrix precision ndarray shape n_features n_features estimated sparse precision matrix costs list objective dual_gap pairs chapter user guide scikitlearn user guide release 0.11 list values objective function dual gap iteration returned return_costs true see also graphlasso graphlassocv notes algorithm employed solve problem glasso algorithm friedman biostatistics paper algorithm glasso package one possible difference glasso package diagonal coefcients penalized 
1261: 1.8.3 sklearn.cross_validation cross validation sklearn.cross_validation module includes utilities cross validation performance evaluation user guide see crossvalidation evaluating estimator performance section details 
1262: cross_validation.bootstrap ... cross_validation.kfold indices ... cross_validation.leaveonelabelout labels ... cross_validation.leaveoneout indices cross_validation.leaveplabelout labels ... cross_validation.leavepout indices cross_validation.stratifiedkfold indices cross_validation.shufflesplit ... cross_validation.stratifiedshufflesplit ... random sampling replacement crossvalidation iterator kfolds cross validation iterator leaveonelabel_out crossvalidation iterator leaveoneout cross validation iterator leaveplabel_out crossvalidation iterator leavepout cross validation iterator stratied kfolds cross validation iterator random permutation crossvalidation iterator stratied shufesplit cross validation iterator sklearn.cross_validation.bootstrap class sklearn.cross_validation.bootstrap n_bootstraps3 train_size0.5 test_sizenone n_trainnone n_testnone random_statenone random sampling replacement crossvalidation iterator provides traintest indices split data train test sets resampling input n_bootstraps times time new random split data performed samples drawn replacement side split build training test sets note contrary crossvalidation strategies bootstrapping allow samples occur several times splits however sample occurs train split never occur test split viceversa want sample occur probably use shufesplit cross validation instead 
1263: parameters int total number elements dataset 
1264: n_bootstraps int default number bootstrapping iterations train_size int oat default 0.5 int number samples include training split smaller total number samples passed dataset 
1265: 1.8. reference scikitlearn user guide release 0.11 oat 0.0 1.0 represent proportion dataset include train split 
1266: test_size int oat none default none int number samples include training set smaller total number samples passed dataset oat 0.0 1.0 represent proportion dataset include test split none n_test set complement n_train 
1267: random_state int randomstate pseudo number generator state used random sampling 
1268: see also shufflesplitcross validation using random permutations 
1269: examples sklearn import cross_validation cross_validation.bootstrap random_state0 len print bootstrap n_bootstraps3 train_size5 test_size4 random_state0 train_index test_index ... ... train test train test train test print train train_index test test_index __init__ n_bootstraps3 dom_statenone train_size0.5 test_sizenone n_trainnone n_testnone ran sklearn.cross_validation.kfold class sklearn.cross_validation.kfold indicestrue shufefalse random_statenone kfolds cross validation iterator provides traintest indices split data train test sets split dataset consecutive folds without shufing fold used validation set remaining fold form training set 
1270: parameters int total number elements int number folds indices boolean optional default true return traintest split arrays indices rather boolean mask array integer indices required dealing sparse matrices since indexed boolean masks 
1271: chapter user guide scikitlearn user guide release 0.11 shufe boolean optional whether shufe data splitting batches random_state int randomstate pseudo number generator state used random sampling 
1272: see also stratifiedkfoldtake label information account avoid building folds classification notes folds size trunc n_samples n_folds last one complementary 
1273: examples sklearn import cross_validation np.array np.array cross_validation.kfold len print sklearn.cross_validation.kfold train_index test_index ... ... ... train test train test print train train_index test test_index x_train x_test train_index test_index y_train y_test train_index test_index __init__ indicestrue shufefalse random_statenone sklearn.cross_validation.leaveonelabelout class sklearn.cross_validation.leaveonelabelout labels indicestrue leaveonelabel_out crossvalidation iterator provides traintest indices split data according thirdparty provided label label information used encode arbitrary domain specic stratications samples integers instance labels could year collection samples thus allow crossvalidation timebased splits 
1274: parameters labels arraylike int shape n_samples arbitrary domainspecic stratication data used draw splits 
1275: indices boolean optional default true return traintest split arrays indices rather boolean mask array integer indices required dealing sparse matrices since indexed boolean masks 
1276: 1.8. reference scikitlearn user guide release 0.11 examples sklearn import cross_validation np.array np.array labels np.array lol cross_validation.leaveonelabelout labels len lol print lol sklearn.cross_validation.leaveonelabelout labels train_index test_index lol ... ... ... ... train test print train train_index test test_index x_train x_test train_index test_index y_train y_test train_index test_index print x_train x_test y_train y_test train test __init__ labels indicestrue sklearn.cross_validation.leaveoneout class sklearn.cross_validation.leaveoneout indicestrue leaveoneout cross validation iterator provides traintest indices split data train test sets sample used test set singleton remaining samples form training set due high number test sets number samples cross validation method costly large datasets one favor kfold stratiedkfold shufesplit 
1277: parameters int total number elements indices boolean optional default true return traintest split arrays indices rather boolean mask array integer indices required dealing sparse matrices since indexed boolean masks 
1278: see also leaveonelabelout domainspecific examples sklearn import cross_validation np.array np.array loo cross_validation.leaveoneout chapter user guide scikitlearn user guide release 0.11 len loo print loo sklearn.cross_validation.leaveoneout train_index test_index loo ... ... ... ... train test train test print train train_index test test_index x_train x_test train_index test_index y_train y_test train_index test_index print x_train x_test y_train y_test __init__ indicestrue sklearn.cross_validation.leaveplabelout class sklearn.cross_validation.leaveplabelout labels indicestrue leaveplabel_out crossvalidation iterator provides traintest indices split data according thirdparty provided label label information used encode arbitrary domain specic stratications samples integers instance labels could year collection samples thus allow crossvalidation timebased splits difference leaveplabelout leaveonelabelout former builds test sets samples assigned different values labels latter uses samples assigned labels 
1279: parameters labels arraylike int shape n_samples arbitrary domainspecic stratication data used draw splits 
1280: int number samples leave test split 
1281: indices boolean optional default true return traintest split arrays indices rather boolean mask array integer indices required dealing sparse matrices since indexed boolean masks 
1282: examples sklearn import cross_validation np.array np.array labels np.array lpl cross_validation.leaveplabelout labels len lpl print lpl sklearn.cross_validation.leaveplabelout labels train_index test_index lpl ... 
1283: print train train_index test test_index x_train x_test train_index test_index 1.8. reference scikitlearn user guide release 0.11 y_train y_test train_index test_index print x_train x_test y_train y_test ... ... train test train test train test __init__ labels indicestrue sklearn.cross_validation.leavepout class sklearn.cross_validation.leavepout indicestrue leavepout cross validation iterator provides traintest indices split data train test sets test set built using samples remaining samples form training set due high number iterations grows number samples cross validation method costly large datasets one favor kfold stratiedkfold shufesplit 
1284: parameters int total number elements int size test sets indices boolean optional default true return traintest split arrays indices rather boolean mask array integer indices required dealing sparse matrices since indexed boolean masks 
1285: examples sklearn import cross_validation np.array np.array lpo cross_validation.leavepout len lpo print lpo sklearn.cross_validation.leavepout train_index test_index lpo ... ... ... train test train test train test train test print train train_index test test_index x_train x_test train_index test_index y_train y_test train_index test_index chapter user guide scikitlearn user guide release 0.11 train test train test __init__ indicestrue sklearn.cross_validation.stratiedkfold class sklearn.cross_validation.stratifiedkfold indicestrue stratied kfolds cross validation iterator provides traintest indices split data train test sets crossvalidation object variation kfold returns stratied folds folds made preserving percentage samples class 
1286: parameters array n_samples samples split folds int number folds indices boolean optional default true return traintest split arrays indices rather boolean mask array integer indices required dealing sparse matrices since indexed boolean masks 
1287: notes folds size trunc n_samples n_folds last one complementary 
1288: examples sklearn import cross_validation np.array np.array skf cross_validation.stratifiedkfold len skf print skf sklearn.cross_validation.stratifiedkfold labels train_index test_index skf ... ... ... train test train test print train train_index test test_index x_train x_test train_index test_index y_train y_test train_index test_index __init__ indicestrue 1.8. reference scikitlearn user guide release 0.11 sklearn.cross_validation.shufesplit class sklearn.cross_validation.shufflesplit n_iterations10 train_sizenone dom_statenone train_fractionnone indicestrue test_size0.1 ran test_fractionnone random permutation crossvalidation iterator yields indices split data training test sets note contrary crossvalidation strategies random splits guarantee folds different although still likely sizeable datasets 
1289: parameters int total number elements dataset 
1290: n_iterations int default number reshufing splitting iterations 
1291: test_size oat default 0.1 int oat 0.0 1.0 represent proportion dataset include test split int represents absolute number test samples 
1292: train_size oat int none default none oat 0.0 1.0 represent proportion dataset include train split int represents absolute number train samples none value automatically set complement test fraction 
1293: indices boolean optional default true return traintest split arrays indices rather boolean mask array integer indices required dealing sparse matrices since indexed boolean masks 
1294: random_state int randomstate pseudorandom number generator state used random sampling 
1295: see also bootstrapcrossvalidation using resampling replacement 
1296: examples test_size.25 random_state0 sklearn import cross_validation cross_validation.shufflesplit n_iterations3 ... len print ... shufflesplit n_iterations3 test_size0.25 indicestrue ... train_index test_index ... ... train test print train train_index test test_index chapter user guide scikitlearn user guide release 0.11 train test train test train_size0.5 test_size.25 random_state0 cross_validation.shufflesplit n_iterations3 ... train_index test_index ... ... train test train test train test print train train_index test test_index __init__ n_iterations10 test_size0.1 train_sizenone indicestrue random_statenone test_fractionnone train_fractionnone sklearn.cross_validation.stratiedshufesplit class sklearn.cross_validation.stratifiedshufflesplit n_iterations10 test_size0.1 indicestrue train_sizenone random_statenone stratied shufesplit cross validation iterator provides traintest indices split data train test sets crossvalidation object merge stratiedkfold shufesplit returns stratied randomized folds folds made preserving percentage samples class note like shufesplit strategy stratied random splits guarantee folds different although still likely sizeable datasets 
1297: parameters array n_samples labels samples 
1298: n_iterations int default number reshufing splitting iterations 
1299: test_size oat default 0.1 int oat 0.0 1.0 represent proportion dataset include test split int represents absolute number test samples 
1300: train_size oat int none default none oat 0.0 1.0 represent proportion dataset include train split int represents absolute number train samples none value automatically set complement test fraction 
1301: indices boolean optional default true return traintest split arrays indices rather boolean mask array integer indices required dealing sparse matrices since indexed boolean masks 
1302: examples 1.8. reference scikitlearn user guide release 0.11 sklearn.cross_validation import stratifiedshufflesplit np.array np.array sss stratifiedshufflesplit test_size0.5 random_state0 len sss print sss stratifiedshufflesplit labels n_iterations3 ... train_index test_index sss ... ... ... train test train test train test print train train_index test test_index x_train x_test train_index test_index y_train y_test train_index test_index __init__ n_iterations10 test_size0.1 train_sizenone indicestrue random_statenone cross_validation.train_test_split arrays ... cross_validation.cross_val_score estimator cross_validation.permutation_test_score ... evaluate signicance crossvalidated score permutations cross_validation.check_cv classier split arrays matrices random train test subsets evaluate score crossvalidation input checker utility building user friendly way 
1303: sklearn.cross_validation.train_test_split sklearn.cross_validation.train_test_split arrays options split arrays matrices random train test subsets quick utility wraps calls check_arrays iter shufflesplit n_samples .next application input data single call splitting optionally subsampling data oneliner 
1304: parameters arrays sequence arrays scipy.sparse matrices shape python lists tuples occurring arrays converted numpy arrays 
1305: test_size oat default 0.25 int oat 0.0 1.0 represent proportion dataset include test split int represents absolute number test samples 
1306: train_size oat int none default none oat 0.0 1.0 represent proportion dataset include train split int represents absolute number train samples none value automatically set complement test fraction 
1307: random_state int randomstate pseudorandom number generator state used random sampling 
1308: dtype numpy dtype instance none default enforce specic dtype 
1309: examples chapter user guide scikitlearn user guide release 0.11 import numpy sklearn.cross_validation import train_test_split np.arange .reshape range array test_size0.33 random_state42 a_train a_test b_train b_test train_test_split ... ... a_train array b_train array a_test array b_test array sklearn.cross_validation.cross_val_score sklearn.cross_validation.cross_val_score estimator score_funcnone ynone cvnone n_jobs1 verbose0 evaluate score crossvalidation parameters estimator estimator object implementing object use data arraylike shape least data 
1310: arraylike optional target variable try predict case supervised learning 
1311: score_func callable optional callable priority score function estimator nonsupervised set ting none takes test data x_test argument supervised setting takes test target y_true test prediction y_pred arguments 
1312: crossvalidation generator optional crossvalidation generator none 3fold cross validation used 3fold strati crossvalidation supplied estimator classier 
1313: n_jobs integer optional number cpus use computation means cpus 
1314: verbose integer optional 1.8. reference scikitlearn user guide release 0.11 verbosity level sklearn.cross_validation.permutation_test_score sklearn.cross_validation.permutation_test_score estimator score_func cvnone n_jobs1 random_state0 ver n_permutations100 labelsnone bose0 evaluate signicance crossvalidated score permutations parameters estimator estimator object implementing object use data arraylike shape least data 
1315: arraylike target variable try predict case supervised learning 
1316: score_func callable callable taking arguments test targets y_test predicted targets y_pred returns oat score functions expected return bigger value better result otherwise returned value correspond pvalue see returns details 
1317: integer crossvalidation generator optional integer passed number fold default specic crossvalidation jects passed see sklearn.cross_validation module list possible objects n_jobs integer optional number cpus use computation means cpus 
1318: labels arraylike shape n_samples optional labels constrain permutation among groups samples label 
1319: random_state randomstate int seed default random number generator instance dene state random permutations generator 
1320: verbose integer optional verbosity level returns score oat true score without permuting targets 
1321: permutation_scores array shape n_permutations scores obtained permutations 
1322: pvalue oat returned value equals pvalue score_func returns bigger numbers better scores e.g. zero_one score_func rather loss function i.e lower better mean_squared_error actually complement pvalue pvalue 
1323: chapter user guide scikitlearn user guide release 0.11 notes function implements test ojala garriga permutation tests studying classier performance journal machine learning research vol sklearn.cross_validation.check_cv sklearn.cross_validation.check_cv xnone ynone classierfalse input checker utility building user friendly way 
1324: parameters integer generator instance none input specifying generator use integer case number folds kfold none case fold used another object used generator 
1325: ndarray data crossval object applied ndarray target variable supervised learning problem classier boolean optional whether task classication task case stratied kfold used 
1326: 1.8.4 sklearn.datasets datasets sklearn.datasets module includes utilities load datasets including methods load fetch popular reference datasets also features articial data generators user guide see dataset loading utilities section details 
1327: loaders deprecated use fetch_20newsgroups instead download_if_missingfalse load lenames newsgroups dataset 
1328: datasets.load_20newsgroups args kwargs datasets.fetch_20newsgroups data_home ... datasets.fetch_20newsgroups_vectorized ... load newsgroups dataset transform tfidf vectors datasets.load_boston datasets.load_diabetes datasets.load_digits n_class datasets.load_files container_path ... datasets.load_iris datasets.load_lfw_pairs download_if_missing datasets.fetch_lfw_pairs subset ... datasets.load_lfw_people download_if_missing datasets.fetch_lfw_people data_home ... datasets.load_linnerud datasets.fetch_olivetti_faces data_home ... datasets.load_sample_image image_name load return boston houseprices dataset regression load return diabetes dataset regression load return digits dataset classication load text les categories subfolder names load return iris dataset classication alias fetch_lfw_pairs download_if_missingfalse loader labeled faces wild lfw pairs dataset alias fetch_lfw_people download_if_missingfalse loader labeled faces wild lfw people dataset load return linnerud dataset multivariate regression loader olivetti faces dataset load numpy array single sample image 1.8. reference continued next page scikitlearn user guide release 0.11 datasets.load_sample_images datasets.load_svmlight_file n_features ... load sample images image manipulation load datasets svmlight libsvm format sparse csr matrix table 1.42 continued previous page sklearn.datasets.load_20newsgroups sklearn.datasets.load_20newsgroups args kwargs deprecated use fetch_20newsgroups instead download_if_missingfalse alias fetch_20newsgroups download_if_missingfalse 
1329: see fetch_20newsgroups.__doc__ documentation parameter list 
1330: sklearn.datasets.fetch_20newsgroups sklearn.datasets.fetch_20newsgroups data_homenone cate goriesnone shufetrue random_state42 load_if_missingtrue subsettrain load lenames newsgroups dataset 
1331: parameters subset train test optional select dataset load train training set test test set shufed ordering 
1332: data_home optional default none specify download cache folder datasets none scikitlearn data stored scikit_learn_data subfolders 
1333: categories none collection string unicode none default load categories none list category names load categories ignored 
1334: shufe bool optional whether shufe data might important models make sumption samples independent identically distributed i.i.d stochastic gradient descent 
1335: random_state numpy random number generator seed integer used shufe dataset 
1336: download_if_missing optional true default false raise ioerror data locally available instead trying download data source site 
1337: sklearn.datasets.fetch_20newsgroups_vectorized sklearn.datasets.fetch_20newsgroups_vectorized subsettrain data_homenone load newsgroups dataset transform tfidf vectors transformation done using default settings convenience function sklearn.feature_extraction.text.vectorizer advanced usage stopword ltering ngram extraction etc combine fetch_20newsgroups custom vectorizer countvectorizer 
1338: tfidf chapter user guide scikitlearn user guide release 0.11 parameters subset train test optional select dataset load train training set test test set shufed ordering 
1339: data_home optional default none specify download cache folder datasets none scikitlearn data stored scikit_learn_data subfolders 
1340: returns bunch bunch object bunch.data sparse matrix shape n_samples n_features bunch.target array shape n_samples bunch.target_names list length n_classes sklearn.datasets.load_boston sklearn.datasets.load_boston load return boston houseprices dataset regression 
1341: samples total dimensionality features targets real positive real 
1342: returns data bunch dictionarylike object interesting attributes data data learn target regression targets target_names meaning labels descr full description dataset 
1343: examples sklearn.datasets import load_boston boston load_boston boston.data.shape sklearn.datasets.load_diabetes sklearn.datasets.load_diabetes load return diabetes dataset regression 
1344: samples total dimensionality features targets real integer returns data bunch dictionarylike object interesting attributes data data learn target regression target sample 
1345: 1.8. reference scikitlearn user guide release 0.11 sklearn.datasets.load_digits sklearn.datasets.load_digits n_class10 load return digits dataset classication datapoint 8x8 image digit 
1346: classes samples per class samples total dimensionality features integers parameters n_class integer optional default10 number classes return 
1347: returns data bunch dictionarylike object interesting attributes data data learn images images corresponding sample target classication labels sample target_names meaning labels descr full description dataset 
1348: examples load data visualize images sklearn.datasets import load_digits digits load_digits digits.data.shape import pylab pl.gray pl.matshow digits.images pl.show sklearn.datasets.load_les sklearn.datasets.load_files container_path load_contenttrue charse_errorstrict random_state0 descriptionnone shufetrue categoriesnone charsetnone load text les categories subfolder names individual samples assumed les stored two levels folder structure following container_folder category_1_folderle_1.txt le_2.txt ... le_42.txt category_2_folderle_43.txt le_44.txt 
1349: folder names used supervised signal label names indivial names important function try extract features numpy array scipy sparse matrix load_content false try load les memory use utf8 text les scikitlearn classication clustering algorithm rst need use sklearn.features.text module build feature extraction transformer suits problem 
1350: addition chapter user guide similar feature extractors build kind unstructured data input images audio video 
1351: scikitlearn user guide release 0.11 parameters container_path string unicode path main folder holding one subfolder per category description string unicode optional defaultnone paragraph describing characteristic dataset source reference etc 
1352: categories collection strings none optional defaultnone none default load categories none list category names load categories ignored 
1353: load_content boolean optional defaulttrue whether load content different les true data attribute con taining text information present data structure returned lenames attribute gives path les 
1354: charset string none default none none try decode content les e.g images nontext content none charset use decode text les load_content true 
1355: charset_error strict ignore replace instruction byte sequence given analyze contains characters given charset default strict meaning unicodedecodeerror raised values ignore replace 
1356: shufe bool optional defaulttrue whether shufe data might important models make sumption samples independent identically distributed i.i.d stochastic gradient descent 
1357: random_state int randomstate instance none optional default0 int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used np.random 
1358: returns data bunch dictionarylike object interesting attributes either data raw text data learn lenames les holding target classication labels integer index target_names meaning labels descr full description dataset 
1359: sklearn.datasets.load_iris sklearn.datasets.load_iris load return iris dataset classication iris dataset classic easy multiclass classication dataset 
1360: 1.8. reference scikitlearn user guide release 0.11 classes samples per class samples total dimensionality features real positive returns data bunch dictionarylike object interesting attributes data data learn target classication labels target_names meaning labels feature_names meaning features descr full description dataset 
1361: examples lets say interested samples want know class name 
1362: sklearn.datasets import load_iris data load_iris data.target array list data.target_names setosa versicolor virginica sklearn.datasets.load_lfw_pairs sklearn.datasets.load_lfw_pairs download_if_missingfalse kwargs alias fetch_lfw_pairs download_if_missingfalse check fetch_lfw_pairs.__doc__ documentation parameter list 
1363: sklearn.datasets.fetch_lfw_pairs sklearn.datasets.fetch_lfw_pairs subsettrain funneledtrue size0.5 colorfalse slice_ slice none slice none download_if_missingtrue data_homenone loader labeled faces wild lfw pairs dataset dataset collection jpeg pictures famous people collected internet details available ofcial website http viswww.cs.umass.edulfw picture centered single face pixel channel color rgb encoded oat range 0.0 1.0. task called face verication given pair two pictures binary classier must predict whether two images person ofcial readme.txt task described restricted task sure implement unrestricted variant correctly left unsupported 
1364: parameters subset optional default train chapter user guide scikitlearn user guide release 0.11 select dataset load train development training set test develop ment test set 10_folds ofcial evaluation set meant used 10folds cross validation 
1365: data_home optional default none specify another download cache folder datasets default scikit learn data stored scikit_learn_data subfolders 
1366: funneled boolean optional default true download use funneled variant dataset 
1367: resize oat optional default 0.5 ratio used resize face picture 
1368: color boolean optional default false keep rgb channels instead averaging single gray level channel color true shape data one dimension shape color false slice_ optional provide custom slice height width extract interesting part jpeg les avoid use statistical correlation background download_if_missing optional true default false raise ioerror data locally available instead trying download data source site 
1369: sklearn.datasets.load_lfw_people sklearn.datasets.load_lfw_people download_if_missingfalse kwargs alias fetch_lfw_people download_if_missingfalse check fetch_lfw_people.__doc__ documentation parameter list 
1370: sklearn.datasets.fetch_lfw_people sklearn.datasets.fetch_lfw_people data_homenone funneledtrue min_faces_per_personnone slice_ slice none download_if_missingtrue resize0.5 colorfalse slice none loader labeled faces wild lfw people dataset dataset collection jpeg pictures famous people collected internet details available ofcial website http viswww.cs.umass.edulfw picture centered single face pixel channel color rgb encoded oat range 0.0 1.0. task called face recognition identication given picture face name person given training set gallery 
1371: parameters data_home optional default none 1.8. reference scikitlearn user guide release 0.11 specify another download cache folder datasets default scikit learn data stored scikit_learn_data subfolders 
1372: funneled boolean optional default true download use funneled variant dataset 
1373: resize oat optional default 0.5 ratio used resize face picture 
1374: min_faces_per_person int optional default none extracted dataset retain pictures people min_faces_per_person different pictures 
1375: least color boolean optional default false keep rgb channels instead averaging single gray level channel color true shape data one dimension shape color false slice_ optional provide custom slice height width extract interesting part jpeg les avoid use statistical correlation background download_if_missing optional true default false raise ioerror data locally available instead trying download data source site 
1376: sklearn.datasets.load_linnerud sklearn.datasets.load_linnerud load return linnerud dataset multivariate regression samples total dimensionality data targets features integer targets integer returns data bunch dictionarylike object interesting attributes data targets two mul tivariate datasets data corresponding exercise targets corresponding physiological measurements well feature_names target_names 
1377: sklearn.datasets.fetch_olivetti_faces sklearn.datasets.fetch_olivetti_faces data_homenone download_if_missingtrue shufefalse random_state0 loader olivetti faces dataset 
1378: parameters data_home optional default none specify another download cache folder datasets default scikit learn data stored scikit_learn_data subfolders 
1379: shufe boolean optional true order dataset shufed avoid images person grouped 
1380: download_if_missing optional true default chapter user guide scikitlearn user guide release 0.11 false raise ioerror data locally available instead trying download data source site 
1381: random_state optional integer randomstate object seed random number generator used shufe data 
1382: notes dataset consists pictures individuals original database available defunct http www.uk.research.att.comfacedatabase.html version retrieved comes matlab format personal web page sam roweis http www.cs.nyu.eduroweis sklearn.datasets.load_sample_image sklearn.datasets.load_sample_image image_name load numpy array single sample image parameters image_name china.jpg ower.jpg name sample image loaded returns img array image numpy array height width color examples sklearn.datasets import load_sample_image china load_sample_image china.jpg china.dtype dtype uint8 china.shape flower load_sample_image flower.jpg flower.dtype dtype uint8 flower.shape sklearn.datasets.load_sample_images sklearn.datasets.load_sample_images load sample images image manipulation loads china flower 
1383: returns data bunch dictionarylike object following attributes images two sample images lenames names images descr full description dataset 
1384: 1.8. reference scikitlearn user guide release 0.11 examples load data visualize images sklearn.datasets import load_sample_images dataset load_sample_images len dataset.images first_img_data dataset.images first_img_data.shape first_img_data.dtype dtype uint8 sklearn.datasets.load_svmlight_le sklearn.datasets.load_svmlight_file n_featuresnone dtype type numpy.oat64 mul tilabelfalse zero_basedauto load datasets svmlight libsvm format sparse csr matrix format textbased format one sample per line store zero valued features hence suitable sparse dataset rst element line used store target variable predict format used default format svmlight libsvm command line programs parsing text based source expensive working repeatedly dataset recom mended wrap loader joblib.memory.cache store memmapped backup csr results rst call benet near instantaneous loading memmapped structures subsequent calls implementation naive allocate much memory slow since written python large datasets recommended use optimized loader https github.commblondelsvmlightloader parameters str lelike open binary mode path load n_features int none number features use none inferred argument useful load several les subsets bigger sliced dataset subset might example every feature hence inferred shape might vary one slice another 
1385: multilabel boolean optional samples may several labels see http www.csie.ntu.edu.twcjlinlibsvmtoolsdatasetsmultilabel.html zero_based boolean auto optional whether column indices zerobased true onebased false set auto heuristic check applied determine contents kinds les occur wild unfortunately selfidentifying using auto true always safe 
1386: returns scipy.sparse matrix shape n_samples n_features chapter user guide scikitlearn user guide release 0.11 ndarray shape n_samples multilabel case list tuples length n_samples 
1387: see also load_svmlight_filessimilar function loading multiple les format enforcing samples generator datasets.make_blobs n_samples n_features ... datasets.make_classification n_samples ... datasets.make_circles n_samples shufe ... datasets.make_friedman1 n_samples ... datasets.make_friedman2 n_samples noise ... datasets.make_friedman3 n_samples noise ... datasets.make_hastie_10_2 n_samples ... datasets.make_low_rank_matrix n_samples ... datasets.make_moons n_samples shufe ... datasets.make_multilabel_classification ... datasets.make_regression n_samples ... datasets.make_s_curve n_samples noise ... datasets.make_sparse_coded_signal n_samples ... generate signal sparse combination dictionary elements datasets.make_sparse_spd_matrix dim ... datasets.make_sparse_uncorrelated ... datasets.make_spd_matrix n_dim random_state datasets.make_swiss_roll n_samples noise ... generate isotropic gaussian blobs clustering generate random nclass classication problem make large circle containing smaller circle 2di generate friedman regression problem generate friedman regression problem generate friedman regression problem generates data binary classication used generate mostly low rank matrix bellshaped singular values make two interleaving half circles generate random multilabel classication problem generate random regression problem generate curve dataset 
1388: generate sparse symetric denite positive matrix generate random regression problem sparse uncorrelated design generate random symmetric positivedenite matrix generate swiss roll dataset 
1389: sklearn.datasets.make_blobs sklearn.datasets.make_blobs n_samples100 n_features2 generate isotropic gaussian blobs clustering 
1390: center_box 10.0 10.0 shufetrue random_statenone centers3 cluster_std1.0 parameters n_samples int optional default100 total number points equally divided among clusters 
1391: n_features int optional default2 number features sample 
1392: centers int array shape n_centers n_features optional default3 number centers generate xed center locations 
1393: cluster_std oat sequence oats optional default1.0 standard deviation clusters 
1394: center_box pair oats min max optional default 10.0 10.0 bounding box cluster center centers generated random 
1395: shufe boolean optional defaulttrue 1.8. reference scikitlearn user guide release 0.11 shufe samples 
1396: random_state int randomstate instance none optional defaultnone int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used np.random 
1397: returns array shape n_samples n_features generated samples 
1398: array shape n_samples integer labels cluster membership sample 
1399: examples sklearn.datasets.samples_generator import make_blobs make_blobs n_samples10 centers3 n_features2 ... x.shape array random_state0 sklearn.datasets.make_classication sklearn.datasets.make_classification n_samples100 n_informative2 n_redundant2 n_classes2 n_clusters_per_class2 weightsnone ip_y0.01 class_sep1.0 hypercubetrue shift0.0 scale1.0 shufetrue random_statenone n_features20 n_repeated0 generate random nclass classication problem 
1400: parameters n_samples int optional default100 number samples 
1401: n_features int optional default20 total number features comprise n_informative informative features n_redundant redundant features n_repeated dupplicated features n_features n_informativen_redundant n_repeated useless features drawn random 
1402: n_informative int optional default2 number informative features class composed number gaussian clusters located around vertices hypercube subspace dimension n_informative cluster informative features drawn independently randomly linearly combined order add covariance clusters placed vertices hypercube 
1403: n_redundant int optional default2 number redundant features features generated random linear com binations informative features 
1404: n_repeated int optional default2 chapter user guide scikitlearn user guide release 0.11 number dupplicated features drawn randomly informative dundant features 
1405: n_classes int optional default2 number classes labels classication problem 
1406: n_clusters_per_class int optional default2 number clusters per class 
1407: weights list oats none defaultnone proportions samples assigned class none classes balanced note len weights n_classes last class weight automatically inferred 
1408: ip_y oat optional default0.01 fraction samples whose class randomly exchanged 
1409: class_sep oat optional default1.0 factor multiplying hypercube dimension 
1410: hypercube boolean optional defaulttrue true clusters put vertices hypercube false clusters put vertices random polytope 
1411: shift oat none optional default0.0 shift features specied value none features shifted random value drawn class_sep class_sep scale oat none optional default1.0 multiply features specied value random value drawn note scaling happens shifting 
1412: none features scaled shufe boolean optional defaulttrue shufe samples features 
1413: random_state int randomstate instance none optional defaultnone int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used np.random 
1414: returns array shape n_samples n_features generated samples 
1415: array shape n_samples integer labels class membership sample 
1416: notes algorithm adapted guyon designed generate madelon dataset 
1417: 1.8. reference scikitlearn user guide release 0.11 references r48 sklearn.datasets.make_circles sklearn.datasets.make_circles n_samples100 shufetrue noisenone random_statenone make large circle containing smaller circle 2di simple toy dataset visualize clustering classication algorithms 
1418: factor0.8 parameters n_samples int optional default100 total number points generated shufe bool optional defaulttrue whether shufe samples 
1419: noise double none defaultnone standard deviation gaussian noise added data 
1420: factor double default.8 scale factor inner outer circle 
1421: sklearn.datasets.make_friedman1 sklearn.datasets.make_friedman1 n_samples100 n_features10 noise0.0 ran dom_statenone generate friedman regression problem dataset described friedman breiman inputs independent features uniformly distributed interval output created according formula sin 0.5 noise 
1422: n_features features actually used compute remaining features independent number features 
1423: parameters n_samples int optional default100 number samples 
1424: n_features int optional default10 number features least 
1425: noise oat optional default0.0 standard deviation gaussian noise applied output 
1426: random_state int randomstate instance none optional defaultnone int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used np.random 
1427: chapter user guide scikitlearn user guide release 0.11 returns array shape n_samples n_features input samples 
1428: array shape n_samples output values 
1429: references r49 r50 sklearn.datasets.make_friedman2 sklearn.datasets.make_friedman2 n_samples100 noise0.0 random_statenone generate friedman regression problem dataset described friedman breiman inputs independent features uniformly distributed intervals 
1430: output created according formula 0.5 noise 
1431: parameters n_samples int optional default100 number samples 
1432: noise oat optional default0.0 standard deviation gaussian noise applied output 
1433: random_state int randomstate instance none optional defaultnone int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used np.random 
1434: returns array shape n_samples input samples 
1435: array shape n_samples output values 
1436: references r51 r52 1.8. reference scikitlearn user guide release 0.11 sklearn.datasets.make_friedman3 sklearn.datasets.make_friedman3 n_samples100 noise0.0 random_statenone generate friedman regression problem dataset described friedman breiman inputs independent features uniformly distributed intervals 
1437: output created according formula arctan noise 
1438: parameters n_samples int optional default100 number samples 
1439: noise oat optional default0.0 standard deviation gaussian noise applied output 
1440: random_state int randomstate instance none optional defaultnone int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used np.random 
1441: returns array shape n_samples input samples 
1442: array shape n_samples output values 
1443: references r53 r54 sklearn.datasets.make_hastie_10_2 sklearn.datasets.make_hastie_10_2 n_samples12000 random_statenone generates data binary classication used hastie example 10.2. ten features standard independent gaussian target dened np.sum 9.34 else parameters n_samples int optional default12000 number samples 
1444: random_state int randomstate instance none optional defaultnone chapter user guide scikitlearn user guide release 0.11 int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used np.random 
1445: returns array shape n_samples input samples 
1446: array shape n_samples output values 
1447: references hastie tibshirani friedman elements statistical learning springer 2009. sklearn.datasets.make_low_rank_matrix sklearn.datasets.make_low_rank_matrix n_samples100 n_features100 effective_rank10 generate mostly low rank matrix bellshaped singular values variance explained bellshaped curve width effective_rank low rank part singular values prole tail_strength0.5 random_statenone tail_strength exp 1.0 effective_rank remaining singular values tail fat decreasing tail_strength exp 0.1 effective_rank 
1448: low rank part prole considered structured signal part data tail considered noisy part data summarized low number linear components singular vectors kind singular proles often seen practice instance gray level pictures faces tfidf vectors text documents crawled web parameters n_samples int optional default100 number samples 
1449: n_features int optional default100 number features 
1450: effective_rank int optional default10 approximate number singular vectors required explain data linear combinations 
1451: tail_strength oat 0.0 1.0 optional default0.5 relative importance fat noisy tail singular values prole random_state int randomstate instance none optional defaultnone 1.8. reference scikitlearn user guide release 0.11 int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used np.random 
1452: returns array shape n_samples n_features matrix 
1453: sklearn.datasets.make_moons sklearn.datasets.make_moons n_samples100 shufetrue noisenone random_statenone make two interleaving half circles simple toy dataset visualize clustering classication algorithms 
1454: parameters n_samples int optional default100 total number points generated 
1455: shufe bool optional defaulttrue whether shufe samples 
1456: noise double none defaultnone standard deviation gaussian noise added data 
1457: sklearn.datasets.make_multilabel_classication sklearn.datasets.make_multilabel_classification n_samples100 n_classes5 n_labels2 allow_unlabeledtrue dom_statenone n_features20 length50 ran generate random multilabel classication problem sample generative process pick number labels poisson n_labels times choose class multinomial theta pick document length poisson length times choose word multinomial theta_c process rejection sampling used make sure never zero n_classes document length never zero likewise reject classes already chosen 
1458: parameters n_samples int optional default100 number samples 
1459: n_features int optional default20 total number features 
1460: n_classes int optional default5 number classes classication problem 
1461: n_labels int optional default2 average number labels per instance number labels follows poisson distri bution never takes value 
1462: chapter user guide scikitlearn user guide release 0.11 length int optional default50 sum features number words documents 
1463: allow_unlabeled bool optional defaulttrue true instances might belong class 
1464: random_state int randomstate instance none optional defaultnone int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used np.random 
1465: returns array shape n_samples n_features generated samples 
1466: list tuples label sets 
1467: sklearn.datasets.make_regression sklearn.datasets.make_regression n_samples100 n_informative10 bias0.0 effective_ranknone tail_strength0.5 noise0.0 shufetrue coeffalse random_statenone n_features100 generate random regression problem input set either well conditioned default low rankfat tail singular prole see make_low_rank_matrix details output generated applying potentially biased random linear regression model n_informative nonzero regressors previously generated input gaussian centered noise adjustable scale 
1468: parameters n_samples int optional default100 number samples 
1469: n_features int optional default100 number features 
1470: n_informative int optional default10 number informative features i.e. number features used build linear model used generate output 
1471: bias oat optional default0.0 bias term underlying linear model 
1472: effective_rank int none optional defaultnone none approximate number singular vectors required explain input data linear combinations using kind singular spectrum input allows generator reproduce correlations often observed practice none input set well conditioned centered gaussian unit variance 
1473: tail_strength oat 0.0 1.0 optional default0.5 relative importance fat noisy tail singular values prole effec tive_rank none 
1474: 1.8. reference scikitlearn user guide release 0.11 noise oat optional default0.0 standard deviation gaussian noise applied output 
1475: shufe boolean optional defaulttrue shufe samples features coef boolean optional defaultfalse true coefcients underlying linear model returned 
1476: random_state int randomstate instance none optional defaultnone int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used np.random 
1477: returns array shape n_samples n_features input samples 
1478: array shape n_samples output values 
1479: coef array shape n_features optional coefcient underlying linear model returned coef true 
1480: sklearn.datasets.make_s_curve sklearn.datasets.make_s_curve n_samples100 noise0.0 random_statenone generate curve dataset 
1481: parameters n_samples int optional default100 number sample points curve 
1482: noise oat optional default0.0 standard deviation gaussian noise 
1483: random_state int randomstate instance none optional defaultnone int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used np.random 
1484: returns array shape n_samples points 
1485: array shape n_samples univariate position sample according main dimension points manifold 
1486: sklearn.datasets.make_sparse_coded_signal sklearn.datasets.make_sparse_coded_signal n_samples generate signal sparse combination dictionary elements 
1487: n_components n_features n_nonzero_coefs random_statenone chapter user guide returns matrix n_features n_components n_components n_samples column exactly n_nonzero_coefs nonzero elements 
1488: scikitlearn user guide release 0.11 parameters n_samples int number samples generate n_components int number components dictionary n_features int number features dataset generate n_nonzero_coefs int number active nonzero coefcients sample random_state int randomstate instance optional defaultnone seed used pseudo random number generator returns data array shape n_features n_samples encoded signal 
1489: dictionary array shape n_features n_components dictionary normalized components code array shape n_components n_samples sparse code column matrix exactly n_nonzero_coefs non zero items 
1490: sklearn.datasets.make_sparse_spd_matrix sklearn.datasets.make_sparse_spd_matrix dim1 alpha0.95 smallest_coef0.1 dom_statenone norm_diagfalse ran largest_coef0.9 generate sparse symetric denite positive matrix 
1491: parameters dim integer optional default1 size random matrix generate 
1492: alpha oat optional default0.95 probability coefcient non zero see notes 
1493: random_state int randomstate instance none optional defaultnone int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used np.random 
1494: returns prec array shape dim dim notes sparsity actually imposed cholesky factor matrix thus alpha translate directly lling fraction matrix 
1495: 1.8. reference scikitlearn user guide release 0.11 sklearn.datasets.make_sparse_uncorrelated sklearn.datasets.make_sparse_uncorrelated n_samples100 n_features10 ran generate random regression problem sparse uncorrelated design dataset described celeux dom_statenone 1.5 rst features informative remaining features useless 
1496: parameters n_samples int optional default100 number samples 
1497: n_features int optional default10 number features 
1498: random_state int randomstate instance none optional defaultnone int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used np.random 
1499: returns array shape n_samples n_features input samples 
1500: array shape n_samples output values 
1501: references r55 sklearn.datasets.make_spd_matrix sklearn.datasets.make_spd_matrix n_dim random_statenone generate random symmetric positivedenite matrix 
1502: parameters n_dim int matrix dimension 
1503: random_state int randomstate instance none optional defaultnone int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used np.random 
1504: returns array shape n_dim n_dim random symmetric positivedenite matrix 
1505: chapter user guide scikitlearn user guide release 0.11 sklearn.datasets.make_swiss_roll sklearn.datasets.make_swiss_roll n_samples100 noise0.0 random_statenone generate swiss roll dataset 
1506: parameters n_samples int optional default100 number sample points curve 
1507: noise oat optional default0.0 standard deviation gaussian noise 
1508: random_state int randomstate instance none optional defaultnone int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used np.random 
1509: returns array shape n_samples points 
1510: array shape n_samples univariate position sample according main dimension points manifold 
1511: notes algorithm marsland 
1512: references r56 1.8.5 sklearn.decomposition matrix decomposition sklearn.decomposition module includes matrix decomposition algorithms including among others pca nmf ica algorithms module regarded dimensionality reduction techniques user guide see decomposing signals components matrix factorization problems section details 
1513: decomposition.pca n_components copy whiten decomposition.probabilisticpca ... decomposition.projectedgradientnmf ... decomposition.randomizedpca n_components ... decomposition.kernelpca n_components ... decomposition.fastica n_components ... decomposition.nmf n_components init ... decomposition.sparsepca n_components ... decomposition.minibatchsparsepca n_components decomposition.sparsecoder dictionary ... decomposition.dictionarylearning n_atoms ... decomposition.minibatchdictionarylearning n_atoms minibatch dictionary learning principal component analysis pca additional layer top pca adds probabilistic evaluationprincipal component analysis pca nonnegative matrix factorization projected gradient nmf principal component analysis pca using randomized svd kernel principal component analysis kpca fastica fast algorithm independent component analysis nonnegative matrix factorization projected gradient nmf sparse principal components analysis sparsepca minibatch sparse principal components analysis sparse coding dictionary learning 1.8. reference scikitlearn user guide release 0.11 sklearn.decomposition.pca class sklearn.decomposition.pca n_componentsnone copytrue whitenfalse principal component analysis pca linear dimensionality reduction using singular value decomposition data keeping signicant singular vectors project data lower dimensional space implementation uses scipy.linalg implementation singular value decomposition works dense arrays scalable large dimensional data time complexity implementation assuming n_samples n_features 
1514: parameters n_components int none string number components keep n_components set components kept n_components min n_samples n_features n_components mle minkas mle used guess dimension n_components select number components amount vari ance needs explained greater percentage specied n_components copy bool false data passed overwritten whiten bool optional true false default components_ vectors divided n_samples times singular values ensure uncorrelated outputs unit componentwise variances whitening remove information transformed signal relative vari ance scales components sometime improve predictive accuracy downstream estimators making data respect hardwired assumptions 
1515: see also probabilisticpca randomizedpca kernelpca sparsepca notes n_componentsmle class uses method thomas minka automatic choice dimensionality pca nips due implementation subtleties singular value decomposition svd used imple mentation running twice matrix lead principal components signs ipped change direction reason important always use estimator object transform data consistent fashion 
1516: examples import numpy sklearn.decomposition import pca np.array pca pca n_components2 pca.fit pca copytrue n_components2 whitenfalse chapter user guide scikitlearn user guide release 0.11 print pca.explained_variance_ratio_ 0.99244.. 
1517: 0.00755 ... attributes compo nents_ array n_components n_features array n_components plained_variance_ratio_ components maximum variance 
1518: percentage variance explained selected components set components stored sum explained variances equal 1.0 methods fit fit_transform get_params deep inverse_transform transform data back original space i.e. set_params params transform set parameters estimator apply dimensionality reduction 
1519: fit model fit model apply dimensionality reduction get parameters estimator __init__ n_componentsnone copytrue whitenfalse fit ynone params fit model 
1520: parameters arraylike shape n_samples n_features training data n_samples number samples n_features number features 
1521: returns self object returns instance fit_transform ynone params fit model apply dimensionality reduction 
1522: parameters arraylike shape n_samples n_features training data n_samples number samples n_features number features 
1523: returns x_new arraylike shape n_samples n_components get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1524: inverse_transform transform data back original space i.e. return input x_original whose transform would parameters arraylike shape n_samples n_components 1.8. reference scikitlearn user guide release 0.11 new data n_samples number samples n_components number components 
1525: returns x_original arraylike shape n_samples n_features notes whitening enabled inverse_transform compute exact inverse operation transform 
1526: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform apply dimensionality reduction 
1527: parameters arraylike shape n_samples n_features new data n_samples number samples n_features number features 
1528: returns x_new arraylike shape n_samples n_components sklearn.decomposition.probabilisticpca class sklearn.decomposition.probabilisticpca n_componentsnone copytrue whitenfalse additional layer top pca adds probabilistic evaluationprincipal component analysis pca linear dimensionality reduction using singular value decomposition data keeping signicant singular vectors project data lower dimensional space implementation uses scipy.linalg implementation singular value decomposition works dense arrays scalable large dimensional data time complexity implementation assuming n_samples n_features 
1529: parameters n_components int none string number components keep n_components set components kept n_components min n_samples n_features n_components mle minkas mle used guess dimension n_components select number components amount vari ance needs explained greater percentage specied n_components copy bool false data passed overwritten whiten bool optional true false default components_ vectors divided n_samples times singular values ensure uncorrelated outputs unit componentwise variances 
1530: chapter user guide scikitlearn user guide release 0.11 whitening remove information transformed signal relative vari ance scales components sometime improve predictive accuracy downstream estimators making data respect hardwired assumptions 
1531: see also probabilisticpca randomizedpca kernelpca sparsepca notes n_componentsmle class uses method thomas minka automatic choice dimensionality pca nips due implementation subtleties singular value decomposition svd used imple mentation running twice matrix lead principal components signs ipped change direction reason important always use estimator object transform data consistent fashion 
1532: examples import numpy sklearn.decomposition import pca np.array pca pca n_components2 pca.fit pca copytrue n_components2 whitenfalse print pca.explained_variance_ratio_ 0.99244.. 
1533: 0.00755 ... attributes compo nents_ array n_components n_features array n_components plained_variance_ratio_ components maximum variance 
1534: percentage variance explained selected components set components stored sum explained variances equal 1.0 methods additionally pca.t learns covariance model fit model apply dimensionality reduction get parameters estimator fit homoscedastic fit_transform get_params deep inverse_transform transform data back original space i.e. score set_params params transform return score associated new data set parameters estimator apply dimensionality reduction 
1535: __init__ n_componentsnone copytrue whitenfalse fit ynone homoscedastictrue 1.8. reference scikitlearn user guide release 0.11 additionally pca.t learns covariance model parameters array shape n_samples n_dim data homoscedastic bool optional true average variance across remaining dimensions fit_transform ynone params fit model apply dimensionality reduction 
1536: parameters arraylike shape n_samples n_features training data n_samples number samples n_features number features 
1537: returns x_new arraylike shape n_samples n_components get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1538: inverse_transform transform data back original space i.e. return input x_original whose transform would parameters arraylike shape n_samples n_components new data n_samples number samples n_components number components 
1539: returns x_original arraylike shape n_samples n_features notes whitening enabled inverse_transform compute exact inverse operation transform 
1540: score ynone return score associated new data parameters array shape n_samples n_dim data test returns array shape n_samples loglikelihood row current model set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform apply dimensionality reduction 
1541: chapter user guide scikitlearn user guide release 0.11 parameters arraylike shape n_samples n_features new data n_samples number samples n_features number features 
1542: returns x_new arraylike shape n_samples n_components sklearn.decomposition.projectedgradientnmf class sklearn.decomposition.projectedgradientnmf n_componentsnone sparsenessnone tol0.0001 nls_max_iter2000 beta1 initnndsvdar eta0.1 max_iter200 nonnegative matrix factorization projected gradient nmf parameters arraylike sparse matrix shape n_samples n_features data model 
1543: n_components int none number components n_components set components kept init nndsvd nndsvda nndsvdar int randomstate method used initialize procedure default nndsvdar valid options nndsvd nonnegative double singular value decomposition nndsvd initialization better sparseness nndsvda nndsvd zeros filled average better sparsity desired nndsvdar nndsvd zeros filled small random values generally faster less accurate alternative nndsvda sparsity desired int seed randomstate nonnegative random matrices sparseness data components none default none enforce sparsity model 
1544: beta double default degree sparseness sparseness none larger values mean sparseness 
1545: eta double default 0.1 degree correctness mantain sparsity none smaller values mean larger error 
1546: tol double default 1e4 tolerance value used stopping conditions 
1547: max_iter int default number iterations compute 
1548: nls_max_iter int default number iterations nls subproblem 
1549: 1.8. reference scikitlearn user guide release 0.11 notes implements c.j lin projected gradient methods nonnegative matrix factorization neural computation 27562779. http www.csie.ntu.edu.twcjlinnmf hoyer nonnegative matrix factorization sparseness constraints journal machine learning search 2004. nndsvd introduced boutsidis gallopoulos svd based initialization head start nonnegative matrix factorization pattern recognition http www.cs.rpi.eduboutsclesnndsvd.pdf examples import numpy np.array 1.2 0.8 sklearn.decomposition import projectedgradientnmf model projectedgradientnmf n_components2 init0 model.fit projectedgradientnmf beta1 eta0.1 init0 max_iter200 n_components2 nls_max_iter2000 sparsenessnone tol0.0001 model.components_ array 0.77032744 0.38526873 0.11118662 0.38228063 model.reconstruction_err_ 0.00746 ... model projectedgradientnmf n_components2 init0 ... model.fit projectedgradientnmf beta1 eta0.1 init0 max_iter200 n_components2 sparsenesscomponents nls_max_iter2000 sparsenesscomponents tol0.0001 model.components_ array 1.67481991 
1550: 0.29614922 0.4681982 model.reconstruction_err_ 0.513.. 
1551: array n_components n_features number attributes compo nents_ recon struc tion_err_ methods nonnegative components data frobenius norm matrix difference training data reconstructed data produced model computed sparse input matrices expensive terms memory 
1552: fit learn nmf model data 
1553: continued next page chapter user guide scikitlearn user guide release 0.11 table 1.47 continued previous page fit_transform learn nmf model data returns transformed data get_params deep set_params params transform get parameters estimator set parameters estimator transform data according tted nmf model __init__ n_componentsnone initnndsvdar sparsenessnone beta1 eta0.1 tol0.0001 max_iter200 nls_max_iter2000 fit ynone params learn nmf model data 
1554: parameters arraylike sparse matrix shape n_samples n_features data matrix decomposed returns self fit_transform ynone learn nmf model data returns transformed data efcient calling followed transform 
1555: parameters arraylike sparse matrix shape n_samples n_features data matrix decomposed returns data array n_samples n_components transformed data get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform transform data according tted nmf model parameters arraylike sparse matrix shape n_samples n_features data matrix transformed model returns data array n_samples n_components transformed data sklearn.decomposition.randomizedpca class sklearn.decomposition.randomizedpca n_components copytrue iterated_power3 principal component analysis pca using randomized svd whitenfalse random_statenone 1.8. reference scikitlearn user guide release 0.11 linear dimensionality reduction using approximated singular value decomposition data keeping signicant singular vectors project data lower dimensional space implementation uses randomized svd implementation handle scipy.sparse numpy dense arrays input 
1556: parameters n_components int maximum number components keep default 
1557: copy bool false data passed overwritten iterated_power int optional number iteration power method default 
1558: whiten bool optional true false default components_ vectors divided singular values ensure uncorrelated outputs unit componentwise variances whitening remove information transformed signal relative vari ance scales components sometime improve predictive accuracy downstream estimators making data respect hardwired assumptions 
1559: random_state int randomstate instance none default pseudo random number generator seed control none use numpy.random sin gleton 
1560: see also pca probabilisticpca references halko2009 mrt examples import numpy sklearn.decomposition import randomizedpca np.array pca randomizedpca n_components2 pca.fit randomizedpca copytrue iterated_power3 n_components2 random_state mtrand.randomstate object ... whitenfalse print pca.explained_variance_ratio_ 0.99244.. 
1561: 0.00755 ... chapter user guide attributes compo nents_ array n_components n_features array n_components plained_variance_ratio_ scikitlearn user guide release 0.11 components maximum variance 
1562: percentage variance explained selected components set components stored sum explained variances equal 1.0 methods fit fit_transform get_params deep inverse_transform transform data back original space i.e. set_params params transform fit model data fit data transform get parameters estimator set parameters estimator apply dimensionality reduction 
1563: __init__ n_components copytrue iterated_power3 whitenfalse random_statenone fit ynone fit model data 
1564: parameters arraylike scipy.sparse matrix shape n_samples n_features training vector n_samples number samples n_features num ber features returns self object returns instance 
1565: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
1566: parameters numpy array shape n_samples n_features training set 
1567: numpy array shape n_samples target values 
1568: returns x_new numpy array shape n_samples n_features_new transformed array 
1569: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
1570: get_params deeptrue get parameters estimator 1.8. reference scikitlearn user guide release 0.11 parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1571: inverse_transform transform data back original space i.e. return input x_original whose transform would parameters arraylike scipy.sparse matrix shape n_samples n_components new data n_samples number samples n_components number components 
1572: returns x_original arraylike shape n_samples n_features notes whitening enabled inverse_transform compute exact inverse operation transform 
1573: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform apply dimensionality reduction 
1574: parameters arraylike scipy.sparse matrix shape n_samples n_features new data n_samples number samples n_features number features 
1575: returns x_new arraylike shape n_samples n_components sklearn.decomposition.kernelpca class sklearn.decomposition.kernelpca n_componentsnone degree3 gamma0 t_inverse_transformfalse tol0 max_iternone coef01 kernellinear alpha1.0 eigen_solverauto kernel principal component analysis kpca nonlinear dimensionality reduction use kernels 
1576: parameters n_components int none number components none nonzero components kept 
1577: kernel linear poly rbf sigmoid precomputed kernel default linear degree int optional degree poly rbf sigmoid kernels default 
1578: gamma oat optional kernel coefcient rbf poly kernels default 1n_features 
1579: chapter user guide scikitlearn user guide release 0.11 coef0 oat optional independent term poly sigmoid kernels 
1580: alpha int hyperparameter ridge regression t_inverse_transformtrue default 1.0 learns inverse transform t_inverse_transform bool learn inverse transform nonprecomputed kernels i.e image point default false learn pre eigen_solver string autodensearpack select eigensolver use n_components much less number training samples arpack may efcient dense eigensolver 
1581: tol oat convergence tolerance arpack default optimal value chosen arpack max_iter int maximum number iterations arpack default none optimal value chosen arpack references kernel pca intoduced bernhard schoelkopf alexander smola klausrobert mueller 1999. kernel principal component analysis advances kernel methods mit press cambridge usa 
1582: attributes lambdas_ alphas_ dual_coef_ x_transformed_t_ eigenvalues eigenvectors centered kernel matrix inverse transform matrix projection tted data kernel principal components methods fit fit_transform get_params deep inverse_transform transform back original space set parameters estimator set_params params transform transform 
1583: fit model data fit model data transform get parameters estimator __init__ n_componentsnone kernellinear gamma0 t_inverse_transformfalse eigen_solverauto tol0 max_iternone degree3 coef01 alpha1.0 fit ynone fit model data 
1584: parameters arraylike shape n_samples n_features 1.8. reference scikitlearn user guide release 0.11 training vector n_samples number samples n_features num ber features returns self object returns instance fit_transform ynone params fit model data transform 
1585: parameters arraylike shape n_samples n_features training vector n_samples number samples n_features num ber features 
1586: returns x_new arraylike shape n_samples n_components get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1587: inverse_transform transform back original space 
1588: parameters arraylike shape n_samples n_components returns x_new arraylike shape n_samples n_features references learning find preimages bakir 
1589: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform transform 
1590: parameters arraylike shape n_samples n_features returns x_new arraylike shape n_samples n_components sklearn.decomposition.fastica class sklearn.decomposition.fastica n_componentsnone algorithmparallel whitentrue fun_argsnone ran funlogcosh max_iter200 dom_statenone fastica fast algorithm independent component analysis fun_prime tol0.0001 w_initnone parameters n_components int optional number components use none passed used 
1591: chapter user guide scikitlearn user guide release 0.11 algorithm parallel deation apply parallel deational algorithm fastica whiten boolean optional whiten false data already considered whitened whitening performed 
1592: fun logcosh exp cube callable nonlinear function used fastica loop approximate negentropy func tion passed derivative passed fun_prime argument 
1593: fun_prime none callable derivative nonlinearity used 
1594: max_iter int optional maximum number iterations tol oat optional tolerance update iteration w_init none n_components n_components ndarray mixing matrix used initialize algorithm 
1595: random_state int randomstate pseudo number generator state used random sampling 
1596: notes implementation based hyvarinen oja independent component analysis algorithms appli cations neural networks attributes unmixing_matrix_ array n_components n_samples unmixing matrix methods fit get_mixing_matrix compute mixing matrix get_params deep set_params params transform get parameters estimator set parameters estimator apply unmixing matrix recover sources __init__ n_componentsnone algorithmparallel whitentrue fun_argsnone max_iter200 tol0.0001 w_initnone random_statenone funlogcosh fun_prime get_mixing_matrix compute mixing matrix get_params deeptrue 1.8. reference scikitlearn user guide release 0.11 get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform apply unmixing matrix recover sources w.t sklearn.decomposition.nmf class sklearn.decomposition.nmf n_componentsnone initnndsvdar sparsenessnone beta1 nonnegative matrix factorization projected gradient nmf eta0.1 tol0.0001 max_iter200 nls_max_iter2000 parameters arraylike sparse matrix shape n_samples n_features data model 
1597: n_components int none number components n_components set components kept init nndsvd nndsvda nndsvdar int randomstate method used initialize procedure default nndsvdar valid options nndsvd nonnegative double singular value decomposition nndsvd initialization better sparseness nndsvda nndsvd zeros filled average better sparsity desired nndsvdar nndsvd zeros filled small random values generally faster less accurate alternative nndsvda sparsity desired int seed randomstate nonnegative random matrices sparseness data components none default none enforce sparsity model 
1598: beta double default degree sparseness sparseness none larger values mean sparseness 
1599: eta double default 0.1 degree correctness mantain sparsity none smaller values mean larger error 
1600: tol double default 1e4 tolerance value used stopping conditions 
1601: chapter user guide scikitlearn user guide release 0.11 max_iter int default number iterations compute 
1602: nls_max_iter int default number iterations nls subproblem 
1603: notes implements c.j lin projected gradient methods nonnegative matrix factorization neural computation 27562779. http www.csie.ntu.edu.twcjlinnmf hoyer nonnegative matrix factorization sparseness constraints journal machine learning search 2004. nndsvd introduced boutsidis gallopoulos svd based initialization head start nonnegative matrix factorization pattern recognition http www.cs.rpi.eduboutsclesnndsvd.pdf examples import numpy np.array 1.2 0.8 sklearn.decomposition import projectedgradientnmf model projectedgradientnmf n_components2 init0 model.fit projectedgradientnmf beta1 eta0.1 init0 max_iter200 n_components2 nls_max_iter2000 sparsenessnone tol0.0001 model.components_ array 0.77032744 0.38526873 0.11118662 0.38228063 model.reconstruction_err_ 0.00746 ... model projectedgradientnmf n_components2 init0 ... model.fit projectedgradientnmf beta1 eta0.1 init0 max_iter200 n_components2 sparsenesscomponents nls_max_iter2000 sparsenesscomponents tol0.0001 model.components_ array 1.67481991 
1604: 0.29614922 0.4681982 model.reconstruction_err_ 0.513.. 
1605: 1.8. reference scikitlearn user guide release 0.11 array n_components n_features number attributes compo nents_ recon struc tion_err_ methods nonnegative components data frobenius norm matrix difference training data reconstructed data produced model computed sparse input matrices expensive terms memory 
1606: learn nmf model data 
1607: fit fit_transform learn nmf model data returns transformed data get_params deep set_params params transform get parameters estimator set parameters estimator transform data according tted nmf model __init__ n_componentsnone max_iter200 nls_max_iter2000 initnndsvdar sparsenessnone beta1 eta0.1 tol0.0001 fit ynone params learn nmf model data 
1608: parameters arraylike sparse matrix shape n_samples n_features data matrix decomposed returns self fit_transform ynone learn nmf model data returns transformed data efcient calling followed transform 
1609: parameters arraylike sparse matrix shape n_samples n_features data matrix decomposed returns data array n_samples n_components transformed data get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self chapter user guide scikitlearn user guide release 0.11 transform transform data according tted nmf model parameters arraylike sparse matrix shape n_samples n_features data matrix transformed model returns data array n_samples n_components transformed data sklearn.decomposition.sparsepca class sklearn.decomposition.sparsepca n_components max_iter1000 u_initnone dom_statenone alpha1 ridge_alpha0.01 tol1e08 methodlars n_jobs1 ran v_initnone verbosefalse sparse principal components analysis sparsepca finds set sparse components optimally reconstruct data amount sparseness control lable coefcient penalty given parameter alpha 
1610: parameters n_components int number sparse atoms extract 
1611: alpha oat sparsity controlling parameter higher values lead sparser components 
1612: ridge_alpha oat amount ridge shrinkage apply order improve conditioning calling transform method 
1613: max_iter int maximum number iterations perform 
1614: tol oat tolerance stopping condition 
1615: method lars lars uses least angle regression method solve lasso problem lin ear_model.lars_path uses coordinate descent method compute lasso lution linear_model.lasso lars faster estimated components sparse 
1616: n_jobs int number parallel jobs run 
1617: u_init array shape n_samples n_atoms initial values loadings warm restart scenarios 
1618: v_init array shape n_atoms n_features initial values components warm restart scenarios 
1619: verbose degree verbosity printed output 
1620: random_state int randomstate 1.8. reference scikitlearn user guide release 0.11 pseudo number generator state used random sampling 
1621: see also pca minibatchsparsepca dictionarylearning attributes components_ error_ array n_components n_features array sparse components extracted data vector errors iteration 
1622: methods fit fit_transform get_params deep set_params params transform ridge_alpha least squares projection data onto sparse components 
1623: fit model data fit data transform get parameters estimator set parameters estimator 
1624: __init__ n_components alpha1 ridge_alpha0.01 max_iter1000 tol1e08 methodlars n_jobs1 u_initnone v_initnone verbosefalse random_statenone fit ynone fit model data 
1625: parameters arraylike shape n_samples n_features training vector n_samples number samples n_features num ber features returns self object returns instance 
1626: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
1627: parameters numpy array shape n_samples n_features training set 
1628: numpy array shape n_samples target values 
1629: returns x_new numpy array shape n_samples n_features_new transformed array 
1630: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
1631: get_params deeptrue get parameters estimator chapter user guide scikitlearn user guide release 0.11 parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform ridge_alphanone least squares projection data onto sparse components avoid instability issues case system underdetermined regularization applied ridge regression via ridge_alpha parameter note sparse pca components orthogonality enforced pca hence one use simple linear projection 
1632: parameters array shape n_samples n_features test data transformed must number features data used train model 
1633: ridge_alpha oat default 0.01 amount ridge shrinkage apply order improve conditioning 
1634: returns x_new array shape n_samples n_components transformed data 
1635: sklearn.decomposition.minibatchsparsepca class sklearn.decomposition.minibatchsparsepca n_components alpha1 call ridge_alpha0.01 backnone chunk_size3 verbosefalse shufetrue n_jobs1 methodlars random_statenone n_iter100 minibatch sparse principal components analysis finds set sparse components optimally reconstruct data amount sparseness control lable coefcient penalty given parameter alpha 
1636: parameters n_components int number sparse atoms extract alpha int sparsity controlling parameter higher values lead sparser components 
1637: ridge_alpha oat amount ridge shrinkage apply order improve conditioning calling transform method 
1638: n_iter int number iterations perform mini batch 1.8. reference scikitlearn user guide release 0.11 callback callable callable gets invoked every iterations chunk_size int number features take mini batch verbose degree output procedure print shufe boolean whether shufe data splitting batches n_jobs int number parallel jobs run autodetect 
1639: method lars uses least angle regression method solve lasso problem lin lars ear_model.lars_path uses coordinate descent method compute lasso lution linear_model.lasso lars faster estimated components sparse 
1640: random_state int randomstate pseudo number generator state used random sampling 
1641: see also pca sparsepca dictionarylearning attributes components_ error_ array n_components n_features array sparse components extracted data vector errors iteration 
1642: methods fit fit_transform get_params deep set_params params transform ridge_alpha least squares projection data onto sparse components 
1643: fit model data fit data transform get parameters estimator set parameters estimator 
1644: __init__ n_components alpha1 ridge_alpha0.01 n_iter100 callbacknone chunk_size3 verbosefalse shufetrue n_jobs1 methodlars random_statenone fit ynone fit model data 
1645: parameters arraylike shape n_samples n_features training vector n_samples number samples n_features num ber features returns self object returns instance 
1646: chapter user guide scikitlearn user guide release 0.11 fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
1647: parameters numpy array shape n_samples n_features training set 
1648: numpy array shape n_samples target values 
1649: returns x_new numpy array shape n_samples n_features_new transformed array 
1650: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
1651: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform ridge_alphanone least squares projection data onto sparse components avoid instability issues case system underdetermined regularization applied ridge regression via ridge_alpha parameter note sparse pca components orthogonality enforced pca hence one use simple linear projection 
1652: parameters array shape n_samples n_features test data transformed must number features data used train model 
1653: ridge_alpha oat default 0.01 amount ridge shrinkage apply order improve conditioning 
1654: returns x_new array shape n_samples n_components transformed data 
1655: 1.8. reference scikitlearn user guide release 0.11 sklearn.decomposition.sparsecoder class sklearn.decomposition.sparsecoder dictionary transform_n_nonzero_coefsnone form_alphanone split_signfalse n_jobs1 transform_algorithmomp trans sparse coding finds sparse representation data xed precomputed dictionary row result solution sparse coding problem goal sparse array code code dictionary parameters dictionary array n_atoms n_features dictionary atoms used sparse coding lines assumed normalized unit norm 
1656: transform_algorithm lasso_lars lasso_cd lars omp threshold algorithm used transform data lars uses least angle regression method lin ear_model.lars_path lasso_lars uses lars compute lasso solution lasso_cd uses coordinate descent method compute lasso solution linear_model.lasso lasso_lars faster estimated components sparse omp uses orthogonal matching pursuit estimate sparse solution threshold squashes zero coef cients less alpha projection dictionary transform_n_nonzero_coefs int 0.1 n_features default number nonzero coefcients target column solution used algorithmlars algorithmomp overridden alpha omp case 
1657: transform_alpha oat default algorithmlasso_lars algorithmlasso_cd alpha penalty applied norm algorithmthreshold alpha absolute value threshold coefcients squashed zero algorithmomp alpha toler ance parameter value reconstruction error targeted case overrides n_nonzero_coefs 
1658: split_sign bool false default whether split sparse feature vector concatenation negative part positive part improve performance downstream classiers 
1659: n_jobs int number parallel jobs run see also dictionarylearning minibatchsparsepca sparse_encode minibatchdictionarylearning sparsepca attributes components_ array n_atoms n_features unchanged dictionary atoms chapter user guide scikitlearn user guide release 0.11 methods fit fit_transform get_params deep set_params params transform nothing return estimator unchanged fit data transform get parameters estimator set parameters estimator encode data sparse combination dictionary atoms 
1660: __init__ dictionary form_alphanone split_signfalse n_jobs1 transform_algorithmomp transform_n_nonzero_coefsnone trans fit ynone nothing return estimator unchanged method implement usual api hence work pipelines 
1661: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
1662: parameters numpy array shape n_samples n_features training set 
1663: numpy array shape n_samples target values 
1664: returns x_new numpy array shape n_samples n_features_new transformed array 
1665: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
1666: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform ynone encode data sparse combination dictionary atoms coding method determined object parameter transform_algorithm 
1667: parameters array shape n_samples n_features 1.8. reference scikitlearn user guide release 0.11 test data transformed must number features data used train model 
1668: returns x_new array shape n_samples n_components transformed data sklearn.decomposition.dictionarylearning class sklearn.decomposition.dictionarylearning n_atoms tol1e08 transform_algorithmomp form_n_nonzero_coefsnone form_alphanone code_initnone bosefalse dom_statenone max_iter1000 t_algorithmlars trans trans n_jobs1 ver ran alpha1 dict_initnone split_signfalse dictionary learning finds dictionary set atoms best used represent data using sparse code solves optimization problem argmin 0.5 _22 alpha v_k n_atoms parameters n_atoms int number dictionary elements extract alpha int sparsity controlling parameter max_iter int maximum number iterations perform tol oat tolerance numerical error t_algorithm lars lars uses least angle regression method solve lasso problem lin ear_model.lars_path uses coordinate descent method compute lasso lution linear_model.lasso lars faster estimated components sparse 
1669: transform_algorithm lasso_lars lasso_cd lars omp threshold algorithm used transform data lars uses least angle regression method lin ear_model.lars_path lasso_lars uses lars compute lasso solution lasso_cd uses coordinate descent method compute lasso solution linear_model.lasso lasso_lars faster estimated components sparse omp uses orthogonal matching pursuit estimate sparse solution threshold squashes zero coef cients less alpha projection dictionary transform_n_nonzero_coefs int 0.1 n_features default chapter user guide scikitlearn user guide release 0.11 number nonzero coefcients target column solution used algorithmlars algorithmomp overridden alpha omp case 
1670: transform_alpha oat default algorithmlasso_lars algorithmlasso_cd alpha penalty applied norm algorithmthreshold alpha absolute value threshold coefcients squashed zero algorithmomp alpha toler ance parameter value reconstruction error targeted case overrides n_nonzero_coefs 
1671: split_sign bool false default whether split sparse feature vector concatenation negative part positive part improve performance downstream classiers 
1672: n_jobs int number parallel jobs run code_init array shape n_samples n_atoms initial value code warm restart dict_init array shape n_atoms n_features initial values dictionary warm restart verbose degree verbosity printed output random_state int randomstate pseudo number generator state used random sampling 
1673: see also sparsecoder minibatchdictionarylearning sparsepca minibatchsparsepca notes references mairal bach http www.di.ens.frsierrapdfsicml09.pdf ponce sapiro online dictionary learning sparse coding attributes components_ error_ array n_atoms n_features array dictionary atoms extracted data vector errors iteration methods fit fit_transform fit model data fit data transform continued next page 1.8. reference scikitlearn user guide release 0.11 table 1.55 continued previous page get_params deep set_params params transform get parameters estimator set parameters estimator encode data sparse combination dictionary atoms 
1674: __init__ n_atoms alpha1 form_algorithmomp n_jobs1 dom_statenone code_initnone dict_initnone verbosefalse max_iter1000 tol1e08 transform_n_nonzero_coefsnone t_algorithmlars trans transform_alphanone split_signfalse ran fit ynone fit model data 
1675: parameters arraylike shape n_samples n_features training vector n_samples number samples n_features num ber features returns self object returns object fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
1676: parameters numpy array shape n_samples n_features training set 
1677: numpy array shape n_samples target values 
1678: returns x_new numpy array shape n_samples n_features_new transformed array 
1679: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
1680: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self chapter user guide scikitlearn user guide release 0.11 transform ynone encode data sparse combination dictionary atoms coding method determined object parameter transform_algorithm 
1681: parameters array shape n_samples n_features test data transformed must number features data used train model 
1682: returns x_new array shape n_samples n_components transformed data sklearn.decomposition.minibatchdictionarylearning class sklearn.decomposition.minibatchdictionarylearning n_atoms n_iter1000 chunk_size3 pha1 t_algorithmlars n_jobs1 shufetrue dict_initnone trans form_algorithmomp trans form_n_nonzero_coefsnone transform_alphanone ver bosefalse split_signfalse random_statenone minibatch dictionary learning finds dictionary set atoms best used represent data using sparse code solves optimization problem argmin 0.5 _22 alpha v_k n_atoms parameters n_atoms int number dictionary elements extract alpha int sparsity controlling parameter n_iter int total number iterations perform t_algorithm lars lars uses least angle regression method solve lasso problem lin ear_model.lars_path uses coordinate descent method compute lasso lution linear_model.lasso lars faster estimated components sparse 
1683: transform_algorithm lasso_lars lasso_cd lars omp threshold algorithm used transform data lars uses least angle regression method lin ear_model.lars_path lasso_lars uses lars compute lasso solution lasso_cd uses coordinate descent method compute lasso solution linear_model.lasso lasso_lars faster estimated components sparse omp uses orthogonal 1.8. reference scikitlearn user guide release 0.11 matching pursuit estimate sparse solution threshold squashes zero coef cients less alpha projection dictionary transform_n_nonzero_coefs int 0.1 n_features default number nonzero coefcients target column solution used algorithmlars algorithmomp overridden alpha omp case 
1684: transform_alpha oat default algorithmlasso_lars algorithmlasso_cd alpha penalty applied norm algorithmthreshold alpha absolute value threshold coefcients squashed zero algorithmomp alpha toler ance parameter value reconstruction error targeted case overrides n_nonzero_coefs 
1685: split_sign bool false default whether split sparse feature vector concatenation negative part positive part improve performance downstream classiers 
1686: n_jobs int number parallel jobs run dict_init array shape n_atoms n_features initial value dictionary warm restart scenarios verbose degree verbosity printed output chunk_size int number samples minibatch shufe bool whether shufe samples forming batches random_state int randomstate pseudo number generator state used random sampling 
1687: see also sparsecoder dictionarylearning sparsepca minibatchsparsepca notes references mairal bach http www.di.ens.frsierrapdfsicml09.pdf ponce sapiro online dictionary learning sparse coding attributes components_ array n_atoms n_features components extracted data chapter user guide methods scikitlearn user guide release 0.11 fit fit_transform get_params deep partial_fit iter_offset updates model using data minibatch set_params params transform fit model data fit data transform get parameters estimator set parameters estimator encode data sparse combination dictionary atoms 
1688: __init__ n_atoms alpha1 n_iter1000 t_algorithmlars n_jobs1 chunk_size3 shuf etrue dict_initnone transform_algorithmomp transform_n_nonzero_coefsnone transform_alphanone verbosefalse split_signfalse random_statenone fit ynone fit model data 
1689: parameters arraylike shape n_samples n_features training vector n_samples number samples n_features num ber features returns self object returns instance 
1690: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
1691: parameters numpy array shape n_samples n_features training set 
1692: numpy array shape n_samples target values 
1693: returns x_new numpy array shape n_samples n_features_new transformed array 
1694: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
1695: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1696: partial_fit ynone iter_offset0 updates model using data minibatch 
1697: parameters arraylike shape n_samples n_features 1.8. reference scikitlearn user guide release 0.11 training vector n_samples number samples n_features num ber features returns self object returns instance 
1698: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform ynone encode data sparse combination dictionary atoms coding method determined object parameter transform_algorithm 
1699: parameters array shape n_samples n_features test data transformed must number features data used train model 
1700: returns x_new array shape n_samples n_components transformed data decomposition.fastica n_components ... decomposition.dict_learning n_atoms alpha decomposition.dict_learning_online ... ... decomposition.sparse_encode dictionary ... perform fast independent component analysis solves dictionary learning matrix factorization problem solves dictionary learning matrix factorization problem online sparse coding sklearn.decomposition.fastica sklearn.decomposition.fastica n_componentsnone algorithmparallel whitentrue fun_args max_iter200 funlogcosh tol0.0001 w_initnone random_statenone fun_prime perform fast independent component analysis 
1701: parameters arraylike shape n_samples n_features training vector n_samples number samples n_features number features 
1702: n_components int optional number components extract none dimension reduction performed 
1703: algorithm parallel deation optional apply parallel deational fastica algorithm 
1704: whiten boolean optional true perform initial whitening data false data assumed already preprocessed centered normed white otherwise get incorrect results case parameter n_components ignored 
1705: fun string function optional chapter user guide scikitlearn user guide release 0.11 functional form function used approximation negentropy could either logcosh exp cube also provide function case derivative provided via argument fun_prime fun_prime empty string function optional see fun 
1706: fun_args dictionary optional empty funlogcosh fun_args take value alpha 1.0 max_iter int optional maximum number iterations perform tol oat optional positive scalar giving tolerance unmixing matrix considered converged w_init n_components n_components array optional initial unmixing array dimension n.comp n.comp none default array normal r.v.s used source_only boolean optional true sources matrix returned random_state int randomstate pseudo number generator state used random sampling 
1707: returns n_components array none whiten true prewhitening matrix projects data onto rst n.comp principal components whiten false none 
1708: n_components n_components array estimated unmixing matrix mixing matrix obtained np.dot k.t w.t w.t n_components array estimated source matrix notes data matrix considered linear combination nongaussian independent components i.e columns contain independent components linear mixing matrix short ica attempts unmix data estimating unmixing matrix implementation originally made data shape n_features n_samples input transposed algorithm applied makes slightly faster fortranordered input implemented using fastica hyvarinen oja independent component analysis algorithms applications neural networks 1.8. reference scikitlearn user guide release 0.11 sklearn.decomposition.dict_learning sklearn.decomposition.dict_learning code_initnone random_statenone n_atoms methodlars n_jobs1 callbacknone solves dictionary learning matrix factorization problem finds best dictionary corresponding sparse code approximating data matrix solving alpha max_iter100 tol1e dict_initnone verbosefalse argmin 0.5 _22 alpha v_k n_atoms dictionary sparse code 
1709: parameters array shape n_samples n_features data matrix n_atoms int number dictionary atoms extract 
1710: alpha int sparsity controlling parameter 
1711: max_iter int maximum number iterations perform 
1712: tol oat tolerance stopping condition 
1713: method lars lars uses least angle regression method solve lasso problem lin ear_model.lars_path uses coordinate descent method compute lasso lution linear_model.lasso lars faster estimated components sparse 
1714: n_jobs int number parallel jobs run autodetect 
1715: dict_init array shape n_atoms n_features initial value dictionary warm restart scenarios 
1716: code_init array shape n_samples n_atoms initial value sparse code warm restart scenarios 
1717: callback callable gets invoked every iterations 
1718: verbose degree output procedure print 
1719: random_state int randomstate pseudo number generator state used random sampling 
1720: returns code array shape n_samples n_atoms chapter user guide sparse code factor matrix factorization dictionary array shape n_atoms n_features dictionary factor matrix factorization 
1721: errors array vector errors iteration 
1722: see also dict_learning_online sparsepca minibatchsparsepca dictionarylearning sklearn.decomposition.dict_learning_online scikitlearn user guide release 0.11 minibatchdictionarylearning sklearn.decomposition.dict_learning_online n_atoms alpha dict_initnone turn_codetrue backnone shufetrue iter_offset0 random_statenone chunk_size3 n_jobs1 n_iter100 call verbosefalse methodlars solves dictionary learning matrix factorization problem online finds best dictionary corresponding sparse code approximating data matrix solving argmin 0.5 _22 alpha v_k n_atoms dictionary sparse code accomplished repeatedly iterating mini batches slicing input data 
1723: parameters array shape n_samples n_features data matrix n_atoms int number dictionary atoms extract alpha int sparsity controlling parameter n_iter int number iterations perform return_code boolean whether also return code dictionary dict_init array shape n_atoms n_features initial value dictionary warm restart scenarios callback callable gets invoked every iterations chunk_size int number samples take batch verbose 1.8. reference scikitlearn user guide release 0.11 degree output procedure print shufe boolean whether shufe data splitting batches n_jobs int number parallel jobs run autodetect 
1724: method lars lars uses least angle regression method solve lasso problem lin ear_model.lars_path uses coordinate descent method compute lasso lution linear_model.lasso lars faster estimated components sparse 
1725: iter_offset int default number previous iterations completed dictionary used initialization random_state int randomstate pseudo number generator state used random sampling 
1726: returns code array shape n_samples n_atoms sparse code returned return_codetrue dictionary array shape n_atoms n_features solutions dictionary learning problem see also dict_learning dictionarylearning minibatchdictionarylearning sparsepca minibatchsparsepca sklearn.decomposition.sparse_encode sklearn.decomposition.sparse_encode dictionary rithmlasso_lars phanone initnone max_iter1000 n_jobs1 copy_gramnone gramnone covnone n_nonzero_coefsnone algo copy_covtrue sparse coding row result solution sparse coding problem goal sparse array code code dictionary parameters array shape n_samples n_features data matrix dictionary array shape n_atoms n_features dictionary matrix solve sparse coding data algorithms assume normalized rows meaningful output 
1727: gram array shape n_atoms n_atoms precomputed gram matrix dictionary dictionary cov array shape n_atoms n_samples chapter user guide scikitlearn user guide release 0.11 precomputed covariance dictionary algorithm lasso_lars lasso_cd lars omp threshold lars uses least angle regression method linear_model.lars_path lasso_lars uses lars compute lasso solution lasso_cd uses coordinate descent method compute lasso solution linear_model.lasso lasso_lars faster timated components sparse omp uses orthogonal matching pursuit estimate sparse solution threshold squashes zero coefcients less alpha projection dictionary n_nonzero_coefs int 0.1 n_features default number nonzero coefcients target column solution used algorithmlars algorithmomp overridden alpha omp case 
1728: alpha oat default algorithmlasso_lars algorithmlasso_cd alpha penalty applied norm algorithmthrehold alpha absolute value threshold coefcients squashed zero algorithmomp alpha toler ance parameter value reconstruction error targeted case overrides n_nonzero_coefs 
1729: init array shape n_samples n_atoms initialization value sparse codes used algorithmlasso_cd 
1730: max_iter int default maximum number iterations perform algorithmlasso_cd 
1731: copy_cov boolean optional whether copy precomputed covariance matrix false may overwritten 
1732: n_jobs int optional number parallel jobs run 
1733: returns code array shape n_samples n_atoms sparse codes see also sklearn.linear_model.lars_path sklearn.linear_model.lasso sparsecoder sklearn.linear_model.orthogonal_mp 1.8.6 sklearn.ensemble ensemble methods sklearn.ensemble module includes ensemblebased methods classication regression user guide see ensemble methods section details 
1734: ensemble.randomforestclassifier ... ensemble.randomforestregressor ... ensemble.extratreesclassifier ... ensemble.extratreesregressor n_estimators ... ensemble.gradientboostingclassifier loss ... gradient boosting classication continued next page random forest classier random forest regressor extratrees classier extratrees regressor 
1735: 1.8. reference scikitlearn user guide release 0.11 ensemble.gradientboostingregressor loss ... gradient boosting regression 
1736: table 1.58 continued previous page sklearn.ensemble.randomforestclassier class sklearn.ensemble.randomforestclassifier n_estimators10 criteriongini min_samples_split1 max_depthnone min_samples_leaf1 min_density0.1 max_featuresauto bootstraptrue com pute_importancesfalse oob_scorefalse n_jobs1 random_statenone verbose0 random forest classier random forest meta estimator number classical decision trees various subsamples dataset use averaging improve predictive accuracy control overtting 
1737: parameters n_estimators integer optional default10 number trees forest 
1738: criterion string optional defaultgini function measure quality split supported criteria gini gini impurity entropy information gain note parameter treespecic 
1739: max_depth integer none optional defaultnone maximum depth tree none nodes expanded leaves pure leaves contain less min_samples_split samples note parameter treespecic 
1740: min_samples_split integer optional default1 minimum number samples required split internal node note parame ter treespecic 
1741: min_samples_leaf integer optional default1 minimum number samples newly created leaves split discarded split one leaves would contain less min_samples_leaf samples note parameter treespecic 
1742: min_density oat optional default0.1 parameter controls tradeoff optimization heuristic controls minimum density sample_mask i.e fraction samples mask density falls threshold mask recomputed input data packed results min_density equals one partitions always represented data copying copies original data otherwise partitions represented bit masks aka sample masks note parameter treespecic 
1743: max_features int string none optional defaultauto number features consider looking best split max_featuressqrt n_features classication tasks auto max_featuresn_features regression problems sqrt max_featuressqrt n_features log2 max_featureslog2 n_features none max_featuresn_features 
1744: chapter user guide scikitlearn user guide release 0.11 note parameter treespecic 
1745: bootstrap boolean optional defaulttrue whether bootstrap samples used building trees 
1746: compute_importances boolean optional defaulttrue whether computed feature_importances_ attribute calling 
1747: importances feature stored oob_score bool whether use outofbag samples estimate generalization error 
1748: n_jobs integer optional default1 number jobs run parallel number jobs set number cores 
1749: random_state int randomstate instance none optional defaultnone int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used np.random 
1750: verbose int optional default0 controlls verbosity tree building process 
1751: see also decisiontreeclassifier extratreesclassifier references r59 attributes fea ture_importances_ oob_score_ array shape n_features oat oob_decision_function_array shape n_samples n_classes feature importances higher important feature score training dataset obtained using outofbag estimate decision function computed outofbag estimate training set 
1752: methods fit fit_transform get_params deep predict predict_log_proba predict_proba score build forest trees training set fit data transform get parameters estimator predict class predict class logprobabilities predict class probabilities returns mean accuracy given test data labels continued next page 1.8. reference scikitlearn user guide release 0.11 set_params params transform threshold reduce important features 
1753: table 1.59 continued previous page set parameters estimator 
1754: __init__ n_estimators10 criteriongini max_depthnone min_samples_leaf1 min_density0.1 max_featuresauto bootstraptrue pute_importancesfalse oob_scorefalse n_jobs1 random_statenone verbose0 min_samples_split1 com fit build forest trees training set 
1755: parameters arraylike shape n_samples n_features training input samples 
1756: arraylike shape n_samples target values integers correspond classes classication real numbers regression 
1757: returns self object returns self 
1758: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
1759: parameters numpy array shape n_samples n_features training set 
1760: numpy array shape n_samples target values 
1761: returns x_new numpy array shape n_samples n_features_new transformed array 
1762: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
1763: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1764: predict predict class predicted class input sample computed majority prediction trees forest 
1765: parameters arraylike shape n_samples n_features input samples 
1766: returns array shape n_samples chapter user guide scikitlearn user guide release 0.11 predicted classes 
1767: predict_log_proba predict class logprobabilities predicted class logprobabilities input sample computed mean predicted class log probabilities trees forest 
1768: parameters arraylike shape n_samples n_features input samples 
1769: returns array shape n_samples class logprobabilities input samples classes ordered arithmetical order predict_proba predict class probabilities predicted class probabilities input sample computed mean predicted class probabilities trees forest 
1770: parameters arraylike shape n_samples n_features input samples 
1771: returns array shape n_samples class probabilities input samples classes ordered arithmetical order 
1772: score returns mean accuracy given test data labels 
1773: parameters arraylike shape n_samples n_features training set 
1774: arraylike shape n_samples labels 
1775: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform thresholdnone reduce important features 
1776: parameters array scipy sparse matrix shape n_samples n_features input samples 
1777: threshold string oat none optional defaultnone threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor e.g. 1.25mean may also used none available object attribute threshold used otherwise mean used default 
1778: 1.8. reference scikitlearn user guide release 0.11 returns x_r array shape n_samples n_selected_features input samples selected features 
1779: sklearn.ensemble.randomforestregressor class sklearn.ensemble.randomforestregressor n_estimators10 criterionmse min_samples_split1 max_depthnone min_density0.1 min_samples_leaf1 max_featuresauto bootstraptrue com pute_importancesfalse oob_scorefalse n_jobs1 random_statenone verbose0 random forest regressor random forest meta estimator number classical decision trees various subsamples dataset use averaging improve predictive accuracy control overtting 
1780: parameters n_estimators integer optional default10 number trees forest 
1781: criterion string optional defaultmse function measure quality split supported criterion mse mean squared error note parameter treespecic 
1782: max_depth integer none optional defaultnone maximum depth tree none nodes expanded leaves pure leaves contain less min_samples_split samples note parameter treespecic 
1783: min_samples_split integer optional default1 minimum number samples required split internal node note parame ter treespecic 
1784: min_samples_leaf integer optional default1 minimum number samples newly created leaves split discarded split one leaves would contain less min_samples_leaf samples note parameter treespecic 
1785: min_density oat optional default0.1 parameter controls tradeoff optimization heuristic controls minimum density sample_mask i.e fraction samples mask density falls threshold mask recomputed input data packed results min_density equals one partitions always represented data copying copies original data otherwise partitions represented bit masks aka sample masks note parameter treespecic 
1786: max_features int string none optional defaultauto number features consider looking best split max_featuressqrt n_features classication tasks auto max_featuresn_features regression problems sqrt max_featuressqrt n_features log2 max_featureslog2 n_features 
1787: chapter user guide scikitlearn user guide release 0.11 none max_featuresn_features 
1788: note parameter treespecic 
1789: bootstrap boolean optional defaulttrue whether bootstrap samples used building trees 
1790: compute_importances boolean optional defaulttrue whether computed feature_importances_ attribute calling 
1791: importances feature stored oob_score bool whether use outofbag samples estimate generalization error 
1792: n_jobs integer optional default1 number jobs run parallel number jobs set number cores 
1793: random_state int randomstate instance none optional defaultnone int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used np.random 
1794: verbose int optional default0 controlls verbosity tree building process 
1795: see also decisiontreeregressor extratreesregressor references r60 attributes fea ture_importances_ oob_score_ array shape n_features oat oob_prediction_ array shape n_samples feature mportances higher important feature score training dataset obtained using outofbag estimate prediction computed outofbag estimate training set 
1796: methods fit fit_transform get_params deep predict score build forest trees training set fit data transform get parameters estimator predict regression target returns coefcient determination prediction continued next page 1.8. reference scikitlearn user guide release 0.11 table 1.60 continued previous page set_params params transform threshold reduce important features 
1797: set parameters estimator 
1798: __init__ n_estimators10 criterionmse max_depthnone min_samples_leaf1 min_density0.1 max_featuresauto bootstraptrue pute_importancesfalse oob_scorefalse n_jobs1 random_statenone verbose0 min_samples_split1 com fit build forest trees training set 
1799: parameters arraylike shape n_samples n_features training input samples 
1800: arraylike shape n_samples target values integers correspond classes classication real numbers regression 
1801: returns self object returns self 
1802: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
1803: parameters numpy array shape n_samples n_features training set 
1804: numpy array shape n_samples target values 
1805: returns x_new numpy array shape n_samples n_features_new transformed array 
1806: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
1807: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1808: predict predict regression target predicted regression target input sample computed mean predicted regression targets trees forest 
1809: parameters arraylike shape n_samples n_features input samples 
1810: chapter user guide scikitlearn user guide release 0.11 returns array shape n_samples predicted values 
1811: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
1812: parameters arraylike shape n_samples n_features training set 
1813: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform thresholdnone reduce important features 
1814: parameters array scipy sparse matrix shape n_samples n_features input samples 
1815: threshold string oat none optional defaultnone threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor e.g. 1.25mean may also used none available object attribute threshold used otherwise mean used default 
1816: returns x_r array shape n_samples n_selected_features input samples selected features 
1817: sklearn.ensemble.extratreesclassier class sklearn.ensemble.extratreesclassifier n_estimators10 max_depthnone min_samples_leaf1 max_featuresauto bootstrapfalse pute_importancesfalse n_jobs1 random_statenone verbose0 criteriongini min_samples_split1 min_density0.1 com oob_scorefalse extratrees classier class implements meta estimator number randomized decision trees a.k.a extratrees various subsamples dataset use averaging improve predictive accuracy control overtting 
1818: parameters n_estimators integer optional default10 number trees forest 
1819: 1.8. reference scikitlearn user guide release 0.11 criterion string optional defaultgini function measure quality split supported criteria gini gini impurity entropy information gain note parameter treespecic 
1820: max_depth integer none optional defaultnone maximum depth tree none nodes expanded leaves pure leaves contain less min_samples_split samples note parameter treespecic 
1821: min_samples_split integer optional default1 minimum number samples required split internal node note parame ter treespecic 
1822: min_samples_leaf integer optional default1 minimum number samples newly created leaves split discarded split one leaves would contain less min_samples_leaf samples note parameter treespecic 
1823: min_density oat optional default0.1 parameter controls tradeoff optimization heuristic controls minimum density sample_mask i.e fraction samples mask density falls threshold mask recomputed input data packed results min_density equals one partitions always represented data copying copies original data otherwise partitions represented bit masks aka sample masks note parameter treespecic 
1824: max_features int string none optional defaultauto number features consider looking best split 
1825: max_featuressqrt n_features classication tasks auto max_featuresn_features regression problems sqrt max_featuressqrt n_features log2 max_featureslog2 n_features none max_featuresn_features 
1826: note parameter treespecic 
1827: bootstrap boolean optional defaultfalse whether bootstrap samples used building trees 
1828: compute_importances boolean optional defaulttrue whether computed feature_importances_ attribute calling 
1829: importances feature stored oob_score bool whether use outofbag samples estimate generalization error 
1830: n_jobs integer optional default1 number jobs run parallel number jobs set number cores 
1831: random_state int randomstate instance none optional defaultnone chapter user guide scikitlearn user guide release 0.11 int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used np.random 
1832: verbose int optional default0 controlls verbosity tree building process 
1833: see also sklearn.tree.extratreeclassifierbase classier ensemble randomforestclassifierensemble classier based trees optimal splits 
1834: references r57 attributes fea ture_importances_ oob_score_ array shape n_features oat oob_decision_function_array shape n_samples n_classes feature mportances higher important feature score training dataset obtained using outofbag estimate decision function computed outofbag estimate training set 
1835: methods fit fit_transform get_params deep predict predict_log_proba predict_proba score set_params params transform threshold reduce important features 
1836: build forest trees training set fit data transform get parameters estimator predict class predict class logprobabilities predict class probabilities returns mean accuracy given test data labels set parameters estimator 
1837: __init__ n_estimators10 criteriongini max_depthnone min_samples_leaf1 min_density0.1 max_featuresauto bootstrapfalse pute_importancesfalse oob_scorefalse n_jobs1 random_statenone verbose0 min_samples_split1 com fit build forest trees training set 
1838: parameters arraylike shape n_samples n_features training input samples 
1839: arraylike shape n_samples target values integers correspond classes classication real numbers regression 
1840: 1.8. reference scikitlearn user guide release 0.11 returns self object returns self 
1841: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
1842: parameters numpy array shape n_samples n_features training set 
1843: numpy array shape n_samples target values 
1844: returns x_new numpy array shape n_samples n_features_new transformed array 
1845: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
1846: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1847: predict predict class predicted class input sample computed majority prediction trees forest 
1848: parameters arraylike shape n_samples n_features input samples 
1849: returns array shape n_samples predicted classes 
1850: predict_log_proba predict class logprobabilities predicted class logprobabilities input sample computed mean predicted class log probabilities trees forest 
1851: parameters arraylike shape n_samples n_features input samples 
1852: returns array shape n_samples class logprobabilities input samples classes ordered arithmetical order predict_proba predict class probabilities 
1853: chapter user guide scikitlearn user guide release 0.11 predicted class probabilities input sample computed mean predicted class probabilities trees forest 
1854: parameters arraylike shape n_samples n_features input samples 
1855: returns array shape n_samples class probabilities input samples classes ordered arithmetical order 
1856: score returns mean accuracy given test data labels 
1857: parameters arraylike shape n_samples n_features training set 
1858: arraylike shape n_samples labels 
1859: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform thresholdnone reduce important features 
1860: parameters array scipy sparse matrix shape n_samples n_features input samples 
1861: threshold string oat none optional defaultnone threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor e.g. 1.25mean may also used none available object attribute threshold used otherwise mean used default 
1862: returns x_r array shape n_samples n_selected_features input samples selected features 
1863: sklearn.ensemble.extratreesregressor class sklearn.ensemble.extratreesregressor n_estimators10 max_depthnone min_samples_leaf1 max_featuresauto pute_importancesfalse n_jobs1 random_statenone verbose0 criterionmse min_samples_split1 min_density0.1 com oob_scorefalse bootstrapfalse extratrees regressor class implements meta estimator number randomized decision trees a.k.a extratrees various subsamples dataset use averaging improve predictive accuracy control overtting 
1864: 1.8. reference scikitlearn user guide release 0.11 parameters n_estimators integer optional default10 number trees forest 
1865: criterion string optional defaultmse function measure quality split supported criterion mse mean squared error note parameter treespecic 
1866: max_depth integer none optional defaultnone none nodes expanded leaves maximum depth tree pure leaves contain less min_samples_split samples note parameter treespecic 
1867: min_samples_split integer optional default1 minimum number samples required split internal node note parame ter treespecic 
1868: min_samples_leaf integer optional default1 minimum number samples newly created leaves split discarded split one leaves would contain less min_samples_leaf samples note parameter treespecic 
1869: min_density oat optional default0.1 parameter controls tradeoff optimization heuristic controls minimum density sample_mask i.e fraction samples mask density falls threshold mask recomputed input data packed results min_density equals one partitions always represented data copying copies original data otherwise partitions represented bit masks aka sample masks note parameter treespecic 
1870: max_features int string none optional defaultauto number features consider looking best split max_featuressqrt n_features classication tasks auto max_featuresn_features regression problems sqrt max_featuressqrt n_features log2 max_featureslog2 n_features none max_featuresn_features 
1871: note parameter treespecic 
1872: bootstrap boolean optional defaultfalse whether bootstrap samples used building trees note parameter tree specic 
1873: compute_importances boolean optional defaulttrue computed whether feature_importances_ attribute calling 
1874: importances feature stored oob_score bool whether use outofbag samples estimate generalization error 
1875: n_jobs integer optional default1 chapter user guide scikitlearn user guide release 0.11 number jobs run parallel number jobs set number cores 
1876: random_state int randomstate instance none optional defaultnone int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used np.random 
1877: verbose int optional default0 controlls verbosity tree building process 
1878: see also sklearn.tree.extratreeregressorbase estimator ensemble randomforestregressorensemble regressor using trees optimal splits 
1879: references r58 attributes fea ture_importances_ oob_score_ array shape n_features oat oob_prediction_ array shape n_samples feature mportances higher important feature score training dataset obtained using outofbag estimate prediction computed outofbag estimate training set 
1880: methods fit fit_transform get_params deep predict score set_params params transform threshold reduce important features 
1881: build forest trees training set fit data transform get parameters estimator predict regression target returns coefcient determination prediction set parameters estimator 
1882: __init__ n_estimators10 criterionmse max_depthnone min_samples_leaf1 min_density0.1 max_featuresauto bootstrapfalse pute_importancesfalse oob_scorefalse n_jobs1 random_statenone verbose0 min_samples_split1 com fit build forest trees training set 
1883: parameters arraylike shape n_samples n_features training input samples 
1884: arraylike shape n_samples 1.8. reference scikitlearn user guide release 0.11 target values integers correspond classes classication real numbers regression 
1885: returns self object returns self 
1886: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
1887: parameters numpy array shape n_samples n_features training set 
1888: numpy array shape n_samples target values 
1889: returns x_new numpy array shape n_samples n_features_new transformed array 
1890: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
1891: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1892: predict predict regression target predicted regression target input sample computed mean predicted regression targets trees forest 
1893: parameters arraylike shape n_samples n_features input samples 
1894: returns array shape n_samples predicted values 
1895: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
1896: parameters arraylike shape n_samples n_features training set 
1897: arraylike shape n_samples returns oat chapter user guide scikitlearn user guide release 0.11 set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform thresholdnone reduce important features 
1898: parameters array scipy sparse matrix shape n_samples n_features input samples 
1899: threshold string oat none optional defaultnone threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor e.g. 1.25mean may also used none available object attribute threshold used otherwise mean used default 
1900: returns x_r array shape n_samples n_selected_features input samples selected features 
1901: sklearn.ensemble.gradientboostingclassier class sklearn.ensemble.gradientboostingclassifier lossdeviance learn_rate0.1 subsam n_estimators100 ple1.0 min_samples_split1 min_samples_leaf1 max_depth3 initnone random_statenone gradient boosting classication builds additive model forward stagewise fashion allows optimization arbitrary differen tiable loss functions stage n_classes_ regression trees negative gradient binomial multinomial deviance loss function binary classication special case single regression tree induced 
1902: parameters loss deviance optional defaultdeviance loss function optimized deviance refers deviance logistic regression classication probabilistic outputs refers least squares regression 
1903: learn_rate oat optional default0.1 learning rate shrinks contribution tree learn_rate tradeoff learn_rate n_estimators 
1904: n_estimators int default100 number boosting stages perform gradient boosting fairly robust tting large number usually results better performance 
1905: max_depth integer optional default3 maximum depth individual regression estimators maximum depth limits number nodes tree tune parameter best performance best value depends interaction input variables 
1906: 1.8. reference scikitlearn user guide release 0.11 min_samples_split integer optional default1 minimum number samples required split internal node 
1907: min_samples_leaf integer optional default1 minimum number samples required leaf node 
1908: subsample oat optional default1.0 fraction samples used tting individual base learners smaller 1.0 results stochastic gradient boosting subsample interacts parameter n_estimators 
1909: see also sklearn.tree.decisiontreeclassifier randomforestclassifier references friedman greedy function approximation gradient boosting machine annals statistics vol 
1910: 10.friedman stochastic gradient boosting hastie tibshirani friedman elements statistical learning springer 
1911: examples samples labels sklearn.ensemble import gradientboostingclassifier gradientboostingclassifier .fit samples labels print gb.predict 0.5 methods fit fit_stage x_argsorted y_pred ... get_params deep predict predict_proba score set_params params staged_decision_function fit gradient boosting model fit another stage n_classes_ trees boosting model get parameters estimator predict class predict class probabilities returns mean accuracy given test data labels set parameters estimator compute decision function 
1912: __init__ lossdeviance learn_rate0.1 n_estimators100 subsample1.0 min_samples_split1 min_samples_leaf1 max_depth3 initnone random_statenone fit fit gradient boosting model 
1913: parameters arraylike shape n_samples n_features training vectors n_samples number samples n_features num chapter user guide scikitlearn user guide release 0.11 ber features use fortranstyle avoid memory copies 
1914: arraylike shape n_samples target values integers classication real numbers regression classication labels must correspond classes ... n_classes_1 returns self object returns self 
1915: fit_stage x_argsorted y_pred sample_mask fit another stage n_classes_ trees boosting model 
1916: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1917: predict predict class 
1918: parameters arraylike shape n_samples n_features input samples 
1919: returns array shape n_samples predicted classes 
1920: predict_proba predict class probabilities 
1921: parameters arraylike shape n_samples n_features input samples 
1922: returns array shape n_samples class probabilities input samples classes ordered arithmetical order 
1923: score returns mean accuracy given test data labels 
1924: parameters arraylike shape n_samples n_features training set 
1925: arraylike shape n_samples labels 
1926: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self 1.8. reference scikitlearn user guide release 0.11 staged_decision_function compute decision function method allows monitoring i.e determine error testing set stage 
1927: parameters arraylike shape n_samples n_features input samples 
1928: returns array shape n_samples n_classes decision function input samples classes ordered arithmetical order regression binary classication special cases n_classes 
1929: sklearn.ensemble.gradientboostingregressor class sklearn.ensemble.gradientboostingregressor lossls n_estimators100 ple1.0 min_samples_leaf1 initnone random_statenone learn_rate0.1 subsam min_samples_split1 max_depth3 gradient boosting regression builds additive model forward stagewise fashion allows optimization arbitrary differ entiable loss functions stage regression tree negative gradient given loss function 
1930: parameters loss lad optional defaultls loss function optimized refers least squares regression lad least absolute deviation highly robust loss function soley based order information input variables 
1931: learn_rate oat optional default0.1 learning rate shrinks contribution tree learn_rate tradeoff learn_rate n_estimators 
1932: n_estimators int default100 number boosting stages perform gradient boosting fairly robust tting large number usually results better performance 
1933: max_depth integer optional default3 maximum depth individual regression estimators maximum depth limits number nodes tree tune parameter best performance best value depends interaction input variables 
1934: min_samples_split integer optional default1 minimum number samples required split internal node 
1935: min_samples_leaf integer optional default1 minimum number samples required leaf node 
1936: subsample oat optional default1.0 fraction samples used tting individual base learners smaller 1.0 results stochastic gradient boosting subsample interacts parameter n_estimators 
1937: chapter user guide scikitlearn user guide release 0.11 see also sklearn.tree.decisiontreeregressor randomforestregressor references friedman greedy function approximation gradient boosting machine annals statistics vol 
1938: 10.friedman stochastic gradient boosting hastie tibshirani friedman elements statistical learning springer 
1939: examples samples labels sklearn.ensemble import gradientboostingregressor gradientboostingregressor .fit samples labels print gb.predict 1.32806997e05 attributes fea ture_importances_ array shape n_features oob_score_ array shape n_estimators train_score_ array shape n_estimators methods feature importances higher important feature 
1940: score training dataset obtained using outofbag estimate ith score oob_score_ deviance loss model iteration outofbag sample ith score train_score_ deviance loss model iteration inbag sample subsample deviance training data 
1941: fit fit_stage x_argsorted y_pred ... get_params deep predict score set_params params staged_decision_function staged_predict fit gradient boosting model fit another stage n_classes_ trees boosting model get parameters estimator predict regression target returns coefcient determination prediction set parameters estimator compute decision function predict regression target stage 
1942: __init__ lossls min_samples_leaf1 max_depth3 initnone random_statenone learn_rate0.1 n_estimators100 subsample1.0 min_samples_split1 fit fit gradient boosting model 
1943: 1.8. reference scikitlearn user guide release 0.11 parameters arraylike shape n_samples n_features training vectors n_samples number samples n_features num ber features use fortranstyle avoid memory copies 
1944: arraylike shape n_samples target values integers classication real numbers regression classication labels must correspond classes ... n_classes_1 returns self object returns self 
1945: fit_stage x_argsorted y_pred sample_mask fit another stage n_classes_ trees boosting model 
1946: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1947: predict predict regression target 
1948: parameters arraylike shape n_samples n_features input samples 
1949: returns array shape n_samples predicted values 
1950: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
1951: parameters arraylike shape n_samples n_features training set 
1952: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self staged_decision_function compute decision function method allows monitoring i.e determine error testing set stage 
1953: parameters arraylike shape n_samples n_features chapter user guide scikitlearn user guide release 0.11 input samples 
1954: returns array shape n_samples n_classes decision function input samples classes ordered arithmetical order regression binary classication special cases n_classes 
1955: staged_predict predict regression target stage method allows monitoring i.e determine error testing set stage 
1956: parameters arraylike shape n_samples n_features input samples 
1957: returns array shape n_samples predicted value input samples 
1958: 1.8.7 sklearn.feature_extraction feature extraction sklearn.feature_extraction module deals feature extraction raw data currently includes methods extract features text images user guide see feature extraction section details 
1959: feature_extraction.dictvectorizer dtype ... transforms lists featurevalue mappings vectors 
1960: sklearn.feature_extraction.dictvectorizer class sklearn.feature_extraction.dictvectorizer dtype type numpy.oat64 separa tor sparsetrue transforms lists featurevalue mappings vectors transformer turns lists mappings dictlike objects feature names feature values numpy arrays scipy.sparse matrices use scikitlearn estimators feature values strings transformer binary onehot aka oneofk coding one boolean valued feature constructed possible string values feature take instance feature take values ham spam become two features output one signifying fham fspam features occur sample mapping zero value resulting arraymatrix 
1961: parameters dtype callable optional type feature values passed numpy arrayscipy.sparse matrix constructors dtype argument 
1962: separator string optional separator string used constructing new features onehot coding 
1963: sparse boolean optional whether transform produce scipy.sparse matrices true default 
1964: 1.8. reference scikitlearn user guide release 0.11 examples sklearn.feature_extraction import dictvectorizer dictvectorizer sparsefalse foo bar foo baz v.fit_transform array v.inverse_transform true v.transform foo unseen_feature array bar 2.0 foo 1.0 baz 1.0 foo 3.0 methods fit fit_transform get_feature_names get_params deep inverse_transform dict_type transform array sparse matrix back feature mappings restrict support indices set_params params transform learn list feature name indices mappings learn list feature name indices mappings transform returns list feature names ordered indices get parameters estimator restrict features support set parameters estimator transform feature value dicts array sparse matrix 
1965: __init__ dtype type numpy.oat64 separator sparsetrue fit ynone learn list feature name indices mappings 
1966: parameters mapping iterable mappings dict mapping feature names arbitrary python objects feature values strings convertible dtype 
1967: ignored returns self fit_transform ynone learn list feature name indices mappings transform like followed transform 
1968: parameters mapping iterable mappings dict mapping feature names arbitrary python objects feature values strings convertible dtype 
1969: ignored returns array sparse matrix feature vectors always 
1970: get_feature_names returns list feature names ordered indices 
1971: chapter user guide scikitlearn user guide release 0.11 oneofk coding applied categorical features include constructed feature names original ones 
1972: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1973: inverse_transform dict_type type dict transform array sparse matrix back feature mappings must produced dictvectorizers transform t_transform method may passed transformers preserve number features order case onehotoneofk coding constructed feature names values returned rather original ones 
1974: parameters arraylike sparse matrix shape n_samples n_features sample matrix 
1975: dict_type callable optional constructor feature mappings must conform collections.mapping api 
1976: returns list dict_type objects length n_samples feature mappings samples 
1977: restrict support indicesfalse restrict features support 
1978: parameters support arraylike boolean mask list indices returned get_support member feature lectors 
1979: indices boolean optional whether support list indices 
1980: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform ynone transform feature value dicts array sparse matrix named features encountered t_transform silently ignored 
1981: parameters mapping iterable mappings length n_samples dict mapping feature names arbitrary python objects feature values strings convertible dtype 
1982: ignored returns array sparse matrix 1.8. reference scikitlearn user guide release 0.11 feature vectors always 
1983: images sklearn.feature_extraction.image submodule gathers utilities extract features images 
1984: feature_extraction.image.img_to_graph img ... feature_extraction.image.grid_to_graph n_x n_y feature_extraction.image.extract_patches_2d ... feature_extraction.image.reconstruct_from_patches_2d ... reconstruct image patches feature_extraction.image.patchextractor ... extracts patches collection images graph pixeltopixel gradient connections graph pixeltopixel connections reshape image collection patches sklearn.feature_extraction.image.img_to_graph sklearn.feature_extraction.image.img_to_graph img masknone return_as class scipy.sparse.coo.coo_matrix dtypenone graph pixeltopixel gradient connections edges weighted gradient values parameters img ndarray image mask ndarray booleans optional optional mask image consider part pixels 
1985: return_as np.ndarray sparse matrix class optional class use build returned adjacency matrix 
1986: dtype none dtype optional data returned sparse matrix default dtype img sklearn.feature_extraction.image.grid_to_graph sklearn.feature_extraction.image.grid_to_graph n_x n_y n_z1 return_as class masknone scipy.sparse.coo.coo_matrix dtype type int graph pixeltopixel connections edges exist voxels connected 
1987: parameters n_x int dimension axis n_y int dimension axis n_z int optional default dimension axis mask ndarray booleans optional chapter user guide scikitlearn user guide release 0.11 optional mask image consider part pixels 
1988: return_as np.ndarray sparse matrix class optional class use build returned adjacency matrix 
1989: dtype dtype optional default int data returned sparse matrix default int sklearn.feature_extraction.image.extract_patches_2d sklearn.feature_extraction.image.extract_patches_2d image max_patchesnone dom_statenone patch_size ran reshape image collection patches resulting patches allocated dedicated array 
1990: parameters image array shape image_height image_width image_height image_width n_channels original image data color images last dimension species channel rgb image would n_channels3 
1991: patch_size tuple ints patch_height patch_width dimensions one patch max_patches integer oat optional default none maximum number patches extract max_patches oat taken proportion total number patches 
1992: random_state int randomstate pseudo number generator state used random sampling use max_patches none 
1993: returns patches array shape n_patches patch_height patch_width n_patches patch_height patch_width n_channels collection patches extracted image n_patches either max_patches total number patches extracted 
1994: examples sklearn.feature_extraction import image one_image np.arange .reshape one_image array patches image.extract_patches_2d one_image patches.shape patches array patches array 1.8. reference scikitlearn user guide release 0.11 patches array sklearn.feature_extraction.image.reconstruct_from_patches_2d sklearn.feature_extraction.image.reconstruct_from_patches_2d patches reconstruct image patches patches assumed overlap image constructed lling patches left right top bottom averaging overlapping regions 
1995: age_size parameters patches array shape n_patches patch_height patch_width n_patches patch_height patch_width n_channels complete set patches patches contain colour information channels indexed along last dimension rgb patches would n_channels3 
1996: image_size tuple ints image_height image_width image_height image_width n_channels size image recon structed returns image array shape image_size reconstructed image sklearn.feature_extraction.image.patchextractor class sklearn.feature_extraction.image.patchextractor patch_size max_patchesnone random_statenone extracts patches collection images parameters patch_size tuple ints patch_height patch_width dimensions one patch max_patches integer oat optional default none maximum number patches per image extract max_patches oat taken mean proportion total number patches 
1997: random_state int randomstate pseudo number generator state used random sampling 
1998: methods fit get_params deep set_params params transform nothing return estimator unchanged get parameters estimator set parameters estimator transforms image samples matrix patch data 
1999: __init__ patch_size max_patchesnone random_statenone chapter user guide scikitlearn user guide release 0.11 fit ynone nothing return estimator unchanged method implement usual api hence work pipelines 
2000: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform transforms image samples matrix patch data 
2001: parameters array shape n_samples image_height image_width n_samples image_height image_width n_channels array images extract patches color images last dimension species channel rgb image would n_channels3 
2002: returns patches array shape n_patches patch_height patch_width n_patches patch_height patch_width n_channels collection patches extracted images n_patches either n_samples max_patches total num ber patches extracted 
2003: text sklearn.feature_extraction.text submodule gathers utilities build feature vectors text doc uments 
2004: feature_extraction.text.countvectorizer ... convert collection raw documents matrix token counts feature_extraction.text.tfidftransformer ... transform count matrix normalized tfidf representation feature_extraction.text.tfidfvectorizer ... convert collection raw documents matrix tfidf features 
2005: 1.8. reference scikitlearn user guide release 0.11 sklearn.feature_extraction.text.countvectorizer class sklearn.feature_extraction.text.countvectorizer inputcontent charsetutf charset_errorstrict low preproces tokenizernone strip_accentsnone ercasetrue sornone stop_wordsnone ken_patternubwwb min_n1 lyzerword max_featuresnone ularynone dtype type long max_n1 ana max_df1.0 vocab binaryfalse convert collection raw documents matrix token counts implementation produces sparse representation counts using scipy.sparse.coo_matrix provide apriori dictionary use analyzer kind feature selection number features equal vocabulary size found analysing data default analyzer simple stop word ltering english 
2006: parameters input string lename content lename sequence passed argument expected list lenames need reading fetch raw content analyze sequence items must read method lelike object called fetch bytes memory otherwise input expected sequence strings bytes items expected analyzed directly 
2007: charset string utf8 default bytes les given analyze charset used decode 
2008: charset_error strict ignore replace instruction byte sequence given analyze contains characters given charset default strict meaning unicodedecodeerror raised values ignore replace 
2009: strip_accents ascii unicode none remove accents preprocessing step ascii fast method works characters direct ascii mapping unicode slightly slower method works characters none default nothing 
2010: analyzer string word char callable whether feature made word character ngrams callable passed used extract sequence features raw unpro cessed input 
2011: preprocessor callable none default override preprocessing string transformation stage preserving tokenizing ngrams generation steps 
2012: tokenizer callable none default chapter user guide scikitlearn user guide release 0.11 override string tokenization step preserving preprocessing ngrams generation steps 
2013: min_n integer lower boundary range nvalues different ngrams extracted 
2014: max_n integer upper boundary range nvalues different ngrams extracted values min_n max_n used 
2015: stop_words string english list none default string passed _check_stop_list appropriate stop list returned currently supported string value list list assumed contain stop words removed resulting tokens none stop words used max_df set value range 0.7 1.0 automatically detect lter stop words based intra corpus document frequency terms 
2016: token_pattern string regular expression denoting constitutes token used tokenize word default regexp select tokens letters characters punctuation completely ignored always treated token separator 
2017: max_df oat range 0.0 1.0 optional 1.0 default building vocabulary ignore terms term frequency strictly higher given threshold corpus specic stop words parameter ignored vocabulary none 
2018: max_features optional none default none build vocabulary consider top max_features ordered term frequency across corpus parameter ignored vocabulary none 
2019: binary boolean false default true non zero counts set useful discrete probabilistic models model binary events rather integer counts 
2020: dtype type optional type matrix returned t_transform transform 
2021: methods build_analyzer build_preprocessor build_tokenizer decode doc fit raw_documents fit_transform raw_documents learn vocabulary dictionary return count vectors return callable handles preprocessing tokenization return function preprocess text tokenization return function split string sequence tokens decode input string unicode symbols learn vocabulary dictionary tokens raw documents 1.8. reference scikitlearn user guide release 0.11 get_feature_names get_params deep get_stop_words inverse_transform set_params params transform raw_documents table 1.71 continued previous page array mapping feature integer indicex feature name get parameters estimator build fetch effective stop words list return terms per document nonzero entries set parameters estimator extract token counts raw text documents using vocabulary tted one provided constructor 
2022: __init__ inputcontent lowercasetrue ken_patternubwwb min_n1 max_n1 max_featuresnone vocabularynone binaryfalse dtype type long strip_accentsnone analyzerword max_df1.0 charsetutf8 preprocessornone charset_errorstrict tokenizernone stop_wordsnone build_analyzer return callable handles preprocessing tokenization build_preprocessor return function preprocess text tokenization build_tokenizer return function split string sequence tokens decode doc decode input string unicode symbols decoding strategy depends vectorizer parameters 
2023: fit raw_documents ynone learn vocabulary dictionary tokens raw documents parameters raw_documents iterable iterable yields either str unicode objects returns self fit_transform raw_documents ynone learn vocabulary dictionary return count vectors efcient calling followed transform 
2024: parameters raw_documents iterable iterable yields either str unicode objects returns vectors array n_samples n_features get_feature_names array mapping feature integer indicex feature name get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2025: get_stop_words build fetch effective stop words list inverse_transform return terms per document nonzero entries 
2026: chapter user guide scikitlearn user guide release 0.11 parameters array sparse matrix shape n_samples n_features returns x_inv list arrays len n_samples list arrays terms 
2027: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform raw_documents extract token counts raw text documents using vocabulary tted one provided constructor 
2028: parameters raw_documents iterable iterable yields either str unicode objects returns vectors sparse matrix n_samples n_features sklearn.feature_extraction.text.tdftransformer class sklearn.feature_extraction.text.tfidftransformer norml2 use_idftrue sublin smooth_idftrue ear_tffalse transform count matrix normalized tfidf representation means termfrequency tfidf means termfrequency times inverse documentfrequency com mon term weighting scheme information retrieval also found good use document classication goal using tfidf instead raw frequencies occurrence token given document scale impact tokens occur frequently given corpus hence empirically less informative features occur small fraction training corpus smart notation used class implements several tfidf variants always natural idf iff use_idf given otherwise normalization iff norml2 iff normnone 
2029: parameters norm none optional norm used normalize term vectors none normalization 
2030: use_idf boolean optional enable inversedocumentfrequency reweighting 
2031: smooth_idf boolean optional smooth idf weights adding one document frequencies extra document seen containing every term collection exactly prevents zero divisions 
2032: sublinear_tf boolean optional apply sublinear scaling i.e replace log 
2033: references yates2011 msr2008 1.8. reference scikitlearn user guide release 0.11 methods fit fit_transform get_params deep set_params params transform copy learn idf vector global term weights fit data transform get parameters estimator set parameters estimator transform count matrix tfidf representation __init__ norml2 use_idftrue smooth_idftrue sublinear_tffalse fit ynone learn idf vector global term weights parameters sparse matrix n_samples n_features matrix termtoken counts fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2034: parameters numpy array shape n_samples n_features training set 
2035: numpy array shape n_samples target values 
2036: returns x_new numpy array shape n_samples n_features_new transformed array 
2037: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
2038: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform copytrue transform count matrix tfidf representation parameters sparse matrix n_samples n_features chapter user guide scikitlearn user guide release 0.11 matrix termtoken counts returns vectors sparse matrix n_samples n_features sklearn.feature_extraction.text.tdfvectorizer class sklearn.feature_extraction.text.tfidfvectorizer inputcontent charsetutf charset_errorstrict low strip_accentsnone ercasetrue preproces sornone tokenizernone ana lyzerword stop_wordsnone token_patternubwwb min_n1 max_df1.0 vocab max_featuresnone binaryfalse ularynone dtype type long norml2 use_idftrue smooth_idftrue sublinear_tffalse max_n1 convert collection raw documents matrix tfidf features equivalent countvectorizer followed tdftransformer see also countvectorizertokenize documents count occurrences token return sparse matrix tfidftransformerapply term frequency inverse document frequency normalization sparse matrix occurrence counts 
2039: methods return callable handles preprocessing tokenization return function preprocess text tokenization return function split string sequence tokens decode input string unicode symbols learn conversion law documents array data build_analyzer build_preprocessor build_tokenizer decode doc fit raw_documents fit_transform raw_documents learn representation return vectors get_feature_names get_params deep get_stop_words inverse_transform set_params params transform raw_documents copy array mapping feature integer indicex feature name get parameters estimator build fetch effective stop words list return terms per document nonzero entries set parameters estimator transform raw text documents tfidf vectors __init__ inputcontent lower charsetutf8 casetrue preprocessornone tokenizernone analyzerword stop_wordsnone token_patternubwwb min_n1 max_n1 max_df1.0 max_featuresnone vocabularynone use_idftrue smooth_idftrue sublinear_tffalse dtype type long charset_errorstrict strip_accentsnone binaryfalse norml2 1.8. reference scikitlearn user guide release 0.11 build_analyzer return callable handles preprocessing tokenization build_preprocessor return function preprocess text tokenization build_tokenizer return function split string sequence tokens decode doc decode input string unicode symbols decoding strategy depends vectorizer parameters 
2040: fit raw_documents learn conversion law documents array data fit_transform raw_documents ynone learn representation return vectors parameters raw_documents iterable iterable yields either str unicode objects returns vectors array n_samples n_features get_feature_names array mapping feature integer indicex feature name get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2041: get_stop_words build fetch effective stop words list inverse_transform return terms per document nonzero entries 
2042: parameters array sparse matrix shape n_samples n_features returns x_inv list arrays len n_samples list arrays terms 
2043: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform raw_documents copytrue transform raw text documents tfidf vectors parameters raw_documents iterable iterable yields either str unicode objects returns vectors sparse matrix n_samples n_features chapter user guide scikitlearn user guide release 0.11 1.8.8 sklearn.feature_selection feature selection sklearn.feature_selection module implements feature selection algorithms currently includes uni variate lter selection methods recursive feature elimination algorithm user guide see feature selection section details 
2044: feature_selection.selectpercentile score_func feature_selection.selectkbest score_func feature_selection.selectfpr score_func alpha feature_selection.selectfdr score_func alpha feature_selection.selectfwe score_func alpha feature_selection.rfe estimator ... step feature_selection.rfecv estimator step ... filter select best percentile p_values filter select lowest pvalues filter select pvalues alpha based fpr test filter select pvalues estimated false discovery rate filter select pvalues corresponding familywise error rate feature ranking recursive feature elimination feature ranking recursive feature elimination crossvalidated selection best number features 
2045: sklearn.feature_selection.selectpercentile class sklearn.feature_selection.selectpercentile score_func percentile10 filter select best percentile p_values parameters score_func callable function taking two arrays returning arrays scores pvalues percentile int optional percent features keep methods fit fit_transform get_params deep get_support indices inverse_transform transform new matrix using selected features set_params params transform evaluate function fit data transform get parameters estimator return mask list featuresindices selected 
2046: set parameters estimator transform new matrix using selected features __init__ score_func percentile10 fit evaluate function fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2047: parameters numpy array shape n_samples n_features training set 
2048: numpy array shape n_samples target values 
2049: returns x_new numpy array shape n_samples n_features_new 1.8. reference scikitlearn user guide release 0.11 transformed array 
2050: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
2051: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2052: get_support indicesfalse return mask list featuresindices selected 
2053: inverse_transform transform new matrix using selected features set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform transform new matrix using selected features sklearn.feature_selection.selectkbest class sklearn.feature_selection.selectkbest score_func k10 filter select lowest pvalues 
2054: parameters score_func callable function taking two arrays returning pair arrays scores pvalues 
2055: int optional number top features select 
2056: notes ties features equal pvalues broken unspecied way 
2057: methods fit fit_transform get_params deep evaluate function fit data transform get parameters estimator continued next page chapter user guide scikitlearn user guide release 0.11 table 1.76 continued previous page get_support indices inverse_transform transform new matrix using selected features set_params params transform set parameters estimator transform new matrix using selected features return mask list featuresindices selected 
2058: __init__ score_func k10 fit evaluate function fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2059: parameters numpy array shape n_samples n_features training set 
2060: numpy array shape n_samples target values 
2061: returns x_new numpy array shape n_samples n_features_new transformed array 
2062: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
2063: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2064: get_support indicesfalse return mask list featuresindices selected 
2065: inverse_transform transform new matrix using selected features set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform transform new matrix using selected features 1.8. reference scikitlearn user guide release 0.11 sklearn.feature_selection.selectfpr class sklearn.feature_selection.selectfpr score_func alpha0.05 filter select pvalues alpha based fpr test fpr test stands false positive rate test controls total amount false detections 
2066: parameters score_func callable function taking two arrays returning arrays scores pvalues alpha oat optional highest pvalue features kept methods fit fit_transform get_params deep get_support indices inverse_transform transform new matrix using selected features set_params params transform evaluate function fit data transform get parameters estimator return mask list featuresindices selected 
2067: set parameters estimator transform new matrix using selected features __init__ score_func alpha0.05 fit evaluate function fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2068: parameters numpy array shape n_samples n_features training set 
2069: numpy array shape n_samples target values 
2070: returns x_new numpy array shape n_samples n_features_new transformed array 
2071: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
2072: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2073: chapter user guide scikitlearn user guide release 0.11 get_support indicesfalse return mask list featuresindices selected 
2074: inverse_transform transform new matrix using selected features set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform transform new matrix using selected features sklearn.feature_selection.selectfdr class sklearn.feature_selection.selectfdr score_func alpha0.05 filter select pvalues estimated false discovery rate uses benjaminihochberg procedure alpha target false discovery rate 
2075: parameters score_func callable function taking two arrays returning arrays scores pvalues alpha oat optional highest uncorrected pvalue features keep methods fit fit_transform get_params deep get_support indices inverse_transform transform new matrix using selected features set_params params transform evaluate function fit data transform get parameters estimator return mask list featuresindices selected 
2076: set parameters estimator transform new matrix using selected features __init__ score_func alpha0.05 fit evaluate function fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2077: parameters numpy array shape n_samples n_features training set 
2078: numpy array shape n_samples 1.8. reference scikitlearn user guide release 0.11 target values 
2079: returns x_new numpy array shape n_samples n_features_new transformed array 
2080: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
2081: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2082: get_support indicesfalse return mask list featuresindices selected 
2083: inverse_transform transform new matrix using selected features set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform transform new matrix using selected features sklearn.feature_selection.selectfwe class sklearn.feature_selection.selectfwe score_func alpha0.05 filter select pvalues corresponding familywise error rate parameters score_func callable function taking two arrays returning arrays scores pvalues alpha oat optional highest uncorrected pvalue features keep methods fit fit_transform get_params deep get_support indices inverse_transform transform new matrix using selected features evaluate function fit data transform get parameters estimator return mask list featuresindices selected 
2084: continued next page chapter user guide scikitlearn user guide release 0.11 table 1.79 continued previous page set_params params transform set parameters estimator transform new matrix using selected features __init__ score_func alpha0.05 fit evaluate function fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2085: parameters numpy array shape n_samples n_features training set 
2086: numpy array shape n_samples target values 
2087: returns x_new numpy array shape n_samples n_features_new transformed array 
2088: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
2089: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2090: get_support indicesfalse return mask list featuresindices selected 
2091: inverse_transform transform new matrix using selected features set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform transform new matrix using selected features 1.8. reference scikitlearn user guide release 0.11 sklearn.feature_selection.rfe class sklearn.feature_selection.rfe estimator n_features_to_select step1 feature ranking recursive feature elimination given external estimator assigns weights features e.g. coefcients linear model goal recursive feature elimination rfe select features recursively considering smaller smaller sets features first estimator trained initial set features weights assigned one features whose absolute weights smallest pruned current set features procedure recursively repeated pruned set desired number features select eventually reached 
2092: parameters estimator object supervised learning estimator method updates coef_ attribute holds tted parameters important features must correspond high absolute values coef_ array instance case supervised learning algorithms support vector classiers generalized linear models svm linear_model mod ules 
2093: n_features_to_select int number features select step int oat optional default1 greater equal step corresponds integer number features remove iteration within 0.0 1.0 step corresponds percentage rounded features remove iteration 
2094: references r61 examples following example shows retrieve right informative features friedman dataset 
2095: sklearn.datasets import make_friedman1 sklearn.feature_selection import rfe sklearn.svm import svr make_friedman1 n_samples50 n_features10 random_state0 estimator svr kernel linear selector rfe estimator step1 selector selector.fit selector.support_ array true true true true true false false false false false dtypebool selector.ranking_ array chapter user guide scikitlearn user guide release 0.11 number selected features mask selected features 
2096: feature ranking ranking_ corresponds ranking position ith feature selected i.e. estimated best features assigned rank 
2097: attributes n_features_int sup port_ rank ing_ array shape n_features array shape n_features methods fit get_params deep predict score set_params params transform fit rfe model underlying estimator selected get parameters estimator reduce selected features predict using reduce selected features return score set parameters estimator reduce selected features elimination 
2098: __init__ estimator n_features_to_select step1 fit fit rfe model underlying estimator selected features 
2099: parameters array shape n_samples n_features training input samples array shape n_samples target values 
2100: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2101: predict reduce selected features predict using underlying estimator 
2102: parameters array shape n_samples n_features input samples 
2103: returns array shape n_samples predicted target values 
2104: score reduce selected features return score underlying estimator 
2105: parameters array shape n_samples n_features input samples 
2106: array shape n_samples target values 
2107: 1.8. reference scikitlearn user guide release 0.11 set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform reduce selected features elimination 
2108: parameters array shape n_samples n_features input samples 
2109: returns x_r array shape n_samples n_selected_features input samples features selected elimination 
2110: sklearn.feature_selection.rfecv class sklearn.feature_selection.rfecv estimator step1 cvnone loss_funcnone feature ranking recursive feature elimination crossvalidatedselection best number fea tures 
2111: parameters estimator object supervised learning estimator method updates coef_ attribute holds tted parameters important features must correspond high absolute values coef_ array instance case supervised learning algorithms support vector classiers generalized linear models svm linear_model mod ules 
2112: step int oat optional default1 greater equal step corresponds integer number features remove iteration within 0.0 1.0 step corresponds percentage rounded features remove iteration 
2113: int crossvalidation generator optional defaultnone int number folds none 3fold crossvalidation performed fault specic crossvalidation objects also passed see sklearn.cross_validation module details 
2114: loss_function function optional defaultnone loss function minimize crossvalidation none score function estimator maximized 
2115: references r62 chapter user guide scikitlearn user guide release 0.11 examples following example shows retrieve apriori known informative features friedman dataset 
2116: sklearn.datasets import make_friedman1 sklearn.feature_selection import rfecv sklearn.svm import svr make_friedman1 n_samples50 n_features10 random_state0 estimator svr kernel linear selector rfecv estimator step1 cv5 selector selector.fit selector.support_ array true true true true true false false false false false dtypebool selector.ranking_ array attributes n_features_int sup port_ rank ing_ array shape n_features array shape n_features cv_scores_array shape n_subsets_of_features number selected features crossvalidation mask selected features 
2117: feature ranking ranking_ corresponds ranking position ith feature selected i.e. estimated best features assigned rank crossvalidation scores cv_scores_ corresponds score ith subset features 
2118: methods fit get_params deep predict score set_params params transform fit rfe model automatically tune number selected get parameters estimator reduce selected features predict using reduce selected features return score set parameters estimator reduce selected features elimination 
2119: __init__ estimator step1 cvnone loss_funcnone fit fit rfe model automatically tune number selected features 
2120: parameters array shape n_samples n_features training vector n_samples number samples n_features total number features 
2121: array shape n_samples target values integers classication real numbers regression 
2122: get_params deeptrue get parameters estimator 1.8. reference scikitlearn user guide release 0.11 parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2123: predict reduce selected features predict using underlying estimator 
2124: parameters array shape n_samples n_features input samples 
2125: returns array shape n_samples predicted target values 
2126: score reduce selected features return score underlying estimator 
2127: parameters array shape n_samples n_features input samples 
2128: array shape n_samples target values 
2129: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform reduce selected features elimination 
2130: parameters array shape n_samples n_features input samples 
2131: returns x_r array shape n_samples n_selected_features input samples features selected elimination 
2132: feature_selection.chi2 feature_selection.f_classif feature_selection.f_regression center univariate linear regression tests compute chisquared statistic classfeature combination compute anova fvalue provided sample sklearn.feature_selection.chi2 sklearn.feature_selection.chi2 compute chisquared statistic classfeature combination transformer used select n_features features highest values chisquare statistic either boolean multinomially distributed data e.g. term counts document classication relative classes recall statistic measures dependence stochastic variables transformer based function weeds features likely independent class therefore irrelevant classication 
2133: chapter user guide parameters arraylike sparse matrix shape n_samples n_features_in scikitlearn user guide release 0.11 sample vectors 
2134: arraylike shape n_samples target vector class labels 
2135: notes complexity algorithm n_classes n_features 
2136: sklearn.feature_selection.f_classif sklearn.feature_selection.f_classif compute anova fvalue provided sample parameters array shape n_samples n_features set regressors sthat tested sequentially array shape n_samples data matrix returns array shape set values pval array shape set pvalues sklearn.feature_selection.f_regression sklearn.feature_selection.f_regression centertrue univariate linear regression tests quick linear model testing effect single regressor sequentially many regressors done steps regressor interest data orthogonalized wrt constant regressors cross correlation data regressors computed converted score pvalue parameters array shape n_samples n_features set regressors sthat tested sequentially array shape n_samples data matrix center true bool true centered returns array shape set values pval array shape set pvalues 1.8. reference scikitlearn user guide release 0.11 1.8.9 sklearn.gaussian_process gaussian processes sklearn.gaussian_process module implements scalar gaussian process based predictions user guide see gaussian processes section details 
2137: gaussian_process.gaussianprocess regr ... gaussian process model class 
2138: sklearn.gaussian_process.gaussianprocess class sklearn.gaussian_process.gaussianprocess regrconstant thetalnone corrsquared_exponential beta0none storage_modefull verbosefalse thetaunone theta00.1 optimizerfmin_cobyla ran normalizetrue dom_start1 nugget2.2204460492503131e15 ran dom_statenone gaussian process model class 
2139: parameters regr string callable optional regression function returning array outputs linear regression functional basis number observations n_samples greater size basis default assumes simple constant regression trend available builtin regression models constant linear quadratic corr string callable optional stationary autocorrelation function returning autocorrelation two points default assumes squaredexponential autocorrelation model builtin corre lation models absolute_exponential squared_exponential generalized_exponential cubic linear beta0 double array_like optional regression weight vector perform ordinary kriging default assumes uni versal kriging vector beta regression weights estimated using maximum likelihood principle 
2140: storage_mode string optional string specifying whether cholesky decomposition correlation matrix stored class storage_mode full storage_mode light default assumes storage_mode full cholesky decomposition cor relation matrix stored might useful parameter one interested mse plan estimate blup correlation matrix required 
2141: verbose boolean optional boolean specifying verbose level default verbose false 
2142: theta0 double array_like optional chapter user guide scikitlearn user guide release 0.11 array shape n_features parameters autocorrelation model thetal thetau also specied theta0 considered starting point maximum likelihood rstimation best set parameters default assumes isotropic autocorrelation model theta0 1e1 
2143: thetal double array_like optional array shape matching theta0s lower bound autocorrelation parame ters maximum likelihood estimation default none skips maximum likelihood estimation uses theta0 
2144: thetau double array_like optional array shape matching theta0s upper bound autocorrelation parame ters maximum likelihood estimation default none skips maximum likelihood estimation uses theta0 
2145: normalize boolean optional input observations centered reduced wrt means standard deviations estimated n_samples observations provided default normalize true data normalized ease maximum likelihood estimation 
2146: nugget double ndarray optional nugget introduce nugget effect allow smooth predictions noisy data ndarray must length number data points used nugget added diagonal assumed training covariance way acts tikhonov regularization problem special case squared exponential correlation function nugget mathematically represents variance input values default assumes nugget close machine precision sake robustness nugget machine_epsilon 
2147: optimizer string optional string specifying optimization algorithm used default uses fmin_cobyla algorithm scipy.optimize available optimizers fmin_cobyla welch welch optimizer dued welch al. see reference wbswm1992 consists iterating several onedimensional optimizations instead running one single multidimensional optimization 
2148: random_start int optional number times maximum likelihood estimation performed random starting point rst mle always uses specied starting point theta0 next starting points picked random according exponential distribution loguniform thetal thetau default use random starting point ran dom_start 
2149: random_state integer numpy.randomstate optional generator used shufe sequence coordinates theta welch opti mizer integer given xes seed defaults global numpy random number generator 
2150: 1.8. reference scikitlearn user guide release 0.11 notes presentation implementation based translation dace matlab toolbox see reference nlns2002 
2151: references nlns2002 wbswm1992 examples import numpy sklearn.gaussian_process import gaussianprocess np.array np.sin .ravel gaussianprocess theta00.1 thetal.001 thetau1 gp.fit gaussianprocess beta0none.. 
2152:  
2153: methods arg_max_reduced_likelihood_function function estimates autocorrelation parameters theta maximizer reduced likelihood function fit get_params deep predict eval_mse batch_size reduced_likelihood_function theta score set_params params gaussian process model tting method get parameters estimator function evaluates gaussian process model function determines blup parameters evaluates reduced returns coefcient determination prediction set parameters estimator 
2154: __init__ regrconstant corrsquared_exponential beta0none theta00.1 thetalnone bosefalse dom_start1 normalizetrue nugget2.2204460492503131e15 random_statenone thetaunone optimizerfmin_cobyla storage_modefull ver ran arg_max_reduced_likelihood_function function estimates autocorrelation parameters theta maximizer reduced likelihood function minimization opposite reduced likelihood function used convenience parameters self parameters stored gaussian process model object returns optimal_theta array_like best set autocorrelation parameters sought maximizer reduced likeli hood function 
2155: optimal_reduced_likelihood_function_value double optimal reduced likelihood function value 
2156: optimal_par dict blup parameters associated thetaopt 
2157: chapter user guide scikitlearn user guide release 0.11 fit gaussian process model tting method 
2158: parameters double array_like array shape n_samples n_features input observations made 
2159: double array_like array shape n_features observations scalar output pre dicted 
2160: returns self tted gaussian process model object awaiting data perform predictions 
2161: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2162: predict eval_msefalse batch_sizenone function evaluates gaussian process model 
2163: parameters array_like array shape n_eval n_features giving point prediction made 
2164: eval_mse boolean optional boolean specifying whether mean squared error evaluated fault assumes evalmse false evaluates blup mean prediction 
2165: batch_size integer optional integer giving maximum number points evaluated simulatneously depending available memory default none given points eval uated time 
2166: returns array_like array shape n_eval best linear unbiased prediction 
2167: mse array_like optional eval_mse true array shape n_eval mean squared error 
2168: reduced_likelihood_function thetanone function determines blup parameters evaluates reduced likelihood function given autocorrelation parameters theta maximizing function wrt autocorrelation parameters theta equivalent maximizing likeli hood assumed joint gaussian distribution observations evaluated onto design experi ments 
2169: parameters theta array_like optional array containing autocorrelation parameters gaussian process model parameters determined default uses builtin autocorrelation rameters theta self.theta 
2170: 1.8. reference scikitlearn user guide release 0.11 returns reduced_likelihood_function_value double value reduced likelihood function associated given autocorrelation parameters theta 
2171: par dict dictionary containing requested gaussian process model parameters sigma2gaussian process variance betageneralized leastsquares regression weights universal kriging given beta0 ordinary kriging 
2172: gammagaussian process weights ccholesky decomposition correlation matrix ftsolution linear equation system gqr decomposition matrix 
2173: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
2174: parameters arraylike shape n_samples n_features training set 
2175: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self gaussian_process.correlation_models.absolute_exponential ... gaussian_process.correlation_models.squared_exponential ... gaussian_process.correlation_models.generalized_exponential ... generalized exponential correlation model gaussian_process.correlation_models.pure_nugget ... gaussian_process.correlation_models.cubic ... gaussian_process.correlation_models.linear ... gaussian_process.regression_models.constant gaussian_process.regression_models.linear gaussian_process.regression_models.quadratic spatial independence correlation model pure nugget cubic correlation model linear correlation model zero order polynomial constant regression model first order polynomial linear regression model second order polynomial quadratic 2n1 regression model 
2176: absolute exponential autocorrelation model squared exponential correlation model radial basis function 
2177: sklearn.gaussian_process.correlation_models.absolute_exponential sklearn.gaussian_process.correlation_models.absolute_exponential theta absolute exponential autocorrelation model ornsteinuhlenbeck stochastic process chapter user guide scikitlearn user guide release 0.11 theta theta exp sum theta_i dx_i parameters theta array_like array shape isotropic anisotropic giving autocorrelation parame ter 
2178: array_like array shape n_eval n_features giving componentwise distances locations correlation model evaluated 
2179: returns array_like array shape n_eval containing values autocorrelation model 
2180: sklearn.gaussian_process.correlation_models.squared_exponential sklearn.gaussian_process.correlation_models.squared_exponential theta squared exponential correlation model radial basis function innitely differentiable stochastic process smooth theta theta exp sum theta_i dx_i parameters theta array_like array shape isotropic anisotropic giving autocorrelation parame ter 
2181: array_like array shape n_eval n_features giving componentwise distances locations correlation model evaluated 
2182: returns array_like array shape n_eval containing values autocorrelation model 
2183: sklearn.gaussian_process.correlation_models.generalized_exponential sklearn.gaussian_process.correlation_models.generalized_exponential theta generalized exponential correlation model useful one know smoothness function predicted theta theta exp sum theta_i dx_ip parameters theta array_like array shape isotropic anisotropic giving autocorrelation rameter theta 
2184: array_like 1.8. reference scikitlearn user guide release 0.11 array shape n_eval n_features giving componentwise distances locations correlation model evaluated 
2185: returns array_like array shape n_eval values autocorrelation model 
2186: sklearn.gaussian_process.correlation_models.pure_nugget sklearn.gaussian_process.correlation_models.pure_nugget theta spatial independence correlation model pure nugget useful one wants solve ordinary least squares problem theta theta sum dx_i otherwise parameters theta array_like none 
2187: array_like array shape n_eval n_features giving componentwise distances locations correlation model evaluated 
2188: returns array_like array shape n_eval values autocorrelation model 
2189: sklearn.gaussian_process.correlation_models.cubic sklearn.gaussian_process.correlation_models.cubic theta cubic correlation model theta theta prod max theta_jd_ij theta_jd_ij ... parameters theta array_like array shape isotropic anisotropic giving autocorrelation parame ter 
2190: array_like array shape n_eval n_features giving componentwise distances locations correlation model evaluated 
2191: returns array_like array shape n_eval values autocorrelation model 
2192: chapter user guide scikitlearn user guide release 0.11 sklearn.gaussian_process.correlation_models.linear sklearn.gaussian_process.correlation_models.linear theta linear correlation model theta theta prod max theta_jd_ij ... parameters theta array_like array shape isotropic anisotropic giving autocorrelation parame ter 
2193: array_like array shape n_eval n_features giving componentwise distances locations correlation model evaluated 
2194: returns array_like array shape n_eval values autocorrelation model 
2195: sklearn.gaussian_process.regression_models.constant sklearn.gaussian_process.regression_models.constant zero order polynomial constant regression model parameters array_like array shape n_eval n_features giving locations regression model evaluated 
2196: returns array_like array shape n_eval values regression model 
2197: sklearn.gaussian_process.regression_models.linear sklearn.gaussian_process.regression_models.linear first order polynomial linear regression model x_1 ... x_n parameters array_like array shape n_eval n_features giving locations regression model evaluated 
2198: returns array_like array shape n_eval values regression model 
2199: 1.8. reference scikitlearn user guide release 0.11 sklearn.gaussian_process.regression_models.quadratic sklearn.gaussian_process.regression_models.quadratic second order polynomial quadratic 2n1 regression model x_i ... x_i x_j ... .ti parameters array_like array shape n_eval n_features giving locations regression model evaluated 
2200: returns array_like array shape n_eval values regression model 
2201: 1.8.10 sklearn.grid_search grid search sklearn.grid_search includes utilities netune parameters estimator user guide see grid search setting estimator parameters section details 
2202: grid_search.gridsearchcv estimator param_grid grid search parameters classier grid_search.itergrid param_grid generators combination various parameter lists given sklearn.grid_search.gridsearchcv class sklearn.grid_search.gridsearchcv estimator param_grid score_funcnone iidtrue pre_dispatch2n_jobs rettrue t_paramsnone cvnone loss_funcnone n_jobs1 verbose0 grid search parameters classier important members predict gridsearchcv implements method predict method like classier except parameters classier used predict optimized crossvalidation 
2203: parameters estimator object type implements predict methods object type instantiated grid point 
2204: param_grid dict dictionary parameters names string keys lists parameter settings try values 
2205: loss_func callable optional function takes arguments compares order evaluate performance prediciton small good none passed score estimator maximized score_func callable optional function takes arguments compares order evaluate perfor mance prediction high good none passed score estimator maximized 
2206: t_params dict optional chapter user guide scikitlearn user guide release 0.11 parameters pass method n_jobs int optional number jobs run parallel default pre_dispatch int string optional controls number jobs get dispatched parallel execution reducing number useful avoid explosion memory consumption jobs get dispatched cpus process parameter none case jobs immediatly created spawned use lightweight fastrunning jobs avoid delays due ondemand spawning jobs int giving exact number total jobs spawned string giving expression function n_jobs 2n_jobs iid boolean optional true data assumed identically distributed across folds loss minimized total loss per sample mean loss across folds 
2207: integer crossvalidation generator optional integer passed number fold default specic crossvalidation jects passed see sklearn.cross_validation module list possible objects ret boolean ret best estimator entire dataset predictions using gridsearch instance tting 
2208: false impossible make verbose integer controls verbosity higher messages 
2209: see also itergridgenerates combinations hyperparameter grid sklearn.cross_validation.train_test_splitutility function split data development set usable tting gridsearchcv instance evaluation set nal evaluation 
2210: notes parameters selected maximize score left data unless explicit score_func passed case used instead loss function loss_func passed overrides score functions minimized n_jobs set value higher one data copied point grid n_jobs times done efciency reasons individual jobs take little time may raise errors dataset large enough memory available workaround case set pre_dispatch memory copied pre_dispatch many times reasonable value pre_dispatch n_jobs 
2211: examples 1.8. reference scikitlearn user guide release 0.11 sklearn import svm grid_search datasets iris datasets.load_iris parameters kernel linear rbf svr svm.svc clf grid_search.gridsearchcv svr parameters clf.fit iris.data iris.target ... gridsearchcv cvnone estimatorsvc c1.0 cache_size ... coef0 ... degree ... gamma ... kernelrbf probabilityfalse shrinkingtrue tol ... fit_params iidtrue loss_funcnone n_jobs1 param_grid ... ... attributes grid_scores_dict oat best_estimator_estimator best_score_ oat best_params_dict methods contains scores parameter combinations param_grid 
2212: estimator choosen grid search i.e estimator gave highest score smallest loss specied left data score best_estimator left data parameter setting gave best results hold data 
2213: fit get_params deep score set_params params run sets parameters get parameters estimator set parameters estimator 
2214: __init__ estimator param_grid loss_funcnone score_funcnone t_paramsnone n_jobs1 iidtrue rettrue cvnone verbose0 pre_dispatch2n_jobs best_estimator deprecated gridsearchcv.best_estimator deprecated removed version 0.12. please use gridsearchcv.best_estimator_ instead 
2215: best_score deprecated gridsearchcv.best_score deprecated removed version 0.12. please use gridsearchcv.best_score_ instead 
2216: fit ynone params run sets parameters returns best classier parameters array n_samples n_features training vector n_samples number samples n_features num ber features 
2217: arraylike shape n_samples optional target vector relative classication none unsupervised learning 
2218: chapter user guide scikitlearn user guide release 0.11 get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.grid_search.itergrid class sklearn.grid_search.itergrid param_grid generators combination various parameter lists given parameters param_grid dict string sequence parameter grid explore dictionary mapping estimator parameters quences allowed values 
2219: returns params dict string yields dictionaries mapping estimator parameter one allowed values 
2220: see also gridsearchcvuses itergrid perform full parallelized grid search 
2221: examples sklearn.grid_search import itergrid param_grid true false list itergrid param_grid true false true false __init__ param_grid 1.8.11 sklearn.hmm hidden markov models sklearn.hmm module implements hidden markov models warning sklearn.hmm orphaned undocumented known numerical stability issues nobody volun teers write documentation make stable module removed version 0.11. user guide see hidden markov models section details 
2222: hmm.gaussianhmm n_components ... hmm.multinomialhmm n_components ... hmm.gmmhmm n_components n_mix startprob ... hidden markov model gaussin mixture emissions hidden markov model gaussian emissions hidden markov model multinomial discrete emissions 1.8. reference scikitlearn user guide release 0.11 sklearn.hmm.gaussianhmm class sklearn.hmm.gaussianhmm n_components1 transmatnone algorithmviterbi means_priornone means_weight0 vars_prior0.01 covars_weight1 random_statenone startprobnone transmat_priornone covariance_typediag startprob_priornone hidden markov model gaussian emissions representation hidden markov model probability distribution class allows easy evaluation sampling maximumlikelihood estimation parameters hmm 
2223: parameters n_components int number states 
2224: _covariance_type string string describing type covariance parameters use must one spherical tied diag full defaults diag 
2225: see also gmmgaussian mixture model examples sklearn.hmm import gaussianhmm gaussianhmm n_components2 ... gaussianhmm algorithmviterbi covariance_typediag covars_prior0.01 covars_weight1 means_priornone means_weight0 n_components2 random_statenone startprobnone startprob_prior1.0 transmatnone transmat_prior1.0 attributes sklearn.hmm.multinomialhmm class sklearn.hmm.multinomialhmm n_components1 prob_priornone random_statenone hidden markov model multinomial discrete emissions see also startprobnone start transmat_priornone algorithmviterbi transmatnone gaussianhmmhmm gaussian emissions examples sklearn.hmm import multinomialhmm multinomialhmm n_components2 ... multinomialhmm algorithmviterbi n_components2 random_statenone chapter user guide scikitlearn user guide release 0.11 startprobnone startprob_prior1.0 transmatnone transmat_prior1.0 attributes n_components n_symbols transmat startprob emissionprob random_state randomstate int seed default methods int int array shape n_components n_components array shape n_components array shape n_components n_symbols number states model number possible symbols emitted model observations matrix transition probabilities states 
2226: initial state occupation distribution 
2227: probability emitting given symbol state 
2228: random number generator instance find likely state sequence corresponding obs compute log probability model compute posteriors decode obs algorithm eval obs fit obs n_iter thresh params init_params estimate model parameters get_params deep predict obs algorithm predict_proba obs rvs args kwargs sample random_state score obs set_params params get parameters estimator find likely state sequence corresponding obs compute posterior probability state model deprecated rvs deprecated 0.11 removed 0.13 use sample instead generate random samples model compute log probability model set parameters estimator 
2229: __init__ n_components1 startprobnone transmatnone startprob_priornone trans mat_priornone algorithmviterbi random_statenone create hidden markov model multinomial emissions 
2230: parameters n_components int number states 
2231: algorithm decoder algorithm decode obs algorithmviterbi find likely state sequence corresponding obs uses selected algorithm decoding 
2232: parameters obs array_like shape n_features list n_featuresdimensional data points row corresponds single data point 
2233: algorithm string one decoder_algorithms decoder algorithm used 1.8. reference scikitlearn user guide release 0.11 returns logprob oat log probability maximum likelihood path hmm state_sequence array_like shape index likely states observation see also evalcompute log probability model posteriors scorecompute log probability model emissionprob_ emission probability distribution state 
2234: eval obs compute log probability model compute posteriors implements rank beam pruning forwardbackward algorithm speed inference large models 
2235: parameters obs array_like shape n_features sequence n_featuresdimensional data points row corresponds single point sequence returns logprob oat log likelihood sequence obs posteriors array_like shape n_components posterior probabilities state observation see also scorecompute log probability model decodefind likely state sequence corresponding obs fit obs n_iter10 thresh0.01 paramsabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz init_paramsabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz kwargs estimate model parameters initialization step performed entering algorithm want avoid step set keyword argument init_params empty string likewise would like initialization call method n_iter0 
2236: parameters obs list list arraylike observation sequences shape n_i n_features 
2237: n_iter int optional number iterations perform 
2238: thresh oat optional convergence threshold params string optional chapter user guide scikitlearn user guide release 0.11 controls parameters updated training process contain com bination startprob transmat means covars etc defaults parameters init_params string optional controls parameters initialized prior training contain combination startprob transmat means covars etc defaults parameters 
2239: notes general logprob nondecreasing unless aggressive pruning used decreasing logprob generally sign overtting e.g covariance parameter getting small getting training data decreasing covars_prior 
2240: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2241: predict obs algorithmviterbi find likely state sequence corresponding obs 
2242: parameters obs array_like shape n_features list n_featuresdimensional data points row corresponds single data point 
2243: returns state_sequence array_like shape index likely states observation predict_proba obs compute posterior probability state model parameters obs array_like shape n_features list n_featuresdimensional data points row corresponds single data point 
2244: returns arraylike shape n_components returns probability sample state model 
2245: rvs args kwargs deprecated rvs deprecated 0.11 removed 0.13 use sample instead sample random_statenone generate random samples model 
2246: parameters int number samples generate 
2247: random_state randomstate int seed default random number generator instance none given objects random_state used returns obs hidden_states obs array_like length list samples 1.8. reference scikitlearn user guide release 0.11 hidden_states array_like length list hidden states score obs compute log probability model 
2248: parameters obs array_like shape n_features sequence n_featuresdimensional data points row corresponds single data point 
2249: returns logprob oat log likelihood obs see also evalcompute log probability model posteriors decodefind likely state sequence corresponding obs set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self startprob_ mixing startprob state 
2250: transmat_ matrix transition probabilities 
2251: sklearn.hmm.gmmhmm class sklearn.hmm.gmmhmm n_components1 n_mix1 hidden markov model gaussin mixture emissions see also start prob_priornone transmat_priornone algorithmviterbi gmmsnone covariance_typediag covars_prior0.01 random_statenone startprobnone transmatnone gaussianhmmhmm gaussian emissions examples sklearn.hmm import gmmhmm gmmhmm n_components2 n_mix10 covariance_typediag ... gmmhmm algorithmviterbi covariance_typediag covars_prior0.01 gmms gmm covariance_typenone init_paramswmc min_covar0.001 n_components10 n_init1 n_iter100 paramswmc random_statenone thresh0.01 gmm covariance_typenone init_paramswmc min_covar0.001 n_components10 n_init1 n_iter100 paramswmc random_statenone thresh0.01 n_components2 n_mix10 random_statenone startprobnone startprob_prior1.0 transmatnone transmat_prior1.0 chapter user guide scikitlearn user guide release 0.11 int array shape n_components n_components array shape n_components array gmm objects length n_components number states model matrix transition probabilities states initial state occupation distribution gmm emission distributions state random number generator instance attributes n_components transmat startprob gmms random_state randomstate int seed default methods find likely state sequence corresponding obs compute log probability model compute posteriors decode obs algorithm eval obs fit obs n_iter thresh params init_params estimate model parameters get_params deep predict obs algorithm predict_proba obs rvs args kwargs sample random_state score obs set_params params get parameters estimator find likely state sequence corresponding obs compute posterior probability state model deprecated rvs deprecated 0.11 removed 0.13 use sample instead generate random samples model compute log probability model set parameters estimator 
2252: __init__ n_components1 n_mix1 transmatnone startprob_priornone transmat_priornone algorithmviterbi gmmsnone covariance_typediag vars_prior0.01 random_statenone startprobnone create hidden markov model gmm emissions 
2253: parameters n_components int number states 
2254: algorithm decoder algorithm covariance_type covariance type model must one spherical tied diag full 
2255: decode obs algorithmviterbi find likely state sequence corresponding obs uses selected algorithm decoding 
2256: parameters obs array_like shape n_features list n_featuresdimensional data points row corresponds single data point 
2257: algorithm string one decoder_algorithms decoder algorithm used returns logprob oat log probability maximum likelihood path hmm 1.8. reference scikitlearn user guide release 0.11 state_sequence array_like shape index likely states observation see also evalcompute log probability model posteriors scorecompute log probability model eval obs compute log probability model compute posteriors implements rank beam pruning forwardbackward algorithm speed inference large models 
2258: parameters obs array_like shape n_features sequence n_featuresdimensional data points row corresponds single point sequence returns logprob oat log likelihood sequence obs posteriors array_like shape n_components posterior probabilities state observation see also scorecompute log probability model decodefind likely state sequence corresponding obs fit obs n_iter10 thresh0.01 paramsabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz init_paramsabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz kwargs estimate model parameters initialization step performed entering algorithm want avoid step set keyword argument init_params empty string likewise would like initialization call method n_iter0 
2259: parameters obs list list arraylike observation sequences shape n_i n_features 
2260: n_iter int optional number iterations perform 
2261: thresh oat optional convergence threshold params string optional controls parameters updated training process contain com bination startprob transmat means covars etc defaults parameters init_params string optional controls parameters initialized prior training contain combination startprob transmat means covars etc defaults parameters 
2262: chapter user guide scikitlearn user guide release 0.11 notes general logprob nondecreasing unless aggressive pruning used decreasing logprob generally sign overtting e.g covariance parameter getting small getting training data decreasing covars_prior 
2263: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2264: predict obs algorithmviterbi find likely state sequence corresponding obs 
2265: parameters obs array_like shape n_features list n_featuresdimensional data points row corresponds single data point 
2266: returns state_sequence array_like shape index likely states observation predict_proba obs compute posterior probability state model parameters obs array_like shape n_features list n_featuresdimensional data points row corresponds single data point 
2267: returns arraylike shape n_components returns probability sample state model 
2268: rvs args kwargs deprecated rvs deprecated 0.11 removed 0.13 use sample instead sample random_statenone generate random samples model 
2269: parameters int number samples generate 
2270: random_state randomstate int seed default random number generator instance none given objects random_state used returns obs hidden_states obs array_like length list samples hidden_states array_like length list hidden states score obs compute log probability model 
2271: parameters obs array_like shape n_features sequence n_featuresdimensional data points row corresponds single data point 
2272: returns logprob oat 1.8. reference scikitlearn user guide release 0.11 log likelihood obs see also evalcompute log probability model posteriors decodefind likely state sequence corresponding obs set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self startprob_ mixing startprob state 
2273: transmat_ matrix transition probabilities 
2274: 1.8.12 sklearn.kernel_approximation kernel approximation sklearn.kernel_approximation module implements several approximate kernel feature maps base fourier transforms user guide see kernel approximation section details 
2275: kernel_approximation.rbfsampler gamma ... kernel_approximation.additivechi2sampler ... approximate feature map additive chi kernel kernel_approximation.skewedchi2sampler ... approximates feature map skewed chisquared kernel monte approximates feature map rbf kernel monte carlo approximation sklearn.kernel_approximation.rbfsampler class sklearn.kernel_approximation.rbfsampler gamma1.0 n_components100.0 ran approximates feature map rbf kernel monte carlo approximation fourier transform 
2276: dom_statenone parameters gamma oat parameter rbf kernel exp gamma n_components int number monte carlo samples per original feature equals dimensionality computed feature space 
2277: random_state int randomstate optional int random_state seed used random number generator randomstate instance random_state random number generator 
2278: notes see random features largescale kernel machines rahimi benjamin recht 
2279: chapter user guide scikitlearn user guide release 0.11 methods fit fit_transform get_params deep set_params params transform fit model fit data transform get parameters estimator set parameters estimator apply approximate feature map 
2280: __init__ gamma1.0 n_components100.0 random_statenone fit ynone fit model samples random projection according n_features 
2281: parameters arraylike sparse matrix shape n_samples n_features training data n_samples number samples n_features number features 
2282: returns self object returns transformer 
2283: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2284: parameters numpy array shape n_samples n_features training set 
2285: numpy array shape n_samples target values 
2286: returns x_new numpy array shape n_samples n_features_new transformed array 
2287: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
2288: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object 
2289: 1.8. reference scikitlearn user guide release 0.11 returns self transform ynone apply approximate feature map 
2290: parameters arraylike sparse matrix shape n_samples n_features new data n_samples number samples n_features number features 
2291: returns x_new arraylike shape n_samples n_components sklearn.kernel_approximation.additivechi2sampler class sklearn.kernel_approximation.additivechi2sampler sample_steps2 sam ple_intervalnone approximate feature map additive chi kernel uses sampling fourier transform kernel characteristic regular intervals since kernel approximated additive components input vectors treated separately entry original space transformed 2sample_steps1 features sample_steps parameter method typical values include optimal choices sampling interval certain data ranges computed see reference default values reasonable 
2292: parameters sample_steps int optional gives number complex sampling points 
2293: sample_interval oat optional sampling interval must specied sample_steps 
2294: notes see efcient additive kernels via explicit feature maps vedaldi zisserman computer vision pattern recognition methods fit fit_transform get_params deep set_params params transform set parameters fit data transform get parameters estimator set parameters estimator apply approximate feature map 
2295: __init__ sample_steps2 sample_intervalnone fit ynone set parameters 
2296: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2297: chapter user guide scikitlearn user guide release 0.11 parameters numpy array shape n_samples n_features training set 
2298: numpy array shape n_samples target values 
2299: returns x_new numpy array shape n_samples n_features_new transformed array 
2300: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
2301: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform ynone apply approximate feature map 
2302: parameters arraylike shape n_samples n_features returns x_new arraylike shape n_samples n_features sklearn.kernel_approximation.skewedchi2sampler class sklearn.kernel_approximation.skewedchi2sampler skewedness1.0 n_components100 dom_statenone ran approximates feature map skewed chisquared kernel monte carlo approximation fourier transform 
2303: parameters skewedness oat skewedness parameter kernel needs crossvalidated 
2304: n_components int number monte carlo samples per original feature equals dimensionality computed feature space 
2305: random_state int randomstate optional int random_state seed used random number generator randomstate instance random_state random number generator 
2306: 1.8. reference scikitlearn user guide release 0.11 notes see random fourier approximations skewed multiplicative histogram kernels fuxin catalin ionescu cristian sminchisescu 
2307: methods fit fit_transform get_params deep set_params params transform fit model fit data transform get parameters estimator set parameters estimator apply approximate feature map 
2308: __init__ skewedness1.0 n_components100 random_statenone fit ynone fit model samples random projection according n_features 
2309: parameters arraylike shape n_samples n_features training data n_samples number samples n_features number features 
2310: returns self object returns transformer 
2311: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2312: parameters numpy array shape n_samples n_features training set 
2313: numpy array shape n_samples target values 
2314: returns x_new numpy array shape n_samples n_features_new transformed array 
2315: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
2316: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2317: chapter user guide scikitlearn user guide release 0.11 set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform ynone apply approximate feature map 
2318: parameters arraylike shape n_samples n_features new data n_samples number samples n_features number features 
2319: returns x_new arraylike shape n_samples n_components 1.8.13 sklearn.semi_supervised semisupervised learning sklearn.semi_supervised module implements semisupervised learning algorithms algorithms utilized small amounts labeled data large amounts unlabeled data classication tasks module includes label propagation user guide see semisupervised section details 
2320: semi_supervised.labelpropagation kernel ... label propagation classier semi_supervised.labelspreading kernel ... labelspreading model semisupervised learning sklearn.semi_supervised.labelpropagation class sklearn.semi_supervised.labelpropagation kernelrbf gamma20 n_neighbors7 alpha1 max_iters30 tol0.001 label propagation classier parameters kernel knn rbf string identier kernel function use rbf knn kernels currently supported.. gamma oat parameter rbf kernel n_neighbors integer parameter knn kernel alpha oat clamping factor max_iters oat change maximum number iterations allowed tol oat convergence tolerance threshold consider system steady state see also 1.8. reference scikitlearn user guide release 0.11 labelspreadingalternate label proagation strategy robust noise references xiaojin zhu zoubin ghahramani bel http pages.cs.wisc.edujerryzhupubcmucald02107.pdf propagation 
2321: technical report cmucald02107 carnegie mellon university learning labeled unlabeled data examples sklearn import datasets sklearn.semi_supervised import labelpropagation label_prop_model labelpropagation iris datasets.load_iris random_unlabeled_points np.where np.random.random_integers ... labels np.copy iris.target labels random_unlabeled_points label_prop_model.fit iris.data labels ... labelpropagation ... sizelen iris.target methods fit get_params deep predict predict_proba score set_params params fit semisupervised label propagation model based get parameters estimator performs inductive inference across model predict probability possible outcome returns mean accuracy given test data labels set parameters estimator 
2322: __init__ kernelrbf gamma20 n_neighbors7 alpha1 max_iters30 tol0.001 fit fit semisupervised label propagation model based input data provided matrix labeled unlabeled corresponding label matrix dedicated marker value unlabeled samples 
2323: parameters arraylike shape n_samples n_features n_samples n_samples size matrix created array_like shape n_samples n_labeled_samples unlabeled points marked unlabeled samples transductively assigned labels returns self returns instance self 
2324: get_params deeptrue get parameters estimator parameters deep boolean optional chapter user guide scikitlearn user guide release 0.11 true return parameters estimator contained subobjects estimators 
2325: predict performs inductive inference across model 
2326: parameters array_like shape n_samples n_features returns array_like shape n_samples predictions input data predict_proba predict probability possible outcome compute probability estimates single sample possible outcome seen training categorical distribution 
2327: parameters array_like shape n_samples n_features returns probabilities array shape n_samples n_classes normalized probability distributions across class labels score returns mean accuracy given test data labels 
2328: parameters arraylike shape n_samples n_features training set 
2329: arraylike shape n_samples labels 
2330: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.semi_supervised.labelspreading class sklearn.semi_supervised.labelspreading kernelrbf gamma20 n_neighbors7 labelspreading model semisupervised learning model similar basic label propgation algorithm uses afnity matrix based normalized graph laplacian soft clamping across labels 
2331: pha0.2 max_iters30 tol0.001 parameters kernel knn rbf string identier kernel function use rbf knn kernels currently supported gamma oat parameter rbf kernel n_neighbors integer 1.8. reference scikitlearn user guide release 0.11 parameter knn kernel alpha oat clamping factor max_iters oat maximum number iterations allowed tol oat convergence tolerance threshold consider system steady state see also labelpropagationunregularized graph based semisupervised learning references dengyong zhou olivier bousquet thomas navin lal jason weston bernhard schlkopf learning local global consistency http citeseer.ist.psu.eduviewdocsummary doi10.1.1.115.3219 examples sklearn import datasets sklearn.semi_supervised import labelspreading label_prop_model labelspreading iris datasets.load_iris random_unlabeled_points np.where np.random.random_integers ... labels np.copy iris.target labels random_unlabeled_points label_prop_model.fit iris.data labels ... labelspreading ... sizelen iris.target methods fit get_params deep predict predict_proba score set_params params fit semisupervised label propagation model based get parameters estimator performs inductive inference across model predict probability possible outcome returns mean accuracy given test data labels set parameters estimator 
2332: __init__ kernelrbf gamma20 n_neighbors7 alpha0.2 max_iters30 tol0.001 fit fit semisupervised label propagation model based input data provided matrix labeled unlabeled corresponding label matrix dedicated marker value unlabeled samples 
2333: parameters arraylike shape n_samples n_features chapter user guide scikitlearn user guide release 0.11 n_samples n_samples size matrix created array_like shape n_samples n_labeled_samples unlabeled points marked unlabeled samples transductively assigned labels returns self returns instance self 
2334: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2335: predict performs inductive inference across model 
2336: parameters array_like shape n_samples n_features returns array_like shape n_samples predictions input data predict_proba predict probability possible outcome compute probability estimates single sample possible outcome seen training categorical distribution 
2337: parameters array_like shape n_samples n_features returns probabilities array shape n_samples n_classes normalized probability distributions across class labels score returns mean accuracy given test data labels 
2338: parameters arraylike shape n_samples n_features training set 
2339: arraylike shape n_samples labels 
2340: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self 1.8.14 sklearn.lda linear discriminant analysis sklearn.lda module implements linear discriminant analysis lda user guide see linear quadratic discriminant analysis section details 
2341: 1.8. reference scikitlearn user guide release 0.11 lda.lda n_components priors linear discriminant analysis lda sklearn.lda.lda class sklearn.lda.lda n_componentsnone priorsnone linear discriminant analysis lda classier linear decision boundary generated tting class conditional densities data using bayes rule model gaussian density class assuming classes share covariance matrix tted model also used reduce dimensionality input projecting discrim inative directions 
2342: parameters n_components int number components n_classes dimensionality reduction priors array optional shape n_classes priors classes see also sklearn.qda.qdaquadratic discriminant analysis examples import numpy sklearn.lda import lda np.array np.array clf lda clf.fit lda n_componentsnone priorsnone print clf.predict 0.8 attributes means_ xbar_ priors_ covariance_ arraylike shape n_classes n_features oat shape n_features arraylike shape n_classes arraylike shape n_features n_features covariance matrix shared classes class means mean class priors sum methods decision_function fit store_covariance tol fit_transform function return decision function values related fit lda model according given training data parameters fit data transform chapter user guide scikitlearn user guide release 0.11 table 1.100 continued previous page get_params deep predict predict_log_proba predict_proba score set_params params transform get parameters estimator function classication array test vectors function return posterior logprobabilities classication function return posterior probabilities classication returns mean accuracy given test data labels set parameters estimator project data maximize class separation large separation projected class means small variance within class 
2343: __init__ n_componentsnone priorsnone decision_function function return decision function values related class array test vectors 
2344: parameters arraylike shape n_samples n_features returns array shape n_samples n_classes fit store_covariancefalse tol0.0001 fit lda model according given training data parameters 
2345: parameters arraylike shape n_samples n_features training vector n_samples number samples n_features num ber features 
2346: array shape n_samples target values integers store_covariance boolean true covariance matrix shared classes computed stored self.covariance_ attribute 
2347: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2348: parameters numpy array shape n_samples n_features training set 
2349: numpy array shape n_samples target values 
2350: returns x_new numpy array shape n_samples n_features_new transformed array 
2351: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
2352: get_params deeptrue get parameters estimator parameters deep boolean optional 1.8. reference scikitlearn user guide release 0.11 true return parameters estimator contained subobjects estimators 
2353: predict function classication array test vectors predicted class sample returned 
2354: parameters arraylike shape n_samples n_features returns array shape n_samples predict_log_proba function return posterior logprobabilities classication according class array test vectors 
2355: parameters arraylike shape n_samples n_features returns array shape n_samples n_classes predict_proba function return posterior probabilities classication according class array test vectors 
2356: parameters arraylike shape n_samples n_features returns array shape n_samples n_classes score returns mean accuracy given test data labels 
2357: parameters arraylike shape n_samples n_features training set 
2358: arraylike shape n_samples labels 
2359: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform project data maximize class separation large separation projected class means small variance within class 
2360: parameters arraylike shape n_samples n_features returns x_new array shape n_samples n_components 1.8.15 sklearn.linear_model generalized linear models includes ridge regression sklearn.linear_model module implements genelarized linear models bayesian regression lasso elastic net estimators computed least angle regression coordinate scent also implements stochastic gradient descent related algorithms user guide see generalized linear models section details 
2361: chapter user guide scikitlearn user guide release 0.11 dense data linear_model.linearregression ... linear_model.ridge alpha t_intercept ... linear_model.ridgeclassifier alpha ... linear_model.ridgeclassifiercv alphas ... linear_model.ridgecv alphas ... linear_model.lasso alpha t_intercept ... linear_model.lassocv eps n_alphas ... linear_model.elasticnet alpha rho ... linear_model.elasticnetcv rho eps ... linear_model.lars t_intercept verbose ... linear_model.lassolars alpha ... linear_model.larscv t_intercept ... linear_model.lassolarscv t_intercept ... linear_model.lassolarsic criterion ... linear_model.logisticregression penalty ... linear_model.orthogonalmatchingpursuit ... linear_model.perceptron penalty alpha ... linear_model.sgdclassifier loss penalty ... linear_model.sgdregressor loss penalty ... linear_model.bayesianridge n_iter tol ... linear_model.ardregression n_iter tol ... linear_model.randomizedlasso alpha ... linear_model.randomizedlogisticregression ... randomized logistic regression ordinary least squares linear regression linear least squares regularization classier using ridge regression ridge classier builtin crossvalidation ridge regression builtin crossvalidation linear model trained prior regularizer aka lasso lasso linear model iterative tting along regularization path linear model trained prior regularizer elastic net model iterative tting along regularization path least angle regression model a.k.a lar lasso model least angle regression a.k.a lars crossvalidated least angle regression model crossvalidated lasso using lars algorithm lasso model lars using bic aic model selection logistic regression aka logit maxent classier orthogonal mathching pursuit model omp perceptron linear model tted minimizing regularized empirical loss sgd linear model tted minimizing regularized empirical loss sgd bayesian ridge regression bayesian ard regression randomized lasso sklearn.linear_model.linearregression class sklearn.linear_model.linearregression t_intercepttrue normalizefalse ordinary least squares linear regression 
2362: parameters t_intercept boolean optional copy_xtrue wether calculate intercept model set false intercept used calculations e.g data expected already centered 
2363: normalize boolean optional true regressors normalized notes implementation point view plain ordinary least squares numpy.linalg.lstsq wrapped predictor object 
2364: attributes coef_ intercept_ array array estimated coefcients linear regression problem independent term linear model 
2365: 1.8. reference scikitlearn user guide release 0.11 methods decision_function decision function linear model fit n_jobs get_params deep predict score set_params params fit linear model get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
2366: __init__ t_intercepttrue normalizefalse copy_xtrue decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2367: fit n_jobs1 fit linear model 
2368: parameters numpy array sparse matrix shape n_samples n_features training data numpy array shape n_samples n_responses target values n_jobs number jobs use computation 
2369: cpus used provide speedup n_response sufcient large problems returns self returns instance self 
2370: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2371: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2372: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
2373: parameters arraylike shape n_samples n_features chapter user guide scikitlearn user guide release 0.11 training set 
2374: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.linear_model.ridge class sklearn.linear_model.ridge alpha1.0 t_intercepttrue normalizefalse copy_xtrue tol0.001 linear least squares regularization model solves regression model loss function linear least squares function regulariza tion given l2norm also known ridge regression tikhonov regularization estimator builtin support multivariate regression i.e. 2darray shape n_samples n_responses 
2375: parameters alpha oat small positive values alpha improve conditioning problem reduce variance estimates alpha corresponds linear models logisticregression linearsvc 
2376: t_intercept boolean whether calculate intercept model set false intercept used calculations e.g data expected already centered 
2377: normalize boolean optional true regressors normalized copy_x boolean optional default true true copied else may overwritten 
2378: tol oat precision solution 
2379: see also ridgeclassifier ridgecv examples sklearn.linear_model import ridge import numpy n_samples n_features np.random.seed np.random.randn n_samples np.random.randn n_samples n_features clf ridge alpha1.0 clf.fit 1.8. reference scikitlearn user guide release 0.11 ridge alpha1.0 copy_xtrue fit_intercepttrue normalizefalse tol0.001 attributes coef_ array shape n_features n_responses n_features weight vector 
2380: methods decision_function fit sample_weight solver get_params deep predict score set_params params decision function linear model fit ridge regression model get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
2381: __init__ alpha1.0 t_intercepttrue normalizefalse copy_xtrue tol0.001 decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2382: fit sample_weight1.0 solverauto fit ridge regression model parameters arraylike sparse matrix shape n_samples n_features training data arraylike shape n_samples n_samples n_responses target values sample_weight oat numpy array shape n_samples individual weights sample solver auto dense_cholesky sparse_cg delse_cholesky use standard solver use computational routines scipy.linalg.solve function sparse_cg use conjugate gradient solver found scipy.sparse.linalg.cg auto chose appropriate depending matrix 
2383: returns self returns instance self 
2384: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2385: chapter user guide scikitlearn user guide release 0.11 predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2386: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
2387: parameters arraylike shape n_samples n_features training set 
2388: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.linear_model.ridgeclassier class sklearn.linear_model.ridgeclassifier alpha1.0 t_intercepttrue normalizefalse copy_xtrue tol0.001 class_weightnone classier using ridge regression 
2389: parameters alpha oat small positive values alpha improve conditioning problem reduce variance estimates alpha corresponds linear models logisticregression linearsvc 
2390: t_intercept boolean whether calculate intercept model set false intercept used calculations e.g data expected already centered 
2391: normalize boolean optional true regressors normalized copy_x boolean optional default true true copied else may overwritten 
2392: tol oat precision solution class_weight dict optional 1.8. reference scikitlearn user guide release 0.11 weights associated classes form class_label weight given classes supposed weight one 
2393: see also ridge ridgeclassifiercv notes multiclass classication n_class classiers trained oneversusall approach concretely implemented taking advantage multivariate response support ridge 
2394: attributes coef_ array shape n_features n_classes n_features weight vector 
2395: methods decision_function fit solver get_params deep predict score set_params params fit ridge regression model get parameters estimator predict target values according tted model returns coefcient determination prediction set parameters estimator 
2396: __init__ alpha1.0 class_weightnone t_intercepttrue normalizefalse copy_xtrue tol0.001 fit solverauto fit ridge regression model 
2397: parameters arraylike sparse matrix shape n_samples n_features training data arraylike shape n_samples target values solver auto dense_cholesky sparse_cg solver use computational routines delse_cholesky use standard scipy.linalg.solve function sparse_cg use conjugate gradient solver found scipy.sparse.linalg.cg auto chose appropriate depending matrix 
2398: returns self returns instance self 
2399: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2400: chapter user guide scikitlearn user guide release 0.11 predict predict target values according tted model 
2401: parameters arraylike shape n_samples n_features returns array shape n_samples score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
2402: parameters arraylike shape n_samples n_features training set 
2403: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.linear_model.ridgeclassiercv class sklearn.linear_model.ridgeclassifiercv alphasarray 
2404: 0.1 normalizefalse t_intercepttrue score_funcnone loss_funcnone cvnone class_weightnone ridge classier builtin crossvalidation default performs generalized crossvalidation form efcient leaveoneout cross validation currently n_features n_samples case handled efciently 
2405: parameters alphas numpy array shape n_alpha array alpha values try small positive values alpha improve conditioning problem reduce variance estimates alpha corresponds linear models logisticregression linearsvc 
2406: t_intercept boolean whether calculate intercept model set false intercept used calculations e.g data expected already centered 
2407: normalize boolean optional true regressors normalized score_func callable optional function takes arguments compares order evaluate performance prediction big good none passed score estimator maximized loss_func callable optional 1.8. reference scikitlearn user guide release 0.11 function takes arguments compares order evaluate performance prediction small good none passed score estimator maximized crossvalidation generator optional none generalized crossvalidation efcient leaveoneout used 
2408: class_weight dict optional weights associated classes form class_label weight given classes supposed weight one 
2409: see also ridgeridge regression ridgeclassifierridge classier ridgecvridge regression builtin cross validation notes multiclass classication n_class classiers trained oneversusall approach concretely implemented taking advantage multivariate response support ridge 
2410: methods decision_function fit sample_weight class_weight get_params deep predict score set_params params fit ridge classier get parameters estimator predict target values according tted model returns coefcient determination prediction set parameters estimator 
2411: __init__ alphasarray 0.1 
2412: t_intercepttrue normalizefalse score_funcnone loss_funcnone cvnone class_weightnone fit sample_weight1.0 class_weightnone fit ridge classier 
2413: parameters arraylike shape n_samples n_features training vectors n_samples number samples n_features num ber features 
2414: arraylike shape n_samples target values 
2415: sample_weight oat numpy array shape n_samples sample weight class_weight dict optional weights associated classes form class_label weight given classes supposed weight one 
2416: returns self object chapter user guide scikitlearn user guide release 0.11 returns self get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2417: predict predict target values according tted model 
2418: parameters arraylike shape n_samples n_features returns array shape n_samples score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
2419: parameters arraylike shape n_samples n_features training set 
2420: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.linear_model.ridgecv class sklearn.linear_model.ridgecv alphasarray 0.1 
2421: t_intercepttrue malizefalse score_funcnone loss_funcnone cvnone gcv_modenone ridge regression builtin crossvalidation default performs generalized crossvalidation form efcient leaveoneout cross validation 
2422: parameters alphas numpy array shape n_alpha array alpha values try small positive values alpha improve conditioning problem reduce variance estimates alpha corresponds linear models logisticregression linearsvc 
2423: t_intercept boolean whether calculate intercept model set false intercept used calculations e.g data expected already centered 
2424: normalize boolean optional 1.8. reference scikitlearn user guide release 0.11 true regressors normalized score_func callable optional function takes arguments compares order evaluate performance prediction big good none passed score estimator maximized loss_func callable optional function takes arguments compares order evaluate performance prediction small good none passed score estimator maximized crossvalidation generator optional none generalized crossvalidation efcient leaveoneout used 
2425: see also ridgeridge regression ridgeclassifierridge classier ridgecvridge regression builtin cross validation attributes coef_ gcv_mode methods shape n_features array n_classes n_features none auto svd eigen tional weight vector 
2426: flag indicating strategy use performing generalized crossvalidation options auto use svd n_samples n_features otherwise use eigen svd force computation via singular value decomposition eigen force computation via eigendecomposition auto mode default intended pick cheaper tion two depending upon shape training data 
2427: decision_function decision function linear model fit sample_weight get_params deep predict score set_params params fit ridge regression model get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
2428: __init__ alphasarray 0.1 
2429: t_intercepttrue normalizefalse score_funcnone loss_funcnone cvnone gcv_modenone decision_function decision function linear model parameters numpy array shape n_samples n_features chapter user guide scikitlearn user guide release 0.11 returns array shape n_samples returns predicted values 
2430: fit sample_weight1.0 fit ridge regression model parameters arraylike shape n_samples n_features training data arraylike shape n_samples n_samples n_responses target values sample_weight oat arraylike shape n_samples sample weight returns self returns self 
2431: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2432: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2433: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
2434: parameters arraylike shape n_samples n_features training set 
2435: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self 1.8. reference scikitlearn user guide release 0.11 sklearn.linear_model.lasso class sklearn.linear_model.lasso alpha1.0 puteauto warm_startfalse positivefalse t_intercepttrue copy_xtrue max_iter1000 linear model trained prior regularizer aka lasso optimization objective lasso normalizefalse precom tol0.0001 n_samples xw2_2 alpha w_1 technically lasso model optimizing objective function elastic net rho1.0 penalty 
2436: parameters alpha oat optional constant multiplies term defaults 1.0 t_intercept boolean whether calculate intercept model set false intercept used calculations e.g data expected already centered 
2437: normalize boolean optional true regressors normalized copy_x boolean optional default true true copied else may overwritten 
2438: precompute true false auto arraylike whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
2439: max_iter int optional maximum number iterations tol oat optional tolerance optimization updates smaller tol optimization code checks dual gap optimality continues smaller tol 
2440: warm_start bool optional set true reuse solution previous call initialization otherwise erase previous solution 
2441: positive bool optional set true forces coefcients positive 
2442: see also lars_path sklearn.decomposition.sparse_encode lasso_path lassolars lassocv lassolarscv notes algorithm used model coordinate descent 
2443: chapter user guide avoid unnecessary memory duplication argument method directly passed fortran contiguous numpy array 
2444: scikitlearn user guide release 0.11 examples sklearn import linear_model clf linear_model.lasso alpha0.1 clf.fit lasso alpha0.1 copy_xtrue fit_intercepttrue max_iter1000 normalizefalse positivefalse precomputeauto tol0.0001 warm_startfalse print clf.coef_ 0.85 print clf.intercept_ 0.15 
2445: attributes coef_ intercept_ array shape n_features oat parameter vector fomulation formula independent term decision function 
2446: methods decision_function decision function linear model fit coef_init get_params deep predict score set_params params fit elastic net model coordinate descent get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
2447: __init__ alpha1.0 t_intercepttrue normalizefalse precomputeauto max_iter1000 tol0.0001 warm_startfalse positivefalse copy_xtrue decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2448: fit xynone coef_initnone fit elastic net model coordinate descent parameters ndarray n_samples n_features data ndarray n_samples target arraylike optional 1.8. reference scikitlearn user guide release 0.11 np.dot x.t precomputed useful gram matrix precomputed 
2449: coef_init ndarray shape n_features initial coefents warmstart optimization notes coordinate descent algorithm considers column data time hence automatically convert input fortran contiguous numpy array necessary avoid memory reallocation advised allocate initial data memory directly using format 
2450: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2451: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2452: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
2453: parameters arraylike shape n_samples n_features training set 
2454: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.linear_model.lassocv class sklearn.linear_model.lassocv eps0.001 n_alphas100 alphasnone t_intercepttrue max_iter1000 precomputeauto lasso linear model iterative tting along regularization path normalizefalse tol0.0001 copy_xtrue cvnone verbosefalse chapter user guide scikitlearn user guide release 0.11 best model selected crossvalidation optimization objective lasso n_samples xw2_2 alpha w_1 parameters eps oat optional length path eps1e3 means alpha_min alpha_max 1e3 
2455: n_alphas int optional number alphas along regularization path alphas numpy array optional list alphas compute models none alphas set automatically precompute true false auto arraylike whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
2456: max_iter int optional maximum number iterations tol oat optional tolerance optimization updates smaller tol optimization code checks dual gap optimality continues smaller tol 
2457: integer crossvalidation generator optional integer passed number fold default specic crossvalidation jects passed see sklearn.cross_validation module list possible objects verbose bool integer amount verbosity see also lars_path lasso_path lassolars lasso lassolarscv notes see exampleslinear_modellasso_path_with_crossvalidation.py example avoid unnecessary memory duplication argument method directly passed fortran contiguous numpy array 
2458: attributes alpha_ oat coef_ intercept_ mse_path_ array shape n_alphas n_folds array shape n_features oat amount penalization choosen cross validation parameter vector fomulation formula independent term decision function mean square error test set fold varying alpha 1.8. reference scikitlearn user guide release 0.11 methods decision_function fit get_params deep path eps n_alphas alphas ... compute lasso path coordinate descent predict score set_params params predict using linear model returns coefcient determination prediction set parameters estimator 
2459: decision function linear model fit linear model coordinate descent along decreasing alphas get parameters estimator __init__ eps0.001 n_alphas100 alphasnone t_intercepttrue normalizefalse precom puteauto max_iter1000 tol0.0001 copy_xtrue cvnone verbosefalse decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2460: fit fit linear model coordinate descent along decreasing alphas using crossvalidation parameters numpy array shape n_samples n_features training data pass directly fortran contiguous data avoid unnecessary memory duplication numpy array shape n_samples target values get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators precomputeauto xynone eps0.001 t_intercepttrue normalizefalse copy_xtrue verbosefalse params static path n_alphas100 alphasnone compute lasso path coordinate descent optimization objective lasso n_samples xw2_2 alpha w_1 parameters numpy array shape n_samples n_features training data pass directly fortran contiguous data avoid unnecessary memory duplication numpy array shape n_samples target values eps oat optional length path eps1e3 means alpha_min alpha_max 1e3 chapter user guide scikitlearn user guide release 0.11 n_alphas int optional number alphas along regularization path alphas numpy array optional list alphas compute models none alphas set automatically precompute true false auto arraylike whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
2461: arraylike optional np.dot x.t precomputed useful gram matrix precomputed 
2462: t_intercept bool fit intercept normalize boolean optional true regressors normalized copy_x boolean optional default true true copied else may overwritten 
2463: verbose bool integer amount verbosity params kwargs keyword arguments passed lasso objects returns models list models along regularization path see also lars_path sklearn.decomposition.sparse_encode lasso lassolars lassocv lassolarscv notes see exampleslinear_modelplot_lasso_coordinate_descent_path.py example avoid unnecessary memory duplication argument method directly passed fortran contiguous numpy array 
2464: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2465: score returns coefcient determination prediction 
2466: 1.8. reference scikitlearn user guide release 0.11 coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
2467: parameters arraylike shape n_samples n_features training set 
2468: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.linear_model.elasticnet class sklearn.linear_model.elasticnet alpha1.0 izefalse copy_xtrue tivefalse linear model trained prior regularizer minimizes objective function t_intercepttrue rho0.5 normal precomputeauto max_iter1000 tol0.0001 warm_startfalse posi n_samples xw2_2 alpha rho w_1 0.5 alpha rho w2_2 interested controlling penalty separately keep mind equivalent alpha rho parameter rho corresponds alpha glmnet package alpha corresponds lambda param eter glmnet specically rho lasso penalty currently rho 0.01 reliable unless supply sequence alpha 
2469: parameters alpha oat constant multiplies penalty terms defaults 1.0 see notes exact mathematical meaning parameter rho oat elasticnet mixing parameter rho rho penalty penalty rho penalty rho penalty combination t_intercept bool whether intercept estimated false data assumed already centered 
2470: normalize boolean optional chapter user guide scikitlearn user guide release 0.11 true regressors normalized precompute true false auto arraylike whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
2471: max_iter int optional maximum number iterations copy_x boolean optional default false true copied else may overwritten 
2472: tol oat optional tolerance optimization updates smaller tol optimization code checks dual gap optimality continues smaller tol 
2473: warm_start bool optional set true reuse solution previous call initialization otherwise erase previous solution 
2474: positive bool optional set true forces coefcients positive 
2475: notes avoid unnecessary memory duplication argument method directly passed fortran contiguous numpy array 
2476: methods decision_function decision function linear model fit coef_init get_params deep predict score set_params params fit elastic net model coordinate descent get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
2477: __init__ alpha1.0 rho0.5 t_intercepttrue normalizefalse precomputeauto max_iter1000 copy_xtrue tol0.0001 warm_startfalse positivefalse decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2478: fit xynone coef_initnone fit elastic net model coordinate descent parameters ndarray n_samples n_features 1.8. reference scikitlearn user guide release 0.11 data ndarray n_samples target arraylike optional np.dot x.t precomputed useful gram matrix precomputed 
2479: coef_init ndarray shape n_features initial coefents warmstart optimization notes coordinate descent algorithm considers column data time hence automatically convert input fortran contiguous numpy array necessary avoid memory reallocation advised allocate initial data memory directly using format 
2480: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2481: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2482: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
2483: parameters arraylike shape n_samples n_features training set 
2484: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self chapter user guide sklearn.linear_model.elasticnetcv scikitlearn user guide release 0.11 class sklearn.linear_model.elasticnetcv rho0.5 eps0.001 n_alphas100 alphasnone t_intercepttrue precom puteauto max_iter1000 tol0.0001 cvnone copy_xtrue verbose0 n_jobs1 normalizefalse elastic net model iterative tting along regularization path best model selected crossvalidation 
2485: parameters rho oat optional oat passed elasticnet scaling penalties rho penalty penalty rho penalty rho penalty combination parameter list case different values tested crossvalidation one giving best prediction score used note good choice list values rho often put values close i.e lasso less close i.e ridge .95 .99 eps oat optional length path eps1e3 means alpha_min alpha_max 1e3 
2486: n_alphas int optional number alphas along regularization path alphas numpy array optional list alphas compute models none alphas set automatically precompute true false auto arraylike whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
2487: max_iter int optional maximum number iterations tol oat optional tolerance optimization updates smaller tol optimization code checks dual gap optimality continues smaller tol 
2488: integer crossvalidation generator optional integer passed number fold default specic crossvalidation jects passed see sklearn.cross_validation module list possible objects verbose bool integer amount verbosity n_jobs integer optional number cpus use cross validation use cpus note used multiple values rho given 
2489: see also enet_path elasticnet 1.8. reference scikitlearn user guide release 0.11 notes see exampleslinear_modellasso_path_with_crossvalidation.py example avoid unnecessary memory duplication argument method directly passed fortran contiguous numpy array parameter rho corresponds alpha glmnet package alpha corresponds lambda param eter glmnet specically optimization objective n_samples xw2_2 alpha rho w_1 0.5 alpha rho w2_2 interested controlling penalty separately keep mind equivalent alpha rho attributes alpha_ oat rho_ oat coef_ intercept_ mse_path_ array shape n_rho n_alpha n_folds methods array shape n_features oat amount penalization choosen cross validation compromise penalization choosen cross validation parameter vector fomulation formula independent term decision function mean square error test set fold varying rho alpha decision_function fit get_params deep path rho eps n_alphas alphas ... compute elasticnet path coordinate descent predict score set_params params predict using linear model returns coefcient determination prediction set parameters estimator 
2490: decision function linear model fit linear model coordinate descent along decreasing alphas get parameters estimator __init__ rho0.5 eps0.001 n_alphas100 alphasnone t_intercepttrue normalizefalse tol0.0001 cvnone copy_xtrue verbose0 precomputeauto max_iter1000 n_jobs1 decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples chapter user guide scikitlearn user guide release 0.11 returns predicted values 
2491: fit fit linear model coordinate descent along decreasing alphas using crossvalidation parameters numpy array shape n_samples n_features training data pass directly fortran contiguous data avoid unnecessary memory duplication numpy array shape n_samples target values get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2492: static path rho0.5 eps0.001 n_alphas100 alphasnone precomputeauto xynone t_intercepttrue normalizefalse copy_xtrue verbosefalse params compute elasticnet path coordinate descent elastic net optimization function n_samples xw2_2 alpha rho w_1 0.5 alpha rho w2_2 parameters numpy array shape n_samples n_features training data pass directly fortran contiguous data avoid unnecessary memory duplication numpy array shape n_samples target values rho oat optional oat passed elasticnet scaling penalties rho1 corresponds lasso eps oat length path eps1e3 means alpha_min alpha_max 1e3 n_alphas int optional number alphas along regularization path alphas numpy array optional list alphas compute models none alphas set automatically precompute true false auto arraylike whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
2493: arraylike optional np.dot x.t precomputed useful gram matrix precomputed 
2494: 1.8. reference scikitlearn user guide release 0.11 t_intercept bool fit intercept normalize boolean optional true regressors normalized copy_x boolean optional default true true copied else may overwritten 
2495: verbose bool integer amount verbosity params kwargs keyword arguments passed lasso objects returns models list models along regularization path see also elasticnet elasticnetcv notes see examplesplot_lasso_coordinate_descent_path.py example 
2496: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2497: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
2498: parameters arraylike shape n_samples n_features training set 
2499: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self chapter user guide sklearn.linear_model.lars scikitlearn user guide release 0.11 class sklearn.linear_model.lars t_intercepttrue verbosefalse normalizetrue precom puteauto n_nonzero_coefs500 eps2.2204460492503131e copy_xtrue least angle regression model a.k.a lar parameters n_nonzero_coefs int optional target number nonzero coefcients use np.inf limit 
2500: t_intercept boolean whether calculate intercept model set false intercept used calculations e.g data expected already centered 
2501: verbose boolean integer optional sets verbosity amount normalize boolean optional true regressors normalized precompute true false auto arraylike whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
2502: copy_x boolean optional default true true copied else may overwritten 
2503: eps oat optional machineprecision regularization computation cholesky diagonal fac tors increase illconditioned systems unlike tol parameter iterative optimizationbased algorithms parameter control tolerance optimization 
2504: see also lars_path larscv sklearn.decomposition.sparse_encode httpen.wikipedia.orgwikileast_angle_regression examples sklearn import linear_model clf linear_model.lars n_nonzero_coefs1 clf.fit 1.1111 1.1111 ... lars copy_xtrue eps ... fit_intercepttrue n_nonzero_coefs1 normalizetrue precomputeauto verbosefalse print clf.coef_ 1.11 ... attributes coef_ intercept_ array shape n_features oat parameter vector fomulation formula independent term decision function 
2505: 1.8. reference scikitlearn user guide release 0.11 methods decision_function decision function linear model fit get_params deep predict score set_params params fit model using training data get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
2506: __init__ t_intercepttrue precomputeauto n_nonzero_coefs500 eps2.2204460492503131e16 copy_xtrue verbosefalse normalizetrue decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2507: fit fit model using training data 
2508: parameters arraylike shape n_samples n_features training data 
2509: arraylike shape n_samples target values returns self object returns instance self 
2510: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2511: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2512: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
2513: parameters arraylike shape n_samples n_features training set 
2514: chapter user guide scikitlearn user guide release 0.11 arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.linear_model.lassolars class sklearn.linear_model.lassolars alpha1.0 t_intercepttrue malizetrue eps2.2204460492503131e16 copy_xtrue precomputeauto verbosefalse max_iter500 lasso model least angle regression a.k.a lars linear model trained prior regularizer optimization objective lasso n_samples xw2_2 alpha w_1 parameters t_intercept boolean whether calculate intercept model set false intercept used calculations e.g data expected already centered 
2515: verbose boolean integer optional sets verbosity amount normalize boolean optional true regressors normalized copy_x boolean optional default true true copied else may overwritten 
2516: precompute true false auto arraylike whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
2517: max_iter integer optional maximum number iterations perform 
2518: eps oat optional machineprecision regularization computation cholesky diagonal fac tors increase illconditioned systems unlike tol parameter iterative optimizationbased algorithms parameter control tolerance optimization 
2519: see also lars_path lasso_path lasso lassocv lassolarscv sklearn.decomposition.sparse_encode httpen.wikipedia.orgwikileast_angle_regression 1.8. reference scikitlearn user guide release 0.11 examples sklearn import linear_model clf linear_model.lassolars alpha0.01 clf.fit ... lassolars alpha0.01 copy_xtrue eps ... fit_intercepttrue max_iter500 normalizetrue precomputeauto verbosefalse print clf.coef_ 
2520: 0.963257 ... attributes coef_ intercept_ array shape n_features oat parameter vector fomulation formula independent term decision function 
2521: methods decision_function decision function linear model fit get_params deep predict score set_params params fit model using training data get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
2522: __init__ alpha1.0 t_intercepttrue verbosefalse normalizetrue precomputeauto max_iter500 eps2.2204460492503131e16 copy_xtrue decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2523: fit fit model using training data 
2524: parameters arraylike shape n_samples n_features training data 
2525: arraylike shape n_samples target values returns self object returns instance self 
2526: get_params deeptrue get parameters estimator parameters deep boolean optional chapter user guide scikitlearn user guide release 0.11 true return parameters estimator contained subobjects estimators 
2527: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2528: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
2529: parameters arraylike shape n_samples n_features training set 
2530: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.linear_model.larscv class sklearn.linear_model.larscv t_intercepttrue verbosefalse max_iter500 normal izetrue precomputeauto cvnone max_n_alphas1000 n_jobs1 eps2.2204460492503131e16 copy_xtrue crossvalidated least angle regression model parameters t_intercept boolean whether calculate intercept model set false intercept used calculations e.g data expected already centered 
2531: verbose boolean integer optional sets verbosity amount normalize boolean optional true regressors normalized copy_x boolean optional default true true copied else may overwritten 
2532: precompute true false auto arraylike whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
2533: 1.8. reference scikitlearn user guide release 0.11 max_iter integer optional maximum number iterations perform 
2534: crossvalidation generator optional see sklearn.cross_validation module none passed default 5fold strategy max_n_alphas integer optional maximum number points path used compute residuals cross validation n_jobs integer optional number cpus use cross validation use cpus eps oat optional machineprecision regularization computation cholesky diagonal fac tors increase illconditioned systems 
2535: see also lars_path lassolars lassolarscv attributes coef_ intercept_ coef_path array shape n_features n_alpha methods array shape n_features oat parameter vector fomulation formula independent term decision function varying values coefcients along path decision_function decision function linear model fit get_params deep predict score set_params params fit model using training data get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
2536: __init__ t_intercepttrue verbosefalse max_iter500 normalizetrue precomputeauto cvnone max_n_alphas1000 n_jobs1 eps2.2204460492503131e16 copy_xtrue decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2537: fit fit model using training data 
2538: parameters arraylike shape n_samples n_features chapter user guide scikitlearn user guide release 0.11 training data 
2539: arraylike shape n_samples target values returns self object returns instance self 
2540: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2541: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2542: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
2543: parameters arraylike shape n_samples n_features training set 
2544: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.linear_model.lassolarscv class sklearn.linear_model.lassolarscv t_intercepttrue verbosefalse max_iter500 precomputeauto n_jobs1 crossvalidated lasso using lars algorithm optimization objective lasso normalizetrue cvnone eps2.2204460492503131e16 copy_xtrue max_n_alphas1000 n_samples xw2_2 alpha w_1 parameters t_intercept boolean 1.8. reference scikitlearn user guide release 0.11 whether calculate intercept model set false intercept used calculations e.g data expected already centered 
2545: verbose boolean integer optional sets verbosity amount normalize boolean optional true regressors normalized precompute true false auto arraylike whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
2546: max_iter integer optional maximum number iterations perform 
2547: crossvalidation generator optional see sklearn.cross_validation module none passed default 5fold strategy max_n_alphas integer optional maximum number points path used compute residuals cross validation n_jobs integer optional number cpus use cross validation use cpus eps oat optional machineprecision regularization computation cholesky diagonal fac tors increase illconditioned systems 
2548: copy_x boolean optional default true true copied else may overwritten 
2549: see also lars_path lassolars larscv lassocv notes object solves problem lassocv object however unlike lassocv relevent alphas values general property stable however fragile heavily multicollinear datasets efcient lassocv small number features selected compared total number instance samples compared number features 
2550: chapter user guide scikitlearn user guide release 0.11 array shape n_features oat parameter vector fomulation formula independent term decision function varying values coefcients along path different values alpha along path values alpha along path different folds mean square error leftout fold along path alpha values given cv_alphas attributes coef_ intercept_ coef_path array shape n_features n_alpha alphas_ array shape n_alpha cv_alphas array shape n_cv_alphas cv_mse_path_ array shape n_folds n_cv_alphas methods decision_function decision function linear model fit get_params deep predict score set_params params fit model using training data get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
2551: __init__ t_intercepttrue verbosefalse max_iter500 normalizetrue precomputeauto cvnone max_n_alphas1000 n_jobs1 eps2.2204460492503131e16 copy_xtrue decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2552: fit fit model using training data 
2553: parameters arraylike shape n_samples n_features training data 
2554: arraylike shape n_samples target values returns self object returns instance self 
2555: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2556: 1.8. reference scikitlearn user guide release 0.11 predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2557: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
2558: parameters arraylike shape n_samples n_features training set 
2559: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.linear_model.lassolarsic class sklearn.linear_model.lassolarsic criterionaic lasso model lars using bic aic model selection optimization objective lasso verbosefalse normalizetrue precomputeauto max_iter500 eps2.2204460492503131e16 copy_xtrue t_intercepttrue n_samples xw2_2 alpha w_1 aic akaike information criterion bic bayes information criterion criteria useful select value regularization parameter making tradeoff goodness complexity model good model explain well data simple 
2560: parameters criterion bic aic type criterion use 
2561: t_intercept boolean whether calculate intercept model set false intercept used calculations e.g data expected already centered 
2562: verbose boolean integer optional sets verbosity amount normalize boolean optional true regressors normalized chapter user guide scikitlearn user guide release 0.11 copy_x boolean optional default true true copied else may overwritten 
2563: precompute true false auto arraylike whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
2564: max_iter integer optional maximum number iterations perform used early stopping 
2565: eps oat optional machineprecision regularization computation cholesky diagonal fac tors increase illconditioned systems unlike tol parameter iterative optimizationbased algorithms parameter control tolerance optimization 
2566: see also lars_path lassolars lassolarscv notes estimation number degrees freedom given degrees freedom lasso hui zou trevor hastie robert tibshirani ann statist volume number 21732192. http en.wikipedia.orgwikiakaike_information_criterion http en.wikipedia.orgwikibayesian_information_criterion examples sklearn import linear_model clf linear_model.lassolarsic criterionbic clf.fit 1.1111 1.1111 ... lassolarsic copy_xtrue criterionbic eps ... fit_intercepttrue max_iter500 normalizetrue precomputeauto verbosefalse print clf.coef_ 
2567: 1.11 ... attributes coef_ intercept_ alpha_ array shape n_features oat oat parameter vector fomulation formula independent term decision function alpha parameter chosen information criterion methods decision_function decision function linear model continued next page 1.8. reference scikitlearn user guide release 0.11 table 1.115 continued previous page fit copy_x get_params deep predict score set_params params fit model using training data get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
2568: __init__ criterionaic t_intercepttrue verbosefalse normalizetrue precomputeauto max_iter500 eps2.2204460492503131e16 copy_xtrue decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2569: fit copy_xtrue fit model using training data 
2570: parameters arraylike shape n_samples n_features training data 
2571: arraylike shape n_samples target values returns self object returns instance self 
2572: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2573: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2574: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
2575: parameters arraylike shape n_samples n_features training set 
2576: arraylike shape n_samples returns oat chapter user guide scikitlearn user guide release 0.11 set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.linear_model.logisticregression class sklearn.linear_model.logisticregression penaltyl2 dualfalse c1.0 cept_scaling1 class_weightnone t_intercepttrue tol0.0001 inter logistic regression aka logit maxent classier multiclass case training algorithm uses onevs.all ova scheme rather true multinomial class implements regularized logistic regression using liblinear library handle dense sparse input use cordered arrays csr matrices containing 64bit oats optimal performance input format converted copied 
2577: parameters penalty string used specify norm used penalization dual boolean dual primal formulation dual formulation implemented penalty prefer dualfalse n_samples n_features 
2578: oat none optional defaultnone species strength regularization smaller bigger regular ization none set n_samples 
2579: t_intercept bool default true species constant a.k.a bias intercept added decision function intercept_scaling oat default self.t_intercept true instance vector becomes self.intercept_scaling i.e synthetic feature constant value equals intercept_scaling appended instance vector intercept becomes intercept_scaling synthetic feature weight note synthetic feature weight subject l1l2 regularization features lessen effect regularization synthetic feature weight therefore intercept intercept_scaling increased tol oat optional tolerance stopping criteria see also linearsvc 1.8. reference scikitlearn user guide release 0.11 notes underlying implementation uses random number generator select features tting model thus uncommon slightly different results input data happens try smaller tol parameter references liblinear library large linear classicationhttp www.csie.ntu.edu.twcjlinliblinear hsiangfu fanglan huang chihjen lin dual coordinate descentmethods machine learning 
2580: regression gistic maximum entropy models http www.csie.ntu.edu.twcjlinpapersmaxent_dual.pdf attributes array shape n_classes1 n_features array shape n_classes1 coef_ ter cept_ methods coefcient features decision function coef_ readonly property derived raw_coef_ follows internal memory layout liblinear intercept a.k.a bias added decision function available parameter intercept set true decision_function decision function value according trained model fit class_weight fit_transform get_params deep predict predict_log_proba predict_proba score set_params params transform threshold reduce important features 
2581: fit model according given training data fit data transform get parameters estimator predict target values according tted model log probability estimates probability estimates returns mean accuracy given test data labels set parameters estimator 
2582: __init__ penaltyl2 dualfalse class_weightnone decision_function tol0.0001 c1.0 t_intercepttrue intercept_scaling1 decision function value according trained model 
2583: parameters arraylike shape n_samples n_features returns arraylike shape n_samples n_class returns decision function sample class model 
2584: fit class_weightnone fit model according given training data 
2585: parameters arraylike sparse matrix shape n_samples n_features training vector n_samples number samples n_features num ber features 
2586: chapter user guide scikitlearn user guide release 0.11 arraylike shape n_samples target vector relative class_weight dict auto optional weights associated classes given classes supposed weight one 
2587: returns self object returns self 
2588: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2589: parameters numpy array shape n_samples n_features training set 
2590: numpy array shape n_samples target values 
2591: returns x_new numpy array shape n_samples n_features_new transformed array 
2592: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
2593: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2594: predict predict target values according tted model 
2595: parameters arraylike sparse matrix shape n_samples n_features returns array shape n_samples predict_log_proba log probability estimates returned estimates classes ordered label classes 
2596: parameters arraylike shape n_samples n_features returns arraylike shape n_samples n_classes returns logprobabilities sample class model classes ordered arithmetical order 
2597: predict_proba probability estimates returned estimates classes ordered label classes 
2598: 1.8. reference scikitlearn user guide release 0.11 parameters arraylike shape n_samples n_features returns arraylike shape n_samples n_classes returns probability sample class model classes ordered arithmetical order 
2599: score returns mean accuracy given test data labels 
2600: parameters arraylike shape n_samples n_features training set 
2601: arraylike shape n_samples labels 
2602: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform thresholdnone reduce important features 
2603: parameters array scipy sparse matrix shape n_samples n_features input samples 
2604: threshold string oat none optional defaultnone threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor e.g. 1.25mean may also used none available object attribute threshold used otherwise mean used default 
2605: returns x_r array shape n_samples n_selected_features input samples selected features 
2606: sklearn.linear_model.orthogonalmatchingpursuit class sklearn.linear_model.orthogonalmatchingpursuit copy_xtrue copy_gramtrue copy_xytrue n_nonzero_coefsnone tolnone normalizetrue pute_gramfalse t_intercepttrue precom orthogonal mathching pursuit model omp parameters n_nonzero_coefs int optional desired number nonzero entries solution none default value set n_features 
2607: tol oat optional chapter user guide scikitlearn user guide release 0.11 maximum norm residual none overrides n_nonzero_coefs 
2608: t_intercept boolean optional whether calculate intercept model set false intercept used calculations e.g data expected already centered 
2609: normalize boolean optional false regressors assumed already normalized 
2610: precompute_gram true false auto whether use precomputed gram matrix speed calculations improves performance n_targets n_samples large note already matrices pass directly method 
2611: copy_x bool optional whether design matrix must copied algorithm false value helpful already fortranordered otherwise copy made anyway 
2612: copy_gram bool optional whether gram matrix must copied algorithm false value helpful already fortranordered otherwise copy made anyway 
2613: copy_xy bool optional whether covariance vector must copied algorithm false may overwritten 
2614: see also orthogonal_mp decomposition.sparse_encode decomposition.sparse_encode_parallel orthogonal_mp_gram lars_path lars lassolars notes orthogonal matching pursuit introduced mallat zhang matching pursuits timefrequency dictionaries ieee transactions signal processing vol december http blanche.polytechnique.frmallatpapiersmallatpursuit93.pdf implementation based rubinstein zibulevsky elad efcient implementation ksvd algorithm using batch orthogonal matching pursuit technical report technion april 2008. http www.cs.technion.ac.ilronrubinpublicationsksvdompv2.pdf attributes coef_ intercept_ array shape n_features n_features n_targets oat array shape n_targets parameter vector fomulation formula independent term decision function 
2615: methods decision_function decision function linear model fit gram get_params deep fit model using training data get parameters estimator continued next page 1.8. reference scikitlearn user guide release 0.11 table 1.117 continued previous page predict score set_params params predict using linear model returns coefcient determination prediction set parameters estimator 
2616: __init__ copy_xtrue copy_gramtrue t_intercepttrue normalizetrue precompute_gramfalse copy_xytrue n_nonzero_coefsnone tolnone decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2617: fit gramnone xynone fit model using training data 
2618: parameters arraylike shape n_samples n_features training data 
2619: arraylike shape n_samples n_samples n_targets target values 
2620: gram arraylike shape n_features n_features optional gram matrix input data x.t arraylike shape n_features n_features n_targets optional input targets multiplied x.t returns self object returns instance self 
2621: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2622: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2623: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
2624: parameters arraylike shape n_samples n_features chapter user guide scikitlearn user guide release 0.11 training set 
2625: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.linear_model.perceptron class sklearn.linear_model.perceptron penaltynone t_intercepttrue n_iter5 shufefalse verbose0 eta01.0 n_jobs1 seed0 class_weightnone warm_startfalse alpha0.0001 perceptron parameters penalty none elasticnet penalty aka regularization term used defaults none 
2626: alpha oat constant multiplies regularization term regularization used defaults 0.0001 t_intercept bool whether intercept estimated false data assumed already centered defaults true 
2627: n_iter int optional number passes training data aka epochs defaults 
2628: shufe bool optional whether training data shufed epoch defaults false 
2629: seed int optional seed pseudo random number generator use shufing data 
2630: verbose integer optional verbosity level n_jobs integer optional number cpus use ova one versus multiclass problems computation means cpus defaults 
2631: eta0 double constant updates multiplied defaults 
2632: class_weight dict class_label 1.8. reference scikitlearn user guide release 0.11 preset class_weight parameter weights associated classes given classes supposed weight one auto mode uses values automatically adjust weights inversely propor tional class frequencies 
2633: warm_start bool optional set true reuse solution previous call initialization otherwise erase previous solution 
2634: see also sgdclassifier notes perceptron sgdclassier share underlying implementation fact perceptron equivalent sgdclassier lossperceptron eta01 learning_rateconstant penaltynone 
2635: references http en.wikipedia.orgwikiperceptron references therein 
2636: attributes coef_ n_features intercept_ methods array shape n_features n_classes else n_classes array shape n_classes else n_classes weights assigned features constants decision function 
2637: decision_function fit coef_init intercept_init ... fit_transform get_params deep partial_fit classes class_weight ... predict predict_proba score set_params params transform threshold predict signed distance hyperplane aka condence score fit linear model stochastic gradient descent fit data transform get parameters estimator fit linear model stochastic gradient descent predict using linear model predict class membership probability returns mean accuracy given test data labels set parameters estimator reduce important features 
2638: __init__ penaltynone alpha0.0001 t_intercepttrue n_iter5 shufefalse verbose0 eta01.0 n_jobs1 seed0 class_weightnone warm_startfalse classes deprecated removed v0.12 use classes_ instead 
2639: decision_function predict signed distance hyperplane aka condence score chapter user guide scikitlearn user guide release 0.11 parameters arraylike sparse matrix shape n_samples n_features returns array shape n_samples n_classes else n_samples n_classes signed distances hyperplane 
2640: fit coef_initnone intercept_initnone class_weightnone sample_weightnone fit linear model stochastic gradient descent 
2641: parameters arraylike sparse matrix shape n_samples n_features training data numpy array shape n_samples target values coef_init array shape n_classes n_features initial coefents warmstart optimization 
2642: intercept_init array shape n_classes initial intercept warmstart optimization 
2643: sample_weight arraylike shape n_samples optional weights applied individual samples provided uniform weights assumed 
2644: returns self returns instance self 
2645: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2646: parameters numpy array shape n_samples n_features training set 
2647: numpy array shape n_samples target values 
2648: returns x_new numpy array shape n_samples n_features_new transformed array 
2649: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
2650: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2651: partial_fit classesnone class_weightnone sample_weightnone fit linear model stochastic gradient descent 
2652: parameters arraylike sparse matrix shape n_samples n_features subset training data 1.8. reference scikitlearn user guide release 0.11 numpy array shape n_samples subset target values classes array shape n_classes classes across calls partial_t obtained via np.unique y_all y_all target vector entire dataset argument required rst call partial_t omitted subsequent calls note doesnt need contain labels classes 
2653: sample_weight arraylike shape n_samples optional weights applied individual samples provided uniform weights assumed 
2654: returns self returns instance self 
2655: predict predict using linear model parameters arraylike sparse matrix shape n_samples n_features returns array shape n_samples array containing predicted class labels 
2656: predict_proba predict class membership probability parameters arraylike sparse matrix shape n_samples n_features returns array shape n_samples n_classes else n_samples n_classes contains membership probabilities positive class 
2657: score returns mean accuracy given test data labels 
2658: parameters arraylike shape n_samples n_features training set 
2659: arraylike shape n_samples labels 
2660: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform thresholdnone reduce important features 
2661: parameters array scipy sparse matrix shape n_samples n_features input samples 
2662: threshold string oat none optional defaultnone chapter user guide scikitlearn user guide release 0.11 threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor e.g. 1.25mean may also used none available object attribute threshold used otherwise mean used default 
2663: returns x_r array shape n_samples n_selected_features input samples selected features 
2664: sklearn.linear_model.sgdclassier class sklearn.linear_model.sgdclassifier losshinge penaltyl2 t_intercepttrue alpha0.0001 shuf learn power_t0.5 rho0.85 n_iter5 efalse verbose0 n_jobs1 seed0 ing_rateoptimal class_weightnone warm_startfalse eta00.0 linear model tted minimizing regularized empirical loss sgd sgd stands stochastic gradient descent gradient loss estimated sample time model updated along way decreasing strength schedule aka learning rate regularizer penalty added loss function shrinks model parameters towards zero vector using either squared euclidean norm absolute norm combination elastic net parameter update crosses 0.0 value regularizer update truncated 0.0 allow learning sparse models achieve online feature selection implementation works data represented dense numpy arrays oating point values features 
2665: parameters loss str hinge log modied_huber loss function used defaults hinge hinge loss margin loss used standard linear svm models log loss loss logistic regression models used probability estimation binary classiers modied_huber another smooth loss brings tolerance outliers 
2666: penalty str elasticnet penalty aka regularization term used defaults standard regularizer linear svm models elasticnet migh bring sparsity model feature selection achievable 
2667: alpha oat constant multiplies regularization term defaults 0.0001 rho oat elastic net mixing parameter rho defaults 0.85 
2668: t_intercept bool whether intercept estimated false data assumed already centered defaults true 
2669: n_iter int optional number passes training data aka epochs defaults 
2670: shufe bool optional whether training data shufed epoch defaults false 
2671: 1.8. reference scikitlearn user guide release 0.11 seed int optional seed pseudo random number generator use shufing data 
2672: verbose integer optional verbosity level n_jobs integer optional number cpus use ova one versus multiclass problems computation means cpus defaults 
2673: learning_rate string optional learning rate constant eta eta0 optimal eta 1.0 tt0 default invscaling eta eta0 pow power_t eta0 double initial learning rate default 0.01 
2674: power_t double exponent inverse scaling learning rate default 0.25 
2675: class_weight dict class_label preset class_weight parameter weights associated classes given classes supposed weight one auto mode uses values automatically adjust weights inversely propor tional class frequencies 
2676: warm_start bool optional set true reuse solution previous call initialization otherwise erase previous solution 
2677: see also linearsvc logisticregression perceptron examples import numpy sklearn import linear_model np.array np.array clf linear_model.sgdclassifier clf.fit ... sgdclassifier alpha0.0001 class_weightnone eta00.0 fit_intercepttrue learning_rateoptimal losshinge n_iter5 n_jobs1 penaltyl2 power_t0.5 rho0.85 seed0 shufflefalse verbose0 warm_startfalse print clf.predict 0.8 chapter user guide scikitlearn user guide release 0.11 array shape n_features n_classes else n_classes array shape n_classes else n_classes weights assigned features constants decision function 
2678: attributes coef_ n_features intercept_ methods decision_function fit coef_init intercept_init ... fit_transform get_params deep partial_fit classes class_weight ... predict predict_proba score set_params params transform threshold predict signed distance hyperplane aka condence score fit linear model stochastic gradient descent fit data transform get parameters estimator fit linear model stochastic gradient descent predict using linear model predict class membership probability returns mean accuracy given test data labels set parameters estimator reduce important features 
2679: __init__ losshinge penaltyl2 alpha0.0001 rho0.85 t_intercepttrue n_iter5 shuf efalse verbose0 n_jobs1 seed0 learning_rateoptimal eta00.0 power_t0.5 class_weightnone warm_startfalse classes deprecated removed v0.12 use classes_ instead 
2680: decision_function predict signed distance hyperplane aka condence score parameters arraylike sparse matrix shape n_samples n_features returns array shape n_samples n_classes else n_samples n_classes signed distances hyperplane 
2681: fit coef_initnone intercept_initnone class_weightnone sample_weightnone fit linear model stochastic gradient descent 
2682: parameters arraylike sparse matrix shape n_samples n_features training data numpy array shape n_samples target values coef_init array shape n_classes n_features initial coefents warmstart optimization 
2683: intercept_init array shape n_classes initial intercept warmstart optimization 
2684: sample_weight arraylike shape n_samples optional weights applied individual samples provided uniform weights assumed 
2685: returns self returns instance self 
2686: 1.8. reference scikitlearn user guide release 0.11 fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2687: parameters numpy array shape n_samples n_features training set 
2688: numpy array shape n_samples target values 
2689: returns x_new numpy array shape n_samples n_features_new transformed array 
2690: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
2691: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2692: partial_fit classesnone class_weightnone sample_weightnone fit linear model stochastic gradient descent 
2693: parameters arraylike sparse matrix shape n_samples n_features subset training data numpy array shape n_samples subset target values classes array shape n_classes classes across calls partial_t obtained via np.unique y_all y_all target vector entire dataset argument required rst call partial_t omitted subsequent calls note doesnt need contain labels classes 
2694: sample_weight arraylike shape n_samples optional weights applied individual samples provided uniform weights assumed 
2695: returns self returns instance self 
2696: predict predict using linear model parameters arraylike sparse matrix shape n_samples n_features returns array shape n_samples array containing predicted class labels 
2697: predict_proba predict class membership probability chapter user guide scikitlearn user guide release 0.11 parameters arraylike sparse matrix shape n_samples n_features returns array shape n_samples n_classes else n_samples n_classes contains membership probabilities positive class 
2698: score returns mean accuracy given test data labels 
2699: parameters arraylike shape n_samples n_features training set 
2700: arraylike shape n_samples labels 
2701: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform thresholdnone reduce important features 
2702: parameters array scipy sparse matrix shape n_samples n_features input samples 
2703: threshold string oat none optional defaultnone threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor e.g. 1.25mean may also used none available object attribute threshold used otherwise mean used default 
2704: returns x_r array shape n_samples n_selected_features input samples selected features 
2705: sklearn.linear_model.sgdregressor class sklearn.linear_model.sgdregressor losssquared_loss penaltyl2 t_intercepttrue verbose0 p0.1 rho0.85 efalse ing_rateinvscaling warm_startfalse n_iter5 seed0 alpha0.0001 shuf learn power_t0.25 eta00.01 linear model tted minimizing regularized empirical loss sgd sgd stands stochastic gradient descent gradient loss estimated sample time model updated along way decreasing strength schedule aka learning rate regularizer penalty added loss function shrinks model parameters towards zero vector using either squared euclidean norm absolute norm combination elastic net 1.8. reference scikitlearn user guide release 0.11 parameter update crosses 0.0 value regularizer update truncated 0.0 allow learning sparse models achieve online feature selection implementation works data represented dense numpy arrays oating point values features 
2706: parameters loss str squared_loss huber loss function used defaults squared_loss refers ordinary least squares huber epsilon insensitive loss function robust regression 
2707: penalty str elasticnet penalty aka regularization term used defaults standard regularizer linear svm models elasticnet migh bring sparsity model feature selection achievable 
2708: alpha oat constant multiplies regularization term defaults 0.0001 rho oat elastic net mixing parameter rho defaults 0.85 
2709: t_intercept bool whether intercept estimated false data assumed already centered defaults true 
2710: n_iter int optional number passes training data aka epochs defaults 
2711: shufe bool optional whether training data shufed epoch defaults false 
2712: seed int optional seed pseudo random number generator use shufing data 
2713: verbose integer optional verbosity level 
2714: oat epsilon epsiloninsensitive huber loss function losshuber 
2715: learning_rate string optional learning rate constant eta eta0 optimal eta 1.0 tt0 invscaling eta eta0 pow power_t default eta0 double optional initial learning rate default 0.01 
2716: power_t double optional exponent inverse scaling learning rate default 0.25 
2717: warm_start bool optional set true reuse solution previous call initialization otherwise erase previous solution 
2718: chapter user guide scikitlearn user guide release 0.11 see also ridge elasticnet lasso svr examples import numpy sklearn import linear_model n_samples n_features np.random.seed np.random.randn n_samples np.random.randn n_samples n_features clf linear_model.sgdregressor clf.fit sgdregressor alpha0.0001 eta00.01 fit_intercepttrue learning_rateinvscaling losssquared_loss n_iter5 p0.1 penaltyl2 power_t0.25 rho0.85 seed0 shufflefalse verbose0 warm_startfalse attributes coef_ intercept_ array shape n_features weights asigned features array shape intercept term 
2719: methods decision_function fit coef_init intercept_init ... fit_transform get_params deep partial_fit sample_weight predict score set_params params transform threshold predict using linear model fit linear model stochastic gradient descent fit data transform get parameters estimator fit linear model stochastic gradient descent predict using linear model returns coefcient determination prediction set parameters estimator reduce important features 
2720: __init__ losssquared_loss penaltyl2 alpha0.0001 rho0.85 t_intercepttrue n_iter5 eta00.01 learning_rateinvscaling seed0 shufefalse power_t0.25 warm_startfalse verbose0 p0.1 decision_function predict using linear model parameters arraylike sparse matrix shape n_samples n_features returns array shape n_samples predicted target values per element 
2721: fit coef_initnone intercept_initnone sample_weightnone fit linear model stochastic gradient descent 
2722: parameters arraylike sparse matrix shape n_samples n_features 1.8. reference scikitlearn user guide release 0.11 training data numpy array shape n_samples target values coef_init array shape n_features initial coefents warmstart optimization 
2723: intercept_init array shape initial intercept warmstart optimization 
2724: sample_weight arraylike shape n_samples optional weights applied individual samples unweighted 
2725: returns self returns instance self 
2726: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2727: parameters numpy array shape n_samples n_features training set 
2728: numpy array shape n_samples target values 
2729: returns x_new numpy array shape n_samples n_features_new transformed array 
2730: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
2731: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2732: partial_fit sample_weightnone fit linear model stochastic gradient descent 
2733: parameters arraylike sparse matrix shape n_samples n_features subset training data numpy array shape n_samples subset target values sample_weight arraylike shape n_samples optional weights applied individual samples provided uniform weights assumed 
2734: returns self returns instance self 
2735: chapter user guide scikitlearn user guide release 0.11 predict predict using linear model parameters arraylike sparse matrix shape n_samples n_features returns array shape n_samples predicted target values per element 
2736: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
2737: parameters arraylike shape n_samples n_features training set 
2738: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform thresholdnone reduce important features 
2739: parameters array scipy sparse matrix shape n_samples n_features input samples 
2740: threshold string oat none optional defaultnone threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor e.g. 1.25mean may also used none available object attribute threshold used otherwise mean used default 
2741: returns x_r array shape n_samples n_selected_features input samples selected features 
2742: sklearn.linear_model.bayesianridge class sklearn.linear_model.bayesianridge n_iter300 alpha_21e06 normalizefalse copy_xtrue verbosefalse compute_scorefalse tol0.001 lambda_11e06 alpha_11e06 lambda_21e t_intercepttrue bayesian ridge regression fit bayesian ridge model optimize regularization parameters lambda precision weights alpha precision noise 
2743: parameters array shape n_samples n_features 1.8. reference scikitlearn user guide release 0.11 training vectors 
2744: array shape length target values training vectors n_iter int optional maximum number iterations default 
2745: tol oat optional stop algorithm converged default 1.e3 
2746: alpha_1 oat optional hyperparameter shape parameter gamma distribution prior alpha parameter default 1.e6 alpha_2 oat optional hyperparameter inverse scale parameter rate parameter gamma distribution prior alpha parameter default 1.e6 
2747: lambda_1 oat optional hyperparameter shape parameter gamma distribution prior lambda parameter default 1.e6 
2748: lambda_2 oat optional hyperparameter inverse scale parameter rate parameter gamma distribution prior lambda parameter default 1.e6 compute_score boolean optional true compute objective function step model default false t_intercept boolean optional wether calculate intercept model set false intercept used calculations e.g data expected already centered default true 
2749: normalize boolean optional default false true regressors normalized copy_x boolean optional default true true copied else may overwritten 
2750: verbose boolean optional default false verbose mode tting model 
2751: notes see exampleslinear_modelplot_bayesian_ridge.py example 
2752: examples chapter user guide scikitlearn user guide release 0.11 sklearn import linear_model clf linear_model.bayesianridge clf.fit ... bayesianridge alpha_11e06 alpha_21e06 compute_scorefalse copy_xtrue fit_intercepttrue lambda_11e06 lambda_21e06 n_iter300 normalizefalse tol0.001 verbosefalse clf.predict array attributes coef_ alpha_ lambda_ scores_ array shape n_features coefcients regression model mean distribution oat array shape n_features oat estimated precision noise estimated precisions weights computed value objective function maximized methods decision_function decision function linear model fit get_params deep predict score set_params params fit model get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
2753: __init__ n_iter300 tol0.001 alpha_11e06 alpha_21e06 lambda_11e06 lambda_21e ver compute_scorefalse t_intercepttrue normalizefalse bosefalse copy_xtrue decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2754: fit fit model parameters numpy array shape n_samples n_features training data numpy array shape n_samples target values returns self returns instance self 
2755: get_params deeptrue get parameters estimator parameters deep boolean optional 1.8. reference scikitlearn user guide release 0.11 true return parameters estimator contained subobjects estimators 
2756: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2757: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
2758: parameters arraylike shape n_samples n_features training set 
2759: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.linear_model.ardregression class sklearn.linear_model.ardregression n_iter300 tol0.001 alpha_21e06 alpha_11e lambda_11e06 lambda_21e06 thresh old_lambda10000.0 t_intercepttrue normal izefalse copy_xtrue verbosefalse compute_scorefalse bayesian ard regression fit weights regression model using ard prior weights regression model assumed gaussian distributions also estimate parameters lambda precisions distributions weights alpha precision distribution noise estimation done iterative procedures evidence maximization parameters array shape n_samples n_features training vectors 
2760: array shape n_samples target values training vectors n_iter int optional maximum number iterations default tol oat optional chapter user guide scikitlearn user guide release 0.11 stop algorithm converged default 1.e3 
2761: alpha_1 oat optional hyperparameter shape parameter gamma distribution prior alpha parameter default 1.e6 
2762: alpha_2 oat optional hyperparameter inverse scale parameter rate parameter gamma distribution prior alpha parameter default 1.e6 
2763: lambda_1 oat optional hyperparameter shape parameter gamma distribution prior lambda parameter default 1.e6 
2764: lambda_2 oat optional hyperparameter inverse scale parameter rate parameter gamma distribution prior lambda parameter default 1.e6 
2765: compute_score boolean optional true compute objective function step model default false 
2766: threshold_lambda oat optional threshold removing pruning weights high precision computation default 1.e4 
2767: t_intercept boolean optional wether calculate intercept model set false intercept used calculations e.g data expected already centered default true 
2768: normalize boolean optional true regressors normalized copy_x boolean optional default true 
2769: true copied else may overwritten 
2770: verbose boolean optional default false verbose mode tting model 
2771: notes see exampleslinear_modelplot_ard.py example 
2772: examples sklearn import linear_model clf linear_model.ardregression clf.fit ... ardregression alpha_11e06 alpha_21e06 compute_scorefalse copy_xtrue fit_intercepttrue lambda_11e06 lambda_21e06 n_iter300 normalizefalse threshold_lambda10000.0 tol0.001 verbosefalse 1.8. reference scikitlearn user guide release 0.11 clf.predict array attributes coef_ alpha_ lambda_ sigma_ scores_ array shape n_features oat array shape n_features array shape n_features n_features oat coefcients regression model mean distribution estimated precision noise estimated precisions weights estimated variancecovariance matrix weights computed value objective function maximized methods decision_function decision function linear model fit get_params deep predict score set_params params fit ardregression model according given training data get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
2773: __init__ n_iter300 tol0.001 alpha_11e06 alpha_21e06 lambda_11e06 lambda_21e normal threshold_lambda10000.0 t_intercepttrue compute_scorefalse izefalse copy_xtrue verbosefalse decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2774: fit fit ardregression model according given training data parameters iterative procedure maximize evidence parameters arraylike shape n_samples n_features training vector n_samples number samples n_features num ber features 
2775: array shape n_samples target values integers returns self returns instance self 
2776: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2777: chapter user guide scikitlearn user guide release 0.11 predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2778: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
2779: parameters arraylike shape n_samples n_features training set 
2780: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.linear_model.randomizedlasso class sklearn.linear_model.randomizedlasso alphaaic scaling0.5 sample_fraction0.75 selection_threshold0.25 verbosefalse precomputeauto eps2.2204460492503131e n_jobs1 mem n_resampling200 t_intercepttrue normalizetrue max_iter500 pre_dispatch3n_jobs orymemory cachedirnone random_statenone randomized lasso randomized lasso works resampling train data computing lasso resampling short features selected often good features also known stability selection 
2781: parameters alpha oat aic bic regularization parameter alpha parameter lasso warning alpha parameter stability selection article scaling 
2782: scaling oat alpha parameter stability selection article used randomly scale features 
2783: sample_fraction oat fraction samples used randomized design samples used 
2784: 1.8. reference scikitlearn user guide release 0.11 t_intercept boolean whether calculate intercept model set false intercept used calculations e.g data expected already centered 
2785: verbose boolean integer optional sets verbosity amount normalize boolean optional true regressors normalized precompute true false auto whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
2786: max_iter integer optional maximum number iterations perform lars algorithm 
2787: eps oat optional machineprecision regularization computation cholesky diagonal fac tors increase illconditioned systems unlike tol parameter iterative optimizationbased algorithms parameter control tolerance optimization 
2788: n_jobs integer optional number cpus use resampling use cpus random_state int randomstate instance none optional defaultnone int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used np.random 
2789: pre_dispatch int string optional controls number jobs get dispatched parallel execution reducing number useful avoid explosion memory consumption jobs get dispatched cpus process parameter none case jobs immediatly created spawned use lightweight fastrunning jobs avoid delays due ondemand spawning jobs int giving exact number total jobs spawned string giving expression function n_jobs 2n_jobs memory instance joblib.memory string used internal caching default caching done string given thepath caching directory 
2790: see also randomizedlogisticregression logisticregression notes see exampleslinear_modelplot_sparse_recovery.py example 
2791: chapter user guide scikitlearn user guide release 0.11 references stability selection nicolai meinshausen peter buhlmann journal royal statistical society series volume issue pages september doi 10.1111j.14679868.2010.00740.x examples sklearn.linear_model import randomizedlasso randomized_lasso randomizedlasso attributes scores_ array shape n_features all_scores_array shape n_features n_reg_parameter methods feature scores 
2792: feature scores values regularization parameter reference article suggests scores_ max all_scores_ 
2793: fit fit_transform get_params deep get_support indices inverse_transform transform new matrix using selected features set_params params transform fit model using training data fit data transform get parameters estimator return mask list featuresindices selected 
2794: set parameters estimator transform new matrix using selected features __init__ alphaaic scaling0.5 sample_fraction0.75 n_resampling200 normalizetrue selec precom random_statenone tion_threshold0.25 puteauto n_jobs1 pre_dispatch3n_jobs memorymemory cachedirnone eps2.2204460492503131e16 t_intercepttrue verbosefalse max_iter500 fit fit model using training data 
2795: parameters arraylike shape n_samples n_features training data 
2796: arraylike shape n_samples target values returns self object returns instance self 
2797: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2798: parameters numpy array shape n_samples n_features 1.8. reference scikitlearn user guide release 0.11 training set 
2799: numpy array shape n_samples target values 
2800: returns x_new numpy array shape n_samples n_features_new transformed array 
2801: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
2802: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2803: get_support indicesfalse return mask list featuresindices selected 
2804: inverse_transform transform new matrix using selected features set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform transform new matrix using selected features sklearn.linear_model.randomizedlogisticregression class sklearn.linear_model.randomizedlogisticregression scaling0.5 sample_fraction0.75 n_resampling200 lection_threshold0.25 tol0.001 t_intercepttrue verbosefalse malizetrue ran dom_statenone n_jobs1 pre_dispatch3n_jobs mem orymemory cachedirnone randomized logistic regression randomized regression works resampling train data computing logisticregression sampling short features selected often good features also known stability selection 
2805: parameters oat chapter user guide scikitlearn user guide release 0.11 regularization parameter logisticregression 
2806: scaling oat alpha parameter stability selection article used randomly scale features 
2807: sample_fraction oat fraction samples used randomized design samples used 
2808: t_intercept boolean whether calculate intercept model set false intercept used calculations e.g data expected already centered 
2809: verbose boolean integer optional sets verbosity amount normalize boolean optional true regressors normalized tol oat optional tolerance stopping criteria logisticregression n_jobs integer optional number cpus use resampling use cpus random_state int randomstate instance none optional defaultnone int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used np.random 
2810: pre_dispatch int string optional controls number jobs get dispatched parallel execution reducing number useful avoid explosion memory consumption jobs get dispatched cpus process parameter none case jobs immediatly created spawned use lightweight fastrunning jobs avoid delays due ondemand spawning jobs int giving exact number total jobs spawned string giving expression function n_jobs 2n_jobs memory instance joblib.memory string used internal caching default caching done string given thepath caching directory 
2811: see also randomizedlasso lasso elasticnet notes see exampleslinear_modelplot_randomized_lasso.py example 
2812: 1.8. reference scikitlearn user guide release 0.11 references stability selection nicolai meinshausen peter buhlmann journal royal statistical society series volume issue pages september doi 10.1111j.14679868.2010.00740.x examples sklearn.linear_model import randomizedlogisticregression randomized_logistic randomizedlogisticregression attributes scores_ array shape n_features all_scores_array shape n_features n_reg_parameter methods feature scores 
2813: feature scores values regularization parameter reference article suggests scores_ max all_scores_ 
2814: fit fit_transform get_params deep get_support indices inverse_transform transform new matrix using selected features set_params params transform fit model using training data fit data transform get parameters estimator return mask list featuresindices selected 
2815: set parameters estimator transform new matrix using selected features __init__ scaling0.5 sample_fraction0.75 n_resampling200 selection_threshold0.25 random_statenone tol0.001 t_intercepttrue n_jobs1 pre_dispatch3n_jobs memorymemory cachedirnone verbosefalse normalizetrue fit fit model using training data 
2816: parameters arraylike shape n_samples n_features training data 
2817: arraylike shape n_samples target values returns self object returns instance self 
2818: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2819: parameters numpy array shape n_samples n_features chapter user guide scikitlearn user guide release 0.11 training set 
2820: numpy array shape n_samples target values 
2821: returns x_new numpy array shape n_samples n_features_new transformed array 
2822: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
2823: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2824: get_support indicesfalse return mask list featuresindices selected 
2825: inverse_transform transform new matrix using selected features set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform transform new matrix using selected features linear_model.lasso_path eps ... linear_model.lars_path gram ... linear_model.orthogonal_mp ... linear_model.orthogonal_mp_gram gram ... gram orthogonal matching pursuit omp linear_model.lasso_stability_path ... compute lasso path coordinate descent compute least angle regression lasso path orthogonal matching pursuit omp stabiliy path based randomized lasso estimates sklearn.linear_model.lasso_path sklearn.linear_model.lasso_path eps0.001 n_alphas100 alphasnone precom puteauto xynone t_intercepttrue normalizefalse copy_xtrue verbosefalse params compute lasso path coordinate descent optimization objective lasso n_samples xw2_2 alpha w_1 parameters numpy array shape n_samples n_features 1.8. reference scikitlearn user guide release 0.11 training data pass directly fortran contiguous data avoid unnecessary memory duplication numpy array shape n_samples target values eps oat optional length path eps1e3 means alpha_min alpha_max 1e3 n_alphas int optional number alphas along regularization path alphas numpy array optional list alphas compute models none alphas set automatically precompute true false auto arraylike whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
2826: arraylike optional np.dot x.t precomputed useful gram matrix precomputed t_intercept bool fit intercept normalize boolean optional true regressors normalized copy_x boolean optional default true true copied else may overwritten 
2827: verbose bool integer amount verbosity params kwargs keyword arguments passed lasso objects returns models list models along regularization path see also lars_path lasso lassolars lassocv lassolarscv sklearn.decomposition.sparse_encode notes see exampleslinear_modelplot_lasso_coordinate_descent_path.py example avoid unnecessary memory duplication argument method directly passed fortran contiguous numpy array 
2828: chapter user guide scikitlearn user guide release 0.11 sklearn.linear_model.lars_path sklearn.linear_model.lars_path xynone gramnone max_iter500 alpha_min0 methodlar copy_xtrue eps2.2204460492503131e16 copy_gramtrue verbosefalse compute least angle regression lasso path optimization objective lasso n_samples xw2_2 alpha w_1 parameters array shape n_samples n_features input data array shape n_samples input targets max_iter integer optional maximum number iterations perform set innity limit gram none auto array shape n_features n_features optional precomputed gram matrix auto gram matrix precomputed given samples features alpha_min oat optional minimum correlation along path alpha parameter lasso 
2829: method lar lasso corresponds regularization parameter species returned model select lar least angle regression lasso lasso 
2830: eps oat optional machineprecision regularization computation cholesky diagonal fac tors increase illconditioned systems 
2831: copy_x bool false overwritten 
2832: copy_gram bool false gram overwritten 
2833: returns alphas array shape max_features maximum covariances absolute value iteration 
2834: active array shape max_features indices active variables end path 
2835: coefs array shape n_features max_features coefcients along path see also lasso_path lassolars lars lassolarscv larscv sklearn.decomposition.sparse_encode 1.8. reference scikitlearn user guide release 0.11 notes http en.wikipedia.orgwikileastangle_regression http en.wikipedia.orgwikilasso_ statistics lasso_method sklearn.linear_model.orthogonal_mp sklearn.linear_model.orthogonal_mp tolnone precom n_nonzero_coefsnone pute_gramfalse copy_xtrue orthogonal matching pursuit omp solves n_targets orthogonal matching pursuit problems instance problem form parametrized number nonzero coefcients using n_nonzero_coefs argmin xgamma2 subject gamma_0 nonzero coefs parametrized error using parameter tol argmin gamma_0 subject xgamma2 tol parameters array shape n_samples n_features input data columns assumed unit norm 
2836: array shape n_samples n_samples n_targets input targets n_nonzero_coefs int desired number nonzero entries solution none default value set n_features 
2837: tol oat maximum norm residual none overrides n_nonzero_coefs 
2838: precompute_gram true false auto whether perform precomputations n_samples large 
2839: copy_x bool optional improves performance n_targets whether design matrix must copied algorithm false value helpful already fortranordered otherwise copy made anyway 
2840: returns coef array shape n_features n_features n_targets coefcients omp solution see also orthogonalmatchingpursuit decomposition.sparse_encode decomposition.sparse_encode_parallel orthogonal_mp_gram lars_path notes orthogonal matching pursuit introduced mallat zhang matching pursuits timefrequency dictionaries ieee transactions signal processing vol december http blanche.polytechnique.frmallatpapiersmallatpursuit93.pdf chapter user guide scikitlearn user guide release 0.11 implementation based rubinstein zibulevsky elad efcient implementation ksvd algorithm using batch orthogonal matching pursuit technical report technion april 2008. http www.cs.technion.ac.ilronrubinpublicationsksvdompv2.pdf sklearn.linear_model.orthogonal_mp_gram sklearn.linear_model.orthogonal_mp_gram gram n_nonzero_coefsnone norms_squarednone copy_xytrue tolnone copy_gramtrue gram orthogonal matching pursuit omp solves n_targets orthogonal matching pursuit problems using gram matrix x.t product x.t 
2841: parameters gram array shape n_features n_features gram matrix input data x.t array shape n_features n_features n_targets input targets multiplied x.t n_nonzero_coefs int desired number nonzero entries solution none default value set n_features 
2842: tol oat maximum norm residual none overrides n_nonzero_coefs 
2843: norms_squared arraylike shape n_targets squared norms lines required tol none 
2844: copy_gram bool optional whether gram matrix must copied algorithm false value helpful already fortranordered otherwise copy made anyway 
2845: copy_xy bool optional whether covariance vector must copied algorithm false may overwritten 
2846: returns coef array shape n_features n_features n_targets coefcients omp solution see also orthogonalmatchingpursuit orthogonal_mp lars_path decomposition.sparse_encode decomposition.sparse_encode_parallel notes orthogonal matching pursuit introduced mallat zhang matching pursuits timefrequency dictionaries ieee transactions signal processing vol december http blanche.polytechnique.frmallatpapiersmallatpursuit93.pdf 1.8. reference scikitlearn user guide release 0.11 implementation based rubinstein zibulevsky elad efcient implementation ksvd algorithm using batch orthogonal matching pursuit technical report technion april 2008. http www.cs.technion.ac.ilronrubinpublicationsksvdompv2.pdf sklearn.linear_model.lasso_stability_path sklearn.linear_model.lasso_stability_path dom_statenone n_grid100 eps8.8817841970012523e16 verbosefalse scaling0.5 ran n_resampling200 sample_fraction0.75 n_jobs1 stabiliy path based randomized lasso estimates parameters arraylike shape n_samples n_features training data 
2847: arraylike shape n_samples target values 
2848: scaling oat alpha parameter stability selection article used randomly scale features 
2849: random_state integer numpy.randomstate optional generator used randomize design 
2850: n_resampling int number randomized models 
2851: n_grid int number grid points path linearly reinterpolated grid computing scores 
2852: sample_fraction oat fraction samples used randomized design samples used 
2853: eps oat smallest value alpha alpha_max considered n_jobs integer optional number cpus use resampling use cpus verbose boolean integer optional sets verbosity amount returns alphas_grid array shape n_grid grid points alphaalpha_max scores_path array shape n_features n_grid scores feature along path 
2854: chapter user guide scikitlearn user guide release 0.11 notes see exampleslinear_modelplot_randomized_lasso.py example 
2855: sparse data sklearn.linear_model.sparse submodule sparse counterpart sklearn.linear_model module user guide see generalized linear models section details 
2856: linear_model.sparse.lasso alpha ... linear_model.sparse.elasticnet alpha rho ... linear_model.sparse.sgdclassifier args ... linear_model.sparse.sgdregressor args kwargs linear_model.logisticregression penalty ... linear model trained prior regularizer linear model trained prior regularizer logistic regression aka logit maxent classier 
2857: sklearn.linear_model.sparse.lasso class sklearn.linear_model.sparse.lasso alpha1.0 t_interceptfalse normalizefalse linear model trained prior regularizer implementation works scipy.sparse dense coef_ technically elastic net penalty set zero 
2858: max_iter1000 tol0.0001 parameters alpha oat constant multiplies term defaults 1.0 coef_ ndarray shape n_features initial coefents warmstart optimization t_intercept bool whether intercept estimated false data assumed already centered 
2859: methods decision_function decision function linear model fit get_params deep predict score set_params params fit current model coordinate descent get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
2860: __init__ alpha1.0 t_interceptfalse normalizefalse max_iter1000 tol0.0001 decision_function decision function linear model parameters scipy.sparse matrix shape n_samples n_features 1.8. reference scikitlearn user guide release 0.11 returns array shape n_samples predicted real values fit fit current model coordinate descent expected sparse matrix maximum efciency use sparse matrix csc format scipy.sparse.csc_matrix get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2861: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2862: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
2863: parameters arraylike shape n_samples n_features training set 
2864: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.linear_model.sparse.elasticnet class sklearn.linear_model.sparse.elasticnet alpha1.0 rho0.5 t_interceptfalse linear model trained prior regularizer implementation works scipy.sparse dense coef_ rho1 lasso penalty currently rho 0.01 reliable unless supply sequence alpha 
2865: malizefalse max_iter1000 tol0.0001 parameters alpha oat constant multiplies term defaults 1.0 rho oat chapter user guide scikitlearn user guide release 0.11 elasticnet mixing parameter rho 
2866: t_intercept bool whether intercept estimated false data assumed already centered todo t_intercepttrue yet implemented notes parameter rho corresponds alpha glmnet package alpha corresponds lambda param eter glmnet 
2867: methods decision_function decision function linear model fit get_params deep predict score set_params params fit current model coordinate descent get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
2868: __init__ alpha1.0 rho0.5 t_interceptfalse normalizefalse max_iter1000 tol0.0001 decision_function decision function linear model parameters scipy.sparse matrix shape n_samples n_features returns array shape n_samples predicted real values fit fit current model coordinate descent expected sparse matrix maximum efciency use sparse matrix csc format scipy.sparse.csc_matrix get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2869: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2870: score returns coefcient determination prediction 
2871: 1.8. reference scikitlearn user guide release 0.11 coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
2872: parameters arraylike shape n_samples n_features training set 
2873: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.linear_model.sparse.sgdclassier class sklearn.linear_model.sparse.sgdclassifier args kwargs methods decision_function fit coef_init intercept_init ... fit_transform get_params deep partial_fit classes class_weight ... predict predict_proba score set_params params transform threshold predict signed distance hyperplane aka condence score fit linear model stochastic gradient descent fit data transform get parameters estimator fit linear model stochastic gradient descent predict using linear model predict class membership probability returns mean accuracy given test data labels set parameters estimator reduce important features 
2874: __init__ args kwargs deprecated removed v0.12 use sklearn.linear_model.sgdclassier directly classes deprecated removed v0.12 use classes_ instead 
2875: decision_function predict signed distance hyperplane aka condence score parameters arraylike sparse matrix shape n_samples n_features returns array shape n_samples n_classes else n_samples n_classes signed distances hyperplane 
2876: fit coef_initnone intercept_initnone class_weightnone sample_weightnone fit linear model stochastic gradient descent 
2877: parameters arraylike sparse matrix shape n_samples n_features chapter user guide scikitlearn user guide release 0.11 training data numpy array shape n_samples target values coef_init array shape n_classes n_features initial coefents warmstart optimization 
2878: intercept_init array shape n_classes initial intercept warmstart optimization 
2879: sample_weight arraylike shape n_samples optional weights applied individual samples provided uniform weights assumed 
2880: returns self returns instance self 
2881: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2882: parameters numpy array shape n_samples n_features training set 
2883: numpy array shape n_samples target values 
2884: returns x_new numpy array shape n_samples n_features_new transformed array 
2885: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
2886: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2887: partial_fit classesnone class_weightnone sample_weightnone fit linear model stochastic gradient descent 
2888: parameters arraylike sparse matrix shape n_samples n_features subset training data numpy array shape n_samples subset target values classes array shape n_classes classes across calls partial_t obtained via np.unique y_all y_all target vector entire dataset argument required rst call partial_t omitted subsequent calls note doesnt need contain labels classes 
2889: 1.8. reference scikitlearn user guide release 0.11 sample_weight arraylike shape n_samples optional weights applied individual samples provided uniform weights assumed 
2890: returns self returns instance self 
2891: predict predict using linear model parameters arraylike sparse matrix shape n_samples n_features returns array shape n_samples array containing predicted class labels 
2892: predict_proba predict class membership probability parameters arraylike sparse matrix shape n_samples n_features returns array shape n_samples n_classes else n_samples n_classes contains membership probabilities positive class 
2893: score returns mean accuracy given test data labels 
2894: parameters arraylike shape n_samples n_features training set 
2895: arraylike shape n_samples labels 
2896: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform thresholdnone reduce important features 
2897: parameters array scipy sparse matrix shape n_samples n_features input samples 
2898: threshold string oat none optional defaultnone threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor e.g. 1.25mean may also used none available object attribute threshold used otherwise mean used default 
2899: returns x_r array shape n_samples n_selected_features input samples selected features 
2900: chapter user guide scikitlearn user guide release 0.11 sklearn.linear_model.sparse.sgdregressor class sklearn.linear_model.sparse.sgdregressor args kwargs methods decision_function fit coef_init intercept_init ... fit_transform get_params deep partial_fit sample_weight predict score set_params params transform threshold predict using linear model fit linear model stochastic gradient descent fit data transform get parameters estimator fit linear model stochastic gradient descent predict using linear model returns coefcient determination prediction set parameters estimator reduce important features 
2901: __init__ args kwargs deprecated removed v0.12 use sklearn.linear_model.sgdregressor directly decision_function predict using linear model parameters arraylike sparse matrix shape n_samples n_features returns array shape n_samples predicted target values per element 
2902: fit coef_initnone intercept_initnone sample_weightnone fit linear model stochastic gradient descent 
2903: parameters arraylike sparse matrix shape n_samples n_features training data numpy array shape n_samples target values coef_init array shape n_features initial coefents warmstart optimization 
2904: intercept_init array shape initial intercept warmstart optimization 
2905: sample_weight arraylike shape n_samples optional weights applied individual samples unweighted 
2906: returns self returns instance self 
2907: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2908: parameters numpy array shape n_samples n_features training set 
2909: 1.8. reference scikitlearn user guide release 0.11 numpy array shape n_samples target values 
2910: returns x_new numpy array shape n_samples n_features_new transformed array 
2911: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
2912: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2913: partial_fit sample_weightnone fit linear model stochastic gradient descent 
2914: parameters arraylike sparse matrix shape n_samples n_features subset training data numpy array shape n_samples subset target values sample_weight arraylike shape n_samples optional weights applied individual samples provided uniform weights assumed 
2915: returns self returns instance self 
2916: predict predict using linear model parameters arraylike sparse matrix shape n_samples n_features returns array shape n_samples predicted target values per element 
2917: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
2918: parameters arraylike shape n_samples n_features training set 
2919: arraylike shape n_samples returns oat set_params params set parameters estimator 
2920: chapter user guide scikitlearn user guide release 0.11 method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform thresholdnone reduce important features 
2921: parameters array scipy sparse matrix shape n_samples n_features input samples 
2922: threshold string oat none optional defaultnone threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor e.g. 1.25mean may also used none available object attribute threshold used otherwise mean used default 
2923: returns x_r array shape n_samples n_selected_features input samples selected features 
2924: sklearn.linear_model.logisticregression class sklearn.linear_model.logisticregression penaltyl2 dualfalse c1.0 cept_scaling1 class_weightnone t_intercepttrue tol0.0001 inter logistic regression aka logit maxent classier multiclass case training algorithm uses onevs.all ova scheme rather true multinomial class implements regularized logistic regression using liblinear library handle dense sparse input use cordered arrays csr matrices containing 64bit oats optimal performance input format converted copied 
2925: parameters penalty string used specify norm used penalization dual boolean dual primal formulation dual formulation implemented penalty prefer dualfalse n_samples n_features 
2926: oat none optional defaultnone species strength regularization smaller bigger regular ization none set n_samples 
2927: t_intercept bool default true species constant a.k.a bias intercept added decision function intercept_scaling oat default self.t_intercept true instance vector becomes self.intercept_scaling i.e synthetic feature constant value equals intercept_scaling appended instance vector intercept becomes intercept_scaling synthetic feature weight note synthetic feature weight subject l1l2 regularization features 
2928: 1.8. reference scikitlearn user guide release 0.11 lessen effect regularization synthetic feature weight therefore intercept intercept_scaling increased tol oat optional tolerance stopping criteria see also linearsvc notes underlying implementation uses random number generator select features tting model thus uncommon slightly different results input data happens try smaller tol parameter references liblinear library large linear classicationhttp www.csie.ntu.edu.twcjlinliblinear hsiangfu fanglan huang chihjen lin dual coordinate descentmethods machine learning 
2929: regression gistic maximum entropy models http www.csie.ntu.edu.twcjlinpapersmaxent_dual.pdf attributes array shape n_classes1 n_features array shape n_classes1 coef_ ter cept_ methods coefcient features decision function coef_ readonly property derived raw_coef_ follows internal memory layout liblinear intercept a.k.a bias added decision function available parameter intercept set true decision_function decision function value according trained model fit class_weight fit_transform get_params deep predict predict_log_proba predict_proba score set_params params transform threshold reduce important features 
2930: fit model according given training data fit data transform get parameters estimator predict target values according tted model log probability estimates probability estimates returns mean accuracy given test data labels set parameters estimator 
2931: __init__ penaltyl2 dualfalse class_weightnone decision_function tol0.0001 c1.0 t_intercepttrue intercept_scaling1 decision function value according trained model 
2932: parameters arraylike shape n_samples n_features chapter user guide scikitlearn user guide release 0.11 returns arraylike shape n_samples n_class returns decision function sample class model 
2933: fit class_weightnone fit model according given training data 
2934: parameters arraylike sparse matrix shape n_samples n_features training vector n_samples number samples n_features num ber features 
2935: arraylike shape n_samples target vector relative class_weight dict auto optional weights associated classes given classes supposed weight one 
2936: returns self object returns self 
2937: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2938: parameters numpy array shape n_samples n_features training set 
2939: numpy array shape n_samples target values 
2940: returns x_new numpy array shape n_samples n_features_new transformed array 
2941: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
2942: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2943: predict predict target values according tted model 
2944: parameters arraylike sparse matrix shape n_samples n_features returns array shape n_samples predict_log_proba log probability estimates returned estimates classes ordered label classes 
2945: 1.8. reference scikitlearn user guide release 0.11 parameters arraylike shape n_samples n_features returns arraylike shape n_samples n_classes returns logprobabilities sample class model classes ordered arithmetical order 
2946: predict_proba probability estimates returned estimates classes ordered label classes 
2947: parameters arraylike shape n_samples n_features returns arraylike shape n_samples n_classes returns probability sample class model classes ordered arithmetical order 
2948: score returns mean accuracy given test data labels 
2949: parameters arraylike shape n_samples n_features training set 
2950: arraylike shape n_samples labels 
2951: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform thresholdnone reduce important features 
2952: parameters array scipy sparse matrix shape n_samples n_features input samples 
2953: threshold string oat none optional defaultnone threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor e.g. 1.25mean may also used none available object attribute threshold used otherwise mean used default 
2954: returns x_r array shape n_samples n_selected_features input samples selected features 
2955: 1.8.16 sklearn.manifold manifold learning sklearn.manifold module implements data embedding techniques user guide see manifold learning section details 
2956: chapter user guide scikitlearn user guide release 0.11 manifold.locallylinearembedding ... manifold.isomap n_neighbors n_components ... locally linear embedding isomap embedding sklearn.manifold.locallylinearembedding class sklearn.manifold.locallylinearembedding n_neighbors5 n_components2 tol1e methodstandard modied_tol1e ran neighbors_algorithmauto eigen_solverauto reg0.001 max_iter100 hessian_tol0.0001 dom_statenone out_dimnone locally linear embedding parameters n_neighbors integer number neighbors consider point 
2957: n_components integer number coordinates manifold reg oat regularization constant multiplies trace local covariance matrix dis tances 
2958: eigen_solver string auto arpack dense auto algorithm attempt choose best method input data arpack use arnoldi iteration shiftinvert mode method may dense matrix sparse matrix general linear operator 
2959: dense use standard dense matrix operations eigenvalue decomposition method must array matrix type method avoided large problems 
2960: tol oat optional tolerance arpack method used eigen_solverdense 
2961: max_iter integer maximum number eigen_solverdense 
2962: iterations arpack solver 
2963: used method string standard hessian modied standard use standard locally linear embedding algorithm see reference hessian use hessian eigenmap method method requires n_neighbors n_components n_components see reference modied use modied locally linear embedding algorithm see reference ltsa use local tangent space alignment algorithm see reference hessian_tol oat optional tolerance hessian eigenmapping method used method hessian modied_tol oat optional tolerance modied lle method used method modied 1.8. reference scikitlearn user guide release 0.11 neighbors_algorithm string autobrutekd_treeball_tree algorithm use nearest neighbors search passed neighbors.nearestneighbors instance random_state numpy.randomstate optional generator used initialize centers defaults numpy.random used deter mine starting vector arpack iterations references r63 r64 r65 r66 attributes embed ding_vectors_ reconstruc tion_error_ nbrs_ arraylike shape n_components n_samples oat nearestneighbors object stores embedding vectors reconstruction error associated embedding_vectors_ stores nearest neighbors instance including balltree kdtree applicable 
2964: methods compute embedding vectors data fit fit_transform compute embedding vectors data transform get_params deep set_params params transform get parameters estimator set parameters estimator transform new points embedding space 
2965: __init__ n_neighbors5 tol1e06 max_iter100 methodstandard hessian_tol0.0001 modied_tol1e12 neigh bors_algorithmauto random_statenone out_dimnone eigen_solverauto n_components2 reg0.001 fit ynone compute embedding vectors data parameters arraylike shape n_samples n_features training set 
2966: returns self returns instance self 
2967: fit_transform ynone compute embedding vectors data transform 
2968: parameters arraylike shape n_samples n_features training set 
2969: returns x_new arraylike shape n_samples n_components get_params deeptrue get parameters estimator chapter user guide scikitlearn user guide release 0.11 parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform transform new points embedding space 
2970: parameters arraylike shape n_samples n_features returns x_new array shape n_samples n_components notes scaling performed method discouraged use together methods scaleinvariant like svms sklearn.manifold.isomap class sklearn.manifold.isomap n_neighbors5 n_components2 tol0 max_iternone path_methodauto neighbors_algorithmauto out_dimnone eigen_solverauto isomap embedding nonlinear dimensionality reduction isometric mapping parameters n_neighbors integer number neighbors consider point 
2971: n_components integer number coordinates manifold eigen_solver autoarpackdense auto attempt choose efcient solver given problem arpack use arnoldi decomposition eigenvalues eigenvectors note arpack handle dense sparse data efciently dense use direct solver i.e lapack eigenvalue decomposition 
2972: tol oat convergence tolerance passed arpack lobpcg used eigen_solver dense max_iter integer maximum number iterations arpack solver used eigen_solver dense path_method string autofwd 1.8. reference scikitlearn user guide release 0.11 method use nding shortest path auto attempt choose best algorithm automatically floydwarshall algorithm dijkstra algorithm fibonacci heaps neighbors_algorithm string autobrutekd_treeball_tree algorithm use nearest neighbors search passed neighbors.nearestneighbors instance references tenenbaum j.b. silva langford j.c. global geometricframework nonlinear dimen sionality reduction science attributes embed ding_ ker nel_pca_ train ing_data_ nbrs_ arraylike shape n_samples n_components kernelpca object used implement embedding arraylike shape n_samples n_features sklearn.neighbors.nearestneighbors instance dist_matrix_ arraylike shape n_samples n_samples methods stores embedding vectors stores training data stores nearest neighbors instance including balltree kdtree applicable stores geodesic distance matrix training data fit fit_transform get_params deep reconstruction_error compute reconstruction error embedding set_params params transform compute embedding vectors data fit model data transform get parameters estimator set parameters estimator transform 
2973: __init__ n_neighbors5 tol0 max_iternone path_methodauto neighbors_algorithmauto out_dimnone n_components2 eigen_solverauto fit ynone compute embedding vectors data parameters arraylike sparse matrix balltree ckdtree nearestneighbors sample data shape n_samples n_features form numpy array sparse array precomputed tree nearestneighbors object 
2974: returns self returns instance self 
2975: fit_transform ynone fit model data transform 
2976: parameters arraylike sparse matrix balltree ckdtree chapter user guide scikitlearn user guide release 0.11 training vector n_samples number samples n_features num ber features 
2977: returns x_new arraylike shape n_samples n_components get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2978: reconstruction_error compute reconstruction error embedding 
2979: returns reconstruction_error oat notes cost function isomap embedding frobenius_norm d_fit n_samples matrix distances input data d_t matrix distances output embedding x_t isomap kernel 0.5 1n_samples 1n_samples set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform transform implemented linking points graph geodesic distances training data first n_neighbors nearest neighbors found training data shortest geodesic distances point point training data computed order construct kernel embedding projection kernel onto embedding vectors training set 
2980: parameters arraylike shape n_samples n_features returns x_new arraylike shape n_samples n_components manifold.locally_linear_embedding ... ... perform locally linear embedding analysis data 
2981: sklearn.manifold.locally_linear_embedding sklearn.manifold.locally_linear_embedding n_neighbors n_components reg0.001 eigen_solverauto tol1e06 max_iter100 methodstandard hessian_tol0.0001 modied_tol1e12 random_statenone out_dimnone perform locally linear embedding analysis data 
2982: 1.8. reference scikitlearn user guide release 0.11 parameters arraylike sparse matrix balltree ckdtree nearestneighbors sample data shape n_samples n_features form numpy array sparse array precomputed tree nearestneighbors object 
2983: n_neighbors integer number neighbors consider point 
2984: n_components integer number coordinates manifold 
2985: reg oat regularization constant multiplies trace local covariance matrix dis tances 
2986: eigen_solver string auto arpack dense auto algorithm attempt choose best method input data arpack use arnoldi iteration shiftinvert mode method may dense matrix sparse matrix general linear operator 
2987: dense use standard dense matrix operations eigenvalue decomposition method must array matrix type method avoided large problems 
2988: tol oat optional tolerance arpack method used eigen_solverdense 
2989: max_iter integer maximum number iterations arpack solver 
2990: method standard hessian modied ltsa standard use standard locally linear embedding algorithm see reference r67 hessian use hessian eigenmap method method requires n_neighbors n_components n_components see reference r68 modied use modied locally linear embedding algorithm see reference r69 ltsa use local tangent space alignment algorithm see reference r70 hessian_tol oat optional tolerance hessian eigenmapping method used method hessian modied_tol oat optional tolerance modied lle method used method modied random_state numpy.randomstate optional generator used initialize centers defaults numpy.random 
2991: returns arraylike shape n_samples n_components embedding vectors 
2992: squared_error oat reconstruction error embedding vectors equivalent norm fro reconstruction weights 
2993: chapter user guide scikitlearn user guide release 0.11 references r67 r68 r69 r70 1.8.17 sklearn.metrics metrics sklearn.metrics module includes score functions performance metrics pairwise metrics distance computations 
2994: classication metrics metrics.confusion_matrix y_true y_pred ... metrics.roc_curve y_true y_score metrics.auc metrics.precision_score y_true y_pred ... metrics.recall_score y_true y_pred ... metrics.fbeta_score y_true y_pred beta ... metrics.f1_score y_true y_pred labels ... metrics.precision_recall_fscore_support ... compute precisions recalls fmeasures support class metrics.classification_report y_true y_pred metrics.precision_recall_curve y_true ... metrics.zero_one_score y_true y_pred metrics.zero_one y_true y_pred metrics.hinge_loss y_true pred_decision ... compute confusion matrix evaluate accuracy classication compute receiver operating characteristic roc compute area curve auc using trapezoidal rule compute precision compute recall compute fbeta score compute score build text report showing main classication metrics compute precisionrecall pairs different probability thresholds zeroone classication score accuracy zeroone classication loss cumulated hinge loss nonregularized 
2995: sklearn.metrics.confusion_matrix sklearn.metrics.confusion_matrix y_true y_pred labelsnone compute confusion matrix evaluate accuracy classication denition confusion matrix equal number observations known group predicted group parameters y_true array shape n_samples true targets y_pred array shape n_samples estimated targets returns array shape n_classes n_classes confusion matrix references http en.wikipedia.orgwikiconfusion_matrix 1.8. reference scikitlearn user guide release 0.11 sklearn.metrics.roc_curve sklearn.metrics.roc_curve y_true y_score compute receiver operating characteristic roc note implementation restricted binary classication task 
2996: parameters y_true array shape n_samples true binary labels y_score array shape n_samples target scores either probability estimates positive class condence values binary decisions 
2997: returns fpr array shape false positive rates tpr array shape true positive rates thresholds array shape thresholds y_score used compute fpr tpr references http en.wikipedia.orgwikireceiver_operating_characteristic examples import numpy sklearn import metrics np.array scores np.array 0.1 0.4 0.35 0.8 fpr tpr thresholds metrics.roc_curve scores fpr array 0.5 0.5 sklearn.metrics.auc sklearn.metrics.auc compute area curve auc using trapezoidal rule parameters array shape coordinates array shape coordinates returns auc oat chapter user guide scikitlearn user guide release 0.11 examples import numpy sklearn import metrics np.array pred np.array 0.1 0.4 0.35 0.8 fpr tpr thresholds metrics.roc_curve pred metrics.auc fpr tpr 0.75 sklearn.metrics.precision_score sklearn.metrics.precision_score y_true labelsnone pos_label1 aver y_pred ageweighted compute precision precision ratio number true positives number false positives precision intuitively ability classier label positive sample negative best value worst value 
2998: parameters y_true array shape n_samples true targets y_pred array shape n_samples predicted targets labels array integer array labels pos_label int binary classication case give label positive class default erything else pos_label considered belong negative class set none case multiclass classication 
2999: average string none micro macro weighted default multiclass classication case determines type averaging performed data macro average classes take imbalance account micro average instances takes imbalance account implies precision recall weighted average weighted support takes imbalance account result score precision recall 
3000: returns precision oat precision positive class binary classication weighted average preci sion class multiclass task sklearn.metrics.recall_score sklearn.metrics.recall_score y_true y_pred labelsnone pos_label1 averageweighted compute recall 1.8. reference scikitlearn user guide release 0.11 recall ratio number true positives number false negatives recall intuitively ability classier positive samples best value worst value 
3001: parameters y_true array shape n_samples true targets y_pred array shape n_samples predicted targets labels array integer array labels pos_label int binary classication case give label positive class default erything else pos_label considered belong negative class set none case multiclass classication 
3002: average string none micro macro weighted default multiclass classication case determines type averaging performed data macro average classes take imbalance account micro average instances takes imbalance account implies precision recall weighted average weighted support takes imbalance account result score precision recall 
3003: returns recall oat recall positive class binary classication weighted average recall class multiclass task 
3004: sklearn.metrics.fbeta_score sklearn.metrics.fbeta_score y_true beta labelsnone pos_label1 aver y_pred ageweighted compute fbeta score f_beta score weighted harmonic mean precision recall reaching optimal value worst value beta parameter determines weight precision combined score beta lends weight precision beta favors precision beta considers precision beta inf recall 
3005: parameters y_true array shape n_samples true targets y_pred array shape n_samples predicted targets beta oat weight precision harmonic mean 
3006: chapter user guide scikitlearn user guide release 0.11 labels array integer array labels pos_label int binary classication case give label positive class default erything else pos_label considered belong negative class set none case multiclass classication 
3007: average string none micro macro weighted default multiclass classication case determines type averaging performed data macro average classes take imbalance account micro average instances takes imbalance account implies precision recall weighted average weighted support takes imbalance account result score precision recall 
3008: returns fbeta_score oat fbeta_score positive class binary classication weighted average fbeta_score class multiclass task 
3009: references baezayates ribeironeto modern information retrieval addison wesley 327328. http en.wikipedia.orgwikif1_score sklearn.metrics.f1_score sklearn.metrics.f1_score y_true y_pred labelsnone pos_label1 averageweighted compute score score interpreted weighted average precision recall score reaches best value worst score relative contribution precision recall score equal formular f_1 score f_1 precision recall precision recall see http en.wikipedia.orgwikif1_score multiclass case weighted average f1score class 
3010: parameters y_true array shape n_samples true targets y_pred array shape n_samples predicted targets labels array integer array labels pos_label int 1.8. reference scikitlearn user guide release 0.11 binary classication case give label positive class default erything else pos_label considered belong negative class set none case multiclass classication 
3011: average string none micro macro weighted default multiclass classication case determines type averaging performed data macro average classes take imbalance account micro average instances takes imbalance account implies precision recall weighted average weighted support takes imbalance account result score precision recall 
3012: returns f1_score oat f1_score positive class binary classication weighted average f1_scores class multiclass task references http en.wikipedia.orgwikif1_score sklearn.metrics.precision_recall_fscore_support sklearn.metrics.precision_recall_fscore_support y_true y_pred beta1.0 belsnone agenone pos_label1 aver compute precisions recalls fmeasures support class precision ratio number true positives number false positives precision intuitively ability classier label positive sample negative recall ratio number true positives number false negatives recall intuitively ability classier positive samples f_beta score interpreted weighted harmonic mean precision recall f_beta score reaches best value worst score f_beta score weights recall beta much precision beta 1.0 means recall precsion equally important support number occurrences class y_true pos_label none function returns average precision recall fmeasure average one micro macro weighted 
3013: parameters y_true array shape n_samples true targets y_pred array shape n_samples predicted targets beta oat 1.0 default strength recall versus precision fscore 
3014: chapter user guide scikitlearn user guide release 0.11 labels array integer array labels pos_label int binary classication case give label positive class default erything else pos_label considered belong negative class set none case multiclass classication 
3015: average string none micro macro weighted default multiclass classication case determines type averaging performed data macro average classes take imbalance account micro average instances takes imbalance account implies precision recall weighted average weighted support takes imbalance account result score precision recall 
3016: returns precision array shape n_unique_labels dtype np.double recall array shape n_unique_labels dtype np.double f1_score array shape n_unique_labels dtype np.double support array shape n_unique_labels dtype np.long references http en.wikipedia.orgwikiprecision_and_recall sklearn.metrics.classication_report sklearn.metrics.classification_report y_true y_pred labelsnone target_namesnone build text report showing main classication metrics parameters y_true array shape n_samples true targets y_pred array shape n_samples estimated targets labels array shape n_labels optional list label indices include report target_names list strings optional display names matching labels order returns report string text summary precision recall f1score class 1.8. reference scikitlearn user guide release 0.11 sklearn.metrics.precision_recall_curve sklearn.metrics.precision_recall_curve y_true probas_pred compute precisionrecall pairs different probability thresholds note implementation restricted binary classication task precision ratio number true positives number false positives precision intuitively ability classier label positive sample negative recall ratio number true positives number false negatives recall intuitively ability classier positive samples last precision recall values respectively corresponding threshold ensures graph starts axis 
3017: parameters y_true array shape n_samples true targets binary classication range probas_pred array shape n_samples estimated probabilities returns precision array shape precision values recall array shape recall values thresholds array shape thresholds y_score used compute precision recall sklearn.metrics.zero_one_score sklearn.metrics.zero_one_score y_true y_pred zeroone classication score accuracy positive integer number good classications best performance return fraction correct predictions y_pred 
3018: parameters y_true arraylike shape n_samples gold standard labels 
3019: y_pred arraylike shape n_samples predicted labels returned classier 
3020: returns score oat sklearn.metrics.zero_one sklearn.metrics.zero_one y_true y_pred zeroone classication loss positive integer number misclassications best performance return number errors chapter user guide scikitlearn user guide release 0.11 parameters y_true arraylike y_pred arraylike returns loss oat sklearn.metrics.hinge_loss sklearn.metrics.hinge_loss y_true pred_decision pos_label1 neg_label1 cumulated hinge loss nonregularized assuming labels y_true encoded prediction mistake made margin y_true pred_decision always negative since signs disagree therefore margin always greater cumulated hinge loss therefore upperbounds number mistakes made classier 
3021: parameters y_true array shape n_samples true target integers pred_decision array shape n_samples n_samples n_classes predicted decisions output decision_function oats regression metrics metrics.r2_score y_true y_pred metrics.mean_squared_error y_true y_pred mean squared error regression loss coefcient determination regression score function sklearn.metrics.r2_score sklearn.metrics.r2_score y_true y_pred coefcient determination regression score function best possible score 1.0 lower values worse 
3022: parameters y_true arraylike y_pred arraylike returns oat score notes symmetric function 
3023: references http en.wikipedia.orgwikicoefcient_of_determination 1.8. reference scikitlearn user guide release 0.11 sklearn.metrics.mean_squared_error sklearn.metrics.mean_squared_error y_true y_pred mean squared error regression loss return positive oating point value best value 0.0 
3024: parameters y_true arraylike y_pred arraylike returns loss oat clustering metrics see clustering section user guide details sklearn.metrics.cluster submodule contains evaluation metrics cluster analysis results two forms evaluation supervised uses ground truth class values sample unsupervised measures quality model 
3025: metrics.adjusted_rand_score labels_true ... metrics.adjusted_mutual_info_score ... metrics.homogeneity_completeness_v_measure ... compute homogeneity completeness vmeasure scores metrics.homogeneity_score labels_true ... metrics.completeness_score labels_true ... metrics.v_measure_score labels_true labels_pred metrics.silhouette_score labels ... homogeneity metric cluster labeling given ground truth completeness metric cluster labeling given ground truth vmeasure cluster labeling given ground truth compute mean silhouette coefcient samples 
3026: rand index adjusted chance adjusted mutual information two clusterings sklearn.metrics.adjusted_rand_score sklearn.metrics.adjusted_rand_score labels_true labels_pred rand index adjusted chance rand index computes similarity measure two clusterings considering pairs samples counting pairs assigned different clusters predicted true clusterings raw score adjusted chance ari score using following scheme ari expected_ri max expected_ri adjusted rand index thus ensured value close 0.0 random labeling independently number clusters samples exactly 1.0 clusterings identical permutation ari symmetric measure adjusted_rand_score adjusted_rand_score parameters labels_true int array shape n_samples ground truth class labels used reference labels_pred array shape n_samples cluster labels evaluate returns ari oat chapter user guide scikitlearn user guide release 0.11 similarity score 1.0 1.0. random labelings ari close 0.0 1.0 stands perfect match 
3027: see also adjusted_mutual_info_scoreadjusted mutual information references hubert1985 examples perfectly maching labelings score even sklearn.metrics.cluster import adjusted_rand_score adjusted_rand_score 1.0 adjusted_rand_score 1.0 labelings assign classes members clusters complete always pure hence penalized adjusted_rand_score 0.57.. 
3028: ari symmetric labelings pure clusters members coming classes unnec essary splits penalized adjusted_rand_score 0.57.. 
3029: classes members completely split across different clusters assignment totally incomplete hence ari low adjusted_rand_score 0.0 sklearn.metrics.adjusted_mutual_info_score sklearn.metrics.adjusted_mutual_info_score labels_true labels_pred adjusted mutual information two clusterings adjusted mutual information ami adjustement mutual information score account chance accounts fact generally higher two clusterings larger number clusters regardless whether actually information shared two clusterings ami given ami max metric independent absolute values labels permutation class cluster label values wont change score value way metric furthermore symmetric switching label_true label_pred return score value useful measure agreement two independent label assignments strategies dataset real ground truth known 
3030: 1.8. reference scikitlearn user guide release 0.11 mindful function order magnitude slower metrics adjusted rand index 
3031: parameters labels_true int array shape n_samples clustering data disjoint subsets 
3032: labels_pred array shape n_samples clustering data disjoint subsets 
3033: returns ami oat score 0.0 1.0 1.0 stands perfectly complete labeling see also adjusted_rand_scoreadjusted rand index mutual_information_scoremutual information adjusted chance examples perfect labelings homogeneous complete hence score 1.0 sklearn.metrics.cluster import adjusted_mutual_info_score adjusted_mutual_info_score 1.0 adjusted_mutual_info_score 1.0 classes members completly splitted across different clusters assignment totally incomplete hence ami null adjusted_mutual_info_score 0.0 sklearn.metrics.homogeneity_completeness_v_measure sklearn.metrics.homogeneity_completeness_v_measure labels_true labels_pred compute homogeneity completeness vmeasure scores metrics based normalized conditional entropy measures clustering labeling evaluate given knowledge ground truth class labels samples clustering result satises homogeneity clusters contain data points members single class clustering result satises completeness data points members given class elements cluster scores positive values 0.0 1.0 larger values desirable metrics independent absolute values labels permutation class cluster label values wont change score values way vmeasure furthermore symmetric swapping labels_true label_pred give score hold homogeneity completeness 
3034: parameters labels_true int array shape n_samples chapter user guide scikitlearn user guide release 0.11 ground truth class labels used reference labels_pred array shape n_samples cluster labels evaluate returns homogeneity oat score 0.0 1.0 1.0 stands perfectly homogeneous labeling completeness oat score 0.0 1.0 1.0 stands perfectly complete labeling v_measure oat harmonic mean rst two see also homogeneity_score completeness_score v_measure_score sklearn.metrics.homogeneity_score sklearn.metrics.homogeneity_score labels_true labels_pred homogeneity metric cluster labeling given ground truth clustering result satises homogeneity clusters contain data points members single class metric independent absolute values labels permutation class cluster label values wont change score value way metric symmetric switching label_true label_pred return completeness_score different general 
3035: parameters labels_true int array shape n_samples ground truth class labels used reference labels_pred array shape n_samples cluster labels evaluate returns homogeneity oat score 0.0 1.0 1.0 stands perfectly homogeneous labeling see also completeness_score v_measure_score references r72 examples perfect labelings homegenous 1.8. reference scikitlearn user guide release 0.11 sklearn.metrics.cluster import homogeneity_score homogeneity_score 1.0 nonpefect labelings futher split classes clusters perfectly homogeneous homogeneity_score 1.0 homogeneity_score 1.0 clusters include samples different classes make homogeneous labeling homogeneity_score 0.0 homogeneity_score 0.0 sklearn.metrics.completeness_score sklearn.metrics.completeness_score labels_true labels_pred completeness metric cluster labeling given ground truth clustering result satises completeness data points members given class elements cluster metric independent absolute values labels permutation class cluster label values wont change score value way metric symmetric switching label_true label_pred return homogeneity_score different general 
3036: parameters labels_true int array shape n_samples ground truth class labels used reference labels_pred array shape n_samples cluster labels evaluate returns completeness oat score 0.0 1.0 1.0 stands perfectly complete labeling see also homogeneity_score v_measure_score references r71 examples perfect labelings complete sklearn.metrics.cluster import completeness_score completeness_score 1.0 chapter user guide scikitlearn user guide release 0.11 nonpefect labelings assign classes members clusters still complete completeness_score 1.0 completeness_score 1.0 classes members splitted across different clusters assignment complete completeness_score 0.0 completeness_score 0.0 sklearn.metrics.v_measure_score sklearn.metrics.v_measure_score labels_true labels_pred vmeasure cluster labeling given ground truth vmeasure hormonic mean homogeneity completeness homogeneity completeness homogeneity completeness metric independent absolute values labels permutation class cluster label values wont change score value way metric furthermore symmetric switching label_true label_pred return score value useful measure agreement two independent label assignments strategies dataset real ground truth known 
3037: parameters labels_true int array shape n_samples ground truth class labels used reference labels_pred array shape n_samples cluster labels evaluate returns completeness oat score 0.0 1.0 1.0 stands perfectly complete labeling see also homogeneity_score completeness_score references rosenberg2007 examples perfect labelings homogeneous complete hence score 1.0 sklearn.metrics.cluster import v_measure_score v_measure_score 1.0 v_measure_score 1.0 1.8. reference scikitlearn user guide release 0.11 labelings assign classes members clusters complete homogeneous hence penal ized v_measure_score 0.8 ... v_measure_score 0.66.. 
3038: labelings pure clusters members coming classes homogeneous necessary splits harms completeness thus penalize vmeasure well v_measure_score 0.8 ... v_measure_score 0.66.. 
3039: classes members completly splitted across different clusters assignment totally incomplete hence vmeasure null v_measure_score 0.0 clusters include samples totally different classes totally destroy homogeneity labeling hence v_measure_score 0.0 sklearn.metrics.silhouette_score sklearn.metrics.silhouette_score labels metriceuclidean sample_sizenone ran dom_statenone kwds compute mean silhouette coefcient samples silhouette coefcient calculated using mean intracluster distance mean nearestcluster distance sample silhouette coefcient sample max clarrify distance sample nearest cluster part function returns mean silhoeutte coefcient samples obtain values sample use silhouette_samples best value worst value values near indicate overlapping clusters negative values generally indicate sample assigned wrong cluster different cluster similar 
3040: parameters array n_samples_a n_samples_a metric precomputed n_samples_a n_features otherwise array pairwise distances samples feature array 
3041: labels array shape n_samples label values sample metric string callable metric string metric use calculating distance instances feature ray must one options allowed met rics.pairwise.pairwise_distances distance array use precomputed metric 
3042: sample_size int none chapter user guide scikitlearn user guide release 0.11 size sample use computing silhouette coefcient sample_size none sampling used 
3043: random_state integer numpy.randomstate optional generator used initialize centers defaults global numpy random number generator 
3044: integer given xes seed 
3045: kwds optional keyword parameters parameters passed directly distance function using scipy.spatial.distance metric parameters still metric dependent see scipy docs usage examples 
3046: returns silhouette oat mean silhouette coefcient samples 
3047: references peter rousseeuw silhouettes graphical aid theinterpretation validation cluster analysis computational applied mathematics 5365. doi10.101603770427 
3048: http en.wikipedia.orgwikisilhouette_ clustering pairwise metrics sklearn.metrics.pairwise submodule implements utilities evaluate pairwise distances afnity sets samples module contains distance metrics kernels brief summary given two distance metrics function objects considered similar objects two objects exactly alike would distance zero one popular examples euclidean distance true metric must obey following four conditions positive definiteness symmetry triangle inequality kernels measures similarity i.e objects considered similar objects kernel must also positive semidenite number ways convert distance metric similarity measure kernel let distance kernel np.exp gamma one heuristic choosing gamma num_features np.max metrics.pairwise.euclidean_distances ... considering rows vectors compute metrics.pairwise.manhattan_distances ... compute distances vectors metrics.pairwise.linear_kernel metrics.pairwise.polynomial_kernel ... metrics.pairwise.rbf_kernel gamma metrics.pairwise.distance_metrics compute linear kernel compute polynomial kernel compute rbf gaussian kernel valid metrics pairwise_distances 1.8. reference continued next page scikitlearn user guide release 0.11 metrics.pairwise.pairwise_distances ... metrics.pairwise.kernel_metrics metrics.pairwise.pairwise_kernels ... compute distance matrix vector array optional valid metrics pairwise_kernels compute kernel arrays optional array 
3049: table 1.139 continued previous page sklearn.metrics.pairwise.euclidean_distances sklearn.metrics.pairwise.euclidean_distances ynone squaredfalse y_norm_squarednone considering rows vectors compute distance matrix pair vectors efciency reasons euclidean distance pair row vector computed dist sqrt dot dot dot formulation two main advantages first computationally efcient dealing sparse data second varies remains unchanged rightmost dotproduct dot precomputed 
3050: parameters arraylike sparse matrix shape n_samples_1 n_features arraylike sparse matrix shape n_samples_2 n_features y_norm_squared arraylike shape n_samples_2 optional precomputed dotproducts vectors e.g. .sum axis1 squared boolean optional return squared euclidean distances 
3051: returns distances array sparse matrix shape n_samples_1 n_samples_2 examples sklearn.metrics.pairwise import euclidean_distances distance rows euclidean_distances array get distance origin euclidean_distances array 
3052: 1.41421356 sklearn.metrics.pairwise.manhattan_distances sklearn.metrics.pairwise.manhattan_distances ynone sum_over_featurestrue compute distances vectors sum_over_features equal false returns componentwise distances 
3053: parameters array_like array shape n_samples_x n_features 
3054: array_like optional array shape n_samples_y n_features 
3055: chapter user guide scikitlearn user guide release 0.11 sum_over_features bool defaulttrue true function returns pairwise distance matrix else returns component wise pairwisedistances 
3056: returns array sum_over_features false shape n_samples_x n_samples_y n_features contains componentwise pairwisedistances absolute difference else shape n_samples_x n_samples_y contains pairwise distances 
3057: examples sklearn.metrics.pairwise import manhattan_distances manhattan_distances array manhattan_distances array manhattan_distances array manhattan_distances array import numpy np.ones np.ones manhattan_distances sum_over_featuresfalse array sklearn.metrics.pairwise.linear_kernel sklearn.metrics.pairwise.linear_kernel ynone compute linear kernel 
3058: parameters array shape n_samples_1 n_features array shape n_samples_2 n_features returns gram matrix array shape n_samples_1 n_samples_2 sklearn.metrics.pairwise.polynomial_kernel sklearn.metrics.pairwise.polynomial_kernel ynone degree3 gamma0 coef01 compute polynomial kernel gamma coef0 degree parameters array shape n_samples_1 n_features array shape n_samples_2 n_features degree int returns gram matrix array shape n_samples_1 n_samples_2 1.8. reference scikitlearn user guide release 0.11 sklearn.metrics.pairwise.rbf_kernel sklearn.metrics.pairwise.rbf_kernel ynone gamma0 compute rbf gaussian kernel exp gamma xy2 parameters array shape n_samples_1 n_features array shape n_samples_2 n_features gamma oat returns gram matrix array shape n_samples_1 n_samples_2 sklearn.metrics.pairwise.distance_metrics sklearn.metrics.pairwise.distance_metrics valid metrics pairwise_distances function simply returns valid pairwise distance metrics description mapping valid strings valid distance metrics function map metric cityblock euclidean manhattan function sklearn.pairwise.manhattan_distances sklearn.pairwise.euclidean_distances sklearn.pairwise.manhattan_distances sklearn.pairwise.euclidean_distances sklearn.pairwise.manhattan_distances sklearn.metrics.pairwise.pairwise_distances exists however allow verbose sklearn.metrics.pairwise.pairwise_distances ynone metriceuclidean n_jobs1 kwds compute distance matrix vector array optional method takes either vector array distance matrix returns distance matrix input vector array distances computed input distances matrix returned instead method provides safe way take distance matrix input preserving compatability many algorithms take vector array given default none returned matrix pairwise distance arrays please note support wise.pairwise_distance_functions valid values metric sparse matrices currently limited metrics listed pair scikitlearn euclidean manhattan cityblock scipy.spatial.distance braycurtis canberra chebyshev correlation cosine dice ham ming russell rao seuclidean sokalmichener sokalsneath sqeucludean yule see documentation scipy.spatial.distance details metrics 
3059: rogerstanimoto mahalanobis matching minkowski jaccard kulsinski chapter user guide scikitlearn user guide release 0.11 note case euclidean cityblock valid scipy.spatial.distance metrics values use scikitlearn implementation faster support sparse matrices verbose description metrics scikitlearn see __doc__ sklearn.pairwise.distance_metrics function 
3060: parameters array n_samples_a n_samples_a metric precomputed n_samples_a n_features otherwise array pairwise distances samples feature array 
3061: array n_samples_b n_features second feature array shape n_samples_a n_features 
3062: metric string callable metric use calculating distance instances feature array metric string must one options allowed scipy.spatial.distance.pdist metric parameter metric listed pairwise.pairwise_distance_functions metric precomputed assumed distance matrix alternatively metric callable function called pair instances rows resulting value recorded callable take two arrays input return value indicating distance 
3063: n_jobs int number jobs use computation works breaking pairwise matrix n_jobs even slices computing parallel cpus used given parallel computing code used useful debuging n_jobs n_cpus n_jobs used thus n_jobs cpus one used 
3064: kwds optional keyword parameters parameters passed directly distance function using scipy.spatial.distance metric parameters still metric dependent see scipy docs usage examples 
3065: returns array n_samples_a n_samples_a n_samples_a n_samples_b distance matrix distance ith jth vectors given matrix none none distance ith array jth array 
3066: sklearn.metrics.pairwise.kernel_metrics sklearn.metrics.pairwise.kernel_metrics valid metrics pairwise_kernels function simply returns valid pairwise distance metrics description mapping valid strings 
3067: valid distance metrics function map exists however allow verbose metric linear poly polynomial rbf sigmoid function sklearn.pairwise.linear_kernel sklearn.pairwise.polynomial_kernel sklearn.pairwise.polynomial_kernel sklearn.pairwise.rbf_kernel sklearn.pairwise.sigmoid_kernel 1.8. reference scikitlearn user guide release 0.11 sklearn.metrics.pairwise.pairwise_kernels sklearn.metrics.pairwise.pairwise_kernels ynone metriclinear ter_paramsfalse n_jobs1 kwds compute kernel arrays optional array method takes either vector array kernel matrix returns kernel matrix input vector array kernels computed input kernel matrix returned instead method provides safe way take kernel matrix input preserving compatability many algorithms take vector array given default none returned matrix pairwise kernel arrays valid values metric rbf sigmoid polynomial poly linear parameters array n_samples_a n_samples_a metric precomputed n_samples_a n_features otherwise array pairwise kernels samples feature array 
3068: array n_samples_b n_features second feature array shape n_samples_a n_features 
3069: metric string callable metric use calculating kernel instances feature array met ric string must one metrics pairwise.pairwise_kernel_functions metric precomputed assumed kernel matrix alternatively met ric callable function called pair instances rows resulting value recorded callable take two arrays input return value indicating distance 
3070: n_jobs int number jobs use computation works breaking pairwise matrix n_jobs even slices computing parallel cpus used given parallel computing code used useful debuging n_jobs n_cpus n_jobs used thus n_jobs cpus one used 
3071: lter_params boolean whether lter invalid parameters 
3072: kwds optional keyword parameters parameters passed directly kernel function 
3073: returns array n_samples_a n_samples_a n_samples_a n_samples_b kernel matrix kernel ith jth vectors given matrix none none kernel ith array jth array 
3074: 1.8.18 sklearn.mixture gaussian mixture models sklearn.mixture module implements mixture modeling algorithms 
3075: chapter user guide scikitlearn user guide release 0.11 user guide see gaussian mixture models section details 
3076: mixture.gmm n_components covariance_type ... gaussian mixture model mixture.dpgmm n_components ... mixture.vbgmm n_components ... variational inference innite gaussian mixture model variational inference gaussian mixture model sklearn.mixture.gmm class sklearn.mixture.gmm n_components1 random_statenone thresh0.01 min_covar0.001 n_iter100 n_init1 paramswmc init_paramswmc covariance_typediag gaussian mixture model representation gaussian mixture model probability distribution class allows easy evaluation sampling maximumlikelihood estimation parameters gmm distribution initializes parameters every mixture component zero mean identity covariance 
3077: parameters n_components int optional number mixture components defaults 
3078: covariance_type string optional string describing type covariance parameters use must one spherical tied diag full defaults diag 
3079: random_state randomstate int seed default random number generator instance min_covar oat optional floor diagonal covariance matrix prevent overtting defaults 1e3 
3080: thresh oat optional convergence threshold 
3081: n_iter int optional number iterations perform 
3082: n_init int optional number initializations perform best results kept params string optional controls parameters updated training process contain combi nation weights means covars defaults wmc 
3083: init_params string optional controls parameters updated initialization process contain combination weights means covars defaults wmc 
3084: see also dpgmmininite gaussian mixture model using dirichlet process variational algorithm vbgmmfinite gaussian mixture model variational algorithm better situations might little data get good estimate covariance matrix 
3085: 1.8. reference scikitlearn user guide release 0.11 examples import numpy sklearn import mixture np.random.seed mixture.gmm n_components2 generate random observations two modes centered use training obs np.concatenate np.random.randn ... g.fit obs gmm covariance_typenone init_paramswmc min_covar0.001 np.random.randn n_components2 n_init1 n_iter100 paramswmc random_statenone thresh0.01 np.round g.weights_ array 0.75 np.round g.means_ array 10.05 0.25 0.06 np.round g.covars_ array 1.02 0.96 g.predict array np.round g.score array 2.19 4.58 1.75 1.21 refit model new data initial parameters remain time even split two modes g.fit gmm covariance_typenone init_paramswmc min_covar0.001 n_components2 n_init1 n_iter100 paramswmc random_statenone thresh0.01 np.round g.weights_ array 0.5 0.5 attributes weights_ means_ covars_ array shape n_components shape array n_features array n_components attribute stores mixing weights mixture compo nent mean parameters mixture component covariance parameters mixture component shape pends covariance_type n_components n_features n_features n_components n_features n_components n_features n_features full converged_ bool true reached false otherwise 
3086: convergence chapter user guide scikitlearn user guide release 0.11 methods akaike information criterion current model aic bayesian information criterion current model bic deprecated removed v0.12 decode args kwargs evaluate model data eval estimate model parameters expectationmaximization algorithm fit kwargs get parameters estimator get_params deep predict label data predict predict posterior probability data gaussian predict_proba rvs args kwargs deprecated removed v0.12 sample n_samples random_state generate random samples model score set_params params compute log probability model set parameters estimator 
3087: __init__ n_components1 thresh0.01 min_covar0.001 n_iter100 n_init1 paramswmc init_paramswmc covariance_typediag random_statenone aic akaike information criterion current model proposed data parameters array shape n_samples n_dimensions returns aic oat lower better bic bayesian information criterion current model proposed data parameters array shape n_samples n_dimensions returns bic oat lower better decode args kwargs deprecated removed v0.12 use score predict method instead depending question find likely mixture components point 
3088: deprecated version 0.10 removed version 0.12 use score predict method instead depending question 
3089: parameters array_like shape n_features list n_featuresdimensional data points row corresponds single data point 
3090: returns logprobs array_like shape n_samples log probability point obs model 
3091: components array_like shape n_samples index likelihod mixture com ponents observation eval evaluate model data compute log probability model return posterior distribution responsibilities mixture component element 
3092: parameters array_like shape n_samples n_features 1.8. reference scikitlearn user guide release 0.11 list n_featuresdimensional data points row corresponds single data point 
3093: returns logprob array_like shape n_samples log probabilities data point responsibilities array_like shape n_samples n_components posterior probabilities mixture component observation fit kwargs estimate model parameters expectationmaximization algorithm initialization step performed entering algorithm want avoid step set keyword argument init_params empty string creating gmm object likewise would like initialization set n_iter0 
3094: parameters array_like shape n_features list n_featuresdimensional data points row corresponds single data point 
3095: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3096: predict predict label data 
3097: parameters arraylike shape n_samples n_features returns array shape n_samples predict_proba predict posterior probability data gaussian model 
3098: parameters arraylike shape n_samples n_features returns responsibilities arraylike shape n_samples n_components returns probability sample gaussian state model 
3099: rvs args kwargs deprecated removed v0.12 use score predict method instead depending question generate random samples model 
3100: deprecated version 0.11 removed version 0.12 use sample stead sample n_samples1 random_statenone generate random samples model parameters n_samples int optional number samples generate defaults returns array_like shape n_samples n_features list samples score compute log probability model 
3101: chapter user guide scikitlearn user guide release 0.11 parameters array_like shape n_samples n_features list n_featuresdimensional data points row corresponds single data point 
3102: returns logprob array_like shape n_samples log probabilities data point set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.mixture.dpgmm class sklearn.mixture.dpgmm n_components1 dom_statenone n_iter10 paramswmc init_paramswmc covariance_typediag thresh0.01 ran verbosefalse min_covarnone alpha1.0 variational inference innite gaussian mixture model dpgmm stands dirichlet process gaussian mixture model innite mixture model dirichlet process prior distribution number clusters practice approximate inference algo rithm uses truncated distribution xed maximum number components almost always number components actually used depends data stickbreaking representation gaussian mixture model probability distribution class allows easy efcient inference approximate posterior distribution parameters gaussian mixture model variable number components smaller truncation parameter n_components initialization normallydistributed means identity covariance proper convergence 
3103: parameters n_components int optional number mixture components defaults 
3104: covariance_type string optional string describing type covariance parameters use must one spherical tied diag full defaults diag 
3105: alpha oat optional real number representing concentration parameter dirichlet process intu itively dirichlet process likely start new cluster point add point cluster alpha elements higher alpha means clusters expected number clusters alphalog defaults 
3106: thresh oat optional convergence threshold 
3107: n_iter int optional maximum number iterations perform convergence 
3108: params string optional controls parameters updated training process contain combi nation weights means covars defaults wmc 
3109: 1.8. reference scikitlearn user guide release 0.11 init_params string optional controls parameters updated initialization process contain combination weights means covars defaults wmc 
3110: see also gmmfinite gaussian mixture model vbgmmfinite gaussian mixture model variational algorithm better data attributes covariance_type string n_components weights_ means_ precisions_ int array shape n_components shape array n_features array n_components string describing type variance parameters used dpgmm must one spher ical tied diag full number mixture components mixing weights mixture component mean parameters mixture component precision inverse covariance rameters mixture compo nent shape depends covari ance_type n_components n_features n_features n_features n_components n_features n_components n_features n_features converged_ bool true reached false otherwise 
3111: convergence methods akaike information criterion current model aic bayesian information criterion current model bic deprecated removed v0.12 decode args kwargs evaluate model data eval estimate model parameters variational algorithm fit kwargs get parameters estimator get_params deep returns lower bound model evidence based membership lower_bound predict label data predict predict posterior probability data gaussian predict_proba rvs args kwargs deprecated removed v0.12 sample n_samples random_state generate random samples model score set_params params compute log probability model set parameters estimator 
3112: chapter user guide scikitlearn user guide release 0.11 __init__ n_components1 covariance_typediag alpha1.0 random_statenone thresh0.01 verbosefalse min_covarnone n_iter10 paramswmc init_paramswmc aic akaike information criterion current model proposed data parameters array shape n_samples n_dimensions returns aic oat lower better bic bayesian information criterion current model proposed data parameters array shape n_samples n_dimensions returns bic oat lower better decode args kwargs deprecated removed v0.12 use score predict method instead depending question find likely mixture components point 
3113: deprecated version 0.10 removed version 0.12 use score predict method instead depending question 
3114: parameters array_like shape n_features list n_featuresdimensional data points row corresponds single data point 
3115: returns logprobs array_like shape n_samples log probability point obs model 
3116: components array_like shape n_samples index likelihod mixture com ponents observation eval evaluate model data compute bound log probability model return posterior distribution respon sibilities mixture component element done computing parameters meaneld observation 
3117: parameters array_like shape n_samples n_features list n_featuresdimensional data points row corresponds single data point 
3118: returns logprob array_like shape n_samples log probabilities data point responsibilities array_like shape n_samples n_components posterior probabilities mixture component observation fit kwargs estimate model parameters variational algorithm full derivation description algorithm see docdpderivationdpderivation.tex initialization step performed entering algorithm want avoid step set keyword argument init_params empty string creating object likewise would like initialization set n_iter0 
3119: 1.8. reference scikitlearn user guide release 0.11 parameters array_like shape n_features list n_featuresdimensional data points row corresponds single data point 
3120: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3121: lower_bound returns lower bound model evidence based membership predict predict label data 
3122: parameters arraylike shape n_samples n_features returns array shape n_samples predict_proba predict posterior probability data gaussian model 
3123: parameters arraylike shape n_samples n_features returns responsibilities arraylike shape n_samples n_components returns probability sample gaussian state model 
3124: rvs args kwargs deprecated removed v0.12 use score predict method instead depending question generate random samples model 
3125: deprecated version 0.11 removed version 0.12 use sample stead sample n_samples1 random_statenone generate random samples model parameters n_samples int optional number samples generate defaults returns array_like shape n_samples n_features list samples score compute log probability model 
3126: parameters array_like shape n_samples n_features list n_featuresdimensional data points row corresponds single data point 
3127: returns logprob array_like shape n_samples log probabilities data point set_params params set parameters estimator 
3128: chapter user guide scikitlearn user guide release 0.11 method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.mixture.vbgmm class sklearn.mixture.vbgmm n_components1 dom_statenone n_iter10 paramswmc init_paramswmc covariance_typediag thresh0.01 ran verbosefalse min_covarnone alpha1.0 variational inference gaussian mixture model variational inference gaussian mixture model probability distribution class allows easy efcient inference approximate posterior distribution parameters gaussian mixture model xed number components initialization normallydistributed means identity covariance proper convergence 
3129: parameters n_components int optional number mixture components defaults 
3130: covariance_type string optional string describing type covariance parameters use must one spherical tied diag full defaults diag 
3131: alpha oat optional real number representing concentration parameter dirichlet distribution intu itively higher value alpha likely variational mixture gaussians model use components defaults 
3132: see also gmmfinite gaussian mixture model dpgmmininite gaussian mixture model using dirichlet process fit 1.8. reference scikitlearn user guide release 0.11 attributes covariance_type string n_features n_components weights_ means_ precisions_ int int readonly array shape n_components shape array n_features array n_components string describing type variance parameters used dpgmm must one spher ical tied diag full dimensionality gaussians number mixture components mixing weights mixture component mean parameters mixture component precision inverse covariance rameters mixture compo nent shape depends covari ance_type n_components n_features n_features n_features n_components n_features n_components n_features n_features converged_ bool true reached false otherwise 
3133: convergence methods aic akaike information criterion current model bic bayesian information criterion current model decode args kwargs deprecated removed v0.12 eval evaluate model data fit kwargs estimate model parameters variational algorithm get_params deep get parameters estimator lower_bound returns lower bound model evidence based membership predict predict label data predict_proba predict posterior probability data gaussian rvs args kwargs deprecated removed v0.12 sample n_samples random_state generate random samples model score set_params params compute log probability model set parameters estimator 
3134: __init__ n_components1 covariance_typediag alpha1.0 random_statenone thresh0.01 verbosefalse min_covarnone n_iter10 paramswmc init_paramswmc aic akaike information criterion current model proposed data parameters array shape n_samples n_dimensions returns aic oat lower better bic bayesian information criterion current model proposed data chapter user guide scikitlearn user guide release 0.11 parameters array shape n_samples n_dimensions returns bic oat lower better decode args kwargs deprecated removed v0.12 use score predict method instead depending question find likely mixture components point 
3135: deprecated version 0.10 removed version 0.12 use score predict method instead depending question 
3136: parameters array_like shape n_features list n_featuresdimensional data points row corresponds single data point 
3137: returns logprobs array_like shape n_samples log probability point obs model 
3138: components array_like shape n_samples index likelihod mixture com ponents observation eval evaluate model data compute bound log probability model return posterior distribution respon sibilities mixture component element done computing parameters meaneld observation 
3139: parameters array_like shape n_samples n_features list n_featuresdimensional data points row corresponds single data point 
3140: returns logprob array_like shape n_samples log probabilities data point responsibilities array_like shape n_samples n_components posterior probabilities mixture component observation fit kwargs estimate model parameters variational algorithm full derivation description algorithm see docdpderivationdpderivation.tex initialization step performed entering algorithm want avoid step set keyword argument init_params empty string creating object likewise would like initialization set n_iter0 
3141: parameters array_like shape n_features list n_featuresdimensional data points row corresponds single data point 
3142: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3143: 1.8. reference scikitlearn user guide release 0.11 lower_bound returns lower bound model evidence based membership predict predict label data 
3144: parameters arraylike shape n_samples n_features returns array shape n_samples predict_proba predict posterior probability data gaussian model 
3145: parameters arraylike shape n_samples n_features returns responsibilities arraylike shape n_samples n_components returns probability sample gaussian state model 
3146: rvs args kwargs deprecated removed v0.12 use score predict method instead depending question generate random samples model 
3147: deprecated version 0.11 removed version 0.12 use sample stead sample n_samples1 random_statenone generate random samples model parameters n_samples int optional number samples generate defaults returns array_like shape n_samples n_features list samples score compute log probability model 
3148: parameters array_like shape n_samples n_features list n_featuresdimensional data points row corresponds single data point 
3149: returns logprob array_like shape n_samples log probabilities data point set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self 1.8.19 sklearn.multiclass multiclass multilabel classication multiclass multilabel classication strategies module implements multiclass learning algorithms chapter user guide scikitlearn user guide release 0.11 onevstherest onevsall onevsone error correcting output codes estimators provided module metaestimators require base estimator provided constructor example possible use estimators turn binary classier regressor multiclass classier also possible use estimators multiclass estimators hope accuracy runtime performance improves user guide see multiclass multilabel algorithms section details 
3150: multiclass.onevsrestclassifier estimator multiclass.onevsoneclassifier estimator multiclass.outputcodeclassifier estimator ... onevstherest ovr multiclassmultilabel strategy onevsone multiclass strategy errorcorrecting outputcode multiclass strategy sklearn.multiclass.onevsrestclassier class sklearn.multiclass.onevsrestclassifier estimator onevstherest ovr multiclassmultilabel strategy also known onevsall strategy consists tting one classier per class classier class tted classes addition computational efciency n_classes classiers needed one advantage approach interpretability since class represented one one classier possible gain knowledge class inspecting corresponding classier commonly used strategy multiclass classication fair default choice strategy also used multilabel learning classier used predict multiple labels instance tting sequence sequences labels e.g. list tuples rather single target vector multilabel learning number classes must least three since otherwise ovr reduces binary classication 
3151: parameters estimator estimator object estimator object implementing one decision_function predict_proba 
3152: attributes estimators_ bel_binarizer_ multilabel_ list n_classes estimators labelbinarizer object boolean estimators used predictions 
3153: object used transform multiclass labels binary labels viceversa whether onevsrestclassier multilabel classier 
3154: methods fit get_params deep predict score set_params params fit underlying estimators get parameters estimator predict multiclass targets using underlying estimators 
3155: set parameters estimator 
3156: 1.8. reference scikitlearn user guide release 0.11 __init__ estimator fit fit underlying estimators 
3157: parameters arraylike sparse matrix shape n_samples n_features data 
3158: arraylike shape n_samples sequence sequences len n_samplesmulticlass targets sequence quences turns multilabel classication 
3159: returns self get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3160: multilabel_ whether multilabel classier predict predict multiclass targets using underlying estimators 
3161: parameters arraylike sparse matrix shape n_samples n_features data 
3162: returns arraylike shape n_samples predicted multiclass targets 
3163: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.multiclass.onevsoneclassier class sklearn.multiclass.onevsoneclassifier estimator onevsone multiclass strategy strategy consists tting one classier per class pair prediction time class received votes selected since requires n_classes n_classes classiers method usually slower onevstherest due n_classes2 complexity however method may advantageous algorithms kernel algorithms dont scale well n_samples individual learning problem involves small subset data whereas onevstherest complete dataset used n_classes times 
3164: parameters estimator estimator object estimator object implementing predict 
3165: chapter user guide scikitlearn user guide release 0.11 attributes estimators_ classes_ list n_classes n_classes estimators numpy array shape n_classes estimators used predictions array containing labels 
3166: methods fit get_params deep predict score set_params params fit underlying estimators get parameters estimator predict multiclass targets using underlying estimators returns mean accuracy given test data labels set parameters estimator 
3167: __init__ estimator fit fit underlying estimators 
3168: parameters arraylike sparse matrix shape n_samples n_features data 
3169: numpy array shape n_samples multiclass targets 
3170: returns self get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3171: predict predict multiclass targets using underlying estimators 
3172: parameters arraylike sparse matrix shape n_samples n_features data 
3173: returns numpy array shape n_samples predicted multiclass targets 
3174: score returns mean accuracy given test data labels 
3175: parameters arraylike shape n_samples n_features training set 
3176: arraylike shape n_samples labels 
3177: returns oat 1.8. reference scikitlearn user guide release 0.11 set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.multiclass.outputcodeclassier class sklearn.multiclass.outputcodeclassifier estimator code_size1.5 ran dom_statenone errorcorrecting outputcode multiclass strategy outputcode based strategies consist representing class binary code array tting time one binary classier per bit code book tted prediction time classiers used project new points class space class closest points chosen main advantage strategies number classiers used controlled user either compressing model code_size making model robust errors code_size see documentation details 
3178: parameters estimator estimator object estimator object implementing one decision_function predict_proba 
3179: code_size oat percentage number classes used create code book number require fewer classiers onevstherest number greater require classiers onevstherest 
3180: random_state numpy.randomstate optional generator used initialize codebook defaults numpy.random 
3181: references r73 r74 r75 attributes estimators_ classes_ code_book_ list int n_classes code_size estimators numpy array shape n_classes numpy array shape n_classes code_size binary array containing code class 
3182: estimators used predictions array containing labels 
3183: methods fit get_params deep predict score set_params params fit underlying estimators get parameters estimator predict multiclass targets using underlying estimators returns mean accuracy given test data labels set parameters estimator 
3184: chapter user guide scikitlearn user guide release 0.11 __init__ estimator code_size1.5 random_statenone fit fit underlying estimators 
3185: parameters arraylike sparse matrix shape n_samples n_features data 
3186: numpy array shape n_samples multiclass targets 
3187: returns self get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3188: predict predict multiclass targets using underlying estimators 
3189: parameters arraylike sparse matrix shape n_samples n_features data 
3190: returns numpy array shape n_samples predicted multiclass targets 
3191: score returns mean accuracy given test data labels 
3192: parameters arraylike shape n_samples n_features training set 
3193: arraylike shape n_samples labels 
3194: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self multiclass.fit_ovr estimator multiclass.predict_ovr estimators ... multiclass.fit_ovo estimator multiclass.predict_ovo estimators classes multiclass.fit_ecoc estimator ... multiclass.predict_ecoc estimators classes ... make predictions using errorcorrecting outputcode strategy 
3195: fit onevstherest strategy make predictions using onevstherest strategy fit onevsone strategy make predictions using onevsone strategy fit errorcorrecting outputcode strategy 
3196: 1.8. reference scikitlearn user guide release 0.11 sklearn.multiclass.t_ovr sklearn.multiclass.fit_ovr estimator fit onevstherest strategy 
3197: sklearn.multiclass.predict_ovr sklearn.multiclass.predict_ovr estimators label_binarizer make predictions using onevstherest strategy 
3198: sklearn.multiclass.t_ovo sklearn.multiclass.fit_ovo estimator fit onevsone strategy 
3199: sklearn.multiclass.predict_ovo sklearn.multiclass.predict_ovo estimators classes make predictions using onevsone strategy 
3200: sklearn.multiclass.t_ecoc sklearn.multiclass.fit_ecoc estimator code_size1.5 random_statenone fit errorcorrecting outputcode strategy 
3201: parameters estimator estimator object estimator object implementing one decision_function predict_proba 
3202: code_size oat optional percentage number classes used create code book 
3203: random_state numpy.randomstate optional generator used initialize codebook defaults numpy.random 
3204: returns estimators list int n_classes code_size estimators estimators used predictions 
3205: classes numpy array shape n_classes array containing labels 
3206: code_book_ numpy array shape n_classes code_size binary array containing code class 
3207: sklearn.multiclass.predict_ecoc sklearn.multiclass.predict_ecoc estimators classes code_book make predictions using errorcorrecting outputcode strategy 
3208: chapter user guide scikitlearn user guide release 0.11 1.8.20 sklearn.naive_bayes naive bayes sklearn.naive_bayes module implements naive bayes algorithms supervised learning methods based applying bayes theorem strong naive feature independence assumptions user guide see naive bayes section details 
3209: naive_bayes.gaussiannb naive_bayes.multinomialnb alpha t_prior naive bayes classier multinomial models naive_bayes.bernoullinb alpha binarize ... naive bayes classier multivariate bernoulli models 
3210: gaussian naive bayes gaussiannb sklearn.naive_bayes.gaussiannb class sklearn.naive_bayes.gaussiannb gaussian naive bayes gaussiannb parameters arraylike shape n_samples n_features training vector n_samples number samples n_features num ber features 
3211: array shape n_samples target vector relative examples import numpy np.array np.array sklearn.naive_bayes import gaussiannb clf gaussiannb clf.fit gaussiannb print clf.predict 0.8 attributes class_prior_ theta_ sigma_ methods array shape n_classes array shape n_classes n_features mean feature per class array shape n_classes n_features probability class 
3212: variance feature per class fit get_params deep predict predict_log_proba return logprobability estimates test vector predict_proba score fit gaussian naive bayes according get parameters estimator perform classication array test vectors 
3213: return probability estimates test vector returns mean accuracy given test data labels continued next page 1.8. reference scikitlearn user guide release 0.11 set_params params set parameters estimator 
3214: table 1.150 continued previous page __init__ x.__init__ ... initializes see help type signature class_prior deprecated gaussiannb.class_prior deprecated removed version 0.12. please use gaussiannb.class_prior_ instead 
3215: fit fit gaussian naive bayes according parameters arraylike shape n_samples n_features training vectors n_samples number samples n_features num ber features 
3216: arraylike shape n_samples target values returns self object returns self get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3217: predict perform classication array test vectors 
3218: parameters arraylike shape n_samples n_features returns array shape n_samples predicted target values predict_log_proba return logprobability estimates test vector 
3219: parameters arraylike shape n_samples n_features returns arraylike shape n_samples n_classes returns logprobability sample class model classes ordered arithmetically 
3220: predict_proba return probability estimates test vector 
3221: parameters arraylike shape n_samples n_features returns arraylike shape n_samples n_classes returns probability sample class model classes ordered arithmetically 
3222: score returns mean accuracy given test data labels 
3223: chapter user guide scikitlearn user guide release 0.11 parameters arraylike shape n_samples n_features training set 
3224: arraylike shape n_samples labels 
3225: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sigma deprecated gaussiannb.sigma deprecated removed version 0.12. please use gaussiannb.sigma_ instead 
3226: theta deprecated gaussiannb.theta deprecated removed version 0.12. please use gaussiannb.theta_ instead 
3227: sklearn.naive_bayes.multinomialnb class sklearn.naive_bayes.multinomialnb alpha1.0 t_priortrue naive bayes classier multinomial models multinomial naive bayes classier suitable classication discrete features e.g. word counts text classication multinomial distribution normally requires integer feature counts however practice fractional counts tfidf may also work 
3228: parameters alpha oat optional default1.0 additive laplacelidstone smoothing parameter smoothing 
3229: t_prior boolean whether learn class prior probabilities false uniform prior used 
3230: notes rationale behind names coef_ intercept_ i.e naive bayes linear classier see rennie tackling poor assumptions naive bayes text classiers icml 
3231: examples import numpy np.random.randint size np.array sklearn.naive_bayes import multinomialnb clf multinomialnb clf.fit multinomialnb alpha1.0 fit_priortrue 1.8. reference scikitlearn user guide release 0.11 print clf.predict attributes intercept_ class_log_prior_ fea ture_log_prob_ coef_ array shape n_classes array shape n_classes n_features methods smoothed empirical log probability class 
3232: empirical log probability features given class x_iy intercept_ coef_ properties referring class_log_prior_ feature_log_prob_ respectively fit sample_weight class_prior get_params deep predict predict_log_proba predict_proba score set_params params fit naive bayes classier according get parameters estimator perform classication array test vectors return logprobability estimates test vector return probability estimates test vector returns mean accuracy given test data labels set parameters estimator 
3233: __init__ alpha1.0 t_priortrue fit sample_weightnone class_priornone fit naive bayes classier according parameters arraylike sparse matrix shape n_samples n_features training vectors n_samples number samples n_features num ber features 
3234: arraylike shape n_samples target values 
3235: sample_weight arraylike shape n_samples optional weights applied individual samples unweighted 
3236: class_prior array shape n_classes custom prior probability per class overrides t_prior parameter 
3237: returns self object returns self get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3238: predict perform classication array test vectors 
3239: chapter user guide scikitlearn user guide release 0.11 parameters arraylike shape n_samples n_features returns array shape n_samples predicted target values predict_log_proba return logprobability estimates test vector 
3240: parameters arraylike shape n_samples n_features returns arraylike shape n_samples n_classes returns logprobability sample class model classes ordered arithmetically 
3241: predict_proba return probability estimates test vector 
3242: parameters arraylike shape n_samples n_features returns arraylike shape n_samples n_classes returns probability sample class model classes ordered arithmetically 
3243: score returns mean accuracy given test data labels 
3244: parameters arraylike shape n_samples n_features training set 
3245: arraylike shape n_samples labels 
3246: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.naive_bayes.bernoullinb class sklearn.naive_bayes.bernoullinb alpha1.0 binarize0.0 t_priortrue naive bayes classier multivariate bernoulli models like multinomialnb classier suitable discrete data difference multinomialnb works occurrence counts bernoullinb designed binaryboolean features 
3247: parameters alpha oat optional default1.0 additive laplacelidstone smoothing parameter smoothing 
3248: binarize oat none optional threshold binarizing mapping booleans sample features none input presumed already consist binary vectors 
3249: t_prior boolean 1.8. reference scikitlearn user guide release 0.11 whether learn class prior probabilities false uniform prior used 
3250: references c.d manning raghavan schtze introduction information retrieval cambridge univer sity press 234265. mccallum nigam comparison event models naive bayes text classication proc aaaiicml98 workshop learning text categorization 4148. metsis androutsopoulos paliouras spam ltering naive bayes naive bayes 3rd conf email antispam ceas 
3251: examples import numpy np.random.randint size np.array sklearn.naive_bayes import bernoullinb clf bernoullinb clf.fit bernoullinb alpha1.0 binarize0.0 fit_priortrue print clf.predict attributes class_log_prior_ array shape n_classes fea array shape n_classes ture_log_prob_ n_features log probability class smoothed empirical log probability features given class x_iy 
3252: methods fit sample_weight class_prior get_params deep predict predict_log_proba predict_proba score set_params params fit naive bayes classier according get parameters estimator perform classication array test vectors return logprobability estimates test vector return probability estimates test vector returns mean accuracy given test data labels set parameters estimator 
3253: __init__ alpha1.0 binarize0.0 t_priortrue fit sample_weightnone class_priornone fit naive bayes classier according parameters arraylike sparse matrix shape n_samples n_features training vectors n_samples number samples n_features num ber features 
3254: arraylike shape n_samples chapter user guide scikitlearn user guide release 0.11 target values 
3255: sample_weight arraylike shape n_samples optional weights applied individual samples unweighted 
3256: class_prior array shape n_classes custom prior probability per class overrides t_prior parameter 
3257: returns self object returns self get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3258: predict perform classication array test vectors 
3259: parameters arraylike shape n_samples n_features returns array shape n_samples predicted target values predict_log_proba return logprobability estimates test vector 
3260: parameters arraylike shape n_samples n_features returns arraylike shape n_samples n_classes returns logprobability sample class model classes ordered arithmetically 
3261: predict_proba return probability estimates test vector 
3262: parameters arraylike shape n_samples n_features returns arraylike shape n_samples n_classes returns probability sample class model classes ordered arithmetically 
3263: score returns mean accuracy given test data labels 
3264: parameters arraylike shape n_samples n_features training set 
3265: arraylike shape n_samples labels 
3266: returns oat set_params params set parameters estimator 
3267: 1.8. reference scikitlearn user guide release 0.11 method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self 1.8.21 sklearn.neighbors nearest neighbors sklearn.neighbors module implements knearest neighbors algorithm user guide see nearest neighbors section details 
3268: neighbors.nearestneighbors n_neighbors ... neighbors.kneighborsclassifier ... neighbors.radiusneighborsclassifier ... neighbors.kneighborsregressor n_neighbors ... neighbors.radiusneighborsregressor radius ... regression based neighbors within xed radius neighbors.balltree neighbors.nearestcentroid metric ... unsupervised learner implementing neighbor searches classier implementing knearest neighbors vote classier implementing vote among neighbors within given radius regression based knearest neighbors 
3269: ball tree fast nearestneighbor searches nearest centroid classier 
3270: sklearn.neighbors.nearestneighbors class sklearn.neighbors.nearestneighbors n_neighbors5 leaf_size30 warn_on_equidistanttrue radius1.0 algorithmauto unsupervised learner implementing neighbor searches parameters n_neighbors int optional default number neighbors use default k_neighbors queries 
3271: radius oat optional default 1.0 range parameter space use default methradius_neighbors queries 
3272: algorithm auto ball_tree kd_tree brute optional algorithm used compute nearest neighbors ball_tree use balltree kd_tree use scipy.spatial.ckdtree brute use bruteforce search auto attempt decide appropriate algorithm based values passed fit method 
3273: note tting sparse input override setting parameter using brute force 
3274: leaf_size int optional default leaf size passed balltree ckdtree affect speed construction query well memory required store tree optimal value depends nature problem 
3275: warn_on_equidistant boolean optional defaults true 
3276: generate warning equidistant neighbors discarded classication regres sion based kneighbors neighbor neighbor identical distances chapter user guide scikitlearn user guide release 0.11 different labels result dependent ordering training data method kd_tree warnings generated 
3277: integer optional default parameter minkowski metric sklearn.metrics.pairwise.pairwise_distances equivalent using manhattan_distance euclidean_distance arbitrary minkowski_distance l_p used 
3278: see also kneighborsclassifier radiusneighborsregressor balltree radiusneighborsclassifier kneighborsregressor notes see nearest neighbors online documentation discussion choice algorithm leaf_size http en.wikipedia.orgwikiknearest_neighbor_algorithm examples sklearn.neighbors import nearestneighbors samples neigh nearestneighbors 0.4 neigh.fit samples nearestneighbors ... neigh.kneighbors 1.3 return_distancefalse array neigh.radius_neighbors 1.3 0.4 return_distancefalse array methods fit get_params deep kneighbors n_neighbors return_distance kneighbors_graph n_neighbors mode radius_neighbors radius return_distance radius_neighbors_graph radius mode set_params params fit model using training data get parameters estimator finds kneighbors point computes weighted graph kneighbors points finds neighbors point within given radius computes weighted graph neighbors points set parameters estimator 
3279: __init__ n_neighbors5 radius1.0 algorithmauto leaf_size30 warn_on_equidistanttrue fit ynone fit model using training data parameters arraylike sparse matrix balltree ckdtree 1.8. reference scikitlearn user guide release 0.11 training data array matrix shape n_samples n_features get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3280: kneighbors n_neighborsnone return_distancetrue finds kneighbors point returns distance parameters arraylike last dimension data new point n_neighbors int number neighbors get default value passed constructor 
3281: return_distance boolean optional defaults true 
3282: false distances returned returns dist array array representing lengths point present return_distancetrue ind array indices nearest points population matrix 
3283: examples following example construct neighborsclassier class array representing data set ask whos closest point samples sklearn.neighbors import nearestneighbors neigh nearestneighbors n_neighbors1 neigh.fit samples nearestneighbors algorithmauto leaf_size30 ... print neigh.kneighbors array 0.5 array ... see returns 0.5 means element distance 0.5 third element samples indexes start also query multiple points neigh.kneighbors return_distancefalse array ... kneighbors_graph n_neighborsnone modeconnectivity computes weighted graph kneighbors points parameters arraylike shape n_samples n_features sample data n_neighbors int chapter user guide scikitlearn user guide release 0.11 number neighbors sample default value passed constructor 
3284: mode connectivity distance optional type returned matrix connectivity return connectivity matrix ones zeros distance edges euclidean distance points returns sparse matrix csr format shape n_samples n_samples_t n_samples_t number samples tted data assigned weight edge connects 
3285: see also nearestneighbors.radius_neighbors_graph examples sklearn.neighbors import nearestneighbors neigh nearestneighbors n_neighbors2 neigh.fit nearestneighbors algorithmauto leaf_size30 ... neigh.kneighbors_graph a.todense matrix radius_neighbors radiusnone return_distancetrue finds neighbors point within given radius returns distance parameters arraylike last dimension data new point 
3286: radius oat limiting distance neighbors return default value passed constructor 
3287: return_distance boolean optional defaults true 
3288: false distances returned returns dist array array representing lengths point present return_distancetrue ind array indices nearest points population matrix 
3289: examples following example construnct neighborsclassier class array representing data set ask whos closest point 1.8. reference scikitlearn user guide release 0.11 samples sklearn.neighbors import nearestneighbors neigh nearestneighbors radius1.6 neigh.fit samples nearestneighbors algorithmauto leaf_size30 ... print neigh.radius_neighbors array 1.5 0.5 ... array ... rst array returned contains distances points closer 1.6 second array returned contains indices general multiple points queried time number neighbors point necessarily equal radius_neighbors returns array objects object array indices 
3290: radius_neighbors_graph radiusnone modeconnectivity computes weighted graph neighbors points neighborhoods restricted points distance lower radius 
3291: parameters arraylike shape n_samples n_features sample data radius oat radius neighborhoods default value passed constructor 
3292: mode connectivity distance optional type returned matrix connectivity return connectivity matrix ones zeros distance edges euclidean distance points 
3293: returns sparse matrix csr format shape n_samples n_samples assigned weight edge connects 
3294: see also kneighbors_graph examples sklearn.neighbors import nearestneighbors neigh nearestneighbors radius1.5 neigh.fit nearestneighbors algorithmauto leaf_size30 ... neigh.radius_neighbors_graph a.todense matrix set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self chapter user guide sklearn.neighbors.kneighborsclassier scikitlearn user guide release 0.11 class sklearn.neighbors.kneighborsclassifier n_neighbors5 algorithmauto warn_on_equidistanttrue weightsuniform leaf_size30 classier implementing knearest neighbors vote 
3295: parameters n_neighbors int optional default number neighbors use default k_neighbors queries 
3296: weights str callable weight function used prediction possible values uniform uniform weights points neighborhood weighted equally distance weight points inverse distance case closer neigh bors query point greater inuence neighbors away callable userdened function accepts array distances returns array shape containing weights 
3297: uniform weights used default 
3298: algorithm auto ball_tree kd_tree brute optional algorithm used compute nearest neighbors ball_tree use balltree kd_tree use scipy.spatial.ckdtree brute use bruteforce search auto attempt decide appropriate algorithm based values passed fit method 
3299: note tting sparse input override setting parameter using brute force 
3300: leaf_size int optional default leaf size passed balltree ckdtree affect speed construction query well memory required store tree optimal value depends nature problem 
3301: warn_on_equidistant boolean optional defaults true 
3302: generate warning equidistant neighbors discarded classication regres sion based kneighbors neighbor neighbor identical distances different labels result dependent ordering training data method kd_tree warnings generated 
3303: integer optional default parameter minkowski metric sklearn.metrics.pairwise.pairwise_distances equivalent using manhattan_distance euclidean_distance arbitrary minkowski_distance l_p used 
3304: see also radiusneighborsclassifier nearestneighbors kneighborsregressor radiusneighborsregressor 1.8. reference scikitlearn user guide release 0.11 notes see nearest neighbors online documentation discussion choice algorithm leaf_size http en.wikipedia.orgwikiknearest_neighbor_algorithm examples sklearn.neighbors import kneighborsclassifier neigh kneighborsclassifier n_neighbors2 neigh.fit kneighborsclassifier ... print neigh.predict 1.5 methods fit get_params deep kneighbors n_neighbors return_distance kneighbors_graph n_neighbors mode predict score set_params params fit model using training data target values get parameters estimator finds kneighbors point computes weighted graph kneighbors points predict class labels provided data returns mean accuracy given test data labels set parameters estimator 
3305: __init__ n_neighbors5 warn_on_equidistanttrue weightsuniform algorithmauto leaf_size30 fit fit model using training data target values parameters arraylike sparse matrix balltree ckdtree training data array matrix shape n_samples n_features arraylike sparse matrix shape n_samples target values array integer values 
3306: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3307: kneighbors n_neighborsnone return_distancetrue finds kneighbors point returns distance parameters arraylike last dimension data chapter user guide scikitlearn user guide release 0.11 new point n_neighbors int number neighbors get default value passed constructor 
3308: return_distance boolean optional defaults true 
3309: false distances returned returns dist array array representing lengths point present return_distancetrue ind array indices nearest points population matrix 
3310: examples following example construct neighborsclassier class array representing data set ask whos closest point samples sklearn.neighbors import nearestneighbors neigh nearestneighbors n_neighbors1 neigh.fit samples nearestneighbors algorithmauto leaf_size30 ... print neigh.kneighbors array 0.5 array ... see returns 0.5 means element distance 0.5 third element samples indexes start also query multiple points neigh.kneighbors return_distancefalse array ... kneighbors_graph n_neighborsnone modeconnectivity computes weighted graph kneighbors points parameters arraylike shape n_samples n_features sample data n_neighbors int number neighbors sample default value passed constructor 
3311: mode connectivity distance optional type returned matrix connectivity return connectivity matrix ones zeros distance edges euclidean distance points returns sparse matrix csr format shape n_samples n_samples_t n_samples_t number samples tted data assigned weight edge connects 
3312: see also nearestneighbors.radius_neighbors_graph 1.8. reference scikitlearn user guide release 0.11 examples sklearn.neighbors import nearestneighbors neigh nearestneighbors n_neighbors2 neigh.fit nearestneighbors algorithmauto leaf_size30 ... neigh.kneighbors_graph a.todense matrix predict predict class labels provided data parameters array array representing test points 
3313: returns labels array list class labels one data sample 
3314: score returns mean accuracy given test data labels 
3315: parameters arraylike shape n_samples n_features training set 
3316: arraylike shape n_samples labels 
3317: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.neighbors.radiusneighborsclassier class sklearn.neighbors.radiusneighborsclassifier radius1.0 weightsuniform algo rithmauto leaf_size30 lier_labelnone classier implementing vote among neighbors within given radius parameters radius oat optional default 1.0 range parameter space use default methradius_neighbors queries 
3318: weights str callable weight function used prediction possible values uniform uniform weights points neighborhood weighted equally 
3319: chapter user guide scikitlearn user guide release 0.11 distance weight points inverse distance case closer neigh bors query point greater inuence neighbors away callable userdened function accepts array distances returns array shape containing weights 
3320: uniform weights used default 
3321: algorithm auto ball_tree kd_tree brute optional algorithm used compute nearest neighbors ball_tree use balltree kd_tree use scipy.spatial.ckdtree brute use bruteforce search auto attempt decide appropriate algorithm based values passed fit method 
3322: note tting sparse input override setting parameter using brute force 
3323: leaf_size int optional default leaf size passed balltree ckdtree affect speed construction query well memory required store tree optimal value depends nature problem 
3324: integer optional default parameter minkowski metric sklearn.metrics.pairwise.pairwise_distances equivalent using manhattan_distance euclidean_distance arbitrary minkowski_distance l_p used 
3325: outlier_label int optional default none label given outlier samples samples neighbors given radius set none valueerror raised outlier detected 
3326: see also kneighborsclassifier nearestneighbors notes radiusneighborsregressor kneighborsregressor see nearest neighbors online documentation discussion choice algorithm leaf_size http en.wikipedia.orgwikiknearest_neighbor_algorithm examples sklearn.neighbors import radiusneighborsclassifier neigh radiusneighborsclassifier radius1.0 neigh.fit radiusneighborsclassifier ... 1.8. reference scikitlearn user guide release 0.11 print neigh.predict 1.5 methods fit get_params deep predict radius_neighbors radius return_distance radius_neighbors_graph radius mode score set_params params fit model using training data target values get parameters estimator predict class labels provided data finds neighbors point within given radius computes weighted graph neighbors points returns mean accuracy given test data labels set parameters estimator 
3327: __init__ radius1.0 weightsuniform algorithmauto leaf_size30 outlier_labelnone fit fit model using training data target values parameters arraylike sparse matrix balltree ckdtree training data array matrix shape n_samples n_features arraylike sparse matrix shape n_samples target values array integer values 
3328: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3329: predict predict class labels provided data parameters array array representing test points 
3330: returns labels array list class labels one data sample 
3331: radius_neighbors radiusnone return_distancetrue finds neighbors point within given radius returns distance parameters arraylike last dimension data new point 
3332: radius oat limiting distance neighbors return default value passed constructor 
3333: return_distance boolean optional defaults true 
3334: false distances returned chapter user guide scikitlearn user guide release 0.11 returns dist array array representing lengths point present return_distancetrue ind array indices nearest points population matrix 
3335: examples following example construnct neighborsclassier class array representing data set ask whos closest point samples sklearn.neighbors import nearestneighbors neigh nearestneighbors radius1.6 neigh.fit samples nearestneighbors algorithmauto leaf_size30 ... print neigh.radius_neighbors array 1.5 0.5 ... array ... rst array returned contains distances points closer 1.6 second array returned contains indices general multiple points queried time number neighbors point necessarily equal radius_neighbors returns array objects object array indices 
3336: radius_neighbors_graph radiusnone modeconnectivity computes weighted graph neighbors points neighborhoods restricted points distance lower radius 
3337: parameters arraylike shape n_samples n_features sample data radius oat radius neighborhoods default value passed constructor 
3338: mode connectivity distance optional type returned matrix connectivity return connectivity matrix ones zeros distance edges euclidean distance points 
3339: returns sparse matrix csr format shape n_samples n_samples assigned weight edge connects 
3340: see also kneighbors_graph examples sklearn.neighbors import nearestneighbors neigh nearestneighbors radius1.5 neigh.fit nearestneighbors algorithmauto leaf_size30 ... neigh.radius_neighbors_graph a.todense 1.8. reference scikitlearn user guide release 0.11 matrix score returns mean accuracy given test data labels 
3341: parameters arraylike shape n_samples n_features training set 
3342: arraylike shape n_samples labels 
3343: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.neighbors.kneighborsregressor class sklearn.neighbors.kneighborsregressor n_neighbors5 algorithmauto warn_on_equidistanttrue weightsuniform leaf_size30 regression based knearest neighbors target predicted local interpolation targets associated nearest neighbors training set 
3344: parameters n_neighbors int optional default number neighbors use default k_neighbors queries 
3345: weights str callable weight function used prediction possible values uniform uniform weights points neighborhood weighted equally distance weight points inverse distance case closer neigh bors query point greater inuence neighbors away callable userdened function accepts array distances returns array shape containing weights 
3346: uniform weights used default 
3347: algorithm auto ball_tree kd_tree brute optional algorithm used compute nearest neighbors ball_tree use balltree kd_tree use scipy.spatial.ckdtree brute use bruteforce search 
3348: chapter user guide scikitlearn user guide release 0.11 auto attempt decide appropriate algorithm based values passed fit method 
3349: note tting sparse input override setting parameter using brute force 
3350: leaf_size int optional default leaf size passed balltree ckdtree affect speed construction query well memory required store tree optimal value depends nature problem 
3351: warn_on_equidistant boolean optional defaults true 
3352: generate warning equidistant neighbors discarded classication regres sion based kneighbors neighbor neighbor identical distances different labels result dependent ordering training data method kd_tree warnings generated 
3353: integer optional default parameter minkowski metric sklearn.metrics.pairwise.pairwise_distances equivalent using manhattan_distance euclidean_distance arbitrary minkowski_distance l_p used 
3354: see also nearestneighbors radiusneighborsclassifier radiusneighborsregressor kneighborsclassifier notes see nearest neighbors online documentation discussion choice algorithm leaf_size http en.wikipedia.orgwikiknearest_neighbor_algorithm examples sklearn.neighbors import kneighborsregressor neigh kneighborsregressor n_neighbors2 neigh.fit kneighborsregressor ... print neigh.predict 1.5 0.5 methods fit get_params deep kneighbors n_neighbors return_distance kneighbors_graph n_neighbors mode predict fit model using training data target values get parameters estimator finds kneighbors point computes weighted graph kneighbors points predict target provided data 1.8. reference continued next page scikitlearn user guide release 0.11 score set_params params returns coefcient determination prediction set parameters estimator 
3355: table 1.157 continued previous page __init__ n_neighbors5 warn_on_equidistanttrue weightsuniform algorithmauto leaf_size30 fit fit model using training data target values parameters arraylike sparse matrix balltree ckdtree training data array matrix shape n_samples n_features arraylike sparse matrix shape n_samples target values array oat values 
3356: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3357: kneighbors n_neighborsnone return_distancetrue finds kneighbors point returns distance parameters arraylike last dimension data new point n_neighbors int number neighbors get default value passed constructor 
3358: return_distance boolean optional defaults true 
3359: false distances returned returns dist array array representing lengths point present return_distancetrue ind array indices nearest points population matrix 
3360: examples following example construct neighborsclassier class array representing data set ask whos closest point samples sklearn.neighbors import nearestneighbors neigh nearestneighbors n_neighbors1 neigh.fit samples nearestneighbors algorithmauto leaf_size30 ... print neigh.kneighbors array 0.5 array ... chapter user guide scikitlearn user guide release 0.11 see returns 0.5 means element distance 0.5 third element samples indexes start also query multiple points neigh.kneighbors return_distancefalse array ... kneighbors_graph n_neighborsnone modeconnectivity computes weighted graph kneighbors points parameters arraylike shape n_samples n_features sample data n_neighbors int number neighbors sample default value passed constructor 
3361: mode connectivity distance optional type returned matrix connectivity return connectivity matrix ones zeros distance edges euclidean distance points returns sparse matrix csr format shape n_samples n_samples_t n_samples_t number samples tted data assigned weight edge connects 
3362: see also nearestneighbors.radius_neighbors_graph examples sklearn.neighbors import nearestneighbors neigh nearestneighbors n_neighbors2 neigh.fit nearestneighbors algorithmauto leaf_size30 ... neigh.kneighbors_graph a.todense matrix predict predict target provided data parameters array array representing test data 
3363: returns array list target values one data sample 
3364: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
3365: 1.8. reference scikitlearn user guide release 0.11 parameters arraylike shape n_samples n_features training set 
3366: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.neighbors.radiusneighborsregressor class sklearn.neighbors.radiusneighborsregressor radius1.0 weightsuniform algo regression based neighbors within xed radius target predicted local interpolation targets associated nearest neighbors training set 
3367: rithmauto leaf_size30 parameters radius oat optional default 1.0 range parameter space use default methradius_neighbors queries 
3368: weights str callable weight function used prediction possible values uniform uniform weights points neighborhood weighted equally distance weight points inverse distance case closer neigh bors query point greater inuence neighbors away callable userdened function accepts array distances returns array shape containing weights 
3369: uniform weights used default 
3370: algorithm auto ball_tree kd_tree brute optional algorithm used compute nearest neighbors ball_tree use balltree kd_tree use scipy.spatial.ckdtree brute use bruteforce search auto attempt decide appropriate algorithm based values passed fit method 
3371: note tting sparse input override setting parameter using brute force 
3372: leaf_size int optional default leaf size passed balltree ckdtree affect speed construction query well memory required store tree optimal value depends nature problem 
3373: integer optional default chapter user guide scikitlearn user guide release 0.11 parameter minkowski metric sklearn.metrics.pairwise.pairwise_distances equivalent using manhattan_distance euclidean_distance arbitrary minkowski_distance l_p used 
3374: see also nearestneighbors radiusneighborsclassifier notes kneighborsregressor kneighborsclassifier see nearest neighbors online documentation discussion choice algorithm leaf_size http en.wikipedia.orgwikiknearest_neighbor_algorithm examples sklearn.neighbors import radiusneighborsregressor neigh radiusneighborsregressor radius1.0 neigh.fit radiusneighborsregressor ... print neigh.predict 1.5 0.5 methods fit get_params deep predict radius_neighbors radius return_distance radius_neighbors_graph radius mode score set_params params fit model using training data target values get parameters estimator predict target provided data finds neighbors point within given radius computes weighted graph neighbors points returns coefcient determination prediction set parameters estimator 
3375: __init__ radius1.0 weightsuniform algorithmauto leaf_size30 fit fit model using training data target values parameters arraylike sparse matrix balltree ckdtree training data array matrix shape n_samples n_features arraylike sparse matrix shape n_samples target values array oat values 
3376: get_params deeptrue get parameters estimator parameters deep boolean optional 1.8. reference scikitlearn user guide release 0.11 true return parameters estimator contained subobjects estimators 
3377: predict predict target provided data parameters array array representing test data 
3378: returns array list target values one data sample 
3379: radius_neighbors radiusnone return_distancetrue finds neighbors point within given radius returns distance parameters arraylike last dimension data new point 
3380: radius oat limiting distance neighbors return default value passed constructor 
3381: return_distance boolean optional defaults true 
3382: false distances returned returns dist array array representing lengths point present return_distancetrue ind array indices nearest points population matrix 
3383: examples following example construnct neighborsclassier class array representing data set ask whos closest point samples sklearn.neighbors import nearestneighbors neigh nearestneighbors radius1.6 neigh.fit samples nearestneighbors algorithmauto leaf_size30 ... print neigh.radius_neighbors array 1.5 0.5 ... array ... rst array returned contains distances points closer 1.6 second array returned contains indices general multiple points queried time number neighbors point necessarily equal radius_neighbors returns array objects object array indices 
3384: radius_neighbors_graph radiusnone modeconnectivity computes weighted graph neighbors points neighborhoods restricted points distance lower radius 
3385: parameters arraylike shape n_samples n_features sample data chapter user guide scikitlearn user guide release 0.11 radius oat radius neighborhoods default value passed constructor 
3386: mode connectivity distance optional type returned matrix connectivity return connectivity matrix ones zeros distance edges euclidean distance points 
3387: returns sparse matrix csr format shape n_samples n_samples assigned weight edge connects 
3388: see also kneighbors_graph examples sklearn.neighbors import nearestneighbors neigh nearestneighbors radius1.5 neigh.fit nearestneighbors algorithmauto leaf_size30 ... neigh.radius_neighbors_graph a.todense matrix score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
3389: parameters arraylike shape n_samples n_features training set 
3390: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.neighbors.balltree class sklearn.neighbors.balltree ball tree fast nearestneighbor searches balltree leaf_size20 p2.0 parameters arraylike shape n_samples n_features 1.8. reference scikitlearn user guide release 0.11 n_samples number points data set n_features dimension parameter space note ccontiguous array doubles data copied otherwise internal copy made 
3391: leaf_size positive integer default number points switch bruteforce changing leaf_size fect results query signicantly impact speed query memory required store built ball tree amount memory needed store tree scales oor log2 n_samples leaf_size specied leaf_size leaf node guaranteed satisfy leaf_size n_points leaf_size except case n_samples leaf_size 
3392: distance metric balltree encodes minkowski pdistance sum must greater equal triangle inequality hold np.inf distance equivalent max examples query knearest neighbors import numpy np.random.seed np.random.random points dimensions ball_tree balltree leaf_size2 dist ind ball_tree.query n_neighbors3 print ind indices closest neighbors print dist distances closest neighbors 
3393: 0.19662693 0.29473397 pickle unpickle ball tree using protocol note state tree saved pickle operation tree rebuilt unpickling import numpy import pickle np.random.seed np.random.random points dimensions ball_tree balltree leaf_size2 pickle.dumps ball_tree protocol2 ball_tree_copy pickle.loads dist ind ball_tree_copy.query print ind indices closest neighbors print dist distances closest neighbors 
3394: 0.19662693 0.29473397 attributes chapter user guide scikitlearn user guide release 0.11 data warning_flag methods query return_distance query_radius query ball tree nearest neighbors query_radius self count_only false __init__ x.__init__ ... initializes see help type signature query return_distancetrue query ball tree nearest neighbors parameters arraylike last dimension self.dim array points query integer default number nearest neighbors return return_distance boolean default true true return tuple false return array returns return_distance false return_distance true array doubles shape x.shape entry gives list distances neighbors corresponding point note distances sorted array integers shape x.shape entry gives list indices neighbors corresponding point note neighbors sorted examples query knearest neighbors import numpy np.random.seed np.random.random points dimensions ball_tree balltree leaf_size2 dist ind ball_tree.query print ind indices closest neighbors print dist distances closest neighbors 
3395: 0.19662693 0.29473397 query_radius query_radius self count_only false query ball tree neighbors within ball size 1.8. reference scikitlearn user guide release 0.11 parameters arraylike last dimension self.dim array points query distance within neighbors returned single value array values shape x.shape different radii desired point 
3396: return_distance boolean default false true return distances neighbors point false return neighbors note unlike balltree.query setting return_distancetrue adds computation time distances need calculated explicitly return_distancefalse results sorted default see sort_results keyword 
3397: count_only boolean default false true return count points within distance false return indices points within distance return_distancetrue setting count_onlytrue result error 
3398: sort_results boolean default false true distances indices sorted returned false results sorted return_distance false setting sort_results true result error 
3399: returns count count_only true ind count_only false return_distance false ind dist count_only false return_distance true count array integers shape x.shape entry gives number neighbors within distance corresponding point 
3400: ind array objects shape x.shape element numpy integer array listing indices neighbors correspond ing point note unlike results balltree.query returned neighbors sorted distance dist array objects shape x.shape element numpy double array listing distances corresponding indices 
3401: examples query neighbors given radius import numpy np.random.seed np.random.random points dimensions ball_tree balltree leaf_size2 print ball_tree.query_radius r0.3 count_onlytrue ind ball_tree.query_radius r0.3 print ind indices neighbors within distance 0.3 chapter user guide scikitlearn user guide release 0.11 sklearn.neighbors.nearestcentroid class sklearn.neighbors.nearestcentroid metriceuclidean shrink_thresholdnone nearest centroid classier class represented centroid test samples classied class nearest centroid 
3402: parameters metric string callable metric use calculating distance instances feature array metric string callable must one options allowed met rics.pairwise.pairwise_distances metric parameter 
3403: shrink_threshold oat optional threshold shrinking centroids remove features 
3404: see also sklearn.neighbors.kneighborsclassifiernearest neighbors classier notes used text classication tfidf vectors classier also known rocchio classier 
3405: references tibshirani hastie narasimhan chu diagnosis multiple cancer types shrunken centroids gene expression proceedings national academy sciences united states america 65676572. national academy sciences 
3406: examples sklearn.neighbors.nearest_centroid import nearestcentroid import numpy np.array np.array clf nearestcentroid clf.fit nearestcentroid metriceuclidean shrink_thresholdnone print clf.predict 0.8 attributes centroids_ arraylike shape n_classes n_features centroid class methods fit get_params deep fit nearestcentroid model according given training data get parameters estimator continued next page 1.8. reference scikitlearn user guide release 0.11 table 1.161 continued previous page predict score set_params params perform classication array test vectors returns mean accuracy given test data labels set parameters estimator 
3407: __init__ metriceuclidean shrink_thresholdnone fit fit nearestcentroid model according given training data 
3408: parameters arraylike sparse matrix shape n_samples n_features training vector n_samples number samples n_features num ber features note centroid shrinking used sparse matrices 
3409: array shape n_samples target values integers get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3410: predict perform classication array test vectors predicted class sample returned 
3411: parameters arraylike shape n_samples n_features returns array shape n_samples notes metric constructor parameter precomputed assumed distance matrix data predicted self.centroids_ 
3412: score returns mean accuracy given test data labels 
3413: parameters arraylike shape n_samples n_features training set 
3414: arraylike shape n_samples labels 
3415: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self chapter user guide scikitlearn user guide release 0.11 neighbors.kneighbors_graph n_neighbors ... computes weighted graph kneighbors points neighbors.radius_neighbors_graph radius computes weighted graph neighbors points sklearn.neighbors.kneighbors_graph sklearn.neighbors.kneighbors_graph n_neighbors modeconnectivity computes weighted graph kneighbors points parameters arraylike balltree shape n_samples n_features sample data form numpy array precomputed balltree 
3416: n_neighbors int number neighbors sample 
3417: mode connectivity distance optional type returned matrix connectivity return connectivity matrix ones zeros distance edges euclidean distance points 
3418: returns sparse matrix csr format shape n_samples n_samples assigned weight edge connects 
3419: see also radius_neighbors_graph examples sklearn.neighbors import kneighbors_graph kneighbors_graph a.todense matrix sklearn.neighbors.radius_neighbors_graph sklearn.neighbors.radius_neighbors_graph radius modeconnectivity computes weighted graph neighbors points neighborhoods restricted points distance lower radius 
3420: parameters arraylike balltree shape n_samples n_features sample data form numpy array precomputed balltree 
3421: radius oat radius neighborhoods 
3422: mode connectivity distance optional type returned matrix connectivity return connectivity matrix ones zeros distance edges euclidean distance points 
3423: 1.8. reference scikitlearn user guide release 0.11 returns sparse matrix csr format shape n_samples n_samples assigned weight edge connects 
3424: see also kneighbors_graph examples sklearn.neighbors import radius_neighbors_graph radius_neighbors_graph 1.5 a.todense matrix 1.8.22 sklearn.pls partial least squares sklearn.pls module implements partial least squares pls user guide see partial least squares section details 
3425: pls.plsregression n_components scale ... pls.plscanonical n_components scale ... pls.cca n_components scale max_iter ... pls.plssvd n_components scale copy pls regression plscanonical implements blocks canonical pls original wold cca canonical correlation analysis cca inherits pls partial least square svd sklearn.pls.plsregression class sklearn.pls.plsregression n_components2 scaletrue max_iter500 tol1e06 copytrue pls regression plsregression implements pls blocks regression known pls2 pls1 case one dimensional response class inherits _pls modea deation_moderegression norm_y_weightsfalse algorithmnipals 
3426: parameters arraylike predictors shape n_samples training vectors n_samples number samples number predictors 
3427: arraylike response shape n_samples training vectors n_samples number samples number response variables 
3428: n_components int default number components keep 
3429: scale boolean default true whether scale data max_iter integer default chapter user guide scikitlearn user guide release 0.11 maximum number iterations nipals inner loop used algo rithmnipals tol nonnegative real tolerance used iterative algorithm default 1e06 
3430: copy boolean default true whether deation done copy let default value true unless dont care side effect notes component weights optimizes max corr var var note maximizes correlations scores intrablock variances residual matrix xk1 block obtained deation current score x_score residual matrix yk1 block obtained deation current score performs pls regression known pls2 mode prediction oriented implementation provides results pls packages provided language rproject mixomics function pls mode regression plspm function plsreg2 pls function oscorespls.t references jacob wegelin survey partial least squares pls methods emphasis twoblock case technical report department statistics university washington seattle 2000. french still reference tenenhaus regression pls theorie pratique paris editions technic 
3431: examples sklearn.pls import plscanonical plsregression cca 1.0.0 2.2.2 2.5.4 0.1 0.2 0.9 1.1 6.2 5.9 11.9 12.3 pls2 plsregression n_components2 pls2.fit ... plsregression copytrue max_iter500 n_components2 scaletrue tol1e06 y_pred pls2.predict 1.8. reference scikitlearn user guide release 0.11 attributes x_weights_ y_weights_ x_loadings_ y_loadings_ x_scores_ y_scores_ x_rotations_ y_rotations_ coefs array methods array n_components array n_components array n_components array n_components array n_samples n_components scores array n_samples n_components scores array n_components array n_components block weights vectors block weights vectors block loadings vectors block loadings vectors 
3432: block latents rotations block latents rotations coecients linear model coefs err fit get_params deep predict copy set_params params transform copy apply dimension reduction learned train data 
3433: get parameters estimator apply dimension reduction learned train data set parameters estimator 
3434: __init__ n_components2 scaletrue max_iter500 tol1e06 copytrue get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators predict copytrue apply dimension reduction learned train data 
3435: parameters arraylike predictors shape n_samples training vectors n_samples number samples number predictors 
3436: copy boolean whether copy perform inplace normalization 
3437: notes call require estimation matrix may issue high dimensional space 
3438: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self chapter user guide scikitlearn user guide release 0.11 transform ynone copytrue apply dimension reduction learned train data 
3439: parameters arraylike predictors shape n_samples training vectors n_samples number samples number predictors 
3440: arraylike response shape n_samples optional training vectors n_samples number samples number response variables 
3441: copy boolean whether copy perform inplace normalization 
3442: returns x_scores given x_scores y_scores otherwise sklearn.pls.plscanonical class sklearn.pls.plscanonical n_components2 scaletrue algorithmnipals max_iter500 tol1e06 copytrue plscanonical implements blocks canonical pls original wold algorithm tenenhaus p.204 refered plsc2a wegelin class inherits pls modea deation_modecanonical norm_y_weightstrue gorithmnipals svd provide similar results numerical errors 
3443: parameters arraylike predictors shape n_samples training vectors n_samples number samples number predictors 
3444: arraylike response shape n_samples training vectors n_samples number samples number response variables 
3445: n_components int number components keep default scale boolean scale data default true algorithm string nipals svd algorithm used estimate weights called n_components times i.e iteration outer loop 
3446: max_iter integer default maximum number iterations nipals inner loop used algo rithmnipals tol nonnegative real default 1e06 tolerance used iterative algorithm copy boolean default true whether deation done copy let default value true unless dont care side effect see also cca plssvd 1.8. reference scikitlearn user guide release 0.11 notes component weights optimize max corr var var note maximizes correlations scores intrablock variances residual matrix xk1 block obtained deation current score x_score residual matrix yk1 block obtained deation current score performs canonical symetric version pls regression slightly different cca mode mostly used modeling implementation provides results plspm package provided language project using function plsca results equal colinear function pls ... mode canonical mixomics package difference relies fact mixomics implmentation exactly implement wold algorithm since normalize y_weights one 
3447: references jacob wegelin survey partial least squares pls methods emphasis twoblock case technical report department statistics university washington seattle 2000. tenenhaus regression pls theorie pratique paris editions technic 
3448: examples sklearn.pls import plscanonical plsregression cca 1.0.0 2.2.2 2.5.4 0.1 0.2 0.9 1.1 6.2 5.9 11.9 12.3 plsca plscanonical n_components2 plsca.fit ... plscanonical algorithmnipals copytrue max_iter500 n_components2 x_c y_c plsca.transform scaletrue tol1e06 attributes x_weights_ y_weights_ x_loadings_ y_loadings_ x_scores_ y_scores_ x_rotations_ y_rotations_ methods array shape n_components array shape n_components array shape n_components array shape n_components array shape n_samples n_components scores array shape n_samples n_components scores array shape n_components array shape n_components block weights vectors block weights vectors block loadings vectors block loadings vectors 
3449: block latents rotations block latents rotations 
3450: fit continued next page chapter user guide scikitlearn user guide release 0.11 table 1.165 continued previous page get_params deep predict copy set_params params transform copy apply dimension reduction learned train data 
3451: get parameters estimator apply dimension reduction learned train data set parameters estimator 
3452: __init__ n_components2 scaletrue algorithmnipals max_iter500 tol1e06 copytrue get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators predict copytrue apply dimension reduction learned train data 
3453: parameters arraylike predictors shape n_samples training vectors n_samples number samples number predictors 
3454: copy boolean whether copy perform inplace normalization 
3455: notes call require estimation matrix may issue high dimensional space 
3456: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform ynone copytrue apply dimension reduction learned train data 
3457: parameters arraylike predictors shape n_samples training vectors n_samples number samples number predictors 
3458: arraylike response shape n_samples optional training vectors n_samples number samples number response variables 
3459: copy boolean whether copy perform inplace normalization 
3460: returns x_scores given x_scores y_scores otherwise 1.8. reference scikitlearn user guide release 0.11 sklearn.pls.cca class sklearn.pls.cca n_components2 scaletrue max_iter500 tol1e06 copytrue pls modeb dea cca canonical correlation analysis tion_modecanonical 
3461: cca inherits parameters arraylike predictors shape n_samples training vectors n_samples number samples number predictors 
3462: arraylike response shape n_samples training vectors n_samples number samples number response variables 
3463: n_components int default 
3464: number components keep 
3465: scale boolean default true whether scale data max_iter integer default maximum number iterations nipals inner loop used algo rithmnipals tol nonnegative real default 1e06 
3466: tolerance used iterative algorithm copy boolean whether deation done copy let default value true unless dont care side effects see also plscanonical plssvd notes component weights maximizes max corr note maximizes correlations scores residual matrix xk1 block obtained deation current score x_score residual matrix yk1 block obtained deation current score 
3467: references jacob wegelin survey partial least squares pls methods emphasis twoblock case technical report department statistics university washington seattle 2000. french still reference tenenhaus regression pls theorie pratique paris editions technic 
3468: chapter user guide scikitlearn user guide release 0.11 examples sklearn.pls import plscanonical plsregression cca 1.0.0 2.2.2 3.5.4 0.1 0.2 0.9 1.1 6.2 5.9 11.9 12.3 cca cca n_components1 cca.fit ... cca copytrue max_iter500 n_components1 scaletrue tol1e06 x_c y_c cca.transform attributes x_weights_ y_weights_ x_loadings_ y_loadings_ x_scores_ y_scores_ x_rotations_ y_rotations_ methods array n_components array n_components array n_components array n_components array n_samples n_components scores array n_samples n_components scores array n_components array n_components block weights vectors block weights vectors block loadings vectors block loadings vectors 
3469: block latents rotations block latents rotations 
3470: fit get_params deep predict copy set_params params transform copy apply dimension reduction learned train data 
3471: get parameters estimator apply dimension reduction learned train data set parameters estimator 
3472: __init__ n_components2 scaletrue max_iter500 tol1e06 copytrue get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators predict copytrue apply dimension reduction learned train data 
3473: parameters arraylike predictors shape n_samples training vectors n_samples number samples number predictors 
3474: copy boolean whether copy perform inplace normalization 
3475: 1.8. reference scikitlearn user guide release 0.11 notes call require estimation matrix may issue high dimensional space 
3476: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform ynone copytrue apply dimension reduction learned train data 
3477: parameters arraylike predictors shape n_samples training vectors n_samples number samples number predictors 
3478: arraylike response shape n_samples optional training vectors n_samples number samples number response variables 
3479: copy boolean whether copy perform inplace normalization 
3480: returns x_scores given x_scores y_scores otherwise sklearn.pls.plssvd class sklearn.pls.plssvd n_components2 scaletrue copytrue partial least square svd simply perform svd crosscovariance matrix iterative deation 
3481: parameters arraylike predictors shape n_samples training vector n_samples number samples number predictors centered analysis 
3482: arraylike response shape n_samples training vector n_samples number samples number response variables centered analysis 
3483: n_components int default 
3484: number components keep 
3485: scale boolean default true scale see also plscanonical cca chapter user guide scikitlearn user guide release 0.11 attributes x_weights_ y_weights_ x_scores_ y_scores_ array n_components array n_components array n_samples n_components scores array n_samples n_components scores 
3486: block weights vectors block weights vectors 
3487: methods fit get_params deep set_params params transform get parameters estimator set parameters estimator apply dimension reduction learned train data 
3488: __init__ n_components2 scaletrue copytrue get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform ynone apply dimension reduction learned train data 
3489: 1.8.23 sklearn.pipeline pipeline sklearn.pipeline module implements utilites build composite estimator chain transforms estimators 
3490: pipeline.pipeline steps pipeline transforms nal estimator 
3491: sklearn.pipeline.pipeline class sklearn.pipeline.pipeline steps pipeline transforms nal estimator sequentially apply list transforms nal estimator intermediate steps pipeline must trans forms must implements transform methods nal estimator needs implements purpose pipeline assemble several steps crossvalidated together setting differ ent parameters enables setting parameters various steps using names parameter 1.8. reference scikitlearn user guide release 0.11 name separated example 
3492: parameters steps list list name transform tuples implementing ttransform chained order chained last object estimator 
3493: examples sklearn import svm sklearn.datasets import samples_generator sklearn.feature_selection import selectkbest sklearn.feature_selection import f_regression sklearn.pipeline import pipeline generate data play samples_generator.make_classification 
3494: n_informative5 n_redundant0 random_state42 anova svmc anova_filter selectkbest f_regression clf svm.svc kernellinear anova_svm pipeline anova anova_filter svc clf set parameters using names issued instance fit using selectkbest parameter svn anova_svm.set_params anova__k10 svc__c.1 .fit ... pipeline steps ... prediction anova_svm.predict anova_svm.score 0.75 attributes steps list name object list named object compose pipeline order applied data 
3495: methods decision_function applies transforms data decision_function method nal estimator fit fit_transform get_params deep inverse_transform predict predict_log_proba predict_proba score applies transforms data predict_proba method nal estimator applies transforms data score method nal estimator 
3496: fit transforms one transform fit transforms one transform data use t_transform transformed data using nal estimator 
3497: applies transforms data predict method nal estimator 
3498: chapter user guide scikitlearn user guide release 0.11 set_params params transform set parameters estimator applies transforms data transform method nal estimator 
3499: table 1.169 continued previous page __init__ steps decision_function applies transforms data decision_function method nal estimator valid nal estimator implements decision_function 
3500: fit ynone t_params fit transforms one transform data transformed data using nal estimator 
3501: fit_transform ynone t_params fit transforms one transform data use t_transform transformed data using nal estimator valid nal estimator implements t_transform 
3502: predict applies transforms data predict method nal estimator valid nal estimator implements predict predict_proba applies transforms data predict_proba method nal estimator valid nal estimator implements predict_proba 
3503: score ynone applies transforms data score method nal estimator valid nal estimator implements score 
3504: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform applies transforms data transform method nal estimator valid nal estimator implements transform 
3505: 1.8.24 sklearn.preprocessing preprocessing normalization user guide see preprocessing data section details 
3506: preprocessing.scaler copy with_mean with_std preprocessing.normalizer norm copy preprocessing.binarizer threshold copy preprocessing.labelbinarizer neg_label ... preprocessing.kernelcenterer standardize features removing mean scaling unit variance normalize samples individually unit norm binarize data set feature values according threshold binarize labels onevsall fashion center kernel matrix 1.8. reference scikitlearn user guide release 0.11 sklearn.preprocessing.scaler class sklearn.preprocessing.scaler copytrue with_meantrue with_stdtrue standardize features removing mean scaling unit variance centering scaling happen indepently feature computing relevant statistics samples training set mean standard deviation stored used later data using transform method standardization dataset common requirement many machine learning estimators might behave badly individual feature less look like standard normally distributed data e.g gaussian mean unit variance instance many elements used objective function learning algorithm rbf kernel support vector machines regularizers linear models assume features centered around variance order feature variance orders magnitude larger others might dominate objective function make estimator unable learn features correctly expected 
3507: parameters with_mean boolean true default true center data scaling 
3508: with_std boolean true default true scale data unit variance equivalently unit standard deviation 
3509: copy boolean optional default true set false perform inplace row normalization avoid copy input already numpy array scipy.sparse csr matrix axis 
3510: see also sklearn.preprocessing.scale scaling sklearn.decomposition.randomizedpca attributes mean_ std_ array oats shape n_features array oats shape n_features mean value feature training set standard deviation feature training set 
3511: methods fit fit_transform get_params deep inverse_transform copy set_params params transform copy compute mean std used later scaling fit data transform get parameters estimator scale back data original representation set parameters estimator perform standardization centering scaling __init__ copytrue with_meantrue with_stdtrue fit ynone compute mean std used later scaling parameters arraylike csr matrix shape n_samples n_features chapter user guide scikitlearn user guide release 0.11 data used compute mean standard deviation used later scaling along features axis 
3512: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
3513: parameters numpy array shape n_samples n_features training set 
3514: numpy array shape n_samples target values 
3515: returns x_new numpy array shape n_samples n_features_new transformed array 
3516: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
3517: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3518: inverse_transform copynone scale back data original representation parameters arraylike shape n_samples n_features data used scale along features axis 
3519: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform ynone copynone perform standardization centering scaling parameters arraylike shape n_samples n_features data used scale along features axis 
3520: sklearn.preprocessing.normalizer class sklearn.preprocessing.normalizer norml2 copytrue normalize samples individually unit norm sample i.e row data matrix least one non zero component rescaled independently samples norm equals one 
3521: 1.8. reference scikitlearn user guide release 0.11 transformer able work dense numpy arrays scipy.sparse matrix use csr format want avoid burden copy conversion scaling inputs unit norms common operation text classication clustering instance instance dot product two l2normalized tfidf vectors cosine similarity vectors base similarity metric vector space model commonly used information retrieval community 
3522: parameters norm optional default norm use normalize non zero sample 
3523: copy boolean optional default true set false perform inplace row normalization avoid copy input already numpy array scipy.sparse csr matrix 
3524: see also sklearn.preprocessing.normalize without notes estimator stateless besides constructor parameters method nothing useful used pipeline 
3525: methods fit fit_transform get_params deep set_params params transform copy nothing return estimator unchanged fit data transform get parameters estimator set parameters estimator scale non zero row unit norm __init__ norml2 copytrue fit ynone nothing return estimator unchanged method implement usual api hence work pipelines 
3526: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
3527: parameters numpy array shape n_samples n_features training set 
3528: numpy array shape n_samples target values 
3529: returns x_new numpy array shape n_samples n_features_new transformed array 
3530: chapter user guide scikitlearn user guide release 0.11 notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
3531: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform ynone copynone scale non zero row unit norm parameters array scipy.sparse matrix shape n_samples n_features data normalize row row scipy.sparse matrices csr format avoid unnecessary copy 
3532: sklearn.preprocessing.binarizer class sklearn.preprocessing.binarizer threshold0.0 copytrue binarize data set feature values according threshold default threshold 0.0 nonzero values set 1.0 zeros left untouched binarization common operation text count data analyst decide consider presence absence feature rather quantied number occurences instance also used preprocessing step estimators consider boolean random variables e.g modeled using bernoulli distribution bayesian setting 
3533: parameters threshold oat optional 0.0 default lower bound triggers feature values replaced 1.0 
3534: copy boolean optional default true set false perform inplace binarization avoid copy input already numpy array scipy.sparse csr matrix 
3535: notes input sparse matrix nonzero values subject update binarizer class estimator stateless besides constructor parameters method nothing useful used pipeline 
3536: 1.8. reference scikitlearn user guide release 0.11 methods fit fit_transform get_params deep set_params params transform copy binarize element nothing return estimator unchanged fit data transform get parameters estimator set parameters estimator 
3537: __init__ threshold0.0 copytrue fit ynone nothing return estimator unchanged method implement usual api hence work pipelines 
3538: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
3539: parameters numpy array shape n_samples n_features training set 
3540: numpy array shape n_samples target values 
3541: returns x_new numpy array shape n_samples n_features_new transformed array 
3542: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
3543: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform ynone copynone binarize element parameters array scipy.sparse matrix shape n_samples n_features data binarize element element scipy.sparse matrices csr format avoid unnecessary copy 
3544: chapter user guide scikitlearn user guide release 0.11 sklearn.preprocessing.labelbinarizer class sklearn.preprocessing.labelbinarizer neg_label0 pos_label1 binarize labels onevsall fashion several regression binary classication algorithms available scikit simple way extend algorithms multiclass classication case use socalled onevsall scheme learning time simply consists learning one regressor binary classier per class one needs convert multiclass labels binary labels belong belong class labelbinarizer makes process easy transform method prediction time one assigns class corresponding model gave greatest condence belbinarizer makes easy inverse_transform method 
3545: parameters neg_label int default value negative labels must encoded 
3546: pos_label int default value positive labels must encoded 
3547: examples sklearn import preprocessing clf preprocessing.labelbinarizer clf.fit labelbinarizer neg_label0 pos_label1 clf.classes_ array clf.transform array clf.fit_transform array clf.classes_ array attributes classes_ array shape n_class holds label class 
3548: methods fit fit_transform get_params deep inverse_transform threshold transform binary labels back multiclass labels set_params params transform fit label binarizer fit data transform get parameters estimator set parameters estimator transform multiclass labels binary labels 1.8. reference scikitlearn user guide release 0.11 __init__ neg_label0 pos_label1 fit fit label binarizer parameters numpy array shape n_samples sequence sequences target values multilabel case nested sequences variable lengths 
3549: returns self returns instance self 
3550: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
3551: parameters numpy array shape n_samples n_features training set 
3552: numpy array shape n_samples target values 
3553: returns x_new numpy array shape n_samples n_features_new transformed array 
3554: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
3555: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3556: inverse_transform thresholdnone transform binary labels back multiclass labels parameters numpy array shape n_samples n_classes target values 
3557: threshold oat none threshold used binary multilabel cases use contains output decision_function classier use 0.5 contains output predict_proba none threshold assumed half way neg_label pos_label 
3558: returns numpy array shape n_samples sequence sequences target values multilabel case nested sequences variable lengths 
3559: chapter user guide scikitlearn user guide release 0.11 notes case binary labels fractional probabilistic inverse_transform chooses class greatest value typically allows use output linear models decision_function method directly input inverse_transform 
3560: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform transform multiclass labels binary labels output transform sometimes referred authors 1ofk coding scheme 
3561: parameters numpy array shape n_samples sequence sequences target values multilabel case nested sequences variable lengths 
3562: returns numpy array shape n_samples n_classes sklearn.preprocessing.kernelcenterer class sklearn.preprocessing.kernelcenterer center kernel matrix equivalent centering phi sklearn.preprocessing.scaler with_stdfalse 
3563: methods fit fit_transform get_params deep set_params params transform copy fit kernelcenterer fit data transform get parameters estimator set parameters estimator center kernel __init__ x.__init__ ... initializes see help type signature fit fit kernelcenterer parameters numpy array shape n_samples n_samples kernel matrix 
3564: returns self returns instance self 
3565: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
3566: parameters numpy array shape n_samples n_features 1.8. reference scikitlearn user guide release 0.11 training set 
3567: numpy array shape n_samples target values 
3568: returns x_new numpy array shape n_samples n_features_new transformed array 
3569: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
3570: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform copytrue center kernel parameters numpy array shape n_samples1 n_samples2 kernel matrix 
3571: returns k_new numpy array shape n_samples1 n_samples2 preprocessing.scale axis with_mean ... standardize dataset along axis preprocessing.normalize norm axis copy normalize dataset along axis preprocessing.binarize threshold copy boolean thresholding arraylike scipy.sparse matrix sklearn.preprocessing.scale sklearn.preprocessing.scale axis0 with_meantrue with_stdtrue copytrue standardize dataset along axis center mean component wise scale unit variance 
3572: parameters arraylike csr matrix 
3573: data center scale 
3574: axis int default axis used compute means standard deviations along standardize feature otherwise standardize sample 
3575: independently with_mean boolean true default chapter user guide scikitlearn user guide release 0.11 true center data scaling 
3576: with_std boolean true default true scale data unit variance equivalently unit standard deviation 
3577: copy boolean optional default true set false perform inplace row normalization avoid copy input already numpy array scipy.sparse csr matrix axis 
3578: see also sklearn.preprocessing.scaler scaling sklearn.pipeline.pipeline notes implementation refuse center scipy.sparse matrices since would make nonsparse would potentially crash program memory exhaustion problems instead caller expected either set explicitly with_meanfalse case variance scaling performed features csr matrix call x.toarray heshe expects materialized dense array memory avoid memory copy caller pass csr matrix 
3579: sklearn.preprocessing.normalize sklearn.preprocessing.normalize norml2 axis1 copytrue normalize dataset along axis parameters array scipy.sparse matrix shape n_samples n_features data normalize element element scipy.sparse matrices csr format avoid unnecessary copy 
3580: norm optional default norm use normalize non zero sample nonzero feature axis 
3581: axis optional default axis used normalize data along independently normalize sample oth erwise normalize feature copy boolean optional default true set false perform inplace row normalization avoid copy input already numpy array scipy.sparse csr matrix axis 
3582: see also sklearn.preprocessing.normalizer using sklearn.pipeline.pipeline sklearn.preprocessing.binarize sklearn.preprocessing.binarize threshold0.0 copytrue boolean thresholding arraylike scipy.sparse matrix parameters array scipy.sparse matrix shape n_samples n_features 1.8. reference scikitlearn user guide release 0.11 data binarize element element scipy.sparse matrices csr format avoid unnecessary copy 
3583: threshold oat optional 0.0 default lower bound triggers feature values replaced 1.0 
3584: copy boolean optional default true set false perform inplace binarization avoid copy input already numpy array scipy.sparse csr matrix axis 
3585: see also sklearn.preprocessing.binarizer using sklearn.pipeline.pipeline 1.8.25 sklearn.qda quadratic discriminant analysis quadratic discriminant analysis user guide see linear quadratic discriminant analysis section details 
3586: qda.qda priors quadratic discriminant analysis qda sklearn.qda.qda class sklearn.qda.qda priorsnone quadratic discriminant analysis qda classier quadratic decision boundary generated tting class conditional densities data using bayes rule model gaussian density class 
3587: parameters priors array optional shape n_classes priors classes see also sklearn.lda.ldalinear discriminant analysis examples sklearn.qda import qda import numpy np.array np.array clf qda clf.fit qda priorsnone print clf.predict 0.8 chapter user guide scikitlearn user guide release 0.11 attributes means_ priors_ covariances_ arraylike shape n_classes n_features arraylike shape n_classes list arraylike shape n_features n_features covariance matrices class class means class priors sum methods decision_function fit store_covariances tol get_params deep predict predict_log_proba predict_proba score set_params params apply decision function array samples fit qda model according given training data parameters get parameters estimator perform classication array test vectors return posterior probabilities classication return posterior probabilities classication returns mean accuracy given test data labels set parameters estimator 
3588: __init__ priorsnone decision_function apply decision function array samples 
3589: parameters arraylike shape n_samples n_features array samples test vectors 
3590: returns array shape n_samples n_classes decision function values related class per sample 
3591: fit store_covariancesfalse tol0.0001 fit qda model according given training data parameters 
3592: parameters arraylike shape n_samples n_features training vector n_samples number samples n_features num ber features 
3593: array shape n_samples target values integers store_covariances boolean true covariance matrices computed stored self.covariances_ tribute 
3594: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3595: predict perform classication array test vectors predicted class sample returned 
3596: 1.8. reference scikitlearn user guide release 0.11 parameters arraylike shape n_samples n_features returns array shape n_samples predict_log_proba return posterior probabilities classication 
3597: parameters arraylike shape n_samples n_features array samplestest vectors 
3598: returns array shape n_samples n_classes posterior logprobabilities classication per class 
3599: predict_proba return posterior probabilities classication 
3600: parameters arraylike shape n_samples n_features array samplestest vectors 
3601: returns array shape n_samples n_classes posterior probabilities classication per class 
3602: score returns mean accuracy given test data labels 
3603: parameters arraylike shape n_samples n_features training set 
3604: arraylike shape n_samples labels 
3605: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self 1.8.26 sklearn.svm support vector machines sklearn.svm module includes support vector machine algorithms user guide see support vector machines section details 
3606: estimators svm.svc kernel degree gamma coef0 ... svm.linearsvc penalty loss dual tol ... svm.nusvc kernel degree gamma ... svm.svr kernel degree gamma coef0 tol ... svm.nusvr kernel degree gamma ... csupport vector classication linear support vector classication nusupport vector classication epsilonsupport vector regression support vector regression 
3607: continued next page chapter user guide scikitlearn user guide release 0.11 svm.oneclasssvm kernel degree gamma ... unsupervised outliers detection 
3608: table 1.179 continued previous page sklearn.svm.svc class sklearn.svm.svc c1.0 kernelrbf degree3 gamma0.0 coef00.0 shrinkingtrue proba bilityfalse tol0.001 cache_size200 class_weightnone verbosefalse csupport vector classication implementations based libsvm time complexity quadratic number samples makes hard scale dataset couple samples multiclass support handled according onevsone scheme details precise mathematical formulation provided kernel functions gamma coef0 degree affect see corresponding section narrative documentation kernel functions 
3609: parameters oat none optional defaultnone penalty parameter error term none set n_samples 
3610: kernel string optional defaultrbf species kernel type used algorithm must one linear poly rbf sigmoid precomputed none given rbf used 
3611: degree int optional default3 degree kernel function signicant poly sigmoid 
3612: gamma oat optional default0.0 kernel coefcient rbf poly gamma 0.0 1n_features used instead 
3613: coef0 oat optional default0.0 independent term kernel function signicant poly sigmoid 
3614: probability boolean optional defaultfalse whether enable probability estimates must enabled prior calling pre dict_proba 
3615: shrinking boolean optional defaulttrue whether use shrinking heuristic 
3616: tol oat optional default1e3 tolerance stopping criterion 
3617: cache_size oat optional specify size kernel cache class_weight dict auto optional set parameter class class_weight svc given classes supposed weight one auto mode uses values automatically adjust weights inversely proportional class frequencies 
3618: verbose bool default false enable verbose output note setting takes advantage perprocess runtime setting libsvm enabled may work properly multithreaded context 
3619: 1.8. reference scikitlearn user guide release 0.11 see also svrsupport vector machine regression implemented using libsvm linearsvcscalable linear support vector machine classifcation implemented using liblinear check see also section linearsvc comparison element 
3620: examples import numpy np.array np.array sklearn.svm import svc clf svc clf.fit svc c1.0 cache_size200 class_weightnone coef00.0 degree3 gamma0.5 kernelrbf probabilityfalse shrinkingtrue tol0.001 verbosefalse print clf.predict 0.8 attributes index support vectors 
3621: support vectors 
3622: number support vector class 
3623: coefcients support vector decision function multiclass coefcient 1vs1 classiers layout coefcients multiclass case somewhat nontrivial see section multiclass classication svm section user guide details weights asigned features coefcients primal problem available case linear kernel coef_ readonly property derived dual_coef_ support_vectors_ constants decision function 
3624: sup port_ sup port_vectors_ arraylike shape n_sv arraylike shape n_sv n_features n_support_arraylike dtypeint32 shape n_class dual_coef_array shape n_class1 n_sv coef_ inter cept_ array shape n_class1 n_features array shape n_class n_class1 methods decision_function fit class_weight sample_weight get_params deep predict predict_log_proba predict_proba distance samples separating hyperplane fit svm model according given training data get parameters estimator perform classication regression samples compute log likehoods possible outcomes samples compute likehoods possible outcomes samples 
3625: continued next page chapter user guide scikitlearn user guide release 0.11 score set_params params returns mean accuracy given test data labels set parameters estimator 
3626: table 1.180 continued previous page __init__ c1.0 kernelrbf degree3 gamma0.0 coef00.0 shrinkingtrue probabil ityfalse tol0.001 cache_size200 class_weightnone verbosefalse decision_function distance samples separating hyperplane 
3627: parameters arraylike shape n_samples n_features returns arraylike shape n_samples n_class n_class1 returns decision function sample class model 
3628: fit class_weightnone sample_weightnone fit svm model according given training data 
3629: parameters arraylike sparse matrix shape n_samples n_features training vectors n_samples number samples n_features num ber features 
3630: arraylike shape n_samples target values integers classication real numbers regression sample_weight arraylike shape n_samples optional weights applied individual samples unweighted 
3631: returns self object returns self 
3632: notes cordered contiguous arrays np.oat64 scipy.sparse.csr_matrix andor may copied dense array methods support sparse matrices input 
3633: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3634: predict perform classication regression samples classication model predicted class sample returned regression model function value calculated returned oneclass model returned 
3635: parameters arraylike sparse matrix shape n_samples n_features returns array shape n_samples 1.8. reference scikitlearn user guide release 0.11 predict_log_proba compute log likehoods possible outcomes samples model need probability information computed training time attribute probability set true 
3636: parameters arraylike shape n_samples n_features returns arraylike shape n_samples n_classes returns logprobabilities sample class model classes ordered arithmetical order 
3637: notes probability model created using cross validation results slightly different obtained predict also meaningless results small datasets 
3638: predict_proba compute likehoods possible outcomes samples model need probability information computed training time attribute probability set true 
3639: parameters arraylike shape n_samples n_features returns arraylike shape n_samples n_classes returns probability sample class model classes ordered arithmetical order 
3640: notes probability model created using cross validation results slightly different obtained predict also meaningless results small datasets 
3641: score returns mean accuracy given test data labels 
3642: parameters arraylike shape n_samples n_features training set 
3643: arraylike shape n_samples labels 
3644: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self chapter user guide sklearn.svm.linearsvc scikitlearn user guide release 0.11 class sklearn.svm.linearsvc penaltyl2 multi_classovr class_weightnone verbose0 lossl2 dualtrue t_intercepttrue tol0.0001 c1.0 intercept_scaling1 linear support vector classication similar svc parameter kernellinear implemented terms liblinear rather libsvm exibility choice penalties loss functions scale better large numbers samples class supports dense sparse input multiclass support handled according onevsthe rest scheme 
3645: parameters oat none optional defaultnone penalty parameter error term none set n_samples 
3646: loss string defaultl2 species loss function squared hinge loss 
3647: hinge loss standard svm penalty string defaultl2 species norm used penalization penalty standard used svc leads coef_ vectors sparse 
3648: dual bool defaulttrue select algorithm either solve dual primal optimization problem prefer dualfalse n_samples n_features 
3649: tol oat optional default1e4 tolerance stopping criteria multi_class string ovr crammer_singer defaultovr determines multiclass strategy contains two classes ovr trains n_classes onevsrest classiers crammer_singer optimizes joint objective classes crammer_singer interesting theoretical perspective consistent seldom used practice rarely leads better accuracy expensive compute crammer_singer choosen options loss penalty dual ignored 
3650: t_intercept boolean optional defaulttrue whether calculate intercept model set false intercept used calculations e.g data expected already centered 
3651: intercept_scaling oat optional default1 self.t_intercept true instance vector becomes self.intercept_scaling i.e synthetic feature constant value equals intercept_scaling appended instance vector intercept becomes intercept_scaling synthetic feature weight note synthetic feature weight subject l1l2 regularization features lessen effect regularization synthetic feature weight therefore intercept intercept_scaling increased class_weight dict auto optional 1.8. reference scikitlearn user guide release 0.11 set parameter class class_weight svc given classes supposed weight one auto mode uses values automatically adjust weights inversely proportional class frequencies 
3652: verbose int default enable verbose output note setting takes advantage perprocess runtime setting liblinear enabled may work properly multithreaded context 
3653: see also svcimplementation support vector machine classier using libsvm kernel nonlinear smo algorithm scale large number samples linearsvc furthermore svc multi class mode implemented using one one scheme linearsvc uses one rest possible implement one rest svc using sklearn.multiclass.onevsrestclassifier wrapper finally svc dense data without memory copy input ccontiguous sparse data still incur memory copy though 
3654: sklearn.linear_model.sgdclassifiersgdclassier optimize cost function lin earsvc adjusting penalty loss parameters furthermore sgdclassier scalable large number samples uses stochastic gradient descent optimizer finally sgdclassier dense sparse data without memory copy input ccontiguous csr 
3655: notes underlying implementation uses random number generator select features tting model thus uncommon slightly different results input data happens try smaller tol parameter underlying implementation liblinear uses sparse internal representation data incur memory copy references liblinear library large linear classication attributes coef_ array shape n_features n_classes else n_classes n_features ter cept_ array shape n_classes else n_classes methods weights asigned features coefcients primal problem available case linear kernel coef_ readonly property derived raw_coef_ follows internal memory layout liblinear constants decision function 
3656: decision_function decision function value according trained model fit class_weight fit_transform get_params deep predict score fit model according given training data fit data transform get parameters estimator predict target values according tted model returns mean accuracy given test data labels continued next page chapter user guide scikitlearn user guide release 0.11 table 1.181 continued previous page set_params params transform threshold reduce important features 
3657: set parameters estimator 
3658: __init__ penaltyl2 lossl2 t_intercepttrue intercept_scaling1 class_weightnone verbose0 dualtrue tol0.0001 c1.0 multi_classovr decision_function decision function value according trained model 
3659: parameters arraylike shape n_samples n_features returns arraylike shape n_samples n_class returns decision function sample class model 
3660: fit class_weightnone fit model according given training data 
3661: parameters arraylike sparse matrix shape n_samples n_features training vector n_samples number samples n_features num ber features 
3662: arraylike shape n_samples target vector relative class_weight dict auto optional weights associated classes given classes supposed weight one 
3663: returns self object returns self 
3664: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
3665: parameters numpy array shape n_samples n_features training set 
3666: numpy array shape n_samples target values 
3667: returns x_new numpy array shape n_samples n_features_new transformed array 
3668: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
3669: get_params deeptrue get parameters estimator parameters deep boolean optional 1.8. reference scikitlearn user guide release 0.11 true return parameters estimator contained subobjects estimators 
3670: predict predict target values according tted model 
3671: parameters arraylike sparse matrix shape n_samples n_features returns array shape n_samples score returns mean accuracy given test data labels 
3672: parameters arraylike shape n_samples n_features training set 
3673: arraylike shape n_samples labels 
3674: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform thresholdnone reduce important features 
3675: parameters array scipy sparse matrix shape n_samples n_features input samples 
3676: threshold string oat none optional defaultnone threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor e.g. 1.25mean may also used none available object attribute threshold used otherwise mean used default 
3677: returns x_r array shape n_samples n_selected_features input samples selected features 
3678: sklearn.svm.nusvc class sklearn.svm.nusvc nu0.5 kernelrbf degree3 gamma0.0 coef00.0 shrinkingtrue probabilityfalse tol0.001 cache_size200 verbosefalse nusupport vector classication similar svc uses parameter control number support vectors implementation based libsvm 
3679: parameters oat optional default0.5 upper bound fraction training errors lower bound fraction support vectors interval 
3680: chapter user guide scikitlearn user guide release 0.11 kernel string optional defaultrbf species kernel type used algorithm one linear poly rbf sigmoid precomputed none given rbf used 
3681: degree int optional default3 degree kernel function signicant poly rbf sigmoid gamma oat optional default0.0 kernel coefcient rbf poly gamma 0.0 1n_features taken 
3682: coef0 oat optional default0.0 independent term kernel function signicant polysigmoid 
3683: probability boolean optional defaultfalse whether enable probability estimates must enabled prior calling pre dict_proba 
3684: shrinking boolean optional defaulttrue whether use shrinking heuristic 
3685: tol oat optional default1e3 tolerance stopping criterion 
3686: cache_size oat optional specify size kernel cache class_weight dict auto optional set parameter class class_weight svc given classes supposed weight one auto mode uses values automatically adjust weights inversely proportional class frequencies 
3687: verbose bool default false enable verbose output note setting takes advantage perprocess runtime setting libsvm enabled may work properly multithreaded context 
3688: see also svcsupport vector machine classication using libsvm linearsvcscalable linear support vector machine classication using liblinear 
3689: examples import numpy np.array np.array sklearn.svm import nusvc clf nusvc clf.fit nusvc cache_size200 coef00.0 degree3 gamma0.5 kernelrbf nu0.5 probabilityfalse shrinkingtrue tol0.001 verbosefalse print clf.predict 0.8 1.8. reference scikitlearn user guide release 0.11 attributes index support vectors 
3690: support vectors 
3691: number support vector class 
3692: coefcients support vector decision function multiclass coefcient 1vs1 classiers layout coefcients multiclass case somewhat nontrivial see section multiclass classication svm section user guide details weights asigned features coefcients primal problem available case linear kernel coef_ readonly property derived dual_coef_ support_vectors_ constants decision function 
3693: sup port_ sup port_vectors_ arraylike shape n_sv arraylike shape n_sv n_features n_support_arraylike dtypeint32 shape n_class dual_coef_array shape n_class1 n_sv coef_ inter cept_ array shape n_class1 n_features array shape n_class n_class1 methods decision_function fit class_weight sample_weight get_params deep predict predict_log_proba predict_proba score set_params params distance samples separating hyperplane fit svm model according given training data get parameters estimator perform classication regression samples compute log likehoods possible outcomes samples compute likehoods possible outcomes samples returns mean accuracy given test data labels set parameters estimator 
3694: __init__ nu0.5 kernelrbf degree3 gamma0.0 coef00.0 shrinkingtrue probabil ityfalse tol0.001 cache_size200 verbosefalse decision_function distance samples separating hyperplane 
3695: parameters arraylike shape n_samples n_features returns arraylike shape n_samples n_class n_class1 returns decision function sample class model 
3696: fit class_weightnone sample_weightnone fit svm model according given training data 
3697: parameters arraylike sparse matrix shape n_samples n_features training vectors n_samples number samples n_features num ber features 
3698: arraylike shape n_samples target values integers classication real numbers regression chapter user guide scikitlearn user guide release 0.11 sample_weight arraylike shape n_samples optional weights applied individual samples unweighted 
3699: returns self object returns self 
3700: notes cordered contiguous arrays np.oat64 scipy.sparse.csr_matrix andor may copied dense array methods support sparse matrices input 
3701: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3702: predict perform classication regression samples classication model predicted class sample returned regression model function value calculated returned oneclass model returned 
3703: parameters arraylike sparse matrix shape n_samples n_features returns array shape n_samples predict_log_proba compute log likehoods possible outcomes samples model need probability information computed training time attribute probability set true 
3704: parameters arraylike shape n_samples n_features returns arraylike shape n_samples n_classes returns logprobabilities sample class model classes ordered arithmetical order 
3705: notes probability model created using cross validation results slightly different obtained predict also meaningless results small datasets 
3706: predict_proba compute likehoods possible outcomes samples model need probability information computed training time attribute probability set true 
3707: parameters arraylike shape n_samples n_features returns arraylike shape n_samples n_classes 1.8. reference scikitlearn user guide release 0.11 returns probability sample class model classes ordered arithmetical order 
3708: notes probability model created using cross validation results slightly different obtained predict also meaningless results small datasets 
3709: score returns mean accuracy given test data labels 
3710: parameters arraylike shape n_samples n_features training set 
3711: arraylike shape n_samples labels 
3712: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.svm.svr class sklearn.svm.svr kernelrbf degree3 gamma0.0 coef00.0 tol0.001 c1.0 epsilon0.1 shrinkingtrue probabilityfalse cache_size200 verbosefalse epsilonsupport vector regression free parameters model epsilon implementations based libsvm 
3713: parameters oat none optional defaultnone penalty parameter error term none set n_samples 
3714: epsilon oat optional default0.1 epsilon epsilonsvr model species epsilontube within penalty associated training loss function points predicted within distance epsilon actual value 
3715: kernel string optional defaultrbf species kernel type used algorithm one linear poly rbf sigmoid precomputed none given rbf used 
3716: degree int optional default3 degree kernel function signicant poly rbf sigmoid gamma oat optional default0.0 kernel coefcient rbf poly gamma 0.0 1n_features taken 
3717: chapter user guide scikitlearn user guide release 0.11 coef0 oat optional default0.0 independent term kernel function signicant polysigmoid 
3718: probability boolean optional defaultfalse whether enable probability estimates must enabled prior calling pre dict_proba 
3719: shrinking boolean optional defaulttrue whether use shrinking heuristic 
3720: tol oat optional default1e3 tolerance stopping criterion 
3721: cache_size oat optional specify size kernel cache verbose bool default false enable verbose output note setting takes advantage perprocess runtime setting libsvm enabled may work properly multithreaded context 
3722: see also nusvrsupport vector machine regression implemented using libsvm using parameter control num ber support vectors 
3723: examples sklearn.svm import svr import numpy n_samples n_features np.random.seed np.random.randn n_samples np.random.randn n_samples n_features clf svr c1.0 epsilon0.2 clf.fit svr c1.0 cache_size200 coef00.0 degree3 epsilon0.2 gamma0.2 kernelrbf probabilityfalse shrinkingtrue tol0.001 verbosefalse 1.8. reference scikitlearn user guide release 0.11 attributes sup port_ sup port_vectors_ arraylike shape n_sv arraylike shape nsv n_features index support vectors 
3724: support vectors 
3725: coef_ dual_coef_array shape n_classes1 n_sv array shape n_classes1 n_features array shape n_class n_class1 inter cept_ coefcients support vector decision function 
3726: weights asigned features coefcients primal problem available case linear kernel coef_ readonly property derived dual_coef_ support_vectors_ constants decision function 
3727: methods decision_function fit class_weight sample_weight get_params deep predict predict_log_proba predict_proba score set_params params distance samples separating hyperplane fit svm model according given training data get parameters estimator perform classication regression samples compute log likehoods possible outcomes samples compute likehoods possible outcomes samples returns coefcient determination prediction set parameters estimator 
3728: __init__ kernelrbf degree3 gamma0.0 coef00.0 tol0.001 c1.0 epsilon0.1 shrink ingtrue probabilityfalse cache_size200 verbosefalse decision_function distance samples separating hyperplane 
3729: parameters arraylike shape n_samples n_features returns arraylike shape n_samples n_class n_class1 returns decision function sample class model 
3730: fit class_weightnone sample_weightnone fit svm model according given training data 
3731: parameters arraylike sparse matrix shape n_samples n_features training vectors n_samples number samples n_features num ber features 
3732: arraylike shape n_samples target values integers classication real numbers regression sample_weight arraylike shape n_samples optional weights applied individual samples unweighted 
3733: returns self object chapter user guide scikitlearn user guide release 0.11 returns self 
3734: notes cordered contiguous arrays np.oat64 scipy.sparse.csr_matrix andor may copied dense array methods support sparse matrices input 
3735: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3736: predict perform classication regression samples classication model predicted class sample returned regression model function value calculated returned oneclass model returned 
3737: parameters arraylike sparse matrix shape n_samples n_features returns array shape n_samples predict_log_proba compute log likehoods possible outcomes samples model need probability information computed training time attribute probability set true 
3738: parameters arraylike shape n_samples n_features returns arraylike shape n_samples n_classes returns logprobabilities sample class model classes ordered arithmetical order 
3739: notes probability model created using cross validation results slightly different obtained predict also meaningless results small datasets 
3740: predict_proba compute likehoods possible outcomes samples model need probability information computed training time attribute probability set true 
3741: parameters arraylike shape n_samples n_features returns arraylike shape n_samples n_classes returns probability sample class model classes ordered arithmetical order 
3742: 1.8. reference scikitlearn user guide release 0.11 notes probability model created using cross validation results slightly different obtained predict also meaningless results small datasets 
3743: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
3744: parameters arraylike shape n_samples n_features training set 
3745: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.svm.nusvr class sklearn.svm.nusvr nu0.5 c1.0 kernelrbf degree3 gamma0.0 coef00.0 shrink ingtrue probabilityfalse tol0.001 cache_size200 verbosefalse support vector regression similar nusvc regression uses parameter control number support vectors however unlike nusvc replaces replaces parameter epsilon svr implementations based libsvm 
3746: parameters oat none optional defaultnone penalty parameter error term none set n_samples 
3747: oat optional upper bound fraction training errors lower bound fraction support vectors interval default 0.5 taken available implnu_svc 
3748: kernel string optional defaultrbf species kernel type used algorithm one linear poly rbf sigmoid precomputed none given rbf used 
3749: degree int optional default3 degree kernel function signicant poly rbf sigmoid gamma oat optional default0.0 kernel coefcient rbf poly gamma 0.0 1n_features taken 
3750: coef0 oat optional default0.0 chapter user guide scikitlearn user guide release 0.11 independent term kernel function signicant polysigmoid 
3751: probability boolean optional defaultfalse whether enable probability estimates must enabled prior calling pre dict_proba 
3752: shrinking boolean optional defaulttrue whether use shrinking heuristic 
3753: tol oat optional default1e3 tolerance stopping criterion 
3754: cache_size oat optional specify size kernel cache verbose bool default false enable verbose output note setting takes advantage perprocess runtime setting libsvm enabled may work properly multithreaded context 
3755: see also nusvcsupport vector machine classication implemented libsvm parameter control number support vectors 
3756: svrepsilon support vector machine regression implemented libsvm 
3757: examples sklearn.svm import nusvr import numpy n_samples n_features np.random.seed np.random.randn n_samples np.random.randn n_samples n_features clf nusvr c1.0 nu0.1 clf.fit nusvr c1.0 cache_size200 coef00.0 degree3 gamma0.2 kernelrbf nu0.1 probabilityfalse shrinkingtrue tol0.001 verbosefalse 1.8. reference scikitlearn user guide release 0.11 attributes sup port_ sup port_vectors_ arraylike shape n_sv arraylike shape nsv n_features index support vectors 
3758: support vectors 
3759: coef_ dual_coef_array shape n_classes1 n_sv array shape n_classes1 n_features array shape n_class n_class1 inter cept_ coefcients support vector decision function 
3760: weights asigned features coefcients primal problem available case linear kernel coef_ readonly property derived dual_coef_ support_vectors_ constants decision function 
3761: methods decision_function fit class_weight sample_weight get_params deep predict predict_log_proba predict_proba score set_params params distance samples separating hyperplane fit svm model according given training data get parameters estimator perform classication regression samples compute log likehoods possible outcomes samples compute likehoods possible outcomes samples returns coefcient determination prediction set parameters estimator 
3762: __init__ nu0.5 c1.0 kernelrbf degree3 gamma0.0 coef00.0 shrinkingtrue probabil ityfalse tol0.001 cache_size200 verbosefalse decision_function distance samples separating hyperplane 
3763: parameters arraylike shape n_samples n_features returns arraylike shape n_samples n_class n_class1 returns decision function sample class model 
3764: fit class_weightnone sample_weightnone fit svm model according given training data 
3765: parameters arraylike sparse matrix shape n_samples n_features training vectors n_samples number samples n_features num ber features 
3766: arraylike shape n_samples target values integers classication real numbers regression sample_weight arraylike shape n_samples optional weights applied individual samples unweighted 
3767: returns self object chapter user guide scikitlearn user guide release 0.11 returns self 
3768: notes cordered contiguous arrays np.oat64 scipy.sparse.csr_matrix andor may copied dense array methods support sparse matrices input 
3769: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3770: predict perform classication regression samples classication model predicted class sample returned regression model function value calculated returned oneclass model returned 
3771: parameters arraylike sparse matrix shape n_samples n_features returns array shape n_samples predict_log_proba compute log likehoods possible outcomes samples model need probability information computed training time attribute probability set true 
3772: parameters arraylike shape n_samples n_features returns arraylike shape n_samples n_classes returns logprobabilities sample class model classes ordered arithmetical order 
3773: notes probability model created using cross validation results slightly different obtained predict also meaningless results small datasets 
3774: predict_proba compute likehoods possible outcomes samples model need probability information computed training time attribute probability set true 
3775: parameters arraylike shape n_samples n_features returns arraylike shape n_samples n_classes returns probability sample class model classes ordered arithmetical order 
3776: 1.8. reference scikitlearn user guide release 0.11 notes probability model created using cross validation results slightly different obtained predict also meaningless results small datasets 
3777: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
3778: parameters arraylike shape n_samples n_features training set 
3779: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn.svm.oneclasssvm class sklearn.svm.oneclasssvm kernelrbf degree3 gamma0.0 coef00.0 tol0.001 nu0.5 shrinkingtrue cache_size200 verbosefalse unsupervised outliers detection estimate support highdimensional distribution implementation based libsvm 
3780: parameters kernel string optional species kernel type used algorithm one linear poly rbf sigmoid precomputed none given rbf used 
3781: oat optional upper bound fraction training errors lower bound fraction support vectors interval default 0.5 taken 
3782: degree int optional degree kernel function signicant poly rbf sigmoid 
3783: gamma oat optional default0.0 kernel coefcient rbf poly gamma 0.0 1n_features taken 
3784: coef0 oat optional independent term kernel function signicant polysigmoid 
3785: tol oat optional tolerance stopping criterion 
3786: chapter user guide scikitlearn user guide release 0.11 shrinking boolean optional whether use shrinking heuristic 
3787: cache_size oat optional specify size kernel cache verbose bool default false enable verbose output note setting takes advantage perprocess runtime setting libsvm enabled may work properly multithreaded context 
3788: attributes sup port_ sup port_vectors_ arraylike shape n_sv arraylike shape nsv n_features index support vectors 
3789: support vectors 
3790: coef_ dual_coef_array shape n_classes1 n_sv array shape n_classes1 n_features array shape n_classes1 inter cept_ coefcient support vector decision function 
3791: weights asigned features coefcients primal problem available case linear kernel coef_ readonly property derived dual_coef_ support_vectors_ constants decision function 
3792: methods decision_function distance samples separating hyperplane fit sample_weight get_params deep predict predict_log_proba compute log likehoods possible outcomes samples predict_proba set_params params detects soft boundary set samples get parameters estimator perform classication regression samples 
3793: compute likehoods possible outcomes samples set parameters estimator 
3794: __init__ kernelrbf degree3 gamma0.0 coef00.0 cache_size200 verbosefalse decision_function distance samples separating hyperplane 
3795: tol0.001 nu0.5 shrinkingtrue parameters arraylike shape n_samples n_features returns arraylike shape n_samples n_class n_class1 returns decision function sample class model 
3796: fit sample_weightnone params detects soft boundary set samples 
3797: parameters arraylike sparse matrix shape n_samples n_features 1.8. reference scikitlearn user guide release 0.11 set samples n_samples number samples n_features number features 
3798: returns self object returns self 
3799: notes cordered contiguous array copied 
3800: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3801: predict perform classication regression samples classication model predicted class sample returned regression model function value calculated returned oneclass model returned 
3802: parameters arraylike sparse matrix shape n_samples n_features returns array shape n_samples predict_log_proba compute log likehoods possible outcomes samples model need probability information computed training time attribute probability set true 
3803: parameters arraylike shape n_samples n_features returns arraylike shape n_samples n_classes returns logprobabilities sample class model classes ordered arithmetical order 
3804: notes probability model created using cross validation results slightly different obtained predict also meaningless results small datasets 
3805: predict_proba compute likehoods possible outcomes samples model need probability information computed training time attribute probability set true 
3806: parameters arraylike shape n_samples n_features returns arraylike shape n_samples n_classes returns probability sample class model classes ordered arithmetical order 
3807: chapter user guide scikitlearn user guide release 0.11 notes probability model created using cross validation results slightly different obtained predict also meaningless results small datasets 
3808: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self svm.l1_min_c loss t_intercept ... return lowest bound l1_min_c innity sklearn.svm.l1_min_c sklearn.svm.l1_min_c lossl2 t_intercepttrue intercept_scaling1.0 return lowest bound l1_min_c empty ear_model.logisticregression penaltyl1 value valid class_weight parameter set 
3809: guaranteed applies penalized classiers linearsvc penaltyl1 lin innity model parameters arraylike sparse matrix shape n_samples n_features training vector n_samples number samples n_features num ber features 
3810: array shape n_samples target vector relative loss log default species loss function loss a.k.a squared hinge loss log loss logistic regression models 
3811: t_intercept bool default true species intercept tted model must match method paramenter 
3812: intercept_scaling oat default t_intercept true instance vector becomes intercept_scaling i.e syn thetic feature constant value equals intercept_scaling appended stance vector must match method parameter 
3813: returns l1_min_c oat minimum value lowlevel methods svm.libsvm.fit svm.libsvm.decision_function train model using libsvm lowlevel method predict margin libsvm name predict_values 1.8. reference continued next page scikitlearn user guide release 0.11 svm.libsvm.predict svm.libsvm.predict_proba svm.libsvm.cross_validation predict target values given model lowlevel method predict probabilities svm_model stores parameters needed predict given value binding crossvalidation routine lowlevel routine table 1.187 continued previous page sklearn.svm.libsvm.t sklearn.svm.libsvm.fit train model using libsvm lowlevel method parameters arraylike dtypeoat64 size n_samples n_features array dtypeoat64 size n_samples target vector svm_type type svm c_svc nusvc oneclasssvm epsilonsvr nusvr respectevely 
3814: kernel linear rbf poly sigmoid precomputed kernel use model linear polynomial rbf sigmoid precomputed 
3815: degree int32 degree polynomial kernel relevant kernel set polynomial gamma oat64 gamma parameter rbf kernel relevant kernel set rbf coef0 oat64 independent parameter polysigmoid kernel 
3816: tol oat64 stopping criteria 
3817: oat64 parameter csupport vector classication oat64 cache_size oat64 returns support array shape n_support index support vectors support_vectors array shape n_support n_features support vectors equivalent support return empty array case precomputed kernel 
3818: n_class_sv array number support vectors class 
3819: sv_coef array coefcients support vectors decision function 
3820: intercept array chapter user guide scikitlearn user guide release 0.11 intercept decision function label labels different classes relevant classication proba probb array probability estimates empty array probabilityfalse sklearn.svm.libsvm.decision_function sklearn.svm.libsvm.decision_function predict margin libsvm name predict_values reconstruct model parameters make sure stay sync python object 
3821: sklearn.svm.libsvm.predict sklearn.svm.libsvm.predict predict target values given model lowlevel method parameters arraylike dtypeoat size n_samples n_features svm_type type svm svc svc one class epsilon svr svr kernel linear rbf poly sigmoid precomputed kernel use model linear polynomial rbf sigmoid precomputed 
3822: degree int degree polynomial kernel relevant kernel set polynomial gamma oat gamma parameter rbf kernel relevant kernel set rbf coef0 oat independent parameter polysigmoid kernel 
3823: eps oat stopping criteria 
3824: oat parameter csupport vector classication returns dec_values array predicted values 
3825: todo probably theres point setting parameters like cache_size weights 1.8. reference scikitlearn user guide release 0.11 sklearn.svm.libsvm.predict_proba sklearn.svm.libsvm.predict_proba predict probabilities svm_model stores parameters needed predict given value speed real work done level function copy_predict libsvm_helper.c reconstruct model parameters make sure stay sync python object see sklearn.svm.predict complete list parameters 
3826: parameters arraylike dtypeoat array target vector kernel linear rbf poly sigmoid precomputed returns dec_values array predicted values 
3827: sklearn.svm.libsvm.cross_validation sklearn.svm.libsvm.cross_validation binding crossvalidation routine lowlevel routine parameters arraylike dtypeoat size n_samples n_features array dtypeoat size n_samples target vector svm_type type svm svc svc one class epsilon svr svr kernel linear rbf poly sigmoid precomputed kernel use model linear polynomial rbf sigmoid precomputed 
3828: degree int degree polynomial kernel relevant kernel set polynomial gamma oat gamma parameter rbf kernel relevant kernel set rbf coef0 oat independent parameter polysigmoid kernel 
3829: tol oat stopping criteria 
3830: oat parameter csupport vector classication oat cache_size oat chapter user guide scikitlearn user guide release 0.11 returns target array oat 1.8.27 sklearn.tree decision trees sklearn.tree module includes decision treebased models classication regression user guide see decision trees section details 
3831: tree.decisiontreeclassifier criterion ... decision tree classier tree.decisiontreeregressor criterion ... tree.extratreeclassifier criterion ... tree.extratreeregressor criterion ... tree regressor extremely randomized tree classier extremely randomized tree regressor 
3832: sklearn.tree.decisiontreeclassier class sklearn.tree.decisiontreeclassifier criteriongini min_samples_split1 min_density0.1 pute_importancesfalse random_statenone max_featuresnone max_depthnone min_samples_leaf1 com decision tree classier 
3833: parameters criterion string optional defaultgini function measure quality split supported criteria gini gini impurity entropy information gain 
3834: max_depth integer none optional defaultnone maximum depth tree none nodes expanded leaves pure leaves contain less min_samples_split samples 
3835: min_samples_split integer optional default1 minimum number samples required split internal node 
3836: min_samples_leaf integer optional default1 minimum number samples required leaf node 
3837: min_density oat optional default0.1 parameter controls tradeoff optimization heuristic controls minimum density sample_mask i.e fraction samples mask density falls threshold mask recomputed input data packed results min_density equals one partitions always represented data copying copies original data otherwise partitions represented bit masks aka sample masks 
3838: max_features int string none optional defaultnone number features consider looking best split auto max_featuressqrt n_features classication tasks max_featuresn_features regression problems log2 max_featureslog2 n_features none max_featuresn_features 
3839: sqrt max_featuressqrt n_features 
3840: compute_importances boolean optional defaulttrue whether computed feature_importances_ attribute calling 
3841: importances feature stored 1.8. reference scikitlearn user guide release 0.11 random_state int randomstate instance none optional defaultnone int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used np.random 
3842: see also decisiontreeregressor references r76 r77 r78 r79 examples sklearn.datasets import load_iris sklearn.cross_validation import cross_val_score sklearn.tree import decisiontreeclassifier clf decisiontreeclassifier random_state0 iris load_iris cross_val_score clf iris.data iris.target cv10 ... ... array 
3843: 0.86 ... 0.93 ... 0.93 ... 
3844: 0.93 ... 
3845: 0.93 ... 0.93 ... 0.93 ... attributes tree_ fea ture_importances_ tree object array shape n_features underlying tree object feature mportances higher important feature importance feature computed normalized total reduction error brought feature also known gini importance r79 
3846: methods fit sample_mask x_argsorted build decision tree training set fit_transform get_params deep predict predict_log_proba predict_proba score set_params params transform threshold fit data transform get parameters estimator predict class regression target predict class logprobabilities input samples predict class probabilities input samples returns mean accuracy given test data labels set parameters estimator reduce important features 
3847: __init__ criteriongini min_samples_leaf1 min_density0.1 max_featuresnone compute_importancesfalse random_statenone min_samples_split1 max_depthnone chapter user guide scikitlearn user guide release 0.11 fit sample_masknone x_argsortednone build decision tree training set 
3848: parameters arraylike shape n_samples n_features training input samples 
3849: arraylike shape n_samples target values integers correspond classes classication real numbers regression 
3850: returns self object returns self 
3851: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
3852: parameters numpy array shape n_samples n_features training set 
3853: numpy array shape n_samples target values 
3854: returns x_new numpy array shape n_samples n_features_new transformed array 
3855: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
3856: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3857: predict predict class regression target classication model predicted class sample returned regression model predicted value based returned 
3858: parameters arraylike shape n_samples n_features input samples 
3859: returns array shape n_samples predicted classes predict values 
3860: predict_log_proba predict class logprobabilities input samples 
3861: parameters arraylike shape n_samples n_features input samples 
3862: 1.8. reference scikitlearn user guide release 0.11 returns array shape n_samples n_classes class logprobabilities input samples classes ordered arithmetical order predict_proba predict class probabilities input samples 
3863: parameters arraylike shape n_samples n_features input samples 
3864: returns array shape n_samples n_classes class probabilities input samples classes ordered arithmetical order 
3865: score returns mean accuracy given test data labels 
3866: parameters arraylike shape n_samples n_features training set 
3867: arraylike shape n_samples labels 
3868: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform thresholdnone reduce important features 
3869: parameters array scipy sparse matrix shape n_samples n_features input samples 
3870: threshold string oat none optional defaultnone threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor e.g. 1.25mean may also used none available object attribute threshold used otherwise mean used default 
3871: returns x_r array shape n_samples n_selected_features input samples selected features 
3872: sklearn.tree.decisiontreeregressor class sklearn.tree.decisiontreeregressor criterionmse tree regressor 
3873: min_samples_split1 min_density0.1 pute_importancesfalse random_statenone max_featuresnone max_depthnone min_samples_leaf1 com chapter user guide scikitlearn user guide release 0.11 parameters criterion string optional defaultmse function measure quality split supported criterion mse mean squared error 
3874: max_depth integer none optional defaultnone maximum depth tree none nodes expanded leaves pure leaves contain less min_samples_split samples 
3875: min_samples_split integer optional default1 minimum number samples required split internal node 
3876: min_samples_leaf integer optional default1 minimum number samples required leaf node 
3877: min_density oat optional default0.1 parameter controls tradeoff optimization heuristic controls minimum density sample_mask i.e fraction samples mask density falls threshold mask recomputed input data packed results min_density equals one partitions always represented data copying copies original data otherwise partitions represented bit masks aka sample masks 
3878: max_features int string none optional defaultnone number features consider looking best split auto max_featuressqrt n_features classication tasks max_featuresn_features regression problems log2 max_featureslog2 n_features none max_featuresn_features 
3879: sqrt max_featuressqrt n_features 
3880: compute_importances boolean optional defaulttrue whether computed feature_importances_ attribute calling 
3881: importances feature stored random_state int randomstate instance none optional defaultnone int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used np.random 
3882: see also decisiontreeclassifier references r80 r81 r82 r83 examples sklearn.datasets import load_boston sklearn.cross_validation import cross_val_score sklearn.tree import decisiontreeregressor boston load_boston regressor decisiontreeregressor random_state0 1.8. reference scikitlearn user guide release 0.11 scores a.k.a coefcient determination 10folds cross_val_score regressor boston.data boston.target cv10 ... ... array 0.61 ... 0.57 ... 0.34 ... 0.41 ... 0.75 ... 0.07 ... 0.29 ... 0.33 ... 1.42 ... 1.77 ... attributes tree_ fea ture_importances_ tree object array shape n_features underlying tree object feature mportances higher important feature importance feature computed normalized total reduction error brought feature also known gini importance r83 
3883: methods fit sample_mask x_argsorted build decision tree training set fit_transform get_params deep predict score set_params params transform threshold fit data transform get parameters estimator predict class regression target returns coefcient determination prediction set parameters estimator reduce important features 
3884: __init__ criterionmse max_depthnone min_samples_leaf1 min_density0.1 max_featuresnone compute_importancesfalse random_statenone min_samples_split1 fit sample_masknone x_argsortednone build decision tree training set 
3885: parameters arraylike shape n_samples n_features training input samples 
3886: arraylike shape n_samples target values integers correspond classes classication real numbers regression 
3887: returns self object returns self 
3888: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
3889: parameters numpy array shape n_samples n_features training set 
3890: numpy array shape n_samples target values 
3891: returns x_new numpy array shape n_samples n_features_new chapter user guide scikitlearn user guide release 0.11 transformed array 
3892: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
3893: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3894: predict predict class regression target classication model predicted class sample returned regression model predicted value based returned 
3895: parameters arraylike shape n_samples n_features input samples 
3896: returns array shape n_samples predicted classes predict values 
3897: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
3898: parameters arraylike shape n_samples n_features training set 
3899: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform thresholdnone reduce important features 
3900: parameters array scipy sparse matrix shape n_samples n_features input samples 
3901: threshold string oat none optional defaultnone 1.8. reference scikitlearn user guide release 0.11 threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor e.g. 1.25mean may also used none available object attribute threshold used otherwise mean used default 
3902: returns x_r array shape n_samples n_selected_features input samples selected features 
3903: sklearn.tree.extratreeclassier class sklearn.tree.extratreeclassifier criteriongini min_samples_split1 min_density0.1 pute_importancesfalse random_statenone max_featuresauto max_depthnone min_samples_leaf1 com extremely randomized tree classier extratrees differ classic decision trees way built looking best split separate samples node two groups random splits drawn max_features randomly selected features best split among chosen max_features set amounts building totally random decision tree warning extratrees used within ensemble methods see also extratreeregressor extratreesclassifier extratreesregressor references r84 methods fit sample_mask x_argsorted build decision tree training set fit_transform get_params deep predict predict_log_proba predict_proba score set_params params transform threshold fit data transform get parameters estimator predict class regression target predict class logprobabilities input samples predict class probabilities input samples returns mean accuracy given test data labels set parameters estimator reduce important features 
3904: __init__ criteriongini min_density0.1 dom_statenone max_depthnone min_samples_split1 max_featuresauto compute_importancesfalse min_samples_leaf1 ran fit sample_masknone x_argsortednone build decision tree training set 
3905: parameters arraylike shape n_samples n_features training input samples 
3906: chapter user guide scikitlearn user guide release 0.11 arraylike shape n_samples target values integers correspond classes classication real numbers regression 
3907: returns self object returns self 
3908: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
3909: parameters numpy array shape n_samples n_features training set 
3910: numpy array shape n_samples target values 
3911: returns x_new numpy array shape n_samples n_features_new transformed array 
3912: notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
3913: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3914: predict predict class regression target classication model predicted class sample returned regression model predicted value based returned 
3915: parameters arraylike shape n_samples n_features input samples 
3916: returns array shape n_samples predicted classes predict values 
3917: predict_log_proba predict class logprobabilities input samples 
3918: parameters arraylike shape n_samples n_features input samples 
3919: returns array shape n_samples n_classes class logprobabilities input samples classes ordered arithmetical order 
3920: 1.8. reference scikitlearn user guide release 0.11 predict_proba predict class probabilities input samples 
3921: parameters arraylike shape n_samples n_features input samples 
3922: returns array shape n_samples n_classes class probabilities input samples classes ordered arithmetical order 
3923: score returns mean accuracy given test data labels 
3924: parameters arraylike shape n_samples n_features training set 
3925: arraylike shape n_samples labels 
3926: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform thresholdnone reduce important features 
3927: parameters array scipy sparse matrix shape n_samples n_features input samples 
3928: threshold string oat none optional defaultnone threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor e.g. 1.25mean may also used none available object attribute threshold used otherwise mean used default 
3929: returns x_r array shape n_samples n_selected_features input samples selected features 
3930: sklearn.tree.extratreeregressor class sklearn.tree.extratreeregressor criterionmse min_samples_split1 min_density0.1 pute_importancesfalse random_statenone max_featuresauto max_depthnone min_samples_leaf1 com extremely randomized tree regressor extratrees differ classic decision trees way built looking best split separate samples node two groups random splits drawn max_features randomly selected features best split among chosen max_features set amounts building totally random decision tree 
3931: chapter user guide scikitlearn user guide release 0.11 warning extratrees used within ensemble methods see also extratreeclassifiera classier base extremely randomized trees sklearn.ensemble.extratreesclassifieran ensemble extratrees classication sklearn.ensemble.extratreesregressoran ensemble extratrees regression references r85 methods fit sample_mask x_argsorted build decision tree training set fit_transform get_params deep predict score set_params params transform threshold fit data transform get parameters estimator predict class regression target returns coefcient determination prediction set parameters estimator reduce important features 
3932: __init__ criterionmse min_density0.1 dom_statenone max_depthnone min_samples_split1 min_samples_leaf1 ran max_featuresauto compute_importancesfalse fit sample_masknone x_argsortednone build decision tree training set 
3933: parameters arraylike shape n_samples n_features training input samples 
3934: arraylike shape n_samples target values integers correspond classes classication real numbers regression 
3935: returns self object returns self 
3936: fit_transform ynone t_params fit data transform fits transformer optional parameters t_params returns transformed version 
3937: parameters numpy array shape n_samples n_features training set 
3938: numpy array shape n_samples target values 
3939: returns x_new numpy array shape n_samples n_features_new transformed array 
3940: 1.8. reference scikitlearn user guide release 0.11 notes method calls transform consecutively i.e. optimized implementation t_transform unlike transformers pca 
3941: get_params deeptrue get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3942: predict predict class regression target classication model predicted class sample returned regression model predicted value based returned 
3943: parameters arraylike shape n_samples n_features input samples 
3944: returns array shape n_samples predicted classes predict values 
3945: score returns coefcient determination prediction coefcient dened regression sum squares y_pred .sum residual sum squares y_true y_true.mean .sum best possible score 1.0 lower values worse 
3946: parameters arraylike shape n_samples n_features training set 
3947: arraylike shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform thresholdnone reduce important features 
3948: parameters array scipy sparse matrix shape n_samples n_features input samples 
3949: threshold string oat none optional defaultnone threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor e.g. 1.25mean may also used none available object attribute threshold used otherwise mean used default 
3950: chapter user guide scikitlearn user guide release 0.11 returns x_r array shape n_samples n_selected_features input samples selected features 
3951: tree.export_graphviz decision_tree ... export decision tree dot format 
3952: sklearn.tree.export_graphviz sklearn.tree.export_graphviz decision_tree out_lenone feature_namesnone export decision tree dot format function generates graphviz representation decision tree written out_le exported graphical renderings generated using example dot tps tree.dot tree.ps dot tpng tree.dot tree.png postscript format png format parameters decision_tree decision tree classier decision tree exported graphviz object string optional defaultnone handle name output 
3953: feature_names list strings optional defaultnone names features 
3954: returns out_le object object tree exported user expected close object done 
3955: examples sklearn.datasets import load_iris sklearn import tree clf tree.decisiontreeclassifier iris load_iris clf clf.fit iris.data iris.target import tempfile out_file tree.export_graphviz clf out_filetempfile.temporaryfile out_file.close 1.8.28 sklearn.utils utilities sklearn.utils module includes various utilites developer guide see utilities developers page details 
3956: utils.check_random_state seed turn seed np.random.randomstate instance utils.resample arrays options utils.shuffle arrays options resample arrays sparse matrices consistent way shufe arrays sparse matrices consistent way 1.8. reference scikitlearn user guide release 0.11 sklearn.utils.check_random_state sklearn.utils.check_random_state seed turn seed np.random.randomstate instance seed none return randomstate singleton used np.random seed int return new ran domstate instance seeded seed seed already randomstate instance return otherwise raise valueerror 
3957: sklearn.utils.resample sklearn.utils.resample arrays options resample arrays sparse matrices consistent way default strategy implements one step bootstrapping procedure 
3958: parameters arrays sequence arrays scipy.sparse matrices shape replace boolean true default implements resampling replacement false implement sliced random permutations 
3959: n_samples int none default number samples generate dimension arrays 
3960: left none automatically set rst random_state int randomstate instance control shufing reproducible behavior 
3961: returns sequence resampled views collections original arrays impacted see also sklearn.cross_validation.bootstrap sklearn.utils.shuffle examples possible mix sparse dense arrays run np.array scipy.sparse import coo_matrix x_sparse coo_matrix sklearn.utils import resample x_sparse resample x_sparse random_state0 array x_sparse 3x2 sparse matrix type type numpy.float64 stored elements compressed sparse row format chapter user guide scikitlearn user guide release 0.11 x_sparse.toarray array array resample n_samples2 random_state0 array sklearn.utils.shufe sklearn.utils.shuffle arrays options shufe arrays sparse matrices consistent way convenience alias resample arrays replacefalse random permutations collections 
3962: parameters arrays sequence arrays scipy.sparse matrices shape random_state int randomstate instance control shufing reproducible behavior 
3963: n_samples int none default number samples generate dimension arrays 
3964: left none automatically set rst returns sequence shufed views collections original arrays impacted see also sklearn.utils.resample examples possible mix sparse dense arrays run np.array scipy.sparse import coo_matrix x_sparse coo_matrix sklearn.utils import shuffle x_sparse shuffle x_sparse random_state0 array x_sparse 3x2 sparse matrix type type numpy.float64 stored elements compressed sparse row format 1.8. reference scikitlearn user guide release 0.11 x_sparse.toarray array array shuffle n_samples2 random_state0 array chapter user guide chapter two example gallery 2.1 examples 2.1.1 general examples generalpurpose introductory examples scikit 
3965: figure 2.1 plot classication probability plot classication probability plot classication probability different classiers use class dataset classify support vector classier well penalized logistic regression logistic regression multiclass classier box result identify rst class 
3966: scikitlearn user guide release 0.11 script output classif_rate linear svc 82.000000 classif_rate logistic 79.333333 classif_rate logistic 76.666667 python source code plot_classification_probability.py print __doc__ author alexandre gramfort alexandre.gramfort inria.fr license bsd style 
3967: import pylab import numpy sklearn.linear_model import logisticregression sklearn.svm import svc sklearn import datasets chapter example gallery scikitlearn user guide release 0.11 iris datasets.load_iris iris.data take first two features visualization iris.target n_features x.shape 1.0 create different classifiers logistic regression multiclass box classifiers logistic logisticregression penaltyl1 logistic logisticregression penaltyl2 linear svc svc kernellinear probabilitytrue n_classifiers len classifiers pl.figure figsize n_classifiers pl.subplots_adjust bottom.2 top.95 index name classifier enumerate classifiers.iteritems classifier.fit y_pred classifier.predict classif_rate np.mean y_pred.ravel y.ravel print classif_rate name classif_rate view probabilities np.linspace np.linspace np.meshgrid xfull np.c_ xx.ravel yy.ravel probas classifier.predict_proba xfull n_classes np.unique y_pred .size range n_classes pl.subplot n_classifiers n_classes index n_classes pl.title class pl.ylabel name imshow_handle pl.imshow probas .reshape extent originlower pl.xticks pl.yticks idx y_pred idx.any pl.scatter idx idx markero pl.axes 0.15 0.04 0.7 0.05 pl.title probability pl.colorbar imshow_handle caxax orientationhorizontal pl.show 2.1. examples scikitlearn user guide release 0.11 figure 2.2 confusion matrix confusion matrix example confusion matrix usage evaluate quality output classier 
3968: script output chapter example gallery scikitlearn user guide release 0.11 python source code plot_confusion_matrix.py print __doc__ import random import pylab sklearn import svm datasets sklearn.metrics import confusion_matrix import data play iris datasets.load_iris iris.data iris.target n_samples n_features x.shape range n_samples random.seed random.shuffle half int n_samples run classifier classifier svm.svc kernellinear classifier.fit half half .predict half compute confusion matrix confusion_matrix half print show confusion matrix pl.matshow pl.title confusion matrix pl.colorbar pl.show figure 2.3 recognizing handwritten digits recognizing handwritten digits example showing scikitlearn used recognize images handwritten digits example commented tutorial section user manual 
3969: 2.1. examples scikitlearn user guide release 0.11 script output classification report classifier svc c1.0 cache_size200 class_weightnone coef00.0 degree3 gamma0.001 kernelrbf probabilityfalse shrinkingtrue tol0.001 verbosefalse precision recall f1score support avg total confusion matrix 0.99 0.98 0.99 0.92 0.97 0.96 0.99 0.97 0.97 0.95 0.97 1.00 0.99 0.99 0.98 0.99 0.95 0.99 0.96 0.94 0.93 0.97 0.99 0.97 0.99 0.87 0.96 0.97 0.99 0.99 1.00 0.98 0.97 chapter example gallery scikitlearn user guide release 0.11 python source code plot_digits_classification.py print __doc__ author gael varoquaux gael dot varoquaux normalesup dot org license simplified bsd standard scientific python imports import pylab import datasets classifiers performance metrics sklearn import datasets svm metrics digits dataset digits datasets.load_digits data interested made 8x8 images digits lets look first images stored images attribute dataset working image files could load using pylab.imread images know digit represent given target dataset index image label enumerate zip digits.images digits.target pl.subplot index pl.axis pl.imshow image cmappl.cm.gray_r interpolationnearest pl.title training label apply classifier data need flatten image turn data samples feature matrix n_samples len digits.images data digits.images.reshape n_samples create classifier support vector classifier classifier svm.svc gamma0.001 learn digits first half digits classifier.fit data n_samples digits.target n_samples predict value digit second half expected digits.target n_samples predicted classifier.predict data n_samples print classification report classifier classifier metrics.classification_report expected predicted print confusion matrix metrics.confusion_matrix expected predicted index image prediction enumerate zip digits.images n_samples predicted pl.subplot index pl.axis pl.imshow image cmappl.cm.gray_r interpolationnearest 2.1. examples scikitlearn user guide release 0.11 pl.title prediction prediction pl.show figure 2.4 pipelining chaining pca logistic regression pipelining chaining pca logistic regression pca unsupervised dimensionality reduction logistic regression prediction use gridsearchcv set dimensionality pca python source code plot_digits_pipe.py print __doc__ code source gael varoqueux modified documentation merge jaques grobler license bsd import numpy import pylab sklearn import linear_model decomposition datasets cross_validation logistic linear_model.logisticregression pca decomposition.pca sklearn.pipeline import pipeline pipe pipeline steps pca pca logistic logistic chapter example gallery scikitlearn user guide release 0.11 digits datasets.load_digits x_digits digits.data y_digits digits.target plot pca spectrum pca.fit x_digits pl.figure figsize pl.clf pl.axes pl.plot pca.explained_variance_ linewidth2 pl.axis tight pl.xlabel n_components pl.ylabel explained_variance_ prediction sklearn.grid_search import gridsearchcv n_components np.logspace parameters pipelines set using separated parameter names estimator gridsearchcv pipe estimator.fit x_digits y_digits dict pca__n_componentsn_components logistic__ccs pl.axvline estimator.best_estimator_.named_steps pca .n_components linestyle labeln_components chosen pl.legend propdict size12 pl.show figure 2.5 univariate feature selection univariate feature selection example showing univariate feature selection noisy non informative features added iris data univariate feature selection applied feature plot pvalues univariate feature selection corresponding weights svm see univariate feature selection selects informative features larger svm weights total set features rst ones signicant see highest score univariate feature selection svm attributes small weights features weight non zero 
3970: 2.1. examples scikitlearn user guide release 0.11 applying univariate feature selection svm increases svm weight attributed signicant features thus improve classication 
3971: python source code plot_feature_selection.py print __doc__ import numpy import pylab sklearn import datasets svm sklearn.feature_selection import selectpercentile f_classif import data play iris dataset iris datasets.load_iris noisy data correlated np.random.normal size len iris.data add noisy data informative features np.hstack iris.data iris.target chapter example gallery scikitlearn user guide release 0.11 pl.figure pl.clf x_indices np.arange x.shape univariate feature selection ftest feature scoring use default selection function significant features selector selectpercentile f_classif percentile10 selector.fit scores np.log10 selector.scores_ scores scores.max pl.bar x_indices .45 scores width.3 labelrunivariate score log value colorg compare weights svm clf svm.svc kernellinear clf.fit svm_weights clf.coef_ .sum axis0 svm_weights svm_weights.max pl.bar x_indices .15 svm_weights width.3 labelsvm weight colorr pl.title comparing feature selection pl.xlabel feature number pl.yticks pl.axis tight pl.legend locupper right pl.show figure 2.6 demonstration sampling hmm demonstration sampling hmm script shows sample points hiden markov model hmm use 4components specied mean covariance plot show sequence observations generated transitions see specied transition matrix transition component 
3972: 2.1. examples scikitlearn user guide release 0.11 python source code plot_hmm_sampling.py import numpy import matplotlib.pyplot plt sklearn import hmm prepare parameters 3components hmm initial population probability start_prob np.array 0.6 0.3 0.1 0.0 transition matrix note transitions possible component trans_mat np.array 0.7 0.2 0.0 0.1 0.3 0.5 0.2 0.0 0.0 0.3 0.5 0.2 0.2 0.0 0.2 0.6 means component means np.array 0.0 0.0 0.0 11.0 9.0 10.0 11.0 1.0 covariance component covars np.tile np.identity chapter example gallery scikitlearn user guide release 0.11 build hmm instance set parameters model hmm.gaussianhmm full start_prob trans_mat random_state42 instead fitting data directly set estimated parameters means covariance components model.means_ means model.covars_ covars generate samples model.sample plot sampled data plt.plot label observations ms6 mfc orange alpha0.7 indicate component numbers enumerate means plt.text component size17 horizontalalignmentcenter bboxdict alpha.7 facecolorw plt.legend locbest plt.show figure 2.7 gaussian hmm stock data gaussian hmm stock data script shows use gaussian hmm uses stock price data obtained yahoo nance information get stock prices matplotlib please refer date_demo1.py matplotlib 
3973: 2.1. examples scikitlearn user guide release 0.11 script output fitting hmm decoding ... done transition matrix 9.82570383e01 1.00013693e02 4.07843572e11 9.22066524e03 4.97052623e19 4.97052623e19 1.08017604e02 2.08227185e19 9.57251401e03 9.86343408e01 2.45954846e08 1.74235836e19 7.42822313e03 9.87982473e01 5.85368960e19 2.79686205e03 4.97087223e19 9.97761575e01 2.23842520e03 2.14011406e19 2.08075306e19 9.89198240e01 4.08407805e03 1.98929510e19 6.09198515e12 means vars hidden state 0th hidden state mean var 1.27509477e02 0.10510603 1.29457075 1.90812491e01 1th hidden state mean var 3.75072729e04 2.64279876 4.05990139e01 110.47500125 2th hidden state chapter example gallery mean var 0.01559469 6.23562441 0.020839 2.01414701 scikitlearn user guide release 0.11 3th hidden state mean var 0.2040404 2.49727960e04 1.48267818e01 2.58432154 4th hidden state mean var 0.38174069 1.13046223e03 2.37575708e01 3.87275215 python source code plot_hmm_stock_analysis.py print __doc__ import datetime import numpy import pylab matplotlib.finance import quotes_historical_yahoo matplotlib.dates import yearlocator monthlocator dateformatter sklearn.hmm import gaussianhmm downloading data date1 datetime.date start date date2 datetime.date end date get quotes yahoo finance quotes quotes_historical_yahoo intc date1 date2 len quotes raise systemexit unpack quotes dates np.array quotes dtypeint close_v np.array quotes volume np.array quotes take diff close value makes len diff len close_t therefore others quantity also need shifted diff close_v close_v dates dates close_v close_v pack diff volume training np.column_stack diff volume run gaussian hmm print fitting hmm decoding ... n_components make hmm instance execute fit model gaussianhmm n_components diag model.fit n_iter1000 predict optimal sequence internal hidden state hidden_states model.predict 2.1. examples scikitlearn user guide release 0.11 print donen print trained parameters plot print transition matrix print model.transmat_ print print means vars hidden state xrange n_components print dth hidden state print mean model.means_ print var np.diag model.covars_ print every year years yearlocator months monthlocator every month yearsfmt dateformatter fig pl.figure fig.add_subplot xrange n_components use fancy indexing plot data state idx hidden_states ax.plot_date dates idx close_v idx label dth hidden state ax.legend format ticks ax.xaxis.set_major_locator years ax.xaxis.set_major_formatter yearsfmt ax.xaxis.set_minor_locator months ax.autoscale_view format coords message box ax.fmt_xdata dateformatter ax.fmt_ydata lambda 1.2f ax.grid true fig.autofmt_xdate pl.show figure 2.8 classiers comparison classiers comparison comparison knearestneighbours logistic regression linear svc classifying iris dataset 
3974: chapter example gallery scikitlearn user guide release 0.11 python source code plot_iris_classifiers.py print __doc__ code source gael varoqueux modified documentation merge jaques grobler license bsd import numpy import pylab sklearn import neighbors datasets linear_model svm import data play iris datasets.load_iris iris.data take first two features iris.target .02 step size mesh classifiers dict knnneighbors.kneighborsclassifier logisticlinear_model.logisticregression c1e5 svmsvm.linearsvc c1e5 lossl1 fignum create instance neighbours classifier fit data 
3975: 2.1. examples scikitlearn user guide release 0.11 name clf classifiers.iteritems clf.fit plot decision boundary asign color point mesh x_min m_max y_min y_max x_min x_max .min .max y_min y_max .min .max np.meshgrid np.arange x_min x_max np.arange y_min y_max clf.predict np.c_ xx.ravel yy.ravel put result color plot z.reshape xx.shape pl.figure fignum figsize pl.pcolormesh cmappl.cm.paired plot also training points pl.scatter cmappl.cm.paired pl.xlabel sepal length pl.ylabel sepal width pl.xlim xx.min xx.max pl.ylim yy.min yy.max pl.xticks pl.yticks fignum pl.show figure 2.9 explicit feature map approximation rbf kernels explicit feature map approximation rbf kernels example shows use rbfsampler appoximate feature map rbf kernel classication svm digits dataset results using linear svm original space linear svm using approximate mapping using kernelized svm compared timings accuracy varying amounts monte carlo samplings approximate mapping shown sampling dimensions clearly leads better classication results comes greater cost means tradeoff runtime accuracy given parameter n_components note solving linear svm also approximate kernel svm could greatly accelerated using stochastic gradient descent via sklearn.linear_model.sgdclassifier easily possible case kernelized svm second plot visualized decision surfaces rbf kernel svm linear svm approximate kernel map plot shows decision surfaces classiers projected onto rst two principal components data visualization taken grain salt since interesting slice decision surface dimensions particular note datapoint represented dot necessarily classied chapter example gallery region lying since lie plane rst two principal components span usage rbfsampler described detail kernel approximation 
3976: scikitlearn user guide release 0.11 python source code plot_kernel_approximation.py print __doc__ author gael varoquaux gael dot varoquaux normalesup dot org license simplified bsd modified andreas mueller standard scientific python imports import pylab import numpy time import time import datasets classifiers performance metrics sklearn import datasets svm pipeline sklearn.kernel_approximation import rbfsampler sklearn.decomposition import pca digits dataset digits datasets.load_digits n_class9 apply classifier data need flatten image turn data samples feature matrix n_samples len digits.data data digits.data 16. data data.mean axis0 learn digits first half digits data_train targets_train data n_samples digits.target n_samples predict value digit second half 2.1. examples scikitlearn user guide release 0.11 data_test targets_test data n_samples digits.target n_samples data_test scaler.transform data_test create classifier support vector classifier kernel_svm svm.svc gamma.2 linear_svm svm.linearsvc create pipeline kernel approximation linear svm feature_map rbfsampler gamma.2 random_state1 approx_kernel_svm pipeline.pipeline feature_map feature_map svm svm.linearsvc fit predict using linear kernel svm kernel_svm_time time kernel_svm.fit data_train targets_train kernel_svm_score kernel_svm.score data_test targets_test kernel_svm_time time kernel_svm_time linear_svm_time time linear_svm.fit data_train targets_train linear_svm_score linear_svm.score data_test targets_test linear_svm_time time linear_svm_time sample_sizes np.arange approx_kernel_scores approx_kernel_times sample_sizes approx_kernel_svm.set_params feature_map__n_componentsd approx_kernel_timing time approx_kernel_svm.fit data_train targets_train approx_kernel_times.append time approx_kernel_timing score approx_kernel_svm.score data_test targets_test approx_kernel_scores.append score plot results accuracy pl.subplot second axis timeings timescale pl.subplot accuracy.plot sample_sizes approx_kernel_scores label approx kernel timescale.plot sample_sizes approx_kernel_times labelapprox kernel horizontal lines exact rbf linear kernels accuracy.plot sample_sizes sample_sizes linear_svm_score linear_svm_score label linear svm timescale.plot sample_sizes sample_sizes linear_svm_time linear_svm_time labellinear svm accuracy.plot sample_sizes sample_sizes kernel_svm_score kernel_svm_score label rbf svm timescale.plot sample_sizes sample_sizes kernel_svm_time kernel_svm_time labelrbf svm vertical line dataset dimensionality chapter example gallery scikitlearn user guide release 0.11 accuracy.plot 0.7 label n_features legends labels accuracy.set_title classification accuracy timescale.set_title training times accuracy.set_xlim sample_sizes sample_sizes accuracy.set_xticks accuracy.set_ylim np.min approx_kernel_scores timescale.set_xlabel sampling steps transformed feature dimension accuracy.set_ylabel classification accuracy timescale.set_ylabel training time seconds accuracy.legend locbest timescale.legend locbest visualize decision surface projected first two principal components dataset pca pca n_components8 .fit data_train pca.transform data_train gemerate grid along first two principal components multiples np.arange 0.1 steps along first component first multiples np.newaxis pca.components_ steps along second component second multiples np.newaxis pca.components_ combine grid first np.newaxis second np.newaxis flat_grid grid.reshape data.shape title plots titles svc rbf kernel svc linear kernel rbf feature mapn n_components100 pl.figure figsize predict plot clf enumerate kernel_svm approx_kernel_svm plot decision boundary asign color point mesh x_min m_max y_min y_max pl.subplot clf.predict flat_grid put result color plot z.reshape grid.shape pl.contourf multiples multiples cmappl.cm.paired pl.axis plot also training points pl.scatter ctargets_train cmappl.cm.paired pl.title titles pl.show linear quadratic discriminant analysis condence ellipsoid plot condence ellipsoids class decision boundary 2.1. examples scikitlearn user guide release 0.11 figure 2.10 linear quadratic discriminant analysis condence ellipsoid python source code plot_lda_qda.py print __doc__ scipy import linalg import numpy import pylab import matplotlib mpl matplotlib import colors sklearn.lda import lda sklearn.qda import qda chapter example gallery scikitlearn user guide release 0.11 colormap cmap colors.linearsegmentedcolormap red_blue_classes red 0.7 0.7 green 0.7 0.7 0.7 0.7 blue 0.7 0.7 pl.cm.register_cmap cmapcmap generate datasets def dataset_fixed_cov generate gaussians samples covariance matrix dim np.random.seed np.array 0.23 0.83 .23 np.r_ np.dot np.random.randn dim np.dot np.random.randn dim np.array np.hstack np.zeros np.ones return def dataset_cov generate gaussians samples different covariance matrices dim np.random.seed np.array 2.5 np.r_ np.dot np.random.randn dim np.dot np.random.randn dim c.t np.array np.hstack np.zeros np.ones return plot functions def plot_data lda y_pred fig_index splot pl.subplot fig_index fig_index pl.title linear discriminant analysis pl.ylabel data fixed covariance elif fig_index pl.title quadratic discriminant analysis elif fig_index pl.ylabel data varying covariances true positive y_pred tp0 tp1 x0_tp x0_fp tp0 tp0 true x1_tp x1_fp tp1 tp1 true xmin xmax .min .max ymin ymax .min .max class dots pl.plot x0_tp x0_tp colorred pl.plot x0_fp x0_fp color dark red class dots pl.plot x1_tp x1_tp colorblue 2.1. examples scikitlearn user guide release 0.11 pl.plot x1_fp x1_fp color dark blue class areas x_min x_max pl.xlim y_min y_max pl.ylim np.meshgrid np.linspace x_min x_max np.linspace y_min y_max lda.predict_proba np.c_ xx.ravel yy.ravel .reshape xx.shape pl.pcolormesh cmapred_blue_classes normcolors.normalize pl.contour 0.5 linewidths2. colorsk means pl.plot lda.means_ lda.means_ colorblack markersize10 pl.plot lda.means_ lda.means_ colorblack markersize10 return splot def plot_ellipse splot mean cov color linalg.eigh cov linalg.norm angle np.arctan angle angle np.pi convert degrees filled gaussian standard deviation ell mpl.patches.ellipse mean 0.5 0.5 angle colorcolor ell.set_clip_box splot.bbox ell.set_alpha 0.5 splot.add_artist ell splot.set_xticks splot.set_yticks def plot_lda_cov lda splot plot_ellipse splot lda.means_ lda.covariance_ red plot_ellipse splot lda.means_ lda.covariance_ blue def plot_qda_cov qda splot plot_ellipse splot qda.means_ qda.covariances_ red plot_ellipse splot qda.means_ qda.covariances_ blue enumerate dataset_fixed_cov dataset_cov lda lda lda y_pred lda.fit store_covariancetrue .predict splot plot_data lda y_pred fig_index2 plot_lda_cov lda splot pl.axis tight qda qda qda chapter example gallery scikitlearn user guide release 0.11 y_pred qda.fit store_covariancestrue .predict splot plot_data qda y_pred fig_index2 plot_qda_cov qda splot pl.axis tight pl.suptitle lda qda pl.show figure 2.11 multilabel classication multilabel classication example simulates multilabel document classication problem dataset generated randomly based following process pick number labels poisson n_labels times choose class multinomial theta pick document length poisson length times choose word multinomial theta_c process rejection sampling used make sure document length never zero likewise reject classes already chosen documents assigned classes plotted surrounded two colored circles classication performed projecting rst two principal components found pca cca visual isation purposes followed using sklearn.multiclass.onevsrestclassifier metaclassier using two svcs linear kernels learn discriminative model class note pca used perform unsupervised dimensionality reduction cca used perform supervised one 
3977: 2.1. examples scikitlearn user guide release 0.11 python source code plot_multilabel.py print __doc__ import numpy import matplotlib.pylab sklearn.datasets import make_multilabel_classification sklearn.multiclass import onevsrestclassifier sklearn.svm import svc sklearn.preprocessing import labelbinarizer sklearn.decomposition import pca sklearn.pls import cca def plot_hyperplane clf min_x max_x linestyle label get separating hyperplane clf.coef_ np.linspace min_x max_x make sure line long enough clf.intercept_ pl.plot linestyle labellabel def plot_subfigure subplot title transform transform pca chapter example gallery scikitlearn user guide release 0.11 pca n_components2 .fit_transform elif transform cca convert list tuples class indicator matrix first y_indicator labelbinarizer .fit .transform cca n_components2 .fit y_indicator .transform else raise valueerror min_x np.min max_x np.max classif onevsrestclassifier svc kernellinear classif.fit pl.subplot subplot pl.title title zero_class np.where one_class np.where pl.scatter s40 cgray pl.scatter zero_class zero_class s160 edgecolorsb facecolorsnone linewidths2 labelclass pl.scatter one_class one_class s80 edgecolorsorange facecolorsnone linewidths2 labelclass pl.axis tight plot_hyperplane classif.estimators_ min_x max_x boundarynfor class plot_hyperplane classif.estimators_ min_x max_x boundarynfor class pl.xticks pl.yticks subplot pl.xlim min_x max_x pl.xlabel first principal component pl.ylabel second principal component pl.legend loc upper left pl.figure figsize make_multilabel_classification n_classes2 n_labels1 allow_unlabeledtrue random_state1 plot_subfigure unlabeled samples cca cca plot_subfigure unlabeled samples pca pca make_multilabel_classification n_classes2 n_labels1 allow_unlabeledfalse random_state1 plot_subfigure without unlabeled samples cca cca plot_subfigure without unlabeled samples pca pca pl.subplots_adjust .04 .02 .97 .94 .09 pl.show 2.1. examples scikitlearn user guide release 0.11 figure 2.12 test permutations signicance classication score test permutations signicance classication score order test classication score signicative technique repeating classication procedure ran domizing permuting labels pvalue given percentage runs score obtained greater classication score obtained rst place 
3978: script output classification score 0.393333333333 pvalue 0.0792079207921 python source code plot_permutation_test_for_classification.py author license bsd alexandre gramfort alexandre.gramfort inria.fr chapter example gallery scikitlearn user guide release 0.11 print __doc__ import numpy import pylab sklearn.svm import svc sklearn.cross_validation import stratifiedkfold permutation_test_score sklearn import datasets sklearn.metrics import zero_one_score loading dataset iris datasets.load_iris iris.data iris.target n_classes np.unique .size noisy data correlated random np.random.randomstate seed0 random.normal size len add noisy data informative features make task harder np.c_ svm svc kernellinear stratifiedkfold score permutation_scores pvalue permutation_test_score svm zero_one_score cvcv n_permutations100 n_jobs1 print classification score pvalue score pvalue view histogram permutation scores pl.hist permutation_scores labelpermutation scores ylim pl.ylim bug vlines ... linestyle fails older versions matplotlib pl.vlines score ylim ylim linestyle pl.vlines 1.0 n_classes ylim ylim linestyle pl.plot score ylim linewidth3 colorg linewidth3 labelclassification score pvalue pvalue colork linewidth3 labelluck labelclassification score pvalue pvalue pl.plot n_classes ylim linewidth3 labelluck pl.ylim ylim pl.legend pl.xlabel score pl.show 2.1. examples scikitlearn user guide release 0.11 figure 2.13 pls partial least squares pls partial least squares simple usage various pls avor plscanonical plsregression multivariate response a.k.a pls2 plsregression univariate response a.k.a pls1 cca given multivariate covarying twodimensional datasets pls extracts directions covariance i.e components datasets explain shared variance datasets apparent scatterplot matrix display components dataset dataset maximaly correlated points lie around rst diagonal also true components dataset however correlation across datasets different components weak point cloud spherical 
3979: script output chapter example gallery scikitlearn user guide release 0.11 0.5 0.07 0.06 0.07 0.07 0.5 0.04 0.06 0.5 corr 
3980: 0.5 0.07 0.04 corr 
3981: 0.01 0.04 0.02 0.54 0.54 0.46 0.04 
3982: 0.46 0.04 0.04 0.01 0.02 true err 2.1 estimated 0.1 
3983:  
3984: estimated betas python source code plot_pls.py print __doc__ import numpy import pylab sklearn.pls import plscanonical plsregression cca dataset based latent variables model latents vars np.random.normal sizen 2.1. examples scikitlearn user guide release 0.11 np.random.normal sizen latents np.array latents np.random.normal size4 .reshape latents np.random.normal size4 .reshape x_train y_train x_test y_test print corr print np.round np.corrcoef x.t print corr print np.round np.corrcoef y.t canonical symetric pls transform data plsca plscanonical n_components2 plsca.fit x_train y_train x_train_r y_train_r plsca.transform x_train y_train x_test_r y_test_r plsca.transform x_test y_test scatter plot scores diagonal plot scores components pl.subplot pl.plot x_train_r y_train_r label train pl.plot x_test_r y_test_r label test pl.xlabel scores pl.ylabel scores pl.title comp test corr .2f np.corrcoef x_test_r y_test_r pl.legend pl.subplot pl.plot x_train_r y_train_r label train pl.plot x_test_r y_test_r label test pl.xlabel scores pl.ylabel scores pl.title comp test corr .2f np.corrcoef x_test_r y_test_r pl.legend diagonal plot components pl.subplot pl.plot x_train_r x_train_r label train pl.plot x_test_r x_test_r label test pl.xlabel comp pl.ylabel comp pl.title comp comp test corr .2f np.corrcoef x_test_r x_test_r pl.legend pl.subplot chapter example gallery scikitlearn user guide release 0.11 pl.plot y_train_r y_train_r label train pl.plot y_test_r y_test_r label test pl.xlabel comp pl.ylabel comp pl.title comp comp test corr .2f np.corrcoef y_test_r y_test_r pl.legend pl.show pls regression multivariate response a.k.a pls2 np.random.normal sizen .reshape np.array 1x1 2x2 noize np.dot np.random.normal sizen .reshape pls2 plsregression n_components3 pls2.fit print true err print compare pls2.coefs print estimated print np.round pls2.coefs pls2.predict pls regression univariate response a.k.a pls1 np.random.normal sizen .reshape np.random.normal sizen pls1 plsregression n_components3 pls1.fit note number compements exceeds dimension print estimated betas print np.round pls1.coefs cca pls mode symetric deflation cca cca n_components2 cca.fit x_train y_train x_train_r y_train_r plsca.transform x_train y_train x_test_r y_test_r plsca.transform x_test y_test precisionrecall example precisionrecall metric evaluate quality output classier 
3985: 2.1. examples scikitlearn user guide release 0.11 figure 2.14 precisionrecall script output area curve 0.82 python source code plot_precision_recall.py print __doc__ import random import pylab import numpy sklearn import svm datasets sklearn.metrics import precision_recall_curve sklearn.metrics import auc chapter example gallery scikitlearn user guide release 0.11 import data play iris datasets.load_iris iris.data iris.target keep also classes n_samples n_features x.shape range n_samples shuffle samples random.seed random.shuffle half int n_samples add noisy features np.random.seed np.c_ np.random.randn n_samples n_features run classifier classifier svm.svc kernellinear probabilitytrue probas_ classifier.fit half half .predict_proba half compute precisionrecall plot curve precision recall thresholds precision_recall_curve half probas_ area auc recall precision print area curve 0.2f area pl.clf pl.plot recall precision labelprecisionrecall curve pl.xlabel recall pl.ylabel precision pl.ylim 0.0 1.05 pl.xlim 0.0 1.0 pl.title precisionrecall example auc 0.2f area pl.legend loc lower left pl.show figure 2.15 recursive feature elimination recursive feature elimination recursive feature elimination example showing relevance pixels digit classication task 
3986: 2.1. examples scikitlearn user guide release 0.11 python source code plot_rfe_digits.py print __doc__ sklearn.svm import svc sklearn.datasets import load_digits sklearn.feature_selection import rfe load digits dataset digits load_digits digits.images.reshape len digits.images digits.target create rfe object rank pixel svc svc kernel linear rfe rfe estimatorsvc n_features_to_select1 step1 rfe.fit ranking rfe.ranking_.reshape digits.images .shape chapter example gallery scikitlearn user guide release 0.11 plot pixel ranking import pylab pl.matshow ranking pl.colorbar pl.title ranking pixels rfe pl.show figure 2.16 recursive feature elimination crossvalidation recursive feature elimination crossvalidation recursive feature elimination example automatic tuning number features selected crossvalidation 
3987: script output 2.1. examples scikitlearn user guide release 0.11 optimal number features python source code plot_rfe_with_cross_validation.py print __doc__ sklearn.svm import svc sklearn.cross_validation import stratifiedkfold sklearn.feature_selection import rfecv sklearn.datasets import make_classification sklearn.metrics import zero_one build classification task using informative features make_classification n_samples1000 n_features25 n_informative3 n_redundant2 n_repeated0 n_classes8 n_clusters_per_class1 random_state0 create rfe object compute crossvalidated score svc svc kernel linear rfecv rfecv estimatorsvc step1 cvstratifiedkfold loss_funczero_one rfecv.fit print optimal number features rfecv.n_features_ plot number features vs. crossvalidation scores import pylab pl.figure pl.xlabel number features selected pl.ylabel cross validation score misclassifications pl.plot xrange len rfecv.cv_scores_ rfecv.cv_scores_ pl.show figure 2.17 receiver operating characteristic roc receiver operating characteristic roc example receiver operating characteristic roc metric evaluate quality output classier 
3988: chapter example gallery scikitlearn user guide release 0.11 script output area roc curve 0.794686 python source code plot_roc.py print __doc__ import numpy import pylab sklearn import svm datasets sklearn.utils import shuffle sklearn.metrics import roc_curve auc random_state np.random.randomstate import data play iris datasets.load_iris iris.data iris.target make binary classification problem removing third class n_samples n_features x.shape add noisy features make problem harder np.c_ random_state.randn n_samples n_features 2.1. examples scikitlearn user guide release 0.11 shuffle split training test sets shuffle random_staterandom_state half int n_samples x_train x_test half half y_train y_test half half run classifier classifier svm.svc kernellinear probabilitytrue probas_ classifier.fit x_train y_train .predict_proba x_test compute roc curve area curve fpr tpr thresholds roc_curve y_test probas_ roc_auc auc fpr tpr print area roc curve roc_auc plot roc curve pl.clf pl.plot fpr tpr labelroc curve area 0.2f roc_auc pl.plot pl.xlim 0.0 1.0 pl.ylim 0.0 1.0 pl.xlabel false positive rate pl.ylabel true positive rate pl.title receiver operating characteristic example pl.legend loc lower right pl.show figure 2.18 receiver operating characteristic roc cross validation receiver operating characteristic roc cross validation example receiver operating characteristic roc metric evaluate quality output classier using crossvalidation 
3989: chapter example gallery scikitlearn user guide release 0.11 python source code plot_roc_crossval.py print __doc__ import numpy scipy import interp import pylab sklearn import svm datasets sklearn.metrics import roc_curve auc sklearn.cross_validation import stratifiedkfold data generation import data play iris datasets.load_iris iris.data iris.target n_samples n_features x.shape add noisy features np.c_ np.random.randn n_samples n_features 2.1. examples scikitlearn user guide release 0.11 classification roc analysis run classifier crossvalidation plot roc curves stratifiedkfold classifier svm.svc kernellinear probabilitytrue mean_tpr 0.0 mean_fpr np.linspace all_tpr train test enumerate probas_ classifier.fit train train .predict_proba test compute roc curve area curve fpr tpr thresholds roc_curve test probas_ mean_tpr interp mean_fpr fpr tpr mean_tpr 0.0 roc_auc auc fpr tpr pl.plot fpr tpr lw1 labelroc fold area 0.2f roc_auc pl.plot color 0.6 0.6 0.6 labelluck mean_tpr len mean_tpr 1.0 mean_auc auc mean_fpr mean_tpr pl.plot mean_fpr mean_tpr labelmean roc area 0.2f mean_auc lw2 pl.xlim 0.05 1.05 pl.ylim 0.05 1.05 pl.xlabel false positive rate pl.ylabel true positive rate pl.title receiver operating characteristic example pl.legend loc lower right pl.show figure 2.19 train error test error train error test error illustration performance estimator unseen data test data performance training data regularization increases performance train decreases performance test optimal within range values regularization parameter example elasticnet regression model performance measured using explained variance a.k.a 
3990: chapter example gallery scikitlearn user guide release 0.11 script output optimal regularization parameter 0.000335292414925 python source code plot_train_error_vs_test_error.py print __doc__ author alexandre gramfort alexandre.gramfort inria.fr license bsd style 
3991: import numpy sklearn import linear_model generate sample data n_samples_train n_samples_test n_features np.random.seed coef np.random.randn n_features coef 0.0 top features impacting model np.random.randn n_samples_train n_samples_test n_features np.dot coef split train test data x_train x_test n_samples_train n_samples_train y_train y_test n_samples_train n_samples_train 2.1. examples scikitlearn user guide release 0.11 compute train test errors alphas np.logspace enet linear_model.elasticnet rho0.7 train_errors list test_errors list alpha alphas enet.set_params alphaalpha enet.fit x_train y_train train_errors.append enet.score x_train y_train test_errors.append enet.score x_test y_test i_alpha_optim np.argmax test_errors alpha_optim alphas i_alpha_optim print optimal regularization parameter alpha_optim estimate coef_ full data optimal regularization parameter enet.set_params alphaalpha_optim coef_ enet.fit .coef_ plot results functions import pylab pl.subplot pl.semilogx alphas train_errors labeltrain pl.semilogx alphas test_errors labeltest pl.vlines alpha_optim pl.ylim np.max test_errors colork linewidth3 labeloptimum test pl.legend loclower left pl.ylim 1.2 pl.xlabel regularization parameter pl.ylabel performance show estimated coef_ true coef pl.subplot pl.plot coef labeltrue coef pl.plot coef_ labelestimated coef pl.legend pl.subplots_adjust 0.09 0.04 0.94 0.94 0.26 0.26 pl.show figure 2.20 classication text documents using sparse features chapter example gallery scikitlearn user guide release 0.11 classication text documents using sparse features example showing scikitlearn used classify documents topics using bagofwords approach example uses scipy.sparse matrix store features instead standard numpy arrays demos various classiers efciently handle sparse matrices dataset used example newsgroups dataset automatically downloaded cached adjust number categories giving names dataset loader setting none get python source code document_classification_20newsgroups.py author peter prettenhofer peter.prettenhofer gmail.com license simplified bsd olivier grisel olivier.grisel ensta.org mathieu blondel mathieu mblondel.org lars buitinck l.j.buitinck uva.nl import logging import numpy optparse import optionparser import sys time import time import pylab sklearn.datasets import fetch_20newsgroups sklearn.feature_extraction.text import tfidfvectorizer sklearn.feature_selection import selectkbest chi2 sklearn.linear_model import ridgeclassifier sklearn.svm import linearsvc sklearn.linear_model import sgdclassifier sklearn.linear_model import perceptron sklearn.naive_bayes import bernoullinb multinomialnb sklearn.neighbors import kneighborsclassifier sklearn.neighbors import nearestcentroid sklearn.utils.extmath import density sklearn import metrics display progress logs stdout logging.basicconfig levellogging.info format asctime levelname message parse commandline arguments optionparser op.add_option report action store_true dest print_report help print detailed classification report op.add_option chi2_select action store type int dest select_chi2 help select number features using chisquared test op.add_option confusion_matrix action store_true dest print_cm help print confusion matrix op.add_option top10 action store_true dest print_top10 2.1. examples scikitlearn user guide release 0.11 help print ten discriminative terms per class every classifier opts args op.parse_args len args op.error script takes arguments sys.exit print __doc__ op.print_help print load categories training set categories alt.atheism talk.religion.misc comp.graphics sci.space uncomment following analysis categories categories none print loading newsgroups dataset categories print categories categories else data_train fetch_20newsgroups subsettrain categoriescategories shuffletrue random_state42 data_test fetch_20newsgroups subsettest categoriescategories shuffletrue random_state42 print data loaded categories data_train.target_names case categories none print documents training set len data_train.data print documents testing set len data_test.data print categories len categories print split training set test set y_train y_test data_train.target data_test.target print extracting features training dataset using sparse vectorizer time vectorizer tfidfvectorizer sublinear_tftrue max_df0.5 stop_wordsenglish x_train vectorizer.fit_transform data_train.data print done time print n_samples n_features x_train.shape print print extracting features test dataset using vectorizer time x_test vectorizer.transform data_test.data print done time print n_samples n_features x_test.shape chapter example gallery scikitlearn user guide release 0.11 print opts.select_chi2 print extracting best features chisquared test opts.select_chi2 time ch2 selectkbest chi2 kopts.select_chi2 x_train ch2.fit_transform x_train y_train x_test ch2.transform x_test print done time print def trim trim string fit terminal assuming 80column display return len else ... mapping integer feature name original token string feature_names vectorizer.get_feature_names benchmark classifiers def benchmark clf print print training print clf time clf.fit x_train y_train train_time time print train time 0.3fs train_time time pred clf.predict x_test test_time time print test time 0.3fs test_time score metrics.f1_score y_test pred print f1score 0.3f score hasattr clf coef_ print dimensionality clf.coef_.shape print density density clf.coef_ opts.print_top10 print top keywords per class category enumerate categories top10 np.argsort clf.coef_ print trim category .join feature_names top10 print opts.print_report print classification report print metrics.classification_report y_test pred target_namescategories 2.1. examples scikitlearn user guide release 0.11 opts.print_cm print confusion matrix print metrics.confusion_matrix y_test pred print clf_descr str clf .split return clf_descr score train_time test_time results clf name ridgeclassifier tol1e1 ridge classifier perceptron n_iter50 perceptron kneighborsclassifier n_neighbors10 knn print print name results.append benchmark clf penalty print print penalty penalty.upper train liblinear model results.append benchmark linearsvc lossl2 penaltypenalty dualfalse tol1e3 train sgd model results.append benchmark sgdclassifier alpha.0001 n_iter50 penaltypenalty train sgd elastic net penalty print print elasticnet penalty results.append benchmark sgdclassifier alpha.0001 n_iter50 penalty elasticnet train nearestcentroid without threshold print print nearestcentroid aka rocchio classifier results.append benchmark nearestcentroid train sparse naive bayes classifiers print print naive bayes results.append benchmark multinomialnb alpha.01 results.append benchmark bernoullinb alpha.01 class l1linearsvc linearsvc def fit self smaller stronger regularization regularization sparsity self.transformer_ linearsvc penalty dualfalse tol1e3 self.transformer_.fit_transform return linearsvc.fit self def predict self self.transformer_.transform chapter example gallery scikitlearn user guide release 0.11 return linearsvc.predict self print print linearsvc l1based feature selection results.append benchmark l1linearsvc make plots indices np.arange len results results results xrange clf_names score training_time test_time results pl.title score pl.barh indices score label score colorr pl.barh indices training_time label training time colorg pl.barh indices test_time label test time colorb pl.yticks pl.legend locbest pl.subplots_adjust left.25 zip indices clf_names pl.text pl.show figure 2.21 clustering text documents using kmeans clustering text documents using kmeans example showing scikitlearn used cluster documents topics using bagofwords approach example uses scipy.sparse matrix store features instead standard numpy arrays two algorithms demoed ordinary kmeans faster cousin minibatch kmeans python source code document_clustering.py author peter prettenhofer peter.prettenhofer gmail.com license simplified bsd lars buitinck l.j.buitinck uva.nl sklearn.datasets import fetch_20newsgroups sklearn.feature_extraction.text import tfidfvectorizer sklearn import metrics 2.1. examples scikitlearn user guide release 0.11 sklearn.cluster import kmeans minibatchkmeans import logging optparse import optionparser import sys time import time import numpy display progress logs stdout logging.basicconfig levellogging.info format asctime levelname message parse commandline arguments optionparser op.add_option nominibatch action store_false dest minibatch defaulttrue help use ordinary kmeans algorithm print __doc__ op.print_help opts args op.parse_args len args op.error script takes arguments sys.exit load categories training set categories alt.atheism talk.religion.misc comp.graphics sci.space uncomment following analysis categories categories none print loading newsgroups dataset categories print categories dataset fetch_20newsgroups subsetall categoriescategories shuffletrue random_state42 print documents len dataset.data print categories len dataset.target_names print labels dataset.target true_k np.unique labels .shape print extracting features training dataset using sparse vectorizer time vectorizer tfidfvectorizer max_df0.5 max_features10000 vectorizer.fit_transform dataset.data stop_wordsenglish chapter example gallery scikitlearn user guide release 0.11 print done time print n_samples n_features x.shape print actual clustering opts.minibatch minibatchkmeans ktrue_k initkmeans n_init1 init_size1000 batch_size1000 verbose1 else kmeans ktrue_k initrandom max_iter100 n_init1 verbose1 print clustering sparse data time km.fit print done 0.3fs time print print homogeneity 0.3f metrics.homogeneity_score labels km.labels_ print completeness 0.3f metrics.completeness_score labels km.labels_ print vmeasure 0.3f metrics.v_measure_score labels km.labels_ print adjusted randindex .3f metrics.adjusted_rand_score labels km.labels_ print silhouette coefficient 0.3f metrics.silhouette_score labels sample_size1000 print figure 2.22 pipeline anova svm pipeline anova svm simple usage pipeline runs successively univariate feature selection anova csvm selected features python source code feature_selection_pipeline.py print __doc__ sklearn import svm sklearn.datasets import samples_generator sklearn.feature_selection import selectkbest f_regression sklearn.pipeline import pipeline 2.1. examples scikitlearn user guide release 0.11 import data play samples_generator.make_classification n_features20 n_informative3 n_redundant0 n_classes4 n_clusters_per_class2 anova svmc anova filter take best ranked features anova_filter selectkbest f_regression svm clf svm.svc kernellinear anova_svm pipeline anova anova_filter svm clf anova_svm.fit anova_svm.predict figure 2.23 parameter estimation using grid search nested crossvalidation parameter estimation using grid search nested crossvalidation classier optimized nested crossvalidation using sklearn.grid_search.gridsearchcv ject development set comprises half available labeled data performance selected hyperparameters trained model measured dedicated evaluation set used model selection step details tools available model selection found sections crossvalidation evaluating estimator performance grid search setting estimator parameters python source code grid_search_digits.py print __doc__ sklearn import datasets sklearn.cross_validation import train_test_split sklearn.grid_search import gridsearchcv sklearn.metrics import classification_report sklearn.metrics import precision_score sklearn.metrics import recall_score sklearn.svm import svc loading digits dataset digits datasets.load_digits apply classifier data need flatten image turn data samples feature matrix n_samples len digits.images digits.images.reshape n_samples chapter example gallery scikitlearn user guide release 0.11 digits.target split dataset two equal parts x_train x_test y_train y_test train_test_split test_fraction0.5 random_state0 set parameters crossvalidation tuned_parameters kernel rbf gamma 1e3 1e4 kernel linear scores precision precision_score recall recall_score score_name score_func scores print tuning hyperparameters score_name print clf gridsearchcv svc tuned_parameters score_funcscore_func clf.fit x_train y_train cv5 print best parameters set found development set print print clf.best_estimator_ print print grid scores development set print params mean_score scores clf.grid_scores_ print 0.3f 0.03f mean_score scores.std params print print detailed classification report print print model trained full development set print scores computed full evaluation set print y_true y_pred y_test clf.predict x_test print classification_report y_true y_pred print note problem easy hyperparameter plateau flat output model precision recall ties quality 
3992: figure 2.24 sample pipeline text feature extraction evaluation 2.1. examples scikitlearn user guide release 0.11 sample pipeline text feature extraction evaluation dataset used example newsgroups dataset automatically downloaded cached reused document classication example adjust number categories giving name dataset loader setting none get sample output run quadcore machine loading newsgroups dataset categories alt.atheism talk.religion.misc documents categories performing grid search ... pipeline vect tfidf clf parameters clf__alpha 1.0000000000000001e05 9.9999999999999995e07 clf__n_iter clf__penalty elasticnet tfidf__use_idf true false vect__max_n vect__max_df 0.5 0.75 1.0 vect__max_features none done 1737.030s best score 0.940 best parameters set clf__alpha 9.9999999999999995e07 clf__n_iter clf__penalty elasticnet tfidf__use_idf true vect__max_n vect__max_df 0.75 vect__max_features python source code grid_search_text_feature_extraction.py print __doc__ author olivier grisel olivier.grisel ensta.org license simplified bsd peter prettenhofer peter.prettenhofer gmail.com mathieu blondel mathieu mblondel.org pprint import pprint time import time import logging sklearn.datasets import fetch_20newsgroups sklearn.feature_extraction.text import countvectorizer sklearn.feature_extraction.text import tfidftransformer sklearn.linear_model import sgdclassifier sklearn.grid_search import gridsearchcv sklearn.pipeline import pipeline display progress logs stdout logging.basicconfig levellogging.info chapter example gallery scikitlearn user guide release 0.11 format asctime levelname message load categories training set categories alt.atheism talk.religion.misc uncomment following analysis categories categories none print loading newsgroups dataset categories print categories data fetch_20newsgroups subsettrain categoriescategories print documents len data.filenames print categories len data.target_names print define pipeline combining text feature extractor simple classifier pipeline pipeline vect countvectorizer tfidf tfidftransformer clf sgdclassifier parameters uncommenting parameters give better exploring power increase processing time combinatorial way vect__max_df 0.5 0.75 1.0 vect__max_features none vect__max_n words bigrams tfidf__use_idf true false tfidf__norm clf__alpha 0.00001 0.000001 clf__penalty elasticnet clf__n_iter __name__ __main__ multiprocessing requires fork happen __main__ protected block find best parameters feature extraction classifier grid_search gridsearchcv pipeline parameters n_jobs1 verbose1 print performing grid search ... print pipeline name name pipeline.steps print parameters pprint parameters time grid_search.fit data.data data.target print done 0.3fs time print 2.1. examples scikitlearn user guide release 0.11 print best score 0.3f grid_search.best_score print best parameters set best_parameters grid_search.best_estimator.get_params param_name sorted parameters.keys print param_name best_parameters param_name figure 2.25 classication text documents using mlcomp dataset classication text documents using mlcomp dataset example showing scikitlearn used classify documents topics using bagofwords approach example uses scipy.sparse matrix store features instead standard numpy arrays dataset used example newsgroups dataset downloaded http mlcomp.org free registration required http mlcomp.orgdatasets379 downloaded unzip archive somewhere lesystem instance mkdir datamlcomp unzip pathtodataset37920news18828_xxxxx.zip datamlcomp get folder datamlcomp379 named metadata subfolders raw train test holding text documents organized newsgroups set mlcomp_datasets_home environment variable pointing root folder holding uncompressed archive export mlcomp_datasets_home datamlcomp ready run example using favorite python shell ipython examplesmlcomp_sparse_document_classification.py python source code mlcomp_sparse_document_classification.py print __doc__ author olivier grisel olivier.grisel ensta.org license simplified bsd time import time import sys import import numpy import scipy.sparse chapter example gallery scikitlearn user guide release 0.11 import pylab sklearn.datasets import load_mlcomp sklearn.feature_extraction.text import tfidfvectorizer sklearn.linear_model import sgdclassifier sklearn.metrics import confusion_matrix sklearn.metrics import classification_report sklearn.naive_bayes import multinomialnb mlcomp_datasets_home os.environ print mlcomp_datasets_home set please follow instructions sys.exit load training set print loading newsgroups training set ... news_train load_mlcomp 20news18828 train print news_train.descr print documents len news_train.filenames print categories len news_train.target_names print extracting features dataset using sparse vectorizer time vectorizer tfidfvectorizer charsetlatin1 x_train vectorizer.fit_transform open .read news_train.filenames print done time print n_samples n_features x_train.shape assert sp.issparse x_train y_train news_train.target print loading newsgroups test set ... news_test load_mlcomp 20news18828 test time print done time print predicting labels test set ... print documents len news_test.filenames print categories len news_test.target_names print extracting features dataset using vectorizer time x_test vectorizer.transform open .read news_test.filenames y_test news_test.target print done time print n_samples n_features x_test.shape benchmark classifiers def benchmark clf_class params name print parameters params time clf clf_class params .fit x_train y_train print done time hasattr clf coef_ print percentage non zeros coef 2.1. examples scikitlearn user guide release 0.11 np.mean clf.coef_ print predicting outcomes testing set time pred clf.predict x_test print done time print classification report test set classifier print clf print print classification_report y_test pred target_namesnews_test.target_names confusion_matrix y_test pred print confusion matrix print show confusion matrix pl.matshow pl.title confusion matrix classifier name pl.colorbar print testbenching linear classifier ... parameters loss hinge penalty n_iter alpha 0.00001 fit_intercept true benchmark sgdclassifier parameters sgd print testbenching multinomialnb classifier ... parameters alpha 0.01 benchmark multinomialnb parameters multinomialnb pl.show 2.1.2 examples based real world datasets applications real world problems medium sized datasets interactive user interface 
3993: figure 2.26 outlier detection real data set chapter example gallery scikitlearn user guide release 0.11 outlier detection real data set example illustrates need robust covariance estimation real data set detection better understanding data structure selected two sets two variables boston housing data set illustration kind analysis done several outlier detection tools purpose vizualisation working twodimensional examples one aware things trivial highdimension pointed examples main result empirical covariance estimate nonrobust one highly inuenced heterogeneous structure observations although robust covariance estimate able focus main mode data distribution sticks assumption data gaussian distributed yielding biased estimation data structure yet accurate extent oneclass svm algorithm useful outlier first example rst example illustrates robust covariance estimation help concentrating relevant cluster one exists many observations confounded one break empirical covariance estima tion course screening tools would pointed presence two clusters support vector machines gaussian mixture models univariate outlier detection ... highdimensional example none could applied easily 
3994: second example second example shows ability minimum covariance determinant robust estimator covariance concentrate main mode data distribution location seems well estimated although covariance hard estimate due bananashaped distribution anyway get rid outlying observations oneclass svm able capture real data structure difculty adjust kernel bandwith parameter obtain good compromise shape data scatter matrix risk overtting data 
3995: 2.1. examples scikitlearn user guide release 0.11 python source code plot_outlier_detection_housing.py print __doc__ author virgile fritsch virgile.fritsch inria.fr license bsd import numpy sklearn.covariance import ellipticenvelope sklearn.svm import oneclasssvm import matplotlib.pyplot plt import matplotlib.font_manager sklearn.datasets import load_boston get data load_boston data two clusters load_boston data banana shaped define classifiers used classifiers empirical covariance ellipticenvelope support_fraction1. contamination0.261 robust covariance minimum covariance determinant ellipticenvelope contamination0.261 ocsvm oneclasssvm nu0.261 gamma0.05 colors legend1 legend2 learn frontier outlier detection several classifiers xx1 yy1 np.meshgrid np.linspace np.linspace xx2 yy2 np.meshgrid np.linspace np.linspace clf_name clf enumerate classifiers.iteritems plt.figure clf.fit clf.decision_function np.c_ xx1.ravel yy1.ravel z1.reshape xx1.shape legend1 clf_name plt.contour xx1 yy1 levels linewidths2 colorscolors plt.figure clf.fit clf.decision_function np.c_ xx2.ravel yy2.ravel z2.reshape xx2.shape legend2 clf_name plt.contour chapter example gallery scikitlearn user guide release 0.11 xx2 yy2 levels linewidths2 colorscolors plot results shape data points cloud plt.figure two clusters plt.title outlier detection real data set boston housing plt.scatter colorblack bbox_args dict boxstyle round 0.8 arrow_args dict arrowstyle plt.annotate several confounded points xycoords data textcoords data xytext bboxbbox_args arrowpropsarrow_args plt.xlim xx1.min xx1.max plt.ylim yy1.min yy1.max plt.legend legend1.values .collections legend1.values .collections legend1.values .collections legend1.keys legend1.keys legend1.keys loc upper center propmatplotlib.font_manager.fontproperties size12 plt.ylabel accessibility radial highways plt.xlabel pupilteatcher ratio town plt.figure banana shape plt.title outlier detection real data set boston housing plt.scatter colorblack plt.xlim xx2.min xx2.max plt.ylim yy2.min yy2.max plt.legend legend2.values .collections legend2.values .collections legend2.values .collections legend2.keys legend2.keys legend2.keys loc upper center propmatplotlib.font_manager.fontproperties size12 plt.ylabel lower status population plt.xlabel average number rooms per dwelling plt.show figure 2.27 species distribution modeling species distribution modeling modeling species geographic distributions important problem conservation biology example model geographic distribution two south american mammals given past observations environmental variables since positive examples unsuccessful observations cast problem density estimation problem use oneclasssvm provided package sklearn.svm modeling tool dataset provided phillips available example uses basemap plot coast lines national 2.1. examples scikitlearn user guide release 0.11 boundaries south america two species bradypus variegatus brownthroated sloth microryzomys minutus also known forest small rice rat rodent lives peru colombia ecuador peru venezuela 
3996: references maximum entropy modeling species geographic distributions phillips anderson schapire ecological modelling 
3997: script output ________________________________________________________________________________ modeling distribution species bradypus variegatus fit oneclasssvm ... plot coastlines coverage predict species distribution done 
3998: area roc curve 0.865318 ________________________________________________________________________________ modeling distribution species microryzomys minutus fit oneclasssvm 
3999: done 
4000: chapter example gallery scikitlearn user guide release 0.11 plot coastlines coverage predict species distribution area roc curve 0.993919 time elapsed 15.32s python source code plot_species_distribution_modeling.py authors peter prettenhoer peter.prettenhofer gmail.com license bsd style 
4001: jake vanderplas vanderplas astro.washington.edu time import time import numpy import pylab sklearn.datasets.base import bunch sklearn.datasets import fetch_species_distributions sklearn.datasets.species_distributions import construct_grids sklearn import svm metrics basemap available well use otherwise well improvise later ... try mpl_toolkits.basemap import basemap basemap true except importerror basemap false print __doc__ def create_species_bunch species_name train test coverages xgrid ygrid create bunch information particular organism use testtrain record arrays extract data specific given species name. bunch bunch name .join species_name.split points dict testtest traintrain label pts points.iteritems choose points associated desired species pts pts pts species species_name bunch pts_ label pts determine coverage values training testing points np.searchsorted xgrid pts long np.searchsorted ygrid pts lat bunch cov_ label coverages 2.1. examples scikitlearn user guide release 0.11 return bunch def plot_species_distribution species bradypus_variegatus_0 microryzomys_minutus_0 plot species distribution. len species print note two species provided first two used time load compressed data data fetch_species_distributions set data grid xgrid ygrid construct_grids data grid coordinates np.meshgrid xgrid ygrid create bunch species bv_bunch create_species_bunch species mm_bunch create_species_bunch species data.train data.test data.coverages xgrid ygrid data.train data.test data.coverages xgrid ygrid background points grid coordinates evaluation np.random.seed background_points np.c_ np.random.randint low0 highdata.ny size10000 np.random.randint low0 highdata.nx size10000 well make use fact coverages measurements land points land_reference data.coverages help decide land water 
4002: fit predict plot species species enumerate bv_bunch mm_bunch print print modeling distribution species species.name standardize features mean species.cov_train.mean axis0 std species.cov_train.std axis0 train_cover_std species.cov_train mean std fit oneclasssvm print fit oneclasssvm ... clf svm.oneclasssvm nu0.1 kernel rbf gamma0.5 clf.fit train_cover_std print done. chapter example gallery scikitlearn user guide release 0.11 plot map south america pl.subplot basemap print plot coastlines using basemap basemap projectioncyl llcrnrlaty.min urcrnrlaty.max llcrnrlonx.min urcrnrlonx.max resolutionc m.drawcoastlines m.drawcountries else print plot coastlines coverage pl.contour land_reference levels colors linestyles solid pl.xticks pl.yticks print predict species distribution predict species distribution using training data np.ones data.ny data.nx dtypenp.float64 well predict land points idx np.where land_reference coverages_land data.coverages idx idx pred clf.decision_function coverages_land mean std pred.min idx idx pred levels np.linspace z.min z.max land_reference plot contours prediction pl.contourf levelslevels cmappl.cm.reds pl.colorbar format .2f scatter trainingtesting points pl.scatter species.pts_train long species.pts_train lat cblack marker labeltrain pl.scatter species.pts_test long species.pts_test lat cblack markerx labeltest pl.legend pl.title species.name pl.axis equal compute auc w.r.t background points pred_background background_points background_points pred_test clf.decision_function species.cov_test mean std scores np.r_ pred_test pred_background np.r_ np.ones pred_test.shape np.zeros pred_background.shape fpr tpr thresholds metrics.roc_curve scores roc_auc metrics.auc fpr tpr pl.text auc .3f roc_auc right print area roc curve roc_auc 2.1. examples scikitlearn user guide release 0.11 print ntime elapsed .2fs time plot_species_distribution pl.show figure 2.28 visualizing stock market structure visualizing stock market structure example employs several unsupervised learning techniques extract stock market structure variations historical quotes quantity use daily variation quote price quotes linked tend couctuate day 
4003: learning graph structure use sparse inverse covariance estimation quotes correlated conditionally others speci cally sparse inverse covariance gives graph list connection symbol symbols connected useful expain uctuations 
4004: clustering use clustering group together quotes behave similarly amongst various clustering techniques available scikitlearn use afnity propagation enforce equalsize clusters choose automatically number clusters data note gives different indication graph graph reects conditional relations variables clustering reects marginal properties variables clustered together considered similar impact level full stock market 
4005: embedding space visualization purposes need lay different symbols canvas use manifold learning techniques retrieve embedding 
4006: visualization output models combined graph nodes represents stocks edges chapter example gallery scikitlearn user guide release 0.11 cluster labels used dene color nodes sparse covariance model used display strength edges embedding used position nodes plan example fair amount visualizationrelated code visualization crucial display graph one challenge position labels minimizing overlap use heuristic based direction nearest neighbor along axis 
4007: script output cluster pepsi coca cola kellogg cluster apple amazon yahoo cluster glaxosmithkline novartis sanofiaventis cluster comcast time warner cablevision cluster conocophillips chevron total valero energy exxon cluster walgreen cvs cluster kraft foods cluster navistar sony marriott caterpillar canon toyota honda mitsubishi xerox unilever cluster kimberlyclark colgatepalmolive procter gamble cluster american express ryder goldman sachs walmart general electrics pfizer wells fargo dupont nemours bank america aig home depot news corp ford jpmorgan chase donalds cluster microsoft sap ibm texas instruments dell cisco cluster raytheon boeing lookheed martin general dynamics northrop grumman 2.1. examples scikitlearn user guide release 0.11 python source code plot_stock_market.py print __doc__ author gael varoquaux gael.varoquaux normalesup.org license bsd import datetime import numpy import pylab matplotlib import finance matplotlib.collections import linecollection sklearn import cluster covariance manifold retrieve data internet choose time period reasonnably calm long ago get hightech firms crash datetime.datetime datetime.datetime symbol_dict tot total xom exxon cvx chevron cop conocophillips vlo valero energy msft microsoft ibm ibm twx time warner cmcsa comcast cvc cablevision yhoo yahoo dell dell hpq amzn amazon toyota caj canon mtu mitsubishi sne sony ford hmc honda nav navistar noc northrop grumman boeing coca cola mmm mcd donalds pep pepsi kft kraft foods kellogg unilever mar marriott procter gamble colgatepalmolive nws news corp chapter example gallery scikitlearn user guide release 0.11 general electrics wfc wells fargo jpm jpmorgan chase aig aig axp american express bac bank america goldman sachs aapl apple sap sap csco cisco txn texas instruments xrx xerox lmt lookheed martin wmt walmart wag walgreen home depot gsk glaxosmithkline pfe pfizer sny sanofiaventis nvs novartis kmb kimberlyclark ryder general dynamics rtn raytheon cvs cvs cat caterpillar dupont nemours symbols names np.array symbol_dict.items quotes finance.quotes_historical_yahoo symbol asobjecttrue symbol symbols open np.array q.open quotes .astype np.float close np.array q.close quotes .astype np.float daily variations quotes carry information variation close open learn graphical structure correlations edge_model covariance.graphlassocv standardize time series using correlations rather covariance efficient structure recovery variation.copy x.std axis0 edge_model.fit cluster using affinity propagation labels cluster.affinity_propagation edge_model.covariance_ n_labels labels.max range n_labels print cluster .join names labels 2.1. examples scikitlearn user guide release 0.11 find lowdimension embedding visualization find best position nodes stocks plane use dense eigen_solver achieve reproducibility arpack initiated random vectors dont control addition use large number neighbors capture largescale structure node_position_model manifold.locallylinearembedding n_components2 eigen_solverdense n_neighbors6 embedding node_position_model.fit_transform x.t visualization pl.figure facecolorw figsize pl.clf pl.axes pl.axis display graph partial correlations partial_correlations edge_model.precision_.copy np.sqrt np.diag partial_correlations partial_correlations partial_correlations np.newaxis non_zero np.abs np.triu partial_correlations 0.02 plot nodes using coordinates embedding pl.scatter embedding embedding s100 clabels cmappl.cm.spectral plot edges start_idx end_idx np.where non_zero sequence line0 line1 line2 linen ... segments embedding start embedding stop start stop zip start_idx end_idx values np.abs partial_correlations non_zero linecollection segments zorder0 cmappl.cm.hot_r normpl.normalize values.max lc.set_array values lc.set_linewidths values ax.add_collection add label node challenge want position labels avoid overlap labels index name label enumerate zip names labels embedding.t embedding index embedding index this_dx np.argmin np.abs this_dy np.argmin np.abs this_dx horizontalalignment left .002 chapter example gallery scikitlearn user guide release 0.11 else horizontalalignment right .002 this_dy verticalalignment bottom .002 else verticalalignment top .002 pl.text name size10 horizontalalignmenthorizontalalignment verticalalignmentverticalalignment bboxdict facecolorw edgecolorpl.cm.spectral label float n_labels alpha.6 pl.xlim embedding .min .15 embedding .ptp embedding .max .10 embedding .ptp pl.ylim embedding .min .03 embedding .ptp embedding .max .03 embedding .ptp pl.show figure 2.29 compressive sensing tomography reconstruction prior lasso compressive sensing tomography reconstruction prior lasso example shows reconstruction image set parallel projections acquired along different angles dataset acquired computed tomography without prior information sample number projections required reconstruct image order linear size image pixels simplicity consider sparse image pixels boundary objects nonzero value data could correspond example cellular material note however images sparse different basis haar wavelets projections acquired therefore necessary use prior information available sample sparsity example compressive sensing tomography projection operation linear transformation addition datadelity term corresponding linear regression penalize norm image account sparsity resulting optimization problem called lasso use class sklearn.linear_model.sparse.lasso uses coordinate descent algorithm importantly implementation computationally efcient sparse matrix projection operator used reconstruction penalization gives result zero error pixels successfully labeled even noise added projections comparison penalization sklearn.linear_model.ridge produces large number labeling errors pixels important artifacts observed reconstructed image contrary penalization note particular circular artifact separating pixels corners contributed fewer projections central disk 
4008: 2.1. examples scikitlearn user guide release 0.11 python source code plot_tomography_l1_reconstruction.py print __doc__ author emmanuelle gouillart emmanuelle.gouillart nsup.org license simplified bsd import numpy scipy import sparse scipy import ndimage sklearn.linear_model.sparse import lasso sklearn.linear_model import ridge import matplotlib.pyplot plt def _weights dx1 orig0 np.ravel floor_x np.floor orig alpha orig floor_x return np.hstack floor_x floor_x np.hstack alpha alpha def _generate_center_coordinates l_x l_x float l_x np.mgrid l_x l_x center l_x 0.5 center 0.5 center return def build_projection_operator l_x n_dir compute tomography design matrix 
4009: parameters l_x int linear size image array chapter example gallery scikitlearn user guide release 0.11 n_dir int number angles projections acquired 
4010: returns sparse matrix shape n_dir l_x l_x2 _generate_center_coordinates l_x angles np.linspace np.pi n_dir endpointfalse data_inds weights camera_inds data_unravel_indices np.arange l_x data_unravel_indices np.hstack data_unravel_indices data_unravel_indices angle enumerate angles xrot np.cos angle np.sin angle inds _weights xrot dx1 origx.min mask np.logical_and inds inds l_x weights list mask camera_inds list inds mask l_x data_inds list data_unravel_indices mask proj_operator sparse.coo_matrix weights camera_inds data_inds return proj_operator def generate_synthetic_data synthetic binary data np.random.randomstate n_pts 36. np.ogrid mask_outer mask np.zeros points rs.rand n_pts mask points .astype np.int points .astype np.int mask ndimage.gaussian_filter mask sigmal n_pts res np.logical_and mask mask.mean mask_outer return res ndimage.binary_erosion res generate synthetic images projections proj_operator build_projection_operator data generate_synthetic_data proj proj_operator data.ravel np.newaxis proj 0.15 np.random.randn proj.shape reconstruction ridge penalization rgr_ridge ridge alpha0.2 rgr_ridge.fit proj_operator proj.ravel rec_l2 rgr_ridge.coef_.reshape reconstruction lasso penalization best value alpha determined using cross validation lassocv rgr_lasso lasso alpha0.001 rgr_lasso.fit proj_operator proj.ravel rec_l1 rgr_lasso.coef_.reshape plt.figure figsize 3.3 2.1. examples scikitlearn user guide release 0.11 plt.subplot plt.imshow data cmapplt.cm.gray interpolationnearest plt.axis plt.title original image plt.subplot plt.imshow rec_l2 cmapplt.cm.gray interpolationnearest plt.title penalization plt.axis plt.subplot plt.imshow rec_l1 cmapplt.cm.gray interpolationnearest plt.title penalization plt.axis plt.subplots_adjust hspace0.01 wspace0.01 top1 bottom0 left0 right1 plt.show figure 2.30 faces recognition example using eigenfaces svms faces recognition example using eigenfaces svms dataset used example preprocessed excerpt labeled faces wild aka lfw http viswww.cs.umass.edulfwlfwfunneled.tgz 233mb expected results top represented people dataset precision recall f1score support gerhard_schroeder donald_rumsfeld tony_blair colin_powell george_w_bush avg total 0.91 0.84 0.65 0.78 0.93 0.86 0.75 0.82 0.82 0.88 0.86 0.84 0.82 0.83 0.73 0.83 0.90 0.85 python source code face_recognition.py print __doc__ time import time import logging import pylab sklearn.cross_validation import train_test_split chapter example gallery scikitlearn user guide release 0.11 sklearn.datasets import fetch_lfw_people sklearn.grid_search import gridsearchcv sklearn.metrics import classification_report sklearn.metrics import confusion_matrix sklearn.decomposition import randomizedpca sklearn.svm import svc display progress logs stdout logging.basicconfig levellogging.info format asctime message download data already disk load numpy arrays lfw_people fetch_lfw_people min_faces_per_person70 resize0.4 introspect images arrays find shapes plotting n_samples lfw_people.images.shape fot machine learning use data directly relative pixel positions info ignored model lfw_people.data n_features x.shape label predict person lfw_people.target target_names lfw_people.target_names n_classes target_names.shape print total dataset size print n_samples n_samples print n_features n_features print n_classes n_classes split training set test set using stratified fold split training testing set x_train x_test y_train y_test train_test_split test_fraction0.25 compute pca eigenfaces face dataset treated unlabeled dataset unsupervised feature extraction dimensionality reduction n_components print extracting top eigenfaces faces n_components x_train.shape time pca randomizedpca n_componentsn_components whitentrue .fit x_train print done 0.3fs time eigenfaces pca.components_.reshape n_components print projecting input data eigenfaces orthonormal basis time 2.1. examples scikitlearn user guide release 0.11 x_train_pca pca.transform x_train x_test_pca pca.transform x_test print done 0.3fs time train svm classification model print fitting classifier training set time param_grid 1e3 5e3 1e4 5e4 1e5 gamma 0.0001 0.0005 0.001 0.005 0.01 0.1 clf gridsearchcv svc kernelrbf class_weightauto param_grid clf clf.fit x_train_pca y_train print done 0.3fs time print best estimator found grid search print clf.best_estimator_ quantitative evaluation model quality test set print predicting people names testing set time y_pred clf.predict x_test_pca print done 0.3fs time print classification_report y_test y_pred target_namestarget_names print confusion_matrix y_test y_pred labelsrange n_classes qualitative evaluation predictions using matplotlib def plot_gallery images titles n_row3 n_col4 helper function plot gallery portraits pl.figure figsize 1.8 n_col 2.4 n_row pl.subplots_adjust bottom0 left.01 right.99 top.90 hspace.35 range n_row n_col pl.subplot n_row n_col pl.imshow images .reshape cmappl.cm.gray pl.title titles size12 pl.xticks pl.yticks plot result prediction portion test set def title y_pred y_test target_names pred_name target_names y_pred .rsplit true_name target_names y_test .rsplit return predicted sntrue pred_name true_name prediction_titles title y_pred y_test target_names range y_pred.shape chapter example gallery scikitlearn user guide release 0.11 plot_gallery x_test prediction_titles plot gallery significative eigenfaces eigenface_titles eigenface range eigenfaces.shape plot_gallery eigenfaces eigenface_titles pl.show figure 2.31 libsvm gui libsvm gui simple graphical frontend libsvm mainly intended didactic purposes create data points point click visualize decision region induced different kernels parameter settings create positive examples click left mouse button create negative examples click right button examples class uses oneclass svm python source code svm_gui.py __future__ import division print __doc__ author peter prettenhoer peter.prettenhofer gmail.com license bsd style 
4011: import matplotlib matplotlib.use tkagg matplotlib.backends.backend_tkagg import figurecanvastkagg matplotlib.backends.backend_tkagg import navigationtoolbar2tkagg matplotlib.figure import figure matplotlib.contour import contourset import tkinter import sys import numpy sklearn import svm sklearn.datasets import dump_svmlight_file y_min y_max x_min x_max 2.1. examples scikitlearn user guide release 0.11 class model object model hold data implements observable observer pattern notifies registered observers change event. def __init__ self self.observers self.surface none self.data self.cls none self.surface_type def changed self event notify observers. observer self.observers observer.update event self def add_observer self observer register observer. self.observers.append observer def set_surface self surface self.surface surface def dump_svmlight_file self file data np.array self.data data data dump_svmlight_file file class controller object def __init__ self model self.model model self.kernel tk.intvar self.surface_type tk.intvar whether model fitted self.fitted false def fit self print fit model train np.array self.model.data train train float self.complexity.get gamma float self.gamma.get coef0 float self.coef0.get degree int self.degree.get kernel_map linear rbf poly len np.unique clf svm.oneclasssvm kernelkernel_map self.kernel.get gammagamma coef0coef0 degreedegree clf.fit else clf svm.svc kernelkernel_map self.kernel.get chapter example gallery scikitlearn user guide release 0.11 gammagamma coef0coef0 degreedegree clf.fit hasattr clf score print accuracy clf.score self.decision_surface clf self.model.clf clf self.model.set_surface self.model.surface_type self.surface_type.get self.fitted true self.model.changed surface def decision_surface self cls delta np.arange x_min x_max delta delta np.arange y_min y_max delta delta np.meshgrid cls.decision_function np.c_ x1.ravel x2.ravel z.reshape x1.shape return def clear_data self self.model.data self.fitted false self.model.changed clear def add_example self label self.model.data.append label self.model.changed example_added update decision surface already fitted self.refit def refit self refit model already fitted. self.fitted self.fit class view object test docstring. def __init__ self root controller figure f.add_subplot ax.set_xticks ax.set_yticks ax.set_xlim x_min x_max ax.set_ylim y_min y_max canvas figurecanvastkagg masterroot canvas.show canvas.get_tk_widget .pack sidetk.top filltk.both expand1 canvas._tkcanvas.pack sidetk.top filltk.both expand1 canvas.mpl_connect button_press_event self.onclick toolbar navigationtoolbar2tkagg canvas root toolbar.update self.controllbar controllbar root controller self.f self.ax self.canvas canvas 2.1. examples scikitlearn user guide release 0.11 self.controller controller self.contours self.c_labels none self.plot_kernels def plot_kernels self self.ax.text linear self.ax.text rbf exp gamma self.ax.text poly gamma def onclick self event event.xdata event.ydata event.button self.controller.add_example event.xdata event.ydata elif event.button self.controller.add_example event.xdata event.ydata def update_example self model idx model.data idx color elif color self.ax.plot color scalex0.0 scaley0.0 def update self event model event examples_loaded xrange len model.data self.update_example model event example_added self.update_example model event clear self.ax.clear self.ax.set_xticks self.ax.set_yticks self.contours self.c_labels none self.plot_kernels event surface self.remove_surface self.plot_support_vectors model.clf.support_vectors_ self.plot_decision_surface model.surface model.surface_type self.canvas.draw def remove_surface self remove old decision surface len self.contours contour self.contours isinstance contour contourset lineset contour.collections lineset.remove else contour.remove self.contours chapter example gallery scikitlearn user guide release 0.11 def plot_support_vectors self support_vectors plot support vectors placing circles corresponding data points adds circle collection contours list self.ax.scatter support_vectors support_vectors s80 edgecolors facecolors none self.contours.append def plot_decision_surface self surface type surface type levels 1.0 0.0 1.0 linestyles dashed solid dashed colors self.contours.append self.ax.contour levels elif type self.contours.append self.ax.contourf colorscolors linestyleslinestyles cmapmatplotlib.cm.bone originlower alpha0.85 self.contours.append self.ax.contour 0.0 colorsk linestyles solid else raise valueerror surface type unknown class controllbar object def __init__ self root controller tk.frame root kernel_group tk.frame tk.radiobutton kernel_group text linear variablecontroller.kernel tk.radiobutton kernel_group text rbf variablecontroller.kernel value0 commandcontroller.refit .pack anchortk.w value1 commandcontroller.refit .pack anchortk.w tk.radiobutton kernel_group text poly variablecontroller.kernel value2 commandcontroller.refit .pack anchortk.w kernel_group.pack sidetk.left valbox tk.frame controller.complexity tk.stringvar controller.complexity.set 1.0 tk.frame valbox tk.label text anchor width7 .pack sidetk.left tk.entry width6 textvariablecontroller.complexity .pack sidetk.left c.pack controller.gamma tk.stringvar controller.gamma.set 0.01 tk.frame valbox tk.label text gamma anchor width7 .pack sidetk.left tk.entry width6 textvariablecontroller.gamma .pack sidetk.left g.pack controller.degree tk.stringvar 2.1. examples scikitlearn user guide release 0.11 controller.degree.set tk.frame valbox tk.label text degree anchor width7 .pack sidetk.left tk.entry width6 textvariablecontroller.degree .pack sidetk.left d.pack controller.coef0 tk.stringvar controller.coef0.set tk.frame valbox tk.label text coef0 anchor width7 .pack sidetk.left tk.entry width6 textvariablecontroller.coef0 .pack sidetk.left r.pack valbox.pack sidetk.left cmap_group tk.frame tk.radiobutton cmap_group text hyperplanes variablecontroller.surface_type value0 commandcontroller.refit .pack anchortk.w tk.radiobutton cmap_group text surface variablecontroller.surface_type value1 commandcontroller.refit .pack anchortk.w cmap_group.pack sidetk.left train_button tk.button textfit width5 commandcontroller.fit train_button.pack fm.pack sidetk.left tk.button textclear width5 commandcontroller.clear_data .pack sidetk.left def get_parser optparse import optionparser optionparser op.add_option output action store type str dest output help path dump data return def main argv get_parser opts args op.parse_args argv root tk.tk model model controller controller model root.wm_title scikitlearn libsvm gui view view root controller model.add_observer view tk.mainloop opts.output model.dump_svmlight_file opts.output __name__ __main__ main sys.argv chapter example gallery scikitlearn user guide release 0.11 figure 2.32 topics extraction nonnegative matrix factorization topics extraction nonnegative matrix factorization proof concept application non negative matrix factorization term frequency matrix corpus documents extract additive model topic structure corpus default parameters n_samples n_features n_topics make example runnable couple tens seconds try increase dimensions problem ware time complexity polynomial sample extracted topics look quite good topic god people bible israel jesus christian true moral think christians believe say human israeli church life children jewish topic drive windows card drivers video scsi software thanks vga graphics help disk uni dos ide controller work topic game team nhl games hockey players buffalo edu year play university teams baseball columbia league player toronto topic window manager application mit motif size display widget program xlib windows user color event informa tion use events x11r5 values topic pitt gordon banks science pittsburgh univ computer soon disease edu reply pain health david article medical medicine python source code topics_extraction_with_nmf.py author olivier grisel olivier.grisel ensta.org license simplified bsd time import time sklearn.feature_extraction import text sklearn import decomposition sklearn import datasets n_samples n_features n_topics n_top_words load newsgroups dataset vectorize using common word frequency tfidf weighting without top stop words time print loading dataset extracting tfidf features ... dataset datasets.fetch_20newsgroups shuffletrue random_state1 vectorizer text.countvectorizer max_df0.95 max_featuresn_features 2.1. examples scikitlearn user guide release 0.11 counts vectorizer.fit_transform dataset.data n_samples tfidf text.tfidftransformer .fit_transform counts print done 0.3fs time fit nmf model print fitting nmf model n_samples n_features ... n_samples n_features nmf decomposition.nmf n_componentsn_topics .fit tfidf print done 0.3fs time inverse vectorizer vocabulary able feature_names vectorizer.get_feature_names topic_idx topic enumerate nmf.components_ print topic topic_idx print .join feature_names print topic.argsort n_top_words figure 2.33 wikipedia principal eigenvector wikipedia principal eigenvector classical way assert relative importance vertices graph compute principal eigenvector adjacency matrix assign vertex values components rst eigenvector centrality score http en.wikipedia.orgwikieigenvector_centrality graph webpages links values called pagerank scores google goal example analyze graph links inside wikipedia articles rank articles relative importance according eigenvector centrality traditional way compute principal eigenvector use power iteration method http en.wikipedia.orgwikipower_iteration computation achieved thanks martinssons randomized svd algoritm implemented scikit graph data fetched dbpedia dumps dbpedia extraction latent structured data wikipedia content python source code wikipedia_principal_eigenvector.py print __doc__ author olivier grisel olivier.grisel ensta.org license simplified bsd chapter example gallery scikitlearn user guide release 0.11 bz2 import bz2file import datetime import datetime pprint import pprint time import time import numpy scipy import sparse sklearn.utils.extmath import randomized_svd sklearn.externals.joblib import memory download data already disk redirects_url http downloads.dbpedia.org3.5.1enredirects_en.nt.bz2 redirects_filename redirects_url.rsplit page_links_url http downloads.dbpedia.org3.5.1enpage_links_en.nt.bz2 page_links_filename page_links_url.rsplit resources redirects_url redirects_filename page_links_url page_links_filename url filename resources os.path.exists filename import urllib print downloading data please wait ... url opener urllib.urlopen url open filename .write opener.read print loading redirect files memory memory cachedir def index redirects index_map find index article name redirect resolution redirects.get return index_map.setdefault len index_map dbpedia_resource_prefix_len len http dbpedia.orgresource shortname_slice slice dbpedia_resource_prefix_len def short_name nt_uri remove uri markers common uri prefix return nt_uri shortname_slice def get_redirects redirects_filename 2.1. examples scikitlearn user guide release 0.11 parse redirections build transitively closed map redirects print parsing redirect file line enumerate bz2file redirects_filename split line.split len split print ignoring malformed line line continue redirects short_name split short_name split print line 08d datetime.now .isoformat compute transitive closure print computing transitive closure redirect relation source enumerate redirects.keys transitive_target none target redirects source seen set source true transitive_target target target redirects.get target target none target seen break seen.add target redirects source transitive_target print line 08d datetime.now .isoformat return redirects disabling joblib pickling large dicts seems much slow memory.cache def get_adjacency_matrix redirects_filename page_links_filename limitnone extract adjacency graph scipy sparse matrix redirects resolved first 
4012: returns scipy sparse adjacency matrix redirects python dict article names article names index_map python dict article names python int article indexes print computing redirect map redirects get_redirects redirects_filename print computing integer index map index_map dict links list line enumerate bz2file page_links_filename split line.split len split print ignoring malformed line line continue index redirects index_map short_name split index redirects index_map short_name split links.append chapter example gallery scikitlearn user guide release 0.11 print line 08d datetime.now .isoformat limit none limit break print computing adjacency matrix sparse.lil_matrix len index_map len index_map dtypenp.float32 links 1.0 del links print converting csr representation x.tocsr print csr conversion done return redirects index_map stop links make possible work ram redirects index_map get_adjacency_matrix redirects_filename page_links_filename limit5000000 names dict name name index_map.iteritems print computing principal singular vectors using randomized_svd time randomized_svd n_iterations3 print done 0.3fs time print names wikipedia related strongest compenents principal singular vector similar highest eigenvector print top wikipedia pages according principal singular vectors pprint names np.abs u.t .argsort pprint names np.abs .argsort def centrality_scores alpha0.85 max_iter100 tol1e10 power iteration computation principal eigenvector method also known google pagerank implementation based one networkx project bsd licensed copyrights aric hagberg hagberg lanl.gov dan schult dschult colgate.edu pieter swart swart lanl.gov x.shape x.copy incoming_counts np.asarray x.sum axis1 .ravel print normalizing graph incoming_counts.nonzero x.data x.indptr x.indptr 1.0 incoming_counts dangle np.asarray np.where x.sum axis1 1.0 .ravel scores np.ones dtypenp.float32 initial guess range max_iter print power iteration prev_scores scores scores alpha scores np.dot dangle prev_scores 2.1. examples scikitlearn user guide release 0.11 alpha prev_scores.sum check convergence normalized l_inf norm scores_max np.abs scores .max scores_max 0.0 scores_max 1.0 err np.abs scores prev_scores .max scores_max print error 0.6f err err tol return scores return scores print computing principal eigenvector score using power iteration method time scores centrality_scores max_iter100 tol1e10 print done 0.3fs time pprint names np.abs scores .argsort 2.1.3 clustering examples concerning sklearn.cluster package 
4013: figure 2.34 adjustment chance clustering performance evaluation adjustment chance clustering performance evaluation following plots demonstrate impact number clusters number samples various clustering performance evaluation metrics nonadjusted measures vmeasure show dependency number clusters number samples mean vmeasure random labeling increases signicantly number clusters closer total number samples used compute measure adjusted chance measure ari display random variations centered around mean score 0.0 number samples clusters adjusted measures hence safely used consensus index evaluate average stability clustering algorithms given value various overlapping subsamples dataset 
4014: chapter example gallery scikitlearn user guide release 0.11 script output computing adjusted_rand_score values n_clusters n_samples100 done 0.253s computing v_measure_score values n_clusters n_samples100 done 2.110s computing adjusted_mutual_info_score values n_clusters n_samples100 done 7.992s computing mutual_info_score values n_clusters n_samples100 done 0.033s computing adjusted_rand_score values n_clusters n_samples1000 done 0.432s computing v_measure_score values n_clusters n_samples1000 done 0.932s computing adjusted_mutual_info_score values n_clusters n_samples1000 done 21.824s computing mutual_info_score values n_clusters n_samples1000 done 0.155s python source code plot_adjusted_for_chance_measures.py print __doc__ author olivier grisel olivier.grisel ensta.org license simplified bsd import numpy import pylab time import time 2.1. examples scikitlearn user guide release 0.11 sklearn import metrics def uniform_labelings_scores score_func n_samples n_clusters_range fixed_n_classesnone n_runs5 seed42 compute score random uniform cluster labelings 
4015: random labelings number clusters value possible value n_clusters_range 
4016: fixed_n_classes none first labeling considered ground truth class assignement fixed number classes. random_labels np.random.randomstate seed .random_integers scores np.zeros len n_clusters_range n_runs fixed_n_classes none labels_a random_labels low0 highfixed_n_classes sizen_samples enumerate n_clusters_range range n_runs fixed_n_classes none labels_a random_labels low0 highk sizen_samples labels_b random_labels low0 highk sizen_samples scores score_func labels_a labels_b return scores score_funcs metrics.adjusted_rand_score metrics.v_measure_score metrics.adjusted_mutual_info_score metrics.mutual_info_score independent random clusterings equal cluster number n_samples n_clusters_range np.linspace n_samples .astype np.int pl.figure plots names score_func score_funcs print computing values n_clusters n_samples score_func.__name__ len n_clusters_range n_samples time scores uniform_labelings_scores score_func n_samples n_clusters_range print done 0.3fs time plots.append pl.errorbar n_clusters_range np.median scores axis1 scores.std axis1 names.append score_func.__name__ pl.title clustering measures random uniform labelingsn equal number clusters pl.xlabel number clusters number samples fixed n_samples chapter example gallery scikitlearn user guide release 0.11 pl.ylabel score value pl.legend plots names pl.ylim ymin0.05 ymax1.05 random labeling varying n_clusters ground class labels fixed number clusters n_samples n_clusters_range np.linspace .astype np.int n_classes pl.figure plots names score_func score_funcs print computing values n_clusters n_samples score_func.__name__ len n_clusters_range n_samples time scores uniform_labelings_scores score_func n_samples n_clusters_range fixed_n_classesn_classes print done 0.3fs time plots.append pl.errorbar n_clusters_range scores.mean axis1 scores.std axis1 names.append score_func.__name__ pl.title clustering measures random uniform labelingn reference assignement classes n_classes pl.xlabel number clusters number samples fixed n_samples pl.ylabel score value pl.ylim ymin0.05 ymax1.05 pl.legend plots names pl.show figure 2.35 demo afnity propagation clustering algorithm demo afnity propagation clustering algorithm reference brendan frey delbert dueck clustering passing messages data points science feb. 2.1. examples scikitlearn user guide release 0.11 script output estimated number clusters homogeneity 0.885 completeness 0.885 vmeasure 0.885 adjusted rand index 0.922 adjusted mutual information 0.884 silhouette coefficient 0.774 python source code plot_affinity_propagation.py print __doc__ import numpy sklearn.cluster import affinitypropagation sklearn import metrics sklearn.datasets.samples_generator import make_blobs generate sample data centers labels_true make_blobs n_samples300 centerscenters cluster_std0.5 compute similarities x_norms np.sum axis1 chapter example gallery scikitlearn user guide release 0.11 x_norms np.newaxis x_norms np.newaxis np.dot x.t np.median compute affinity propagation affinitypropagation .fit cluster_centers_indices af.cluster_centers_indices_ labels af.labels_ n_clusters_ len cluster_centers_indices print estimated number clusters n_clusters_ print homogeneity 0.3f metrics.homogeneity_score labels_true labels print completeness 0.3f metrics.completeness_score labels_true labels print vmeasure 0.3f metrics.v_measure_score labels_true labels print adjusted rand index 0.3f metrics.adjusted_rand_score labels_true labels print adjusted mutual information 0.3f metrics.adjusted_mutual_info_score labels_true labels np.min print silhouette coefficient 0.3f metrics.silhouette_score labels metricprecomputed plot result import pylab itertools import cycle pl.close pl.figure pl.clf colors cycle bgrcmykbgrcmykbgrcmykbgrcmyk col zip range n_clusters_ colors class_members labels cluster_center cluster_centers_indices pl.plot class_members class_members col pl.plot cluster_center cluster_center markerfacecolorcol markeredgecolork markersize14 class_members pl.plot cluster_center cluster_center col pl.title estimated number clusters n_clusters_ pl.show comparing different clustering algorithms toy datasets example aims showing characteristics different clustering algorithms datasets interesting still last dataset example null situation clustering data homogeneous good clustering examples give intuition algorithms intuition might apply high dimensional data results could improved tweaking parameters clustering strategy instance setting number clusters methods needs parameter specied note afnity propagation tendency create many clusters thus example two parameters damping perpoint preference set mitigate 2.1. examples scikitlearn user guide release 0.11 figure 2.36 comparing different clustering algorithms toy datasets behavior 
4017: python source code plot_cluster_comparison.py print __doc__ import time import numpy import pylab sklearn import cluster datasets sklearn.metrics import euclidean_distances chapter example gallery scikitlearn user guide release 0.11 sklearn.neighbors import kneighbors_graph sklearn.preprocessing import scaler np.random.seed generate datasets choose size big enough see scalability algorithms big avoid long running times n_samples noisy_circles datasets.make_circles n_samplesn_samples factor.5 noisy_moons datasets.make_moons n_samplesn_samples noise.05 blobs datasets.make_blobs n_samplesn_samples random_state8 no_structure np.random.rand n_samples none noise.05 colors np.array bgrcmykbgrcmykbgrcmykbgrcmyk colors np.hstack colors pl.figure figsize 9.5 pl.subplots_adjust left.001 right.999 bottom.001 top.96 wspace.05 hspace.01 plot_num i_dataset dataset enumerate noisy_circles noisy_moons blobs no_structure dataset normalize dataset easier parameter selection scaler .fit_transform estimate bandwidth mean shift bandwidth cluster.estimate_bandwidth quantile0.3 connectivity matrix structured ward connectivity kneighbors_graph n_neighbors10 make connectivity symmetric connectivity 0.5 connectivity connectivity.t compute distances distances euclidean_distances create clustering estimators cluster.meanshift bandwidthbandwidth bin_seedingtrue two_means cluster.minibatchkmeans ward_five cluster.ward n_clusters2 connectivityconnectivity spectral cluster.spectralclustering modearpack dbscan cluster.dbscan eps.2 affinity_propagation cluster.affinitypropagation damping.9 algorithm two_means affinity_propagation spectral ward_five dbscan predict cluster memberships time.time algorithm spectral algorithm.fit connectivity elif algorithm affinity_propagation set low preference avoid creating many clusters parameter hard set practice algorithm.fit distances p50 distances.max else 2.1. examples scikitlearn user guide release 0.11 algorithm.fit time.time hasattr algorithm labels_ y_pred algorithm.labels_.astype np.int else y_pred algorithm.predict plot pl.subplot plot_num i_dataset pl.title str algorithm .split size18 pl.scatter colorcolors y_pred .tolist s10 hasattr algorithm cluster_centers_ centers algorithm.cluster_centers_ center_colors colors len centers pl.scatter centers centers s100 ccenter_colors pl.xlim pl.ylim pl.xticks pl.yticks pl.text .99 .01 .2fs .lstrip transformpl.gca .transaxes size15 horizontalalignmentright plot_num pl.show figure 2.37 kmeans clustering kmeans clustering plots display rstly kmeans algorithm would yield using three clusters shown effect bad initialization classication process setting n_init default amount times algorithm run different centroid seeds reduced next plot displays using eight clusters would deliver nally ground truth 
4018: chapter example gallery scikitlearn user guide release 0.11 python source code plot_cluster_iris.py print __doc__ code source gael varoqueux modified documentation merge jaques grobler license bsd import numpy import pylab mpl_toolkits.mplot3d import axes3d sklearn.cluster import kmeans sklearn import datasets np.random.seed centers iris datasets.load_iris iris.data iris.target estimators k_means_iris_3 kmeans k_means_iris_8 kmeans k_means_iris_bad_init kmeans n_init1 initrandom 2.1. examples scikitlearn user guide release 0.11 fignum name est estimators.iteritems fig pl.figure fignum figsize pl.clf axes3d fig rect .95 elev48 azim134 pl.cla est.fit labels est.labels_ ax.scatter clabels.astype np.float ax.w_xaxis.set_ticklabels ax.w_yaxis.set_ticklabels ax.w_zaxis.set_ticklabels ax.set_xlabel petal width ax.set_ylabel sepal length ax.set_zlabel petal length fignum fignum plot ground truth fig pl.figure fignum figsize pl.clf axes3d fig rect .95 elev48 azim134 pl.cla name label setosa versicolour virginica ax.text3d label .mean label .mean 1.5 label .mean name horizontalalignmentcenter bboxdict alpha.5 edgecolorw facecolorw reorder labels colors matching cluster results np.choose .astype np.float ax.scatter ax.w_xaxis.set_ticklabels ax.w_yaxis.set_ticklabels ax.w_zaxis.set_ticklabels ax.set_xlabel petal width ax.set_ylabel sepal length ax.set_zlabel petal length pl.show color quantization using kmeans performs pixelwise vector quantization image summer palace china reducing number colors required show image unique colors preserving overall appearance quality example pixels represented 3dspace kmeans used color clusters image processing literature codebook obtained kmeans cluster centers called color palette using single byte colors addressed whereas rgb encoding requires bytes per pixel gif chapter example gallery scikitlearn user guide release 0.11 figure 2.38 color quantization using kmeans format example uses palette comparison quantized image using random codebook colors picked randomly also shown 
4019: 2.1. examples scikitlearn user guide release 0.11 script output fitting estimator small subsample data done 1.301s predicting color indices full image kmeans done 1.353s predicting color indices full image random done 1.830s 
4020: python source code plot_color_quantization.py authors robert layton robertlayton gmail.com license bsd olivier grisel olivier.grisel ensta.org mathieu blondel mathieu mblondel.org print __doc__ import numpy import pylab sklearn.cluster import kmeans sklearn.metrics import euclidean_distances sklearn.datasets import load_sample_image sklearn.utils import shuffle time import time n_colors load summer palace photo china load_sample_image china.jpg convert floats instead default bits integer coding dividing important pl.imshow behaves works well foat data need range china np.array china dtypenp.float64 load image transform numpy array original_shape tuple china.shape assert image_array np.reshape china print fitting estimator small subsample data time image_array_sample shuffle image_array random_state0 chapter example gallery scikitlearn user guide release 0.11 kmeans kmeans kn_colors random_state0 .fit image_array_sample print done 0.3fs time get labels points print predicting color indices full image kmeans time labels kmeans.predict image_array print done 0.3fs time codebook_random shuffle image_array random_state0 n_colors print predicting color indices full image random time dist euclidean_distances codebook_random image_array squaredtrue labels_random dist.argmin axis0 print done 0.3fs time def recreate_image codebook labels recreate compressed image code book labels codebook.shape image np.zeros label_idx range range image codebook labels label_idx label_idx return image display results alongside original image pl.figure pl.clf pl.axes pl.axis pl.title original image colors pl.imshow china pl.figure pl.clf pl.axes pl.axis pl.title quantized image colors kmeans pl.imshow recreate_image kmeans.cluster_centers_ labels pl.figure pl.clf pl.axes pl.axis pl.title quantized image colors random pl.imshow recreate_image codebook_random labels_random pl.show demo dbscan clustering algorithm finds core samples high density expands clusters 
4021: 2.1. examples scikitlearn user guide release 0.11 figure 2.39 demo dbscan clustering algorithm script output estimated number clusters homogeneity 0.560 completeness 0.802 vmeasure 0.659 adjusted rand index 0.541 adjusted mutual information 0.559 silhouette coefficient 0.417 python source code plot_dbscan.py print __doc__ import numpy chapter example gallery scikitlearn user guide release 0.11 scipy.spatial import distance sklearn.cluster import dbscan sklearn import metrics sklearn.datasets.samples_generator import make_blobs generate sample data centers labels_true make_blobs n_samples750 centerscenters cluster_std0.4 compute similarities distance.squareform distance.pdist np.max compute dbscan dbscan .fit eps0.95 min_samples10 core_samples db.core_sample_indices_ labels db.labels_ number clusters labels ignoring noise present n_clusters_ len set labels labels else print estimated number clusters n_clusters_ print homogeneity 0.3f metrics.homogeneity_score labels_true labels print completeness 0.3f metrics.completeness_score labels_true labels print vmeasure 0.3f metrics.v_measure_score labels_true labels print adjusted rand index 0.3f metrics.adjusted_rand_score labels_true labels print adjusted mutual information 0.3f metrics.adjusted_mutual_info_score labels_true labels print silhouette coefficient 0.3f metrics.silhouette_score labels metricprecomputed plot result import pylab itertools import cycle pl.close pl.figure pl.clf black removed used noise instead colors cycle bgrcmybgrcmybgrcmybgrcmy col zip set labels colors black used noise col markersize class_members index index np.argwhere labels cluster_core_samples index index core_samples labels index index class_members index index core_samples 2.1. examples scikitlearn user guide release 0.11 markersize else markersize pl.plot markerfacecolorcol markeredgecolork markersizemarkersize pl.title estimated number clusters n_clusters_ pl.show figure 2.40 feature agglomeration feature agglomeration images similiar features merged together using feature agglomeration 
4022: python source code plot_digits_agglomeration.py print __doc__ code source gael varoqueux modified documentation merge jaques grobler license bsd import numpy import pylab chapter example gallery scikitlearn user guide release 0.11 sklearn import datasets cluster sklearn.feature_extraction.image import grid_to_graph digits datasets.load_digits images digits.images np.reshape images len images connectivity grid_to_graph images .shape agglo cluster.wardagglomeration connectivityconnectivity n_clusters32 agglo.fit x_reduced agglo.transform x_restored agglo.inverse_transform x_reduced images_restored np.reshape x_restored images.shape pl.figure figsize 3.5 pl.clf pl.subplots_adjust left.01 right.99 bottom.01 top.91 range pl.subplot pl.imshow images cmappl.cm.gray vmax16 interpolationnearest pl.xticks pl.yticks pl.title original data pl.subplot pl.imshow images_restored cmappl.cm.gray vmax16 interpolationnearest pl.title agglomerated data pl.xticks pl.yticks pl.subplot pl.imshow np.reshape agglo.labels_ images .shape interpolationnearest cmappl.cm.spectral pl.xticks pl.yticks pl.title labels figure 2.41 feature agglomeration vs. univariate selection feature agglomeration vs. univariate selection example compares dimensionality reduction strategies univariate feature selection anova feature agglomeration ward hierarchical clustering methods compared regression problem using bayesianridge supervised estimator 
4023: 2.1. examples scikitlearn user guide release 0.11 script output ________________________________________________________________________________ memory calling sklearn.cluster.hierarchical.ward_tree ... ward_tree array 0.451933 ... 0.675318 ... 0.275706 ... 1.085711 1600x1600 sparse matrix type type numpy.int32 stored elements coordinate format copytrue n_components1 ________________________________________________________ward_tree 0.3s 0.0min ________________________________________________________________________________ memory calling sklearn.cluster.hierarchical.ward_tree ... ward_tree array 0.905206 ... 0.161245 ... 0.849835 ... 1.091621 1600x1600 sparse matrix type type numpy.int32 stored elements coordinate format copytrue n_components1 ________________________________________________________ward_tree 0.3s 0.0min ________________________________________________________________________________ memory calling sklearn.cluster.hierarchical.ward_tree ... ward_tree array 0.905206 ... 0.675318 ... 0.849835 ... 1.085711 1600x1600 sparse matrix type type numpy.int32 stored elements coordinate format copytrue n_components1 ________________________________________________________ward_tree 0.3s 0.0min ________________________________________________________________________________ memory calling sklearn.feature_selection.univariate_selection.f_regression ... f_regression array 0.451933 ... 0.275706 ... 0.675318 ... 1.085711 array 25.267703 ... 25.026711 _____________________________________________________f_regression 0.0s 0.0min ________________________________________________________________________________ memory calling sklearn.feature_selection.univariate_selection.f_regression ... f_regression array 0.905206 ... 0.849835 ... 0.161245 ... 1.091621 array 27.447268 ... 112.638768 _____________________________________________________f_regression 0.0s 0.0min ________________________________________________________________________________ memory calling sklearn.feature_selection.univariate_selection.f_regression ... f_regression array 0.905206 ... 0.849835 chapter example gallery scikitlearn user guide release 0.11 ... 0.675318 ... 1.085711 array 27.447268 ... 25.026711 _____________________________________________________f_regression 0.0s 0.0min python source code plot_feature_agglomeration_vs_univariate_selection.py author alexandre gramfort alexandre.gramfort inria.fr license bsd style 
4024: print __doc__ import shutil import tempfile import numpy import pylab scipy import linalg ndimage sklearn.feature_extraction.image import grid_to_graph sklearn import feature_selection sklearn.cluster import wardagglomeration sklearn.linear_model import bayesianridge sklearn.pipeline import pipeline sklearn.grid_search import gridsearchcv sklearn.externals.joblib import memory sklearn.cross_validation import kfold generate data n_samples size image size roi_size snr np.random.seed mask np.ones size size dtypenp.bool coef np.zeros size size coef roi_size roi_size coef roi_size roi_size 
4025: np.random.randn n_samples size smooth data ndimage.gaussian_filter x.reshape size size sigma1.0 .ravel x.mean axis0 x.std axis0 np.dot coef.ravel noise np.random.randn y.shape noise_coef linalg.norm np.exp snr linalg.norm noise noise_coef noise add noise compute coefs bayesian ridge gridsearch kfold len crossvalidation generator model selection ridge bayesianridge cachedir tempfile.mkdtemp mem memory cachedircachedir verbose1 2.1. examples scikitlearn user guide release 0.11 ward agglomeration followed bayesianridge grid_to_graph n_xsize n_ysize ward wardagglomeration n_clusters10 connectivitya memorymem n_components1 clf pipeline ward ward ridge ridge select optimal number parcels grid search clf gridsearchcv clf ward__n_clusters n_jobs1 cvcv clf.fit set best parameters coef_ clf.best_estimator_.steps .coef_ coef_ clf.best_estimator_.steps .inverse_transform coef_ coef_agglomeration_ coef_.reshape size size anova univariate feature selection followed bayesianridge f_regression mem.cache feature_selection.f_regression caching function anova feature_selection.selectpercentile f_regression clf pipeline anova anova ridge ridge select optimal percentage features grid search clf gridsearchcv clf anova__percentile cvcv clf.fit set best parameters coef_ clf.best_estimator_.steps .coef_ coef_ clf.best_estimator_.steps .inverse_transform coef_ coef_selection_ coef_.reshape size size inverse transformation plot results image pl.close pl.figure figsize 7.3 2.7 pl.subplot pl.imshow coef interpolation nearest cmappl.cm.rdbu_r pl.title true weights pl.subplot pl.imshow coef_selection_ interpolation nearest cmappl.cm.rdbu_r pl.title feature selection pl.subplot pl.imshow coef_agglomeration_ interpolation nearest cmappl.cm.rdbu_r pl.title feature agglomeration pl.subplots_adjust 0.04 0.0 0.98 0.94 0.16 0.26 pl.show attempt remove temporary cachedir dont worry fails shutil.rmtree cachedir ignore_errorstrue figure 2.42 demo kmeans clustering handwritten digits data demo kmeans clustering handwritten digits data example compare various initialization strategies kmeans terms runtime quality results 
4026: chapter example gallery scikitlearn user guide release 0.11 ground truth known also apply different cluster quality metrics judge goodness cluster labels ground truth cluster quality metrics evaluated see clustering performance evaluation denitions discussions met rics shorthand homo compl vmeas ari ami silhouette full name homogeneity score completeness score measure adjusted rand index adjusted mutual information silhouette coefcient script output n_samples n_features time inertia homo compl vmeas ari ami silhouette n_digits _______________________________________________________________________________ init kmeans random pcabased _______________________________________________________________________________ 2.15s 2.05s 0.18s 0.602 0.669 0.673 0.465 0.553 0.567 0.598 0.666 0.670 0.146 0.147 0.150 0.650 0.710 0.715 0.625 0.689 0.693 python source code plot_kmeans_digits.py 2.1. examples scikitlearn user guide release 0.11 print __doc__ time import time import numpy import pylab sklearn import metrics sklearn.cluster import kmeans sklearn.datasets import load_digits sklearn.decomposition import pca sklearn.preprocessing import scale np.random.seed digits load_digits data scale digits.data n_samples n_features data.shape n_digits len np.unique digits.target labels digits.target sample_size print n_digits n_samples n_features n_digits n_samples n_features print print init time inertia homo compl vmeas ari ami silhouette def bench_k_means estimator name data time estimator.fit data print .2fs .3f .3f .3f .3f .3f .3f name time estimator.inertia_ metrics.homogeneity_score labels estimator.labels_ metrics.completeness_score labels estimator.labels_ metrics.v_measure_score labels estimator.labels_ metrics.adjusted_rand_score labels estimator.labels_ metrics.adjusted_mutual_info_score labels metrics.silhouette_score data estimator.labels_ estimator.labels_ metriceuclidean sample_sizesample_size bench_k_means kmeans initkmeans kn_digits n_init10 name kmeans datadata bench_k_means kmeans initrandom kn_digits n_init10 name random datadata case seeding centers deterministic hence run kmeans algorithm n_init1 pca pca n_componentsn_digits .fit data bench_k_means kmeans initpca.components_ kn_digits n_init1 name pcabased chapter example gallery scikitlearn user guide release 0.11 print datadata visualize results pcareduced data reduced_data pca n_components2 .fit_transform data kmeans kmeans initkmeans kn_digits n_init10 .fit reduced_data step size mesh decrease increase quality .02 point mesh x_min m_max y_min y_max 
4027: plot decision boundary asign color x_min x_max reduced_data .min reduced_data .max y_min y_max reduced_data .min reduced_data .max np.meshgrid np.arange x_min x_max np.arange y_min y_max obtain labels point mesh use last trained model kmeans.predict np.c_ xx.ravel yy.ravel put result color plot z.reshape xx.shape pl.figure pl.clf pl.imshow interpolationnearest extent xx.min xx.max yy.min yy.max cmappl.cm.paired aspectauto originlower pl.plot reduced_data reduced_data markersize2 plot centroids white centroids kmeans.cluster_centers_ pl.scatter centroids centroids markerx s169 linewidths3 colorw zorder10 pl.title kmeans clustering digits dataset pcareduced data centroids marked white cross pl.xlim x_min x_max pl.ylim y_min y_max pl.xticks pl.yticks pl.show figure 2.43 empirical evaluation impact kmeans initialization 2.1. examples scikitlearn user guide release 0.11 empirical evaluation impact kmeans initialization evaluate ability kmeans initializations strategies make algorithm convergence robust measured relative standard deviation inertia clustering i.e sum distances nearest cluster center rst plot shows best inertia reached combination model kmeans minibatchkmeans init method init random init kmeans increasing values n_init parameter controls number initializations second plot demonstrate one single run minibatchkmeans estimator using init random n_init1 run leads bad convergence local optimum estimated centers stucked ground truth clusters dataset used evaluation grid isotropic gaussian clusters widely spaced 
4028: script output evaluation kmeans kmeans init evaluation kmeans random init evaluation minibatchkmeans kmeans init evaluation minibatchkmeans random init python source code plot_kmeans_stability_low_dim_dense.py print __doc__ author olivier grisel olivier.grisel ensta.org license simplified bsd import numpy chapter example gallery scikitlearn user guide release 0.11 import pylab import matplotlib.cm sklearn.utils import shuffle sklearn.utils import check_random_state sklearn.cluster import minibatchkmeans sklearn.cluster import kmeans random_state np.random.randomstate number run randomly generated dataset strategy able compute estimate standard deviation n_runs kmeans models several random inits able trade cpu time convergence robustness n_init_range np.array datasets generation parameters n_samples_per_center grid_size scale 0.1 n_clusters grid_size def make_data random_state n_samples_per_center grid_size scale random_state check_random_state random_state centers np.array range grid_size range grid_size n_clusters_true n_featues centers.shape noise random_state.normal scalescale size n_samples_per_center centers.shape np.concatenate noise centers np.concatenate n_samples_per_center range n_clusters_true return shuffle random_staterandom_state part quantitative evaluation various init methods fig pl.figure plots legends cases kmeans kmeans kmeans random minibatchkmeans kmeans max_no_improvement minibatchkmeans random max_no_improvement init_size factory init params cases print evaluation init factory.__name__ init inertia np.empty len n_init_range n_runs run_id range n_runs 2.1. examples scikitlearn user guide release 0.11 make_data run_id n_samples_per_center grid_size scale n_init enumerate n_init_range factory kn_clusters initinit random_staterun_id n_initn_init params .fit inertia run_id km.inertia_ pl.errorbar n_init_range inertia.mean axis1 inertia.std axis1 plots.append legends.append init factory.__name__ init pl.xlabel n_init pl.ylabel inertia pl.legend plots legends pl.title mean inertia various kmeans init across runs n_runs part qualitative visual inspection convergence make_data random_state n_samples_per_center grid_size scale minibatchkmeans kn_clusters initrandom n_init1 random_staterandom_state .fit fig pl.figure range n_clusters my_members km.labels_ color cm.spectral float n_clusters pl.plot my_members my_members marker. ccolor cluster_center km.cluster_centers_ pl.plot cluster_center cluster_center markerfacecolorcolor markeredgecolork markersize6 pl.title example cluster allocation single random initn minibatchkmeans pl.show figure 2.44 vector quantization example vector quantization example classic image processing example lena 8bit grayscale bitdepth sized image used illustrate kmeans used vector quantization 
4029: chapter example gallery scikitlearn user guide release 0.11 python source code plot_lena_compress.py print __doc__ code source gael varoqueux modified documentation merge jaques grobler license bsd import numpy import scipy import pylab sklearn import cluster n_clusters np.random.seed try lena sp.lena except attributeerror newer versions scipy lena misc scipy import misc lena misc.lena lena.reshape need n_sample n_feature array k_means cluster.kmeans kn_clusters n_init4 k_means.fit values k_means.cluster_centers_.squeeze labels k_means.labels_ create array labels values lena_compressed np.choose labels values lena_compressed.shape lena.shape vmin lena.min vmax lena.max 2.1. examples scikitlearn user guide release 0.11 original lena pl.figure figsize 2.2 pl.imshow lena cmappl.cm.gray vminvmin vmax256 compressed lena pl.figure figsize 2.2 pl.imshow lena_compressed cmappl.cm.gray vminvmin vmaxvmax equal bins lena regular_values np.linspace n_clusters regular_labels np.searchsorted regular_values lena regular_values regular_values regular_values mean regular_lena np.choose regular_labels.ravel regular_values regular_lena.shape lena.shape pl.figure figsize 2.2 pl.imshow regular_lena cmappl.cm.gray vminvmin vmaxvmax histogram pl.figure figsize 2.2 pl.clf pl.axes .01 .01 .98 .98 pl.hist bins256 color.5 edgecolor.5 pl.yticks pl.xticks regular_values values np.sort values center_1 center_2 zip values values pl.axvline center_1 center_2 colorb center_1 center_2 zip regular_values regular_values pl.axvline center_1 center_2 colorb linestyle pl.show figure 2.45 segmenting picture lena regions segmenting picture lena regions example uses spectral clustering graph created voxeltovoxel difference image break image multiple partlyhomogenous regions procedure spectral clustering image efcient approximate solution nding normalized graph cuts 
4030: chapter example gallery scikitlearn user guide release 0.11 python source code plot_lena_segmentation.py print __doc__ author gael varoquaux gael.varoquaux normalesup.org license bsd import numpy import scipy import pylab sklearn.feature_extraction import image sklearn.cluster import spectral_clustering lena sp.misc.lena downsample image factor lena lena lena lena lena lena lena lena lena lena convert image graph value gradient edges graph image.img_to_graph lena take decreasing function gradient exponential smaller beta independent segmentation 2.1. examples scikitlearn user guide release 0.11 actual image beta1 segmentation close voronoi beta eps 1e6 graph.data np.exp beta graph.data lena.std eps apply spectral clustering step goes much faster pyamg installed n_regions labels spectral_clustering graph kn_regions labels labels.reshape lena.shape visualize resulting regions pl.figure figsize pl.imshow lena range n_regions cmappl.cm.gray pl.contour labels contours1 colors pl.cm.spectral float n_regions pl.xticks pl.yticks pl.show figure 2.46 demo structured ward hierarchical clustering lena image demo structured ward hierarchical clustering lena image compute segmentation image ward hierarchical clustering clustering spatially constrained order segmented region one piece 
4031: chapter example gallery scikitlearn user guide release 0.11 script output compute structured hierarchical clustering ... elaspsed time number pixels number clusters 18.7588250637 python source code plot_lena_ward_segmentation.py author vincent michel license bsd style 
4032: alexandre gramfort print __doc__ import time time import numpy import scipy import pylab sklearn.feature_extraction.image import grid_to_graph sklearn.cluster import ward generate data lena sp.misc.lena downsample image factor 2.1. examples scikitlearn user guide release 0.11 lena lena lena lena lena np.reshape lena define structure data pixels connected neighbors connectivity grid_to_graph lena.shape compute clustering print compute structured hierarchical clustering ... time.time n_clusters number regions ward ward n_clustersn_clusters connectivityconnectivity .fit label np.reshape ward.labels_ lena.shape print elaspsed time time.time print number pixels label.size print number clusters np.unique label .size plot results image pl.figure figsize pl.imshow lena cmappl.cm.gray range n_clusters pl.contour label contours1 colors pl.cm.spectral float n_clusters pl.xticks pl.yticks pl.show figure 2.47 demo meanshift clustering algorithm demo meanshift clustering algorithm reference dorin comaniciu peter meer mean shift robust approach toward feature space analysis ieee transactions pattern analysis machine intelligence 2002. 
4033: chapter example gallery scikitlearn user guide release 0.11 script output number estimated clusters python source code plot_mean_shift.py print __doc__ import numpy sklearn.cluster import meanshift estimate_bandwidth sklearn.datasets.samples_generator import make_blobs generate sample data centers make_blobs n_samples10000 centerscenters cluster_std0.6 compute clustering meanshift following bandwidth automatically detected using bandwidth estimate_bandwidth quantile0.2 n_samples500 meanshift bandwidthbandwidth bin_seedingtrue ms.fit labels ms.labels_ cluster_centers ms.cluster_centers_ 2.1. examples scikitlearn user guide release 0.11 labels_unique np.unique labels n_clusters_ len labels_unique print number estimated clusters n_clusters_ plot result import pylab itertools import cycle pl.figure pl.clf colors cycle bgrcmykbgrcmykbgrcmykbgrcmyk col zip range n_clusters_ colors my_members labels cluster_center cluster_centers pl.plot my_members my_members col pl.plot cluster_center cluster_center markerfacecolorcol markeredgecolork markersize14 pl.title estimated number clusters n_clusters_ pl.show figure 2.48 demo means clustering algorithm demo means clustering algorithm want compare performance minibatchkmeans kmeans minibatchkmeans faster gives slightly different results see mini batch kmeans cluster set data rst kmeans minibatchkmeans plot results also plot points labelled differently two algorithms 
4034: python source code plot_mini_batch_kmeans.py chapter example gallery scikitlearn user guide release 0.11 print __doc__ import time import numpy import pylab sklearn.cluster import minibatchkmeans kmeans sklearn.metrics.pairwise import euclidean_distances sklearn.datasets.samples_generator import make_blobs generate sample data np.random.seed batch_size centers n_clusters len centers labels_true make_blobs n_samples3000 centerscenters cluster_std0.7 compute clustering means k_means kmeans initkmeans n_init10 time.time k_means.fit t_batch time.time k_means_labels k_means.labels_ k_means_cluster_centers k_means.cluster_centers_ k_means_labels_unique np.unique k_means_labels compute clustering minibatchkmeans mbk minibatchkmeans initkmeans batch_sizebatch_size n_init10 max_no_improvement10 verbose0 time.time mbk.fit t_mini_batch time.time mbk_means_labels mbk.labels_ mbk_means_cluster_centers mbk.cluster_centers_ mbk_means_labels_unique np.unique mbk_means_labels plot result fig pl.figure figsize fig.subplots_adjust left0.02 right0.98 bottom0.05 top0.9 colors 4eacc5 ff9c34 4e9a06 want colors cluster minibatchkmeans kmeans algorithm lets pair cluster centers per closest one 
4035: distance euclidean_distances k_means_cluster_centers mbk_means_cluster_centers squaredtrue order distance.argmin axis1 2.1. examples scikitlearn user guide release 0.11 kmeans fig.add_subplot col zip range n_clusters colors my_members k_means_labels cluster_center k_means_cluster_centers ax.plot my_members my_members markerfacecolorcol marker ax.plot cluster_center cluster_center markerfacecolorcol markeredgecolork markersize6 ax.set_title kmeans ax.set_xticks ax.set_yticks pl.text 3.5 1.8 train time .2fsninertia t_batch k_means.inertia_ minibatchkmeans fig.add_subplot col zip range n_clusters colors my_members mbk_means_labels order cluster_center mbk_means_cluster_centers order ax.plot my_members my_members markerfacecolorcol marker ax.plot cluster_center cluster_center markerfacecolorcol markeredgecolork markersize6 ax.set_title minibatchkmeans ax.set_xticks ax.set_yticks pl.text 3.5 1.8 train time .2fsninertia t_mini_batch mbk.inertia_ initialise different array false different mbk_means_labels fig.add_subplot range n_clusters different k_means_labels mbk_means_labels order identic np.logical_not different ax.plot identic identic markerfacecolor bbbbbb marker ax.plot different different markerfacecolorm marker ax.set_title difference ax.set_xticks ax.set_yticks pl.show spectral clustering image segmentation example image connected circles generated spectral clustering used separate circles settings spectral clustering approach solves problem know normalized graph cuts image seen graph connected voxels spectral clustering algorithm amounts choosing graph cuts dening regions minimizing ratio gradient along cut volume region algorithm tries balance volume balance region sizes take circles different sizes chapter example gallery scikitlearn user guide release 0.11 figure 2.49 spectral clustering image segmentation segmentation fails addition useful information intensity image gradient choose perform spectral clustering graph weakly informed gradient close performing voronoi partition graph addition use mask objects restrict graph outline objects example interested separating objects one background 
4036: 2.1. examples scikitlearn user guide release 0.11 python source code plot_segmentation_toy.py print __doc__ authors license bsd emmanuelle gouillart emmanuelle.gouillart normalesup.org gael varoquaux gael.varoquaux normalesup.org import numpy import pylab sklearn.feature_extraction import image sklearn.cluster import spectral_clustering np.indices center1 center2 chapter example gallery scikitlearn user guide release 0.11 center3 center4 radius1 radius2 radius3 radius4 circle1 center1 center1 radius1 circle2 center2 center2 radius2 circle3 center3 center3 radius3 circle4 center4 center4 radius4 circles img circle1 circle2 circle3 circle4 mask img.astype bool img img.astype float img 0.2 np.random.randn img.shape convert image graph value gradient edges graph image.img_to_graph img maskmask take decreasing function gradient take weakly dependant gradient segmentation close voronoi graph.data np.exp graph.data graph.data.std force solver arpack since amg numerically unstable example labels spectral_clustering graph modearpack label_im np.ones mask.shape label_im mask labels pl.matshow img pl.matshow label_im circles img circle1 circle2 mask img.astype bool img img.astype float img 0.2 np.random.randn img.shape graph image.img_to_graph img maskmask graph.data np.exp graph.data graph.data.std labels spectral_clustering graph modearpack label_im np.ones mask.shape label_im mask labels pl.matshow img pl.matshow label_im pl.show 2.1. examples scikitlearn user guide release 0.11 figure 2.50 hierarchical clustering structured unstructured ward hierarchical clustering structured unstructured ward example builds swiss roll dataset runs hierarchical clustering position rst step hierarchical clustering without connectivity constraints structure solely based distance whereas second step clustering restricted knearest neighbors graph hierarchical clustering structure prior clusters learned without connectivity constraints respect structure swiss roll extend across different folds manifolds opposite opposing connectivity constraints clusters form nice parcellation swiss roll 
4037: script output compute unstructured hierarchical clustering ... elapsed time number points 0.912338972092 chapter example gallery scikitlearn user guide release 0.11 compute structured hierarchical clustering ... elapsed time number points 0.172855138779 python source code plot_ward_structured_vs_unstructured.py authors vincent michel license bsd alexandre gramfort gael varoquaux print __doc__ import time time import numpy import pylab import mpl_toolkits.mplot3d.axes3d sklearn.cluster import ward sklearn.datasets.samples_generator import make_swiss_roll generate data swiss roll dataset n_samples noise 0.05 make_swiss_roll n_samples noise make thinner compute clustering print compute unstructured hierarchical clustering ... time.time ward ward n_clusters6 .fit label ward.labels_ print elapsed time time.time print number points label.size plot result fig pl.figure p3.axes3d fig ax.view_init np.unique label ax.plot3d label label label colorpl.cm.jet np.float np.max label pl.title without connectivity constraints define structure data nearest neighbors sklearn.neighbors import kneighbors_graph connectivity kneighbors_graph n_neighbors10 compute clustering print compute structured hierarchical clustering ... time.time ward ward n_clusters6 connectivityconnectivity .fit 2.1. examples scikitlearn user guide release 0.11 label ward.labels_ print elapsed time time.time print number points label.size plot result fig pl.figure p3.axes3d fig ax.view_init np.unique label ax.plot3d label label label colorpl.cm.jet float np.max label pl.title connectivity constraints pl.show 2.1.4 covariance estimation examples concerning sklearn.covariance package 
4038: figure 2.51 ledoitwolf covariance simple estimation ledoitwolf covariance simple estimation usual covariance maximum likelihood estimate regularized using shrinkage ledoit wolf proposed close formula compute asymptotical optimal shrinkage parameter minimizing mse criterion yielding ledoitwolf covariance estimate chen proposed improvement ledoitwolf shrinkage parameter oas coefcient whose convergence signicantly better assumption data gaussian example compute likelihood unseen data different values shrinkage parameter highlighting oas estimates ledoitwolf estimate stays close likelihood criterion optimal value artifact method since asymptotic working small number observations oas estimate deviates likelihood criterion optimal value better approximate mse optimal value especially small number observations 
4039: chapter example gallery scikitlearn user guide release 0.11 python source code plot_covariance_estimation.py print __doc__ import numpy import pylab scipy import linalg generate sample data n_features n_samples base_x_train np.random.normal size n_samples n_features base_x_test np.random.normal size n_samples n_features color samples coloring_matrix np.random.normal size n_features n_features x_train np.dot base_x_train coloring_matrix x_test np.dot base_x_test coloring_matrix compute ledoitwolf covariances grid shrinkages sklearn.covariance import ledoitwolf oas shrunkcovariance log_likelihood empirical_covariance ledoitwolf optimal shrinkage coefficient estimate 2.1. examples scikitlearn user guide release 0.11 ledoitwolf loglik_lw lw.fit x_train assume_centeredtrue .score x_test assume_centeredtrue oas coefficient estimate oas loglik_oa oa.fit x_train assume_centeredtrue .score x_test assume_centeredtrue spanning range possible shrinkage coefficient values shrinkages np.logspace negative_logliks shrunkcovariance shrinkages .fit x_train assume_centeredtrue .score x_test assume_centeredtrue shrinkages getting likelihood real model real_cov np.dot coloring_matrix.t coloring_matrix emp_cov empirical_covariance x_train loglik_real log_likelihood emp_cov linalg.inv real_cov plot results pl.figure pl.title regularized covariance likelihood shrinkage coefficient pl.xlabel shrinkage pl.ylabel negative loglikelihood range shrinkage curve pl.loglog shrinkages negative_logliks real likelihood reference bug hlines ... linestyle breaks older versions matplotlib pl.hlines loglik_real pl.xlim pl.xlim colorred pl.plot pl.xlim loglik_real label real covariance likelihood label real covariance likelihood linestyle adjust view lik_max np.amax negative_logliks lik_min np.amin negative_logliks ylim0 lik_min np.log pl.ylim pl.ylim ylim1 lik_max np.log lik_max lik_min likelihood pl.vlines lw.shrinkage_ ylim0 loglik_lw colorg linewidth3 labelledoitwolf estimate oas likelihood pl.vlines oa.shrinkage_ ylim0 loglik_oa colororange linewidth3 labeloas estimate pl.ylim ylim0 ylim1 pl.xlim shrinkages shrinkages pl.legend pl.show ledoitwolf oas estimation usual covariance maximum likelihood estimate regularized using shrinkage ledoit wolf proposed close formula compute asymptotical optimal shrinkage parameter minimizing mse criterion yielding chapter example gallery scikitlearn user guide release 0.11 figure 2.52 ledoitwolf oas estimation ledoitwolf covariance estimate chen proposed improvement ledoitwolf shrinkage parameter oas coefcient whose convergence signicantly better assumption data gaussian example inspired chens publication shows comparison estimated mse oas methods using gaussian distributed data shrinkage algorithms mmse covariance estimation chen al. ieee trans sign proc. volume issue october 
4040: python source code plot_lw_vs_oas.py print __doc__ 2.1. examples scikitlearn user guide release 0.11 import numpy import pylab scipy.linalg import toeplitz cholesky sklearn.covariance import ledoitwolf oas n_features simulation covariance matrix process 0.1 real_cov toeplitz np.arange n_features coloring_matrix cholesky real_cov n_samples_range np.arange repeat lw_mse np.zeros n_samples_range.size repeat oa_mse np.zeros n_samples_range.size repeat lw_shrinkage np.zeros n_samples_range.size repeat oa_shrinkage np.zeros n_samples_range.size repeat n_samples enumerate n_samples_range range repeat np.dot np.random.normal size n_samples n_features coloring_matrix.t ledoitwolf store_precisionfalse lw.fit assume_centeredtrue lw_mse lw.error_norm real_cov scalingfalse lw_shrinkage lw.shrinkage_ oas store_precisionfalse oa.fit assume_centeredtrue oa_mse oa.error_norm real_cov scalingfalse oa_shrinkage oa.shrinkage_ plot mse pl.subplot pl.errorbar n_samples_range lw_mse.mean yerrlw_mse.std labelledoitwolf colorg pl.errorbar n_samples_range oa_mse.mean yerroa_mse.std labeloas colorr pl.ylabel squared error pl.legend loc upper right pl.title comparison covariance estimators pl.xlim plot shrinkage coefficient pl.subplot pl.errorbar n_samples_range lw_shrinkage.mean yerrlw_shrinkage.std labelledoitwolf colorg pl.errorbar n_samples_range oa_shrinkage.mean yerroa_shrinkage.std labeloas colorr pl.xlabel n_samples pl.ylabel shrinkage pl.legend loc lower right pl.ylim pl.ylim pl.ylim pl.ylim pl.xlim pl.show chapter example gallery scikitlearn user guide release 0.11 figure 2.53 robust covariance estimation mahalanobis distances relevance robust covariance estimation mahalanobis distances relevance gaussian ditributed data distance observation mode distribution computed using mahalanobis distance cid48 location covariance underlying gaussian distribution practice replaced estimates usual covariance maximum likelihood estimate sensitive presence outliers data set therefor corresponding mahalanobis distances one would better use robust estimator covariance garanty estimation resistant errorneous observations data set associated mahalanobis distances accurately reect true organisation observations minimum covariance determinant estimator robust highbreakdown point i.e used estimate covariance matrix highly contaminated datasets math rac n_samplesn_features1 outliers estimator covariance idea math rac n_samplesn_features1 observations whose empirical covariance smallest determinant yielding pure subset observations compute standards estimates location covariance minimum covariance determinant estimator mcd introduced p.j.rousseuw example illustrates mahalanobis distances affected outlying data observations drawn contaminating distribution distinguishable observations comming real gaussian distribution one may want work using mcdbased mahalanobis distances two populations become distinguish able associated applications outliers detection observations ranking clustering ... vizualisation purpose cubique root mahalanobis distances represented boxplot wilson hilferty suggest rousseeuw least median squares regression stat ass wilson hilferty distribution chisquare proceedings national academy sciences united states america 
4041: 2.1. examples scikitlearn user guide release 0.11 python source code plot_mahalanobis_distances.py print __doc__ import numpy import pylab sklearn.covariance import empiricalcovariance mincovdet n_samples n_outliers n_features generate data gen_cov np.eye n_features gen_cov np.dot np.random.randn n_samples n_features gen_cov add outliers outliers_cov np.eye n_features outliers_cov np.arange n_features np.arange n_features n_outliers np.dot np.random.randn n_outliers n_features outliers_cov fit minimum covariance determinant mcd robust estimator data robust_cov mincovdet .fit compare estimators learnt full data set true parameters chapter example gallery scikitlearn user guide release 0.11 emp_cov empiricalcovariance .fit display results fig pl.figure pl.subplots_adjust hspace.1 wspace.4 top.95 bottom.05 show data set subfig1 pl.subplot inlier_plot subfig1.scatter colorblack labelinliers outlier_plot subfig1.scatter n_outliers n_outliers subfig1.set_xlim subfig1.get_xlim subfig1.set_title mahalanobis distances contaminated data set colorred labeloutliers show contours distance functions np.meshgrid np.linspace pl.xlim pl.xlim np.linspace pl.ylim pl.ylim np.c_ xx.ravel yy.ravel mahal_emp_cov emp_cov.mahalanobis mahal_emp_cov mahal_emp_cov.reshape xx.shape emp_cov_contour subfig1.contour np.sqrt mahal_emp_cov cmappl.cm.pubu_r linestylesdashed mahal_robust_cov robust_cov.mahalanobis mahal_robust_cov mahal_robust_cov.reshape xx.shape robust_contour subfig1.contour np.sqrt mahal_robust_cov cmappl.cm.ylorbr_r linestylesdotted subfig1.legend emp_cov_contour.collections robust_contour.collections inlier_plot outlier_plot mle dist robust dist inliers outliers loc upper right borderaxespad0 pl.xticks pl.yticks plot scores point emp_mahal emp_cov.mahalanobis np.mean 0.33 subfig2 pl.subplot subfig2.boxplot emp_mahal n_outliers emp_mahal n_outliers widths.25 subfig2.plot 1.26 np.ones n_samples n_outliers emp_mahal n_outliers markeredgewidth1 subfig2.plot 2.26 np.ones n_outliers emp_mahal n_outliers markeredgewidth1 subfig2.axes.set_xticklabels inliers outliers size15 subfig2.set_ylabel sqrt mahal dist size16 subfig2.set_title nonrobust estimatesn maximum likelihood pl.yticks robust_mahal robust_cov.mahalanobis robust_cov.location_ 0.33 subfig3 pl.subplot subfig3.boxplot robust_mahal n_outliers robust_mahal n_outliers subfig3.plot 1.26 np.ones n_samples n_outliers widths.25 2.1. examples scikitlearn user guide release 0.11 robust_mahal n_outliers markeredgewidth1 subfig3.plot 2.26 np.ones n_outliers robust_mahal n_outliers markeredgewidth1 subfig3.axes.set_xticklabels inliers outliers size15 subfig3.set_ylabel sqrt mahal dist size16 subfig3.set_title robust estimatesn minimum covariance determinant pl.yticks pl.show figure 2.54 outlier detection several methods 
4042: outlier detection several methods 
4043: example illustrates two ways performing novelty outlier detection amount contamination known based robust estimator covariance assuming data gaussian distributed performs better oneclass svm case 
4044: using oneclass svm ability capture shape data set hence performing better data strongly nongaussian i.e two wellseparated clusters ground truth inliers outliers given points colors orangelled area indicates points reported outliers method assume know fraction outliers datasets thus rather using predict method objects set threshold decision_function separate corresponding fraction 
4045: chapter example gallery scikitlearn user guide release 0.11 python source code plot_outlier_detection.py print __doc__ import numpy import pylab import matplotlib.font_manager scipy import stats sklearn import svm sklearn.covariance import ellipticenvelope example settings n_samples outliers_fraction 0.25 clusters_separation define two outlier detection tools compared classifiers oneclass svm svm.oneclasssvm nu0.95 outliers_fraction 0.05 kernel rbf gamma0.1 robust covariance estimator ellipticenvelope contamination.1 compare given classifiers given settings np.meshgrid np.linspace np.linspace n_inliers int outliers_fraction n_samples n_outliers int outliers_fraction n_samples ground_truth np.ones n_samples dtypeint ground_truth n_outliers fit problem varying cluster separation offset enumerate clusters_separation np.random.seed data generation 0.3 np.random.randn 0.5 n_inliers offset 0.3 np.random.randn 0.5 n_inliers offset np.r_ add outliers np.r_ np.random.uniform low6 high6 size n_outliers fit model oneclass svm pl.figure figsize clf_name clf enumerate classifiers.iteritems fit data tag outliers clf.fit y_pred clf.decision_function .ravel threshold stats.scoreatpercentile y_pred 2.1. examples scikitlearn user guide release 0.11 outliers_fraction y_pred y_pred threshold n_errors y_pred ground_truth .sum plot levels lines points clf.decision_function np.c_ xx.ravel yy.ravel z.reshape xx.shape subplot pl.subplot subplot.set_title outlier detection subplot.contourf levelsnp.linspace z.min threshold cmappl.cm.blues_r subplot.contour levels threshold linewidths2 colorsred subplot.contourf levels threshold z.max colorsorange subplot.scatter n_outliers n_outliers cwhite subplot.scatter n_outliers n_outliers cblack subplot.axis tight subplot.legend a.collections learned decision function true inliers true outliers propmatplotlib.font_manager.fontproperties size11 subplot.set_xlabel errors clf_name n_errors subplot.set_xlim subplot.set_ylim pl.subplots_adjust 0.04 0.1 0.96 0.94 0.1 0.26 pl.show figure 2.55 robust empirical covariance estimate robust empirical covariance estimate usual covariance maximum likelihood estimate sensitive presence outliers data set case one would better use robust estimator covariance garanty estimation resistant errorneous observations data set minimum covariance determinant estimator robust highbreakdown point i.e used estimate covariance matrix highly contaminated datasets math rac n_samplesn_features1 outliers estimator covariance idea math rac n_samplesn_features1 observations whose empirical covariance smallest determinant yielding pure subset observations compute standards estimates location covariance correction step aiming compensating fact estimates learnt portion initial data end robust estimates data set location covariance minimum covariance determinant estimator mcd introduced p.j.rousseuw example compare estimation errors made using three types location covariance estimates contaminated gaussian distributed data sets chapter example gallery scikitlearn user guide release 0.11 mean empirical covariance full dataset break soon outliers data set robust mcd low error provided n_samples n_features mean empirical covariance observations known good ones consid ered perfect mcd estimation one trust implementation comparing case 
4046: rousseeuw least median squares regression stat ass johanna hardin david rocke journal computational graphical statistics december 
4047: python source code plot_robust_vs_empirical_covariance.py print __doc__ import numpy import pylab import matplotlib.font_manager sklearn.covariance import empiricalcovariance mincovdet example settings n_samples n_features repeat 2.1. examples scikitlearn user guide release 0.11 range_n_outliers np.concatenate np.linspace n_samples np.linspace n_samples n_samples definition arrays store results err_loc_mcd np.zeros range_n_outliers.size repeat err_cov_mcd np.zeros range_n_outliers.size repeat err_loc_emp_full np.zeros range_n_outliers.size repeat err_cov_emp_full np.zeros range_n_outliers.size repeat err_loc_emp_pure np.zeros range_n_outliers.size repeat err_cov_emp_pure np.zeros range_n_outliers.size repeat computation n_outliers enumerate range_n_outliers range repeat generate data np.random.randn n_samples n_features add outliers outliers_index np.random.permutation n_samples n_outliers outliers_offset np.random.randint size n_outliers n_features 0.5 outliers_index outliers_offset inliers_mask np.ones n_samples .astype bool inliers_mask outliers_index false fit minimum covariance determinant mcd robust estimator data mincovdet .fit compare raw robust estimates true location covariance err_loc_mcd np.sum s.location_ err_cov_mcd s.error_norm np.eye n_features compare estimators learnt full data set true parameters err_loc_emp_full np.sum x.mean err_cov_emp_full empiricalcovariance .fit .error_norm np.eye n_features compare empirical covariance learnt pure data set i.e perfect mcd pure_x inliers_mask pure_location pure_x.mean pure_emp_cov empiricalcovariance .fit pure_x err_loc_emp_pure np.sum pure_location err_cov_emp_pure pure_emp_cov.error_norm np.eye n_features display results font_prop matplotlib.font_manager.fontproperties size11 pl.subplot pl.errorbar range_n_outliers err_loc_mcd.mean yerrerr_loc_mcd.std np.sqrt repeat label robust location colorm pl.errorbar range_n_outliers err_loc_emp_full.mean yerrerr_loc_emp_full.std np.sqrt repeat label full data set mean colorgreen pl.errorbar range_n_outliers err_loc_emp_pure.mean yerrerr_loc_emp_pure.std np.sqrt repeat label pure data set mean colorblack pl.title influence outliers location estimation pl.ylabel error hat _22 pl.legend loc upper left propfont_prop chapter example gallery scikitlearn user guide release 0.11 pl.subplot x_size range_n_outliers.size pl.errorbar range_n_outliers err_cov_mcd.mean yerrerr_cov_mcd.std label robust covariance mcd colorm pl.errorbar range_n_outliers x_size err_cov_emp_full.mean x_size yerrerr_cov_emp_full.std x_size label full data set empirical covariance colorgreen pl.plot range_n_outliers x_size x_size err_cov_emp_full.mean x_size x_size colorgreen pl.errorbar range_n_outliers err_cov_emp_pure.mean yerrerr_cov_emp_pure.std label pure data set empirical covariance colorblack pl.title influence outliers covariance estimation pl.xlabel amount contamination pl.ylabel rmse pl.legend loc upper center propfont_prop pl.show figure 2.56 sparse inverse covariance estimation sparse inverse covariance estimation using graphlasso estimator learn covariance sparse precision small number samples estimate probabilistic model e.g gaussian model estimating precision matrix inverse covari ance matrix important estimating covariance matrix indeed gaussian model parametrized precision matrix favorable recovery conditions sample data model sparse inverse covariance matrix addition ensure data much correlated limiting largest coefcient precision matrix small coefcients precision matrix recovered addition small number observations easier recover correlation matrix rather covariance thus scale time series number samples slightly larger number dimensions thus empirical covariance still invertible however observations strongly correlated empirical covariance matrix illconditioned result inverse empirical precision matrix far ground truth use shrinkage ledoitwolf estimator number samples small need shrink lot result ledoitwolf precision fairly close ground truth precision far diagonal offdiagonal structure lost l1penalized estimator recover part offdiagonal structure able recover exact sparsity pattern detects many nonzero coefcients however highest nonzero coefcients estimated correspond nonzero coefcients ground truth finally coefcients learns sparse precision 
4048: 2.1. examples scikitlearn user guide release 0.11 precision estimate biased toward zero penalty smaller corresponding ground truth value seen gure note color range precision matrices tweeked improve readibility gure full range values empirical precision displayed alpha parameter graphlasso setting sparsity model set internal crossvalidation graphlassocv seen gure grid compute crossvalidation score iteratively rened neighborhood maximum 
4049: python source code plot_sparse_cov.py print __doc__ author gael varoquaux gael.varoquaux inria.fr license bsd style copyright inria import numpy scipy import linalg sklearn.datasets import make_sparse_spd_matrix sklearn.covariance import graphlassocv ledoit_wolf import pylab generate data n_samples n_features prng np.random.randomstate prec make_sparse_spd_matrix n_features alpha.98 smallest_coef.4 largest_coef.7 random_stateprng cov linalg.inv prec np.sqrt np.diag cov cov cov np.newaxis chapter example gallery scikitlearn user guide release 0.11 prec prec np.newaxis prng.multivariate_normal np.zeros n_features cov sizen_samples x.mean axis0 x.std axis0 estimate covariance emp_cov np.dot x.t n_samples model graphlassocv model.fit cov_ model.covariance_ prec_ model.precision_ lw_cov_ ledoit_wolf lw_prec_ linalg.inv lw_cov_ plot results pl.figure figsize pl.subplots_adjust left0.02 right0.98 plot covariances covs empirical emp_cov ledoitwolf lw_cov_ graphlasso cov_ true cov vmax cov_.max name this_cov enumerate covs pl.subplot pl.imshow this_cov interpolationnearest vminvmax vmaxvmax cmappl.cm.rdbu_r pl.xticks pl.yticks pl.title covariance name plot precisions precs empirical linalg.inv emp_cov ledoitwolf lw_prec_ graphlasso prec_ true prec vmax prec_.max name this_prec enumerate precs pl.subplot pl.imshow np.ma.masked_equal this_prec interpolationnearest vminvmax vmaxvmax cmappl.cm.rdbu_r pl.xticks pl.yticks pl.title precision name ax.set_axis_bgcolor plot model selection metric pl.figure figsize pl.axes .15 .75 pl.plot model.cv_alphas_ np.mean model.cv_scores axis1 pl.axvline model.alpha_ color.5 pl.title model selection pl.ylabel crossvalidation score pl.xlabel alpha 2.1. examples scikitlearn user guide release 0.11 pl.show 2.1.5 dataset examples examples concerning sklearn.datasets package 
4050: figure 2.57 digit dataset digit dataset dataset made 8x8 images image like one shown handwritten digit order ultilise 8x8 gure like wed rst transform feature vector lengh 64. see information dataset 
4051: python source code plot_digits_last_image.py print __doc__ code source gael varoqueux modified documentation merge jaques grobler license bsd sklearn import datasets import pylab load digits dataset chapter example gallery scikitlearn user guide release 0.11 digits datasets.load_digits display first digit pl.figure figsize pl.imshow digits.images cmappl.cm.gray_r interpolationnearest pl.show figure 2.58 iris dataset iris dataset data sets consists different types irises setosa versicolour virginica petal sepal length stored 150x4 numpy.ndarray rows samples columns sepal length sepal width petal length petal width plot uses rst two features see information dataset 
4052: python source code plot_iris_dataset.py print __doc__ code source gael varoqueux modified documentation merge jaques grobler license bsd import pylab sklearn import datasets import data play iris datasets.load_iris 2.1. examples scikitlearn user guide release 0.11 iris.data take first two features iris.target x_min x_max .min .max y_min y_max .min .max pl.figure figsize pl.clf plot also training points pl.scatter cmappl.cm.paired pl.xlabel sepal length pl.ylabel sepal width pl.xlim x_min x_max pl.ylim y_min y_max pl.xticks pl.yticks pl.show figure 2.59 plot randomly generated classication dataset plot randomly generated classication dataset plot several randomly generated classication datasets example illustrates datasets.make_classication function three binary two multiclass classication datasets generated different numbers informative features clusters per class 
4053: chapter example gallery scikitlearn user guide release 0.11 python source code plot_random_dataset.py print __doc__ import pylab sklearn.datasets import make_classification pl.figure figsize pl.subplots_adjust bottom.05 top.9 left.05 right.95 pl.subplot pl.title one informative feature one cluster fontsizesmall make_classification n_features2 n_redundant0 n_informative1 n_clusters_per_class1 pl.scatter markero cy1 pl.subplot pl.title two informative features one cluster fontsizesmall make_classification n_features2 n_redundant0 n_informative2 n_clusters_per_class1 pl.scatter markero cy1 pl.subplot pl.title two informative features two clusters fontsizesmall make_classification n_features2 n_redundant0 n_informative2 2.1. examples scikitlearn user guide release 0.11 pl.scatter markero cy2 pl.subplot pl.title multiclass two informative features one cluster fontsizesmall make_classification n_features2 n_redundant0 n_informative2 n_clusters_per_class1 n_classes3 pl.scatter markero cy1 pl.show 2.1.6 decomposition examples concerning sklearn.decomposition package 
4054: figure 2.60 faces dataset decompositions faces dataset decompositions example applies olivetti faces dataset different unsupervised matrix decomposition dimension reduction methods module sklearn.decomposition see documentation chapter decomposing signals components matrix factorization problems 
4055: chapter example gallery scikitlearn user guide release 0.11 2.1. examples scikitlearn user guide release 0.11 script output dataset consists faces extracting top eigenfaces randomizedpca ... done 0.395s extracting top nonnegative components nmf ... done 2.598s extracting top independent components fastica ... done 2.912s extracting top sparse comp minibatchsparsepca ... done 1.705s extracting top minibatchdictionarylearning ... done 1.368s extracting top cluster centers minibatchkmeans ... done 0.447s python source code plot_faces_decomposition.py print __doc__ authors vlad niculae alexandre gramfort license bsd import logging time import time numpy.random import randomstate import pylab sklearn.datasets import fetch_olivetti_faces chapter example gallery scikitlearn user guide release 0.11 sklearn.cluster import minibatchkmeans sklearn import decomposition display progress logs stdout logging.basicconfig levellogging.info format asctime levelname message n_row n_col n_components n_row n_col image_shape rng randomstate load faces data dataset fetch_olivetti_faces shuffletrue random_staterng faces dataset.data n_samples n_features faces.shape global centering faces_centered faces faces.mean axis0 local centering faces_centered faces_centered.mean axis1 .reshape n_samples print dataset consists faces n_samples def plot_gallery title images pl.figure figsize n_col 2.26 n_row pl.suptitle title size16 comp enumerate images pl.subplot n_row n_col vmax max comp.max comp.min pl.imshow comp.reshape image_shape cmappl.cm.gray interpolationnearest vminvmax vmaxvmax pl.xticks pl.yticks pl.subplots_adjust 0.01 0.05 0.99 0.93 0.04 list different estimators whether center transpose problem whether transformer uses clustering api estimators eigenfaces randomizedpca decomposition.randomizedpca n_componentsn_components whitentrue true false nonnegative components nmf decomposition.nmf n_componentsn_components initnndsvda beta5.0 tol5e3 sparsenesscomponents false false independent components fastica decomposition.fastica n_componentsn_components whitentrue true true max_iter10 2.1. examples scikitlearn user guide release 0.11 sparse comp minibatchsparsepca decomposition.minibatchsparsepca n_componentsn_components alpha0.8 n_iter100 chunk_size3 random_staterng true false minibatchdictionarylearning decomposition.minibatchdictionarylearning n_atoms15 alpha0.1 n_iter50 chunk_size3 random_staterng true false cluster centers minibatchkmeans minibatchkmeans kn_components tol1e3 batch_size20 max_iter50 random_staterng true false plot sample input data plot_gallery first centered olivetti faces faces_centered n_components estimation plot name estimator center transpose estimators print extracting top ... n_components name time data faces center data faces_centered transpose data data.t estimator.fit data train_time time print done 0.3fs train_time hasattr estimator cluster_centers_ components_ estimator.cluster_centers_ else components_ estimator.components_ transpose components_ components_.t plot_gallery train time .1fs name train_time components_ n_components pl.show blind source separation using fastica independent component analysis ica used estimate sources given noisy measurements imagine instruments playing simultaneously microphones recording mixed signals ica used recover sources played instrument 
4056: chapter example gallery scikitlearn user guide release 0.11 figure 2.61 blind source separation using fastica python source code plot_ica_blind_source_separation.py print __doc__ import numpy import pylab sklearn.decomposition import fastica generate sample data np.random.seed n_samples time np.linspace n_samples np.sin time signal sinusoidal signal 2.1. examples scikitlearn user guide release 0.11 np.sign np.sin time signal square signal np.c_ 0.2 np.random.normal sizes.shape add noise s.std axis0 standardize data mix data np.array 0.5 mixing matrix np.dot a.t generate observations compute ica ica fastica ica.fit .transform get estimated sources ica.get_mixing_matrix get estimated mixing matrix assert np.allclose np.dot a_.t plot results pl.figure pl.subplot pl.plot pl.title true sources pl.subplot pl.plot pl.title observations mixed signal pl.subplot pl.plot pl.title ica estimated sources pl.subplots_adjust 0.09 0.04 0.94 0.94 0.26 0.36 pl.show figure 2.62 fastica point clouds fastica point clouds illustrate visually results independent component analysis ica principal component analysis pca feature space representing ica feature space gives view geometric ica ica algorithm nds directions feature space corresponding projections high nongaussianity directions need orthogonal original feature space orthogonal whitened feature space directions correspond variance pca hand nds orthogonal directions raw feature space correspond directions accounting maximum variance simulate independent sources using highly nongaussian process student low number degrees freedom top left gure mix create observations top right gure raw observation space rections identied pca represented green vectors represent signal pca space whitening chapter example gallery variance corresponding pca vectors lower left running ica corresponds nding rotation space identify directions largest nongaussianity lower right 
4057: scikitlearn user guide release 0.11 python source code plot_ica_vs_pca.py print __doc__ authors alexandre gramfort gael varoquaux license bsd import numpy import pylab sklearn.decomposition import pca fastica generate sample data rng np.random.randomstate rng.standard_t 1.5 size 
4058: mix data np.array mixing matrix np.dot a.t generate observations 2.1. examples scikitlearn user guide release 0.11 pca pca s_pca_ pca.fit .transform ica fastica random_staterng s_ica_ ica.fit .transform estimate sources s_ica_ s_ica_.std axis0 plot results def plot_samples axis_listnone pl.scatter markero linewidths0 zorder10 axis_list none colors 0.6 0.6 color axis zip colors axis_list axis axis.std x_axis y_axis axis trick get legend work pl.plot 0.1 x_axis 0.1 y_axis linewidth2 colorcolor pl.quiver x_axis y_axis x_axis y_axis zorder11 width0.01 pl.quiver x_axis y_axis zorder11 width0.01 scale6 colorcolor pl.hlines pl.vlines pl.xlim pl.ylim pl.xlabel pl.ylabel pl.subplot plot_samples s.std pl.title true independent sources axis_list pca.components_.t ica.get_mixing_matrix pl.subplot plot_samples np.std axis_listaxis_list pl.legend pca ica locupper left pl.title observations pl.subplot plot_samples s_pca_ np.std s_pca_ axis0 pl.title pca scores pl.subplot plot_samples s_ica_ np.std s_ica_ pl.title ica estimated sources pl.subplots_adjust 0.09 0.04 0.94 0.94 0.26 0.26 pl.show chapter example gallery scikitlearn user guide release 0.11 figure 2.63 image denoising using dictionary learning image denoising using dictionary learning example comparing effect reconstructing noisy fragments lena using online dictionary learning various transform methods dictionary tted nondistorted left half image subsequently used reconstruct right half common practice evaluating results image denoising looking difference recon struction original image reconstruction perfect look like gaussian noise seen plots results orthogonal matching pursuit omp two nonzero coefcients bit less biased keeping one edges look less prominent addition closer ground truth frobenius norm result least angle regression much strongly biased difference reminiscent local intensity value original image thresholding clearly useful denoising show produce suggestive output high speed thus useful tasks object classication performance necessarily related visualisation 
4059: 2.1. examples scikitlearn user guide release 0.11 script output distorting image ... extracting clean patches ... done 0.27s learning dictionary ... done 9.67s extracting noisy patches ... done 0.19s orthogonal matching pursuit atom ... done 6.15s orthogonal matching pursuit chapter example gallery scikitlearn user guide release 0.11 atoms ... done 9.46s leastangle regression atoms ... done 50.56s thresholding alpha0.1 ... done 0.97s 
4060: python source code plot_image_denoising.py print __doc__ time import time import pylab import numpy scipy.misc import lena sklearn.decomposition import minibatchdictionarylearning sklearn.feature_extraction.image import extract_patches_2d sklearn.feature_extraction.image import reconstruct_from_patches_2d load lena image extract patches lena lena 256.0 downsample higher speed lena lena lena lena lena lena 4.0 height width lena.shape distort right half image print distorting image ... distorted lena.copy distorted height 0.075 np.random.randn width height extract clean patches left half image print extracting clean patches ... time patch_size data extract_patches_2d distorted height patch_size data data.reshape data.shape data np.mean data axis0 data np.std data axis0 print done .2fs time learn dictionary clean patches print learning dictionary ... time dico minibatchdictionarylearning n_atoms100 alpha1 n_iter500 dico.fit data .components_ time print done .2fs 2.1. examples scikitlearn user guide release 0.11 pl.figure figsize 4.2 comp enumerate pl.subplot pl.imshow comp.reshape patch_size cmappl.cm.gray_r interpolationnearest pl.xticks pl.yticks pl.suptitle dictionary learned lena patchesn train time .1fs patches len data fontsize16 pl.subplots_adjust 0.08 0.02 0.92 0.85 0.08 0.23 display distorted image def show_with_diff image reference title helper function display denoising pl.figure figsize 3.3 pl.subplot pl.title image pl.imshow image vmin0 vmax1 cmappl.cm.gray interpolationnearest pl.xticks pl.yticks pl.subplot difference image reference pl.title difference norm .2f np.sqrt np.sum difference pl.imshow difference vmin0.5 vmax0.5 cmappl.cm.puor interpolationnearest pl.xticks pl.yticks pl.suptitle title size16 pl.subplots_adjust 0.02 0.02 0.98 0.79 0.02 0.2 show_with_diff distorted lena distorted image extract noisy patches reconstruct using dictionary print extracting noisy patches ... time data extract_patches_2d distorted height patch_size data data.reshape data.shape intercept np.mean data axis0 data intercept print done .2fs time transform_algorithms orthogonal matching pursuitn1 atom omp transform_n_nonzero_coefs orthogonal matching pursuitn2 atoms omp transform_n_nonzero_coefs leastangle regressionn5 atoms lars thresholdingn alpha0.1 threshold transform_alpha transform_n_nonzero_coefs reconstructions chapter example gallery scikitlearn user guide release 0.11 title transform_algorithm kwargs transform_algorithms print title ... reconstructions title lena.copy time dico.set_params transform_algorithmtransform_algorithm kwargs code dico.transform data patches np.dot code transform_algorithm threshold patches patches.min patches patches.max patches intercept patches patches.reshape len data patch_size transform_algorithm threshold patches patches.min patches patches.max reconstructions title height reconstruct_from_patches_2d patches width height time print done .2fs show_with_diff reconstructions title lena title time .1fs pl.show figure 2.64 kernel pca kernel pca example shows kernel pca able projection data makes data linearly separable 
4061: 2.1. examples scikitlearn user guide release 0.11 python source code plot_kernel_pca.py print __doc__ authors mathieu blondel andreas mueller license bsd import numpy import pylab sklearn.decomposition import pca kernelpca sklearn.datasets import make_circles np.random.seed make_circles n_samples400 factor.3 noise.05 kpca kernelpca kernel rbf fit_inverse_transformtrue gamma10 x_kpca kpca.fit_transform x_back kpca.inverse_transform x_kpca pca pca x_pca pca.fit_transform plot results chapter example gallery scikitlearn user guide release 0.11 pl.figure pl.subplot aspectequal pl.title original space reds blues pl.plot reds reds pl.plot blues blues pl.xlabel x_1 pl.ylabel x_2 np.meshgrid np.linspace 1.5 1.5 np.linspace 1.5 1.5 x_grid np.array np.ravel np.ravel projection first principal component phi space z_grid kpca.transform x_grid .reshape x1.shape pl.contour z_grid colorsgrey linewidths1 originlower pl.subplot aspectequal pl.plot x_pca reds x_pca reds pl.plot x_pca blues x_pca blues pl.title projection pca pl.xlabel 1st principal component pl.ylabel 2nd component pl.subplot aspectequal pl.plot x_kpca reds x_kpca reds pl.plot x_kpca blues x_kpca blues pl.title projection kpca pl.xlabel 1st principal component space induced phi pl.ylabel 2nd component pl.subplot aspectequal pl.plot x_back reds x_back reds pl.plot x_back blues x_back blues pl.title original space inverse transform pl.xlabel x_1 pl.ylabel x_2 pl.subplots_adjust 0.02 0.10 0.98 0.94 0.04 0.35 pl.show figure 2.65 principal component analysis principal component analysis gures aid illustrating point cloud one direction pca would come choose direction 
4062: 2.1. examples scikitlearn user guide release 0.11 python source code plot_pca_3d.py print __doc__ code source gael varoqueux modified documentation merge jaques grobler license bsd import pylab import numpy scipy import stats mpl_toolkits.mplot3d import axes3d np.exp np.random.seed def pdf return 0.5 stats.norm scale0.25 .pdf stats.norm scale4 .pdf np.random.normal scale0.5 size np.random.normal scale0.5 size np.random.normal scale0.1 sizelen density pdf pdf pdf_z pdf density pdf_z norm np.sqrt a.var b.var norm norm chapter example gallery scikitlearn user guide release 0.11 plot figures def plot_figs fig_num elev azim fig pl.figure fig_num figsize pl.clf axes3d fig rect .95 elevelev azimazim ax.scatter cdensity marker alpha.4 np.c_ pca_score np.linalg.svd full_matricesfalse x_pca_axis y_pca_axis z_pca_axis v.t pca_score pca_score.min x_pca_axis y_pca_axis z_pca_axis v.t x_pca_plane np.r_ x_pca_axis x_pca_axis y_pca_plane np.r_ y_pca_axis y_pca_axis z_pca_plane np.r_ z_pca_axis z_pca_axis x_pca_plane.shape y_pca_plane.shape z_pca_plane.shape ax.plot_surface x_pca_plane y_pca_plane z_pca_plane ax.w_xaxis.set_ticklabels ax.w_yaxis.set_ticklabels ax.w_zaxis.set_ticklabels elev azim plot_figs elev azim elev azim plot_figs elev azim pl.show figure 2.66 pca example iris dataset 2.1. examples scikitlearn user guide release 0.11 pca example iris dataset python source code plot_pca_iris.py print __doc__ code source gael varoqueux license bsd import numpy import pylab mpl_toolkits.mplot3d import axes3d sklearn import decomposition sklearn import datasets np.random.seed centers iris datasets.load_iris iris.data iris.target fig pl.figure figsize pl.clf axes3d fig rect .95 elev48 azim134 pl.cla pca decomposition.pca n_components3 pca.fit pca.transform name label setosa versicolour virginica ax.text3d label .mean label .mean 1.5 label .mean name chapter example gallery scikitlearn user guide release 0.11 horizontalalignmentcenter bboxdict alpha.5 edgecolorw facecolorw reorder labels colors matching cluster results np.choose .astype np.float ax.scatter cmappl.cm.spectral x_surf .min .max .min .max y_surf .max .max .min .min x_surf np.array x_surf y_surf np.array y_surf pca.transform pca.components_ pca.transform pca.components_ ax.w_xaxis.set_ticklabels ax.w_yaxis.set_ticklabels ax.w_zaxis.set_ticklabels pl.show figure 2.67 comparison lda pca projection iris dataset comparison lda pca projection iris dataset iris dataset represents kind iris owers setosa versicolour virginica attributes sepal length sepal width petal length petal width principal component analysis pca applied data identies combination attributes principal compo nents directions feature space account variance data plot different samples rst principal components linear discriminant analysis lda tries identify attributes account variance classes particular lda constrast pca supervised method using known class labels 
4063: 2.1. examples scikitlearn user guide release 0.11 script output explained variance ratio first two components 0.92461621 0.05301557 python source code plot_pca_vs_lda.py print __doc__ import pylab sklearn import datasets sklearn.decomposition import pca sklearn.lda import lda iris datasets.load_iris iris.data iris.target target_names iris.target_names pca pca n_components2 x_r pca.fit .transform lda lda n_components2 x_r2 lda.fit .transform percentage variance explained components print explained variance ratio first two components pca.explained_variance_ratio_ chapter example gallery scikitlearn user guide release 0.11 pl.figure target_name zip rgb target_names pl.scatter x_r x_r labeltarget_name pl.legend pl.title pca iris dataset pl.figure target_name zip rgb target_names pl.scatter x_r2 x_r2 labeltarget_name pl.legend pl.title lda iris dataset pl.show figure 2.68 sparse coding precomputed dictionary sparse coding precomputed dictionary transform signal sparse combination ricker wavelets example visually compares different sparse coding methods using sklearn.decomposition.sparsecoder estimator ricker also known mexican hat second derivative gaussian particularily good kernel represent piecewise constant signals like one therefore seen much adding different widths atoms matters therefore motivates learning dictionary best type signals richer dictionary right larger size heavier subsampling performed order stay order magnitude 
4064: 2.1. examples scikitlearn user guide release 0.11 python source code plot_sparse_coding.py print __doc__ import numpy import matplotlib.pylab sklearn.decomposition import sparsecoder def ricker_function resolution center width discrete subsampled ricker mexican hat wavelet np.linspace resolution resolution np.sqrt width np.pi center width np.exp center width return def ricker_matrix width resolution n_atoms dictionary ricker mexican hat wavelets centers np.linspace resolution n_atoms np.empty n_atoms resolution center enumerate centers ricker_function resolution center width np.sqrt np.sum axis1 np.newaxis return resolution subsampling subsampling factor width n_atoms resolution subsampling compute wavelet dictionary d_fixed ricker_matrix widthwidth resolutionresolution n_atomsn_atoms d_multi np.r_ tuple ricker_matrix widthw resolutionresolution n_atomsnp.floor n_atoms generate signal np.linspace resolution resolution first_quarter resolution first_quarter np.logical_not first_quarter 
4065: list different sparse coding methods following format title transform_algorithm transform_alpha transform_n_nozero_coefs estimators omp omp none lasso lasso_cd none pl.figure figsize subplot title enumerate zip d_fixed d_multi fixed width multiple widths pl.subplot subplot pl.title sparse coding dictionary title pl.plot lsdotted labeloriginal signal wavelet approximation chapter example gallery scikitlearn user guide release 0.11 title algo alpha n_nonzero estimators coder sparsecoder dictionaryd transform_n_nonzero_coefsn_nonzero transform_alphaalpha transform_algorithmalgo coder.transform density len np.flatnonzero np.ravel np.dot squared_error np.sum pl.plot label nonzero coefs .2f error title density squared_error soft thresholding debiasing coder sparsecoder dictionaryd transform_algorithmthreshold transform_alpha20 coder.transform idx np.where idx np.linalg.lstsq idx np.ravel np.dot squared_error np.sum pl.plot labelthresholding debiasing nonzero coefs .2f error len idx squared_error pl.axis tight pl.legend pl.subplots_adjust .04 .07 .97 .90 .09 pl.show 2.1.7 ensemble methods examples concerning sklearn.ensemble package 
4066: figure 2.69 feature importances forests trees feature importances forests trees examples shows use forests trees evaluate importance features artical classication task red plots feature importances individual tree blue plot feature importance whole forest expected knee blue plot suggests features informative remaining 
4067: 2.1. examples scikitlearn user guide release 0.11 script output feature ranking feature 0.245865 feature 0.194416 feature 0.174455 feature 0.057138 feature 0.055967 feature 0.055516 feature 0.055179 feature 0.054639 feature 0.053921 10. feature 0.052904 python source code plot_forest_importances.py print __doc__ import numpy sklearn.datasets import make_classification sklearn.ensemble import extratreesclassifier build classification task using informative features make_classification n_samples1000 n_features10 chapter example gallery scikitlearn user guide release 0.11 n_informative3 n_redundant0 n_repeated0 n_classes2 random_state0 shufflefalse build forest compute feature importances forest extratreesclassifier n_estimators250 compute_importancestrue random_state0 forest.fit importances forest.feature_importances_ indices np.argsort importances print feature ranking print feature ranking xrange print feature indices importances indices plot feature importances trees forest import pylab pl.figure pl.title feature importances tree forest.estimators_ pl.plot xrange tree.feature_importances_ indices pl.plot xrange importances indices pl.show figure 2.70 pixel importances parallel forest trees pixel importances parallel forest trees example shows use forests trees evaluate importance pixels image classication task faces hotter pixel important code also illustrates construction computation predictions parallelized within multiple jobs 
4068: 2.1. examples scikitlearn user guide release 0.11 script output fitting extratreesclassifier faces data cores ... done 25.886s python source code plot_forest_importances_faces.py print __doc__ time import time import pylab sklearn.datasets import fetch_olivetti_faces sklearn.ensemble import extratreesclassifier number cores use perform parallel fitting forest model n_jobs loading digits dataset chapter example gallery scikitlearn user guide release 0.11 data fetch_olivetti_faces data.images.reshape len data.images data.target mask limit classes mask mask build forest compute pixel importances print fitting extratreesclassifier faces data cores ... n_jobs time forest extratreesclassifier n_estimators1000 max_features128 compute_importancestrue n_jobsn_jobs random_state0 forest.fit print done 0.3fs time importances forest.feature_importances_ importances importances.reshape data.images .shape plot pixel importances pl.matshow importances cmappl.cm.hot pl.title pixel importances forests trees pl.show figure 2.71 plot decision surfaces ensembles trees iris dataset plot decision surfaces ensembles trees iris dataset plot decision surfaces forests randomized trees trained pairs features iris dataset plot compares decision surfaces learned decision tree classier rst column random forest classi second column extra trees classier third column rst row classiers built using sepal width sepal length features second row using petal length sepal length third row using petal width petal length 
4069: 2.1. examples scikitlearn user guide release 0.11 python source code plot_forest_iris.py print __doc__ import numpy import pylab sklearn import clone sklearn.datasets import load_iris sklearn.ensemble import randomforestclassifier extratreesclassifier sklearn.tree import decisiontreeclassifier parameters n_classes n_estimators plot_colors bry plot_step 0.02 load data iris load_iris plot_idx pair model decisiontreeclassifier randomforestclassifier n_estimatorsn_estimators chapter example gallery scikitlearn user guide release 0.11 extratreesclassifier n_estimatorsn_estimators take two corresponding features iris.data pair iris.target shuffle idx np.arange x.shape np.random.seed np.random.shuffle idx idx idx standardize mean x.mean axis0 std x.std axis0 mean std train clf clone model clf model.fit plot decision boundary pl.subplot plot_idx x_min x_max .min .max y_min y_max .min .max np.meshgrid np.arange x_min x_max plot_step np.arange y_min y_max plot_step isinstance model decisiontreeclassifier model.predict np.c_ xx.ravel yy.ravel z.reshape xx.shape pl.contourf cmappl.cm.paired else tree model.estimators_ tree.predict np.c_ xx.ravel yy.ravel z.reshape xx.shape pl.contourf alpha0.1 cmappl.cm.paired pl.axis tight plot training points zip xrange n_classes plot_colors idx np.where pl.scatter idx idx labeliris.target_names cmappl.cm.paired pl.axis tight plot_idx pl.suptitle decision surfaces decision tree random forest extratrees classifier pl.show 2.1. examples scikitlearn user guide release 0.11 figure 2.72 gradient boosting regression gradient boosting regression demonstrate gradient boosting boston housing dataset example gradient boosting model least squares loss regression trees depth 
4070: script output mse 6.2736 python source code plot_gradient_boosting_regression.py print __doc__ author peter prettenhofer peter.prettenhofer gmail.com license bsd import numpy import pylab sklearn import ensemble sklearn import datasets sklearn.utils import shuffle sklearn.metrics import mean_squared_error chapter example gallery scikitlearn user guide release 0.11 load data boston datasets.load_boston shuffle boston.data boston.target random_state13 x.astype np.float32 offset int x.shape 0.9 x_train y_train offset offset x_test y_test offset offset fit regression model params n_estimators max_depth min_samples_split learn_rate 0.01 loss clf ensemble.gradientboostingregressor params clf.fit x_train y_train mse mean_squared_error y_test clf.predict x_test print mse .4f mse plot training deviance compute test set deviance test_score np.zeros params n_estimators dtypenp.float64 y_pred enumerate clf.staged_decision_function x_test test_score clf.loss_ y_test y_pred pl.figure figsize pl.subplot pl.title deviance pl.plot np.arange params n_estimators clf.train_score_ labeltraining set deviance pl.plot np.arange params n_estimators test_score labeltest set deviance pl.legend locupper right pl.xlabel boosting iterations pl.ylabel deviance plot feature importance feature_importance clf.feature_importances_ make importances relative max importance feature_importance 100.0 feature_importance feature_importance.max sorted_idx np.argsort feature_importance pos np.arange sorted_idx.shape pl.subplot pl.barh pos feature_importance sorted_idx aligncenter pl.yticks pos boston.feature_names sorted_idx pl.xlabel relative importance pl.title variable importance pl.show gradient boosting regularization illustration effect different regularization strategies gradient boosting example taken hastie 
4071: 2.1. examples scikitlearn user guide release 0.11 figure 2.73 gradient boosting regularization loss function used binomial deviance combination shrinkage stochastic gradient boosting sample 0.5 produce accurate models subsampling without shrinkage usually poorly 
4072: python source code plot_gradient_boosting_regularization.py print __doc__ author peter prettenhofer peter.prettenhofer gmail.com license bsd import numpy import pylab sklearn import ensemble chapter example gallery scikitlearn user guide release 0.11 sklearn import datasets datasets.make_hastie_10_2 n_samples12000 random_state1 x.astype np.float32 x_train x_test y_train y_test original_params n_estimators max_depth random_state min_samples_split pl.figure label color setting shrinkage orange learn_rate 1.0 subsample 1.0 shrink0.1 turquoise learn_rate 0.1 subsample 1.0 sample0.5 blue learn_rate 1.0 subsample 0.5 shrink0.1 sample0.5 gray learn_rate 0.1 subsample 0.5 params dict original_params params.update setting clf ensemble.gradientboostingclassifier params clf.fit x_train y_train compute test set deviance test_deviance np.zeros params n_estimators dtypenp.float64 y_pred enumerate clf.staged_decision_function x_test test_deviance clf.loss_ y_test y_pred pl.plot np.arange test_deviance.shape test_deviance colorcolor labellabel pl.title deviance pl.legend locupper left pl.xlabel boosting iterations pl.ylabel test set deviance pl.show 2.1.8 tutorial exercices exercises tutorials figure 2.74 crossvalidation diabetes dataset exercise 2.1. examples scikitlearn user guide release 0.11 crossvalidation diabetes dataset exercise exercise used crossvalidated estimators part model selection choosing estimators parameters section tutorial statisticallearning scientic data processing 
4073: script output 0.10000000000000001 0.10000000000000001 0.10000000000000001 python source code plot_cv_diabetes.py print __doc__ import numpy import pylab sklearn import cross_validation datasets linear_model diabetes datasets.load_diabetes diabetes.data diabetes.target lasso linear_model.lasso alphas np.logspace scores list scores_std list alpha alphas lasso.alpha alpha this_scores cross_validation.cross_val_score lasso n_jobs1 scores.append np.mean this_scores scores_std.append np.std this_scores pl.figure figsize 2.5 pl.clf pl.axes .25 pl.semilogx alphas scores pl.semilogx alphas np.array scores np.array scores_std pl.semilogx alphas np.array scores np.array scores_std pl.yticks pl.ylabel score pl.xlabel alpha pl.axhline np.max scores linestyle color.5 chapter example gallery scikitlearn user guide release 0.11 pl.text 2e4 np.max scores 1e4 .489 bonus much trust selection alpha k_fold cross_validation.kfold len print lasso.fit train train .alpha train k_fold figure 2.75 crossvalidation digits dataset exercise crossvalidation digits dataset exercise exercise used crossvalidation generators part model selection choosing estimators parameters section tutorial statisticallearning scientic data processing 
4074: python source code plot_cv_digits.py print __doc__ import numpy sklearn import cross_validation datasets svm digits datasets.load_digits digits.data digits.target svc svm.svc c_s np.logspace scores list scores_std list c_s svc.c this_scores cross_validation.cross_val_score svc n_jobs1 scores.append np.mean this_scores scores_std.append np.std this_scores import pylab pl.figure figsize 2.5 2.1. examples scikitlearn user guide release 0.11 pl.clf pl.axes .25 pl.semilogx c_s scores pl.semilogx c_s np.array scores np.array scores_std pl.semilogx c_s np.array scores np.array scores_std pl.yticks pl.ylabel score pl.xlabel parameter pl.ylim 1.1 pl.axhline np.max scores linestyle color.5 pl.text c_s np.argmax scores np.max scores .3f np.max scores verticalalignmenttop horizontalalignmentcenter pl.show figure 2.76 digits classication exercise digits classication exercise exercise used classication part supervised learning predicting output variable high dimensional observations section tutorial statisticallearning scientic data processing script output knn score 0.961111 logisticregression score 0.938889 python source code plot_digits_classification_exercise.py print __doc__ sklearn import datasets neighbors linear_model digits datasets.load_digits x_digits digits.data y_digits digits.target n_samples len x_digits x_train x_digits n_samples y_train y_digits n_samples x_test x_digits n_samples y_test y_digits n_samples knn neighbors.kneighborsclassifier logistic linear_model.logisticregression print knn score chapter example gallery scikitlearn user guide release 0.11 knn.fit x_train y_train .score x_test y_test print logisticregression score logistic.fit x_train y_train .score x_test y_test figure 2.77 svm exercise svm exercise exercise used using kernels part supervised learning predicting output variable high dimensional observations section tutorial statisticallearning scientic data processing 
4075: python source code plot_iris_exercise.py print __doc__ import numpy import pylab 2.1. examples scikitlearn user guide release 0.11 sklearn import datasets svm iris datasets.load_iris iris.data iris.target n_sample len np.random.seed order np.random.permutation n_sample order order .astype np.float x_train n_sample y_train n_sample x_test n_sample y_test n_sample fit model fig_num kernel enumerate linear rbf poly clf svm.svc kernelkernel gamma10 clf.fit x_train y_train pl.figure fig_num pl.clf pl.scatter zorder10 cmappl.cm.paired circle test data pl.scatter x_test x_test s80 facecolorsnone zorder10 pl.axis tight x_min .min x_max .max y_min .min y_max .max np.mgrid x_min x_max200j y_min y_max200j clf.decision_function np.c_ xx.ravel yy.ravel put result color plot z.reshape xx.shape pl.pcolormesh cmappl.cm.paired pl.contour colors linestyles levels pl.title kernel pl.show 2.1.9 gaussian process machine learning examples concerning sklearn.gaussian_process package 
4076: chapter example gallery scikitlearn user guide release 0.11 figure 2.78 gaussian processes classication example exploiting probabilistic output gaussian processes classication example exploiting probabilistic output twodimensional regression exercise postprocessing allowing probabilistic classication thanks gaussian property prediction gure illustrates probability prediction negative respect remaining uncertainty prediction red blue lines corresponds condence interval prediction zero level set 
4077: python source code plot_gp_probabilistic_classification_after_regression.py print __doc__ author vincent dubourg vincent.dubourg gmail.com license bsd style 2.1. examples scikitlearn user guide release 0.11 import numpy scipy import stats sklearn.gaussian_process import gaussianprocess matplotlib import pyplot matplotlib import standard normal distribution functions phi stats.distributions.norm .pdf phi stats.distributions.norm .cdf phiinv stats.distributions.norm .ppf constants lim def function predict classification consist predicting whether return 
4078: design experiments np.array 4.61611719 6.00099547 4.10469096 5.32782448 0.00000000 0.50000000 6.17289014 4.6984743 1.3109306 6.93271427 5.03823144 3.10584743 2.87600388 6.74310541 5.21301203 4.26386883 observations instanciate fit gaussian process model gaussianprocess theta05e1 dont perform mle youll get perfect prediction simple example gp.fit evaluate real function prediction mse grid res np.meshgrid np.linspace lim lim res np.vstack x1.reshape x1.size x2.reshape x2.size np.linspace lim lim res y_true y_pred mse gp.predict eval_msetrue sigma np.sqrt mse y_true y_true.reshape res res y_pred y_pred.reshape res res sigma sigma.reshape res res phiinv .975 plot probabilistic classification isovalues using gaussian property prediction fig pl.figure fig.add_subplot ax.axes.set_aspect equal chapter example gallery scikitlearn user guide release 0.11 pl.xticks pl.yticks ax.set_xticklabels ax.set_yticklabels pl.xlabel x_1 pl.ylabel x_2 cax pl.imshow np.flipud phi y_pred sigma cmapcm.gray_r alpha0.8 extent lim lim lim lim norm pl.matplotlib.colors.normalize vmin0. vmax0.9 pl.colorbar cax ticks 0.2 0.4 0.6 0.8 normnorm cb.set_label mathbb left widehat mathbf leq 0right pl.plot markersize12 pl.plot markersize12 pl.contour y_true colorsk linestylesdashdot pl.contour phi y_pred sigma 0.025 colorsb pl.clabel fontsize11 linestylessolid pl.contour phi y_pred sigma 0.5 colorsk pl.clabel fontsize11 linestylesdashed pl.contour phi y_pred sigma 0.975 colorsr pl.clabel fontsize11 linestylessolid pl.show figure 2.79 gaussian processes regression basic introductory example gaussian processes regression basic introductory example simple onedimensional regression exercise computed two different ways noisefree case cubic correlation model noisy case squared euclidean correlation model cases model parameters estimated using maximum likelihood principle gures illustrate interpolating property gaussian process model well probabilistic nature form pointwise condence interval 
4079: 2.1. examples scikitlearn user guide release 0.11 note parameter nugget applied tikhonov regularization assumed covariance training points special case squared euclidean correlation model nugget mathematically equivalent normalized variance cid20 cid21 nuggeti python source code plot_gp_regression.py print __doc__ author vincent dubourg vincent.dubourg gmail.com license bsd style jake vanderplas vanderplas astro.washington.edu import numpy sklearn.gaussian_process import gaussianprocess matplotlib import pyplot np.random.seed def function predict return np.sin chapter example gallery scikitlearn user guide release 0.11 first noiseless case np.atleast_2d observations .ravel mesh input space evaluations real function prediction mse np.atleast_2d np.linspace instanciate gaussian process model gaussianprocess corrcubic theta01e2 thetal1e4 thetau1e1 random_start100 fit data using maximum likelihood estimation parameters gp.fit make prediction meshed xaxis ask mse well y_pred mse gp.predict eval_msetrue sigma np.sqrt mse plot function prediction confidence interval based mse fig pl.figure pl.plot labelu sin pl.plot markersize10 labeluobservations pl.plot y_pred labeluprediction pl.fill np.concatenate np.concatenate y_pred 1.9600 sigma alpha.5 fcb ecnone label95 confidence interval y_pred 1.9600 sigma pl.xlabel pl.ylabel pl.ylim pl.legend locupper left noisy case np.linspace 0.1 9.9 np.atleast_2d observations noise .ravel 0.5 1.0 np.random.random y.shape noise np.random.normal noise mesh input space evaluations real function prediction mse np.atleast_2d np.linspace instanciate gaussian process model gaussianprocess corrsquared_exponential theta01e1 thetal1e3 thetau1 nugget random_start100 fit data using maximum likelihood estimation parameters 2.1. examples scikitlearn user guide release 0.11 gp.fit make prediction meshed xaxis ask mse well y_pred mse gp.predict eval_msetrue sigma np.sqrt mse plot function prediction confidence interval based mse fig pl.figure pl.plot labelu sin pl.errorbar x.ravel fmtr. markersize10 labeluobservations pl.plot y_pred labeluprediction pl.fill np.concatenate np.concatenate y_pred 1.9600 sigma alpha.5 fcb ecnone label95 confidence interval y_pred 1.9600 sigma pl.xlabel pl.ylabel pl.ylim pl.legend locupper left pl.show figure 2.80 gaussian processes regression goodnessoft diabetes dataset gaussian processes regression goodnessoft diabetes dataset example consists tting gaussian process model onto diabetes dataset correlation parameters determined means maximum likelihood estimation mle anisotropic squared exponential correlation model constant regression model assumed also used nugget 1e2 order account strong noise targets compute compute crossvalidation estimate coefcient determination without reperforming mle using set correlation parameters found whole dataset python source code gp_diabetes_dataset.py print __doc__ author vincent dubourg vincent.dubourg gmail.com license bsd style sklearn import datasets sklearn.gaussian_process import gaussianprocess sklearn.cross_validation import cross_val_score kfold chapter example gallery scikitlearn user guide release 0.11 load dataset scikits data sets diabetes datasets.load_diabetes diabetes.data diabetes.target instanciate model gaussianprocess regrconstant corrabsolute_exponential theta0 1e4 thetal 1e12 thetau 1e2 nugget1e2 optimizerwelch fit model data performing maximum likelihood estimation gp.fit deactivate maximum likelihood estimation crossvalidation loop gp.theta0 gp.theta given correlation parameter mle gp.thetal gp.thetau none none none bounds deactivate mle perform crossvalidation estimate coefficient determination using cross_validation module using cpus available machine cross_val_score cvkfold y.size n_jobs1 .mean print dfolds estimate coefficient determination folds 2.1.10 generalized linear models examples concerning sklearn.linear_model package 
4080: figure 2.81 automatic relevance determination regression ard automatic relevance determination regression ard fit regression model bayesian ridge regression compared ols ordinary least squares estimator coefcient weights slightly shifted toward zeros wich stabilises histogram estimated weights peaked sparsityinducing prior implied weights estimation model done iteratively maximizing marginal loglikelihood observations 
4081: 2.1. examples scikitlearn user guide release 0.11 python source code plot_ard.py print __doc__ import numpy import pylab scipy import stats sklearn.linear_model import ardregression linearregression chapter example gallery scikitlearn user guide release 0.11 generating simulated data gaussian weigthts parameters example np.random.seed n_samples n_features create gaussian data np.random.randn n_samples n_features create weigts precision lambda_ lambda_ np.zeros n_features keep weights interest relevant_features np.random.randint n_features relevant_features stats.norm.rvs loc0 scale1 np.sqrt lambda_ create noite precision alpha 50. alpha_ 50. noise stats.norm.rvs loc0 scale1 np.sqrt alpha_ sizen_samples create target np.dot noise fit ard regression clf ardregression compute_scoretrue clf.fit ols linearregression ols.fit plot true weights estimated weights histogram weights pl.figure figsize pl.title weights model pl.plot clf.coef_ label ard estimate pl.plot ols.coef_ label ols estimate pl.plot label ground truth pl.xlabel features pl.ylabel values weights pl.legend loc1 pl.figure figsize pl.title histogram weights pl.hist clf.coef_ binsn_features logtrue pl.plot clf.coef_ relevant_features np.ones len relevant_features label relevant features pl.ylabel features pl.xlabel values weights pl.legend loc1 pl.figure figsize pl.title marginal loglikelihood pl.plot clf.scores_ pl.ylabel score pl.xlabel iterations pl.show 2.1. examples scikitlearn user guide release 0.11 figure 2.82 bayesian ridge regression bayesian ridge regression computes bayesian ridge regression synthetic dataset compared ols ordinary least squares estimator coefcient weights slightly shifted toward zeros wich stabilises prior weights gaussian prior histogram estimated weights gaussian estimation model done iteratively maximizing marginal loglikelihood observations 
4082: chapter example gallery scikitlearn user guide release 0.11 python source code plot_bayesian_ridge.py print __doc__ import numpy import pylab scipy import stats sklearn.linear_model import bayesianridge linearregression generating simulated data gaussian weigthts np.random.seed n_samples n_features np.random.randn n_samples n_features create gaussian data create weigts precision lambda_ lambda_ np.zeros n_features keep weights interest relevant_features np.random.randint n_features relevant_features stats.norm.rvs loc0 scale1 np.sqrt lambda_ create noise precision alpha 50. alpha_ 50. noise stats.norm.rvs loc0 scale1 np.sqrt alpha_ sizen_samples create target np.dot noise fit bayesian ridge regression ols comparison clf bayesianridge compute_scoretrue clf.fit ols linearregression ols.fit plot true weights estimated weights histogram weights pl.figure figsize pl.title weights model pl.plot clf.coef_ label bayesian ridge estimate pl.plot label ground truth pl.plot ols.coef_ label ols estimate 2.1. examples scikitlearn user guide release 0.11 pl.xlabel features pl.ylabel values weights pl.legend loc best propdict size12 pl.figure figsize pl.title histogram weights pl.hist clf.coef_ binsn_features logtrue pl.plot clf.coef_ relevant_features np.ones len relevant_features label relevant features pl.ylabel features pl.xlabel values weights pl.legend loc lower left pl.figure figsize pl.title marginal loglikelihood pl.plot clf.scores_ pl.ylabel score pl.xlabel iterations pl.show figure 2.83 logistic regression 3class classier logistic regression 3class classier show logisticregression classiers decision boundaries iris dataset datapoints colored according labels 
4083: python source code plot_iris_logistic.py print __doc__ chapter example gallery scikitlearn user guide release 0.11 code source gael varoqueux modified documentation merge jaques grobler license bsd import numpy import pylab sklearn import linear_model datasets import data play iris datasets.load_iris iris.data take first two features iris.target .02 step size mesh logreg linear_model.logisticregression c1e5 create instance neighbours classifier fit data logreg.fit plot decision boundary asign color point mesh x_min m_max y_min y_max x_min x_max .min .max y_min y_max .min .max np.meshgrid np.arange x_min x_max np.arange y_min y_max logreg.predict np.c_ xx.ravel yy.ravel put result color plot z.reshape xx.shape pl.figure figsize pl.pcolormesh cmappl.cm.paired plot also training points pl.scatter edgecolorsk cmappl.cm.paired pl.xlabel sepal length pl.ylabel sepal width pl.xlim xx.min xx.max pl.ylim yy.min yy.max pl.xticks pl.yticks pl.show figure 2.84 lasso elastic net sparse signals 2.1. examples scikitlearn user guide release 0.11 lasso elastic net sparse signals script output lasso alpha0.1 copy_xtrue fit_intercepttrue max_iter1000 normalizefalse positivefalse precomputeauto tol0.0001 warm_startfalse test data 0.384710 elasticnet alpha0.1 copy_xtrue fit_intercepttrue max_iter1000 normalizefalse positivefalse precomputeauto rho0.7 tol0.0001 warm_startfalse test data 0.240176 python source code plot_lasso_and_elasticnet.py print __doc__ import numpy import pylab sklearn.metrics import r2_score generate sparse data play np.random.seed chapter example gallery scikitlearn user guide release 0.11 n_samples n_features np.random.randn n_samples n_features coef np.random.randn n_features inds np.arange n_features np.random.shuffle inds coef inds sparsify coef np.dot coef add noise 0.01 np.random.normal n_samples split data train set test set n_samples x.shape x_train y_train n_samples n_samples x_test y_test n_samples n_samples lasso sklearn.linear_model import lasso alpha 0.1 lasso lasso alphaalpha y_pred_lasso lasso.fit x_train y_train .predict x_test r2_score_lasso r2_score y_test y_pred_lasso print lasso print test data r2_score_lasso elasticnet sklearn.linear_model import elasticnet enet elasticnet alphaalpha rho0.7 y_pred_enet enet.fit x_train y_train .predict x_test r2_score_enet r2_score y_test y_pred_enet print enet print test data r2_score_enet pl.plot enet.coef_ labelelastic net coefficients pl.plot lasso.coef_ labellasso coefficients pl.plot coef labeloriginal coefficients pl.legend locbest pl.title lasso elastic net r2_score_lasso r2_score_enet pl.show figure 2.85 lasso elastic net 2.1. examples scikitlearn user guide release 0.11 lasso elastic net lasso elastic net penalisation implemented using coordinate descent coefcients forced positive 
4084: script output computing regularization path using lasso ... computing regularization path using positive lasso ... computing regularization path using elastic net ... computing regularization path using positve elastic net.. 
4085: python source code plot_lasso_coordinate_descent_path.py chapter example gallery scikitlearn user guide release 0.11 print __doc__ author alexandre gramfort alexandre.gramfort inria.fr license bsd style 
4086: import numpy import pylab sklearn.linear_model import lasso_path enet_path sklearn import datasets diabetes datasets.load_diabetes diabetes.data diabetes.target x.std standardize data easier set rho parameter compute paths eps 5e3 smaller longer path print computing regularization path using lasso ... models lasso_path epseps alphas_lasso np.array model.alpha model models coefs_lasso np.array model.coef_ model models print computing regularization path using positive lasso ... models lasso_path epseps positivetrue alphas_positive_lasso np.array model.alpha model models coefs_positive_lasso np.array model.coef_ model models print computing regularization path using elastic net ... models enet_path epseps rho0.8 alphas_enet np.array model.alpha model models coefs_enet np.array model.coef_ model models print computing regularization path using positve elastic net ... models enet_path epseps rho0.8 positivetrue alphas_positive_enet np.array model.alpha model models coefs_positive_enet np.array model.coef_ model models display results pl.figure pl.gca ax.set_color_cycle pl.plot coefs_lasso pl.plot coefs_enet linestyle pl.xlabel log lambda pl.ylabel weights pl.title lasso elasticnet paths pl.legend lasso elasticnet loclower left pl.axis tight 2.1. examples scikitlearn user guide release 0.11 pl.figure pl.gca ax.set_color_cycle pl.plot coefs_lasso pl.plot coefs_positive_lasso linestyle pl.xlabel log lambda pl.ylabel weights pl.title lasso positive lasso pl.legend lasso positive lasso loclower left pl.axis tight pl.figure pl.gca ax.set_color_cycle pl.plot coefs_enet pl.plot coefs_positive_enet linestyle pl.xlabel log lambda pl.ylabel weights pl.title elasticnet positive elasticnet pl.legend elasticnet positive elasticnet loclower left pl.axis tight pl.show figure 2.86 lasso path using lars lasso path using lars computes lasso path along regularization parameter using lars algorithm diabetest dataset 
4087: chapter example gallery scikitlearn user guide release 0.11 script output computing regularization path using lars ... 
4088: python source code plot_lasso_lars.py print __doc__ author fabian pedregosa fabian.pedregosa inria.fr license bsd style 
4089: alexandre gramfort alexandre.gramfort inria.fr import numpy import pylab sklearn import linear_model sklearn import datasets diabetes datasets.load_diabetes diabetes.data diabetes.target print computing regularization path using lars ... alphas coefs linear_model.lars_path methodlasso verbosetrue np.sum np.abs coefs.t axis1 2.1. examples scikitlearn user guide release 0.11 pl.plot coefs.t ymin ymax pl.ylim pl.vlines ymin ymax linestyledashed pl.xlabel coef maxcoef pl.ylabel coefficients pl.title lasso path pl.axis tight pl.show figure 2.87 lasso model selection crossvalidation aic bic lasso model selection crossvalidation aic bic use akaike information criterion aic bayes information criterion bic crossvalidation select optimal value regularization parameter alpha lasso estimator results obtained lassolarsic based aicbic criteria informationcriterion based model selection fast relies proper estimation degrees freedom derived large samples asymptotic results assume model correct i.e data actually generated model also tend break problem badly conditioned features samples crossvalidation use 20fold algorithms compute lasso path coordinate descent implemented lassocv class lars least angle regression implemented lassolarscv class algorithms give roughly results differ regards execution speed sources numerical errors lars computes path solution kink path result efcient kinks case features samples also able compute full path without setting meta parameter opposite coordinate descent compute path points prespecied grid use default thus efcient number grid points smaller number kinks path strategy interesting number features really large enough samples select large amount terms numerical errors heavily correlated variables lars accumulate erros coordinate descent algorithm sample path grid note optimal value alpha varies fold illustrates nestedcross validation necessary trying evaluate performance method parameter chosen crossvalidation choice parameter may optimal unseen data 
4090: chapter example gallery scikitlearn user guide release 0.11 script output computing regularization path using coordinate descent lasso ... computing regularization path using lars lasso.. 
4091: python source code plot_lasso_model_selection.py print __doc__ author olivier grisel gael varoquaux alexandre gramfort license bsd style 
4092: import time 2.1. examples scikitlearn user guide release 0.11 import numpy import pylab sklearn.linear_model import lassocv lassolarscv lassolarsic sklearn import datasets diabetes datasets.load_diabetes diabetes.data diabetes.target rng np.random.randomstate np.c_ rng.randn x.shape add bad features normalize data done lars allow comparison np.sqrt np.sum axis0 lassolarsic least angle regression bicaic criterion model_bic lassolarsic criterionbic time.time model_bic.fit t_bic time.time alpha_bic_ model_bic.alpha_ model_aic lassolarsic criterionaic model_aic.fit alpha_aic_ model_aic.alpha_ def plot_ic_criterion model name color alpha_ model.alpha_ alphas_ model.alphas_ criterion_ model.criterion_ pl.plot np.log10 alphas_ criterion_ colorcolor linewidth3 label criterion name pl.axvline np.log10 alpha_ colorcolor linewidth3 labelalpha estimate name pl.xlabel log lambda pl.ylabel criterion pl.figure plot_ic_criterion model_aic aic plot_ic_criterion model_bic bic pl.legend pl.title informationcriterion model selection training time .3fs t_bic lassocv coordinate descent compute paths print computing regularization path using coordinate descent lasso ... time.time model lassocv cv20 .fit t_lasso_cv time.time display results chapter example gallery scikitlearn user guide release 0.11 m_log_alphas np.log10 model.alphas pl.figure ymin ymax pl.plot m_log_alphas model.mse_path_ pl.plot m_log_alphas model.mse_path_.mean axis1 labelaverage across folds linewidth2 pl.axvline np.log10 model.alpha linestyle colork labelalpha estimate pl.legend pl.xlabel log lambda pl.ylabel mean square error pl.title mean square error fold coordinate descent train time .2fs t_lasso_cv pl.axis tight pl.ylim ymin ymax lassolarscv least angle regression compute paths print computing regularization path using lars lasso ... time.time model lassolarscv cv20 .fit t_lasso_lars_cv time.time display results m_log_alphas np.log10 model.cv_alphas pl.figure pl.plot m_log_alphas model.cv_mse_path_ pl.plot m_log_alphas model.cv_mse_path_.mean axis1 labelaverage across folds linewidth2 pl.axvline np.log10 model.alpha linestyle colork labelalpha pl.legend pl.xlabel log lambda pl.ylabel mean square error pl.title mean square error fold lars train time .2fs t_lasso_lars_cv pl.axis tight pl.ylim ymin ymax pl.show figure 2.88 logit function 2.1. examples scikitlearn user guide release 0.11 logit function show plot logistic regression would synthetic dataset classify values either i.e class one two using logitcurve 
4093: python source code plot_logistic.py print __doc__ code source gael varoqueux license bsd import numpy import pylab sklearn import linear_model test set straight line gaussian noise xmin xmax n_samples np.random.seed np.random.normal sizen_samples .astype np.float np.random.normal sizen_samples np.newaxis run classifier clf linear_model.logisticregression c1e5 clf.fit plot result pl.figure figsize pl.clf pl.scatter x.ravel colorblack zorder20 x_test np.linspace chapter example gallery scikitlearn user guide release 0.11 def model return np.exp loss model x_test clf.coef_ clf.intercept_ .ravel pl.plot x_test loss colorblue linewidth3 ols linear_model.linearregression ols.fit pl.plot x_test ols.coef_ x_test ols.intercept_ linewidth1 pl.axhline color.5 pl.ylabel pl.xlabel pl.xticks pl.yticks pl.ylim .25 1.25 pl.xlim pl.show figure 2.89 penalty sparsity logistic regression penalty sparsity logistic regression comparison sparsity percentage zero coefcients solutions penalty used different values see large values give freedom model conversely smaller values constrain model penalty case leads sparser solutions classify 8x8 images digits two classes 59. visualization shows coefcients models varying 
4094: 2.1. examples scikitlearn user guide release 0.11 script output c10.000000 sparsity penalty 6.250000 score penalty 0.910406 sparsity penalty 4.687500 score penalty 0.909293 c100.000000 sparsity penalty 4.687500 score penalty 0.908737 sparsity penalty 4.687500 score penalty 0.909850 c1000.000000 sparsity penalty 4.687500 score penalty 0.910406 sparsity penalty 4.687500 score penalty 0.909850 python source code plot_logistic_l1_l2_sparsity.py print __doc__ authors alexandre gramfort alexandre.gramfort inria.fr license bsd style 
4095: mathieu blondel mathieu mblondel.org andreas mueller amueller ais.unibonn.de chapter example gallery scikitlearn user guide release 0.11 import numpy import pylab sklearn.linear_model import logisticregression sklearn import datasets sklearn.preprocessing import scaler digits datasets.load_digits digits.data digits.target scaler .fit_transform classify small large digits .astype np.int set regularization parameter enumerate np.arange turn tolerance short training time clf_l1_lr logisticregression penaltyl1 tol0.01 clf_l2_lr logisticregression penaltyl2 tol0.01 clf_l1_lr.fit clf_l2_lr.fit coef_l1_lr clf_l1_lr.coef_.ravel coef_l2_lr clf_l2_lr.coef_.ravel coef_l1_lr contains zeros due sparsity inducing norm sparsity_l1_lr np.mean coef_l1_lr sparsity_l2_lr np.mean coef_l2_lr print print sparsity penalty sparsity_l1_lr print score penalty clf_l1_lr.score print sparsity penalty sparsity_l2_lr print score penalty clf_l2_lr.score l1_plot pl.subplot l2_plot pl.subplot l1_plot.set_title penalty l2_plot.set_title penalty l1_plot.imshow np.abs coef_l1_lr.reshape interpolationnearest cmapbinary vmax1 vmin0 l2_plot.imshow np.abs coef_l2_lr.reshape interpolationnearest cmapbinary vmax1 vmin0 pl.text l1_plot.set_xticks l1_plot.set_yticks l2_plot.set_xticks l2_plot.set_yticks pl.show 2.1. examples scikitlearn user guide release 0.11 figure 2.90 path logistic regression path logistic regression computes path iris dataset 
4096: script output computing regularization path ... took 00000.024074 python source code plot_logistic_path.py print __doc__ author alexandre gramfort alexandre.gramfort inria.fr license bsd style 
4097: chapter example gallery scikitlearn user guide release 0.11 datetime import datetime import numpy import pylab sklearn import linear_model sklearn import datasets sklearn.svm import l1_min_c iris datasets.load_iris iris.data iris.target np.mean demo path functions l1_min_c losslog np.logspace print computing regularization path ... start datetime.now clf linear_model.logisticregression c1.0 penaltyl1 tol1e6 coefs_ clf.set_params clf.fit coefs_.append clf.coef_.ravel .copy print took datetime.now start coefs_ np.array coefs_ pl.plot np.log10 coefs_ ymin ymax pl.ylim pl.xlabel log pl.ylabel coefficients pl.title logistic regression path pl.axis tight pl.show figure 2.91 linear regression example 2.1. examples scikitlearn user guide release 0.11 linear regression example example uses rst feature diabetes dataset order illustrate twodimensional plot regression technique straight line seen plot showing linear regression attempts draw straight line best minimize residual sum squares observed responses dataset responses predicted linear approximation coefcients residual sum squares variance score also calculated 
4098: script output coefficients 938.23786125 residual sum squares 2548.07 variance score 0.47 python source code plot_ols.py print __doc__ code source jaques grobler license bsd import pylab import numpy chapter example gallery scikitlearn user guide release 0.11 sklearn import datasets linear_model load diabetes dataset diabetes datasets.load_diabetes use one feature diabetes_x diabetes.data np.newaxis diabetes_x_temp diabetes_x split data trainingtesting sets diabetes_x_train diabetes_x_temp diabetes_x_test diabetes_x_temp sklearn.datasets.samples_generator import make_regression test set straight line gaussian noise make_regression n_samples100 n_features1 n_informative1 random_state0 noise35 split targets trainingtesting sets diabetes_y_train diabetes.target diabetes_y_test diabetes.target create linear regression object regr linear_model.linearregression train model using training sets regr.fit diabetes_x_train diabetes_y_train coefficients print coefficients regr.coef_ mean square error print residual sum squares .2f np.mean regr.predict diabetes_x_test diabetes_y_test explained variance score perfect prediction print variance score .2f regr.score diabetes_x_test diabetes_y_test plot outputs pl.scatter diabetes_x_test diabetes_y_test pl.plot diabetes_x_test regr.predict diabetes_x_test colorblue colorblack linewidth3 pl.xticks pl.yticks pl.show figure 2.92 sparsity example fitting features 2.1. examples scikitlearn user guide release 0.11 sparsity example fitting features features diabetesdataset tted plotted illustrates although feature strong coefcient full model give much regarding compared feautre python source code plot_ols_3d.py print __doc__ code source gael varoqueux modified documentation merge jaques grobler license bsd import pylab import numpy mpl_toolkits.mplot3d import axes3d sklearn import datasets linear_model diabetes datasets.load_diabetes indices x_train diabetes.data indices x_test diabetes.data indices y_train diabetes.target y_test diabetes.target ols linear_model.linearregression chapter example gallery scikitlearn user guide release 0.11 ols.fit x_train y_train plot figure def plot_figs fig_num elev azim x_train clf fig pl.figure fig_num figsize pl.clf axes3d fig elevelev azimazim ax.scatter x_train x_train y_train marker ax.plot_surface np.array .15 .15 np.array .15 .15 clf.predict np.array .15 .15 .15 .15 .reshape alpha.5 ax.set_xlabel x_1 ax.set_ylabel x_2 ax.set_zlabel ax.w_xaxis.set_ticklabels ax.w_yaxis.set_ticklabels ax.w_zaxis.set_ticklabels generate three different figures different views elev 43.5 azim plot_figs elev azim x_train ols elev azim plot_figs elev azim x_train ols elev azim plot_figs elev azim x_train ols pl.show figure 2.93 ordinary least squares ridge regression variance ordinary least squares ridge regression variance due points dimension straight line linear regression uses follow points well noise observations cause great variace shown rst plot every lines slope vary quite bit prediction due noise induced observations ridge regression basically minimizing penalised version leastsquared function penalising shrinks value regression coefcients despite data points dimension slope prediction much stable variance line greatly reduced comparison standard linear regression 2.1. examples scikitlearn user guide release 0.11 python source code plot_ols_ridge_variance.py print __doc__ code source gael varoqueux modified documentation merge jaques grobler license bsd import numpy import pylab sklearn import linear_model x_train np.c_ y_train x_test np.c_ np.random.seed classifiers dict olslinear_model.linearregression ridgelinear_model.ridge alpha.1 fignum name clf classifiers.iteritems fig pl.figure fignum figsize pl.clf pl.axes .12 .12 range this_x np.random.normal size x_train clf.fit this_x y_train ax.plot x_test clf.predict x_test color.5 ax.scatter this_x y_train c.5 markero zorder10 chapter example gallery scikitlearn user guide release 0.11 clf.fit x_train y_train ax.plot x_test clf.predict x_test linewidth2 colorblue ax.scatter x_train y_train s30 marker zorder10 ax.set_xticks ax.set_yticks ax.set_ylim 1.6 ax.set_xlabel ax.set_ylabel ax.set_xlim fignum pl.show figure 2.94 orthogonal matching pursuit orthogonal matching pursuit using orthogonal matching pursuit recovering sparse signal noisy measurement encoded dictionary 2.1. examples scikitlearn user guide release 0.11 python source code plot_omp.py print __doc__ import pylab import numpy sklearn.linear_model import orthogonal_mp sklearn.datasets import make_sparse_coded_signal n_components n_features n_atoms generate data x_0 n_atoms make_sparse_coded_signal n_samples1 n_componentsn_components n_featuresn_features n_nonzero_coefsn_atoms random_state0 idx x.nonzero chapter example gallery scikitlearn user guide release 0.11 distort clean signal y_noisy 0.05 np.random.randn len plot sparse signal pl.subplot pl.xlim pl.title sparse signal pl.stem idx idx plot noisefree reconstruction x_r orthogonal_mp n_atoms idx_r x_r.nonzero pl.subplot pl.xlim pl.title recovered signal noisefree measurements pl.stem idx_r x_r idx_r plot noisy reconstruction x_r orthogonal_mp y_noisy n_atoms idx_r x_r.nonzero pl.subplot pl.xlim pl.title recovered signal noisy measurements pl.stem idx_r x_r idx_r pl.subplots_adjust 0.06 0.04 0.94 0.90 0.20 0.38 pl.suptitle sparse signal recovery orthogonal matching pursuit fontsize16 pl.show figure 2.95 polynomial interpolation polynomial interpolation example demonstrates approximate function polynomial degree n_degree using ridge regression concretely n_samples points sufces build vandermonde matrix n_samples n_degree1 following form x_1 x_1 x_1 ... x_2 x_2 x_2 ... ... intuitively matrix interpreted matrix pseudo features points raised power matrix akin different matrix induced polynomial kernel 
4099: 2.1. examples scikitlearn user guide release 0.11 example shows nonlinear regression linear model manually adding nonlinear features kernel methods extend idea induce high even innite dimensional feature spaces 
4100: python source code plot_polynomial_interpolation.py print __doc__ author mathieu blondel license bsd style 
4101: import numpy import pylab sklearn.linear_model import ridge def function approximate polynomial interpolation return np.sin generate points used plot x_plot np.linspace generate points keep subset np.linspace rng np.random.randomstate chapter example gallery scikitlearn user guide release 0.11 rng.shuffle np.sort pl.plot x_plot x_plot label ground truth pl.scatter label training points degree ridge ridge ridge.fit np.vander degree pl.plot x_plot ridge.predict np.vander x_plot degree label degree degree pl.legend loclower left pl.show figure 2.96 plot ridge coefcients function regularization plot ridge coefcients function regularization shows effect collinearity coefcients ridge end path alpha tends toward zero solution tends towards ordinary least squares coefcients exhibit big oscillations 
4102: 2.1. examples scikitlearn user guide release 0.11 python source code plot_ridge_path.py author fabian pedregosa fabian.pedregosa inria.fr license bsd style 
4103: print __doc__ import numpy import pylab sklearn import linear_model 10x10 hilbert matrix np.arange np.arange np.newaxis np.ones compute paths n_alphas alphas np.logspace n_alphas clf linear_model.ridge fit_interceptfalse coefs alphas clf.set_params alphaa clf.fit chapter example gallery scikitlearn user guide release 0.11 coefs.append clf.coef_ display results pl.gca ax.set_color_cycle ax.plot alphas coefs ax.set_xscale log ax.set_xlim ax.get_xlim reverse axis pl.xlabel alpha pl.ylabel weights pl.title ridge coefficients function regularization pl.axis tight pl.show figure 2.97 plot multiclass sgd iris dataset plot multiclass sgd iris dataset plot decision surface multiclass sgd iris dataset hyperplanes corresponding three oneversusall ova classiers represented dashed lines 
4104: 2.1. examples scikitlearn user guide release 0.11 python source code plot_sgd_iris.py print __doc__ import numpy import pylab sklearn import datasets sklearn.linear_model import sgdclassifier import data play iris datasets.load_iris iris.data take first two features could avoid ugly slicing using twodim dataset iris.target colors bry shuffle idx np.arange x.shape np.random.seed np.random.shuffle idx idx idx standardize mean x.mean axis0 std x.std axis0 chapter example gallery scikitlearn user guide release 0.11 mean std .02 step size mesh clf sgdclassifier alpha0.001 n_iter100 .fit create mesh plot x_min x_max .min .max y_min y_max .min .max np.meshgrid np.arange x_min x_max np.arange y_min y_max plot decision boundary asign color point mesh x_min m_max y_min y_max clf.predict np.c_ xx.ravel yy.ravel put result color plot z.reshape xx.shape pl.contourf cmappl.cm.paired pl.axis tight plot also training points color zip clf.classes_ colors idx np.where pl.scatter idx idx ccolor labeliris.target_names cmappl.cm.paired pl.title decision surface multiclass sgd pl.axis tight plot three oneagainstall classifiers xmin xmax pl.xlim ymin ymax pl.ylim coef clf.coef_ intercept clf.intercept_ def plot_hyperplane color def line return coef intercept coef pl.plot xmin xmax line xmin line xmax colorcolor color zip clf.classes_ colors plot_hyperplane color pl.legend pl.show figure 2.98 sgd convex loss functions 2.1. examples scikitlearn user guide release 0.11 sgd convex loss functions plot convex loss functions supported sklearn.linear_model.stochastic_gradient 
4105: python source code plot_sgd_loss_functions.py print __doc__ import numpy import pylab sklearn.linear_model.sgd_fast import hinge modifiedhuber squaredloss define loss funcitons xmin xmax hinge hinge log_loss lambda np.log2 1.0 np.exp modified_huber modifiedhuber squared_loss squaredloss plot loss funcitons np.linspace xmin xmax pl.plot xmin xmax label zeroone loss chapter example gallery scikitlearn user guide release 0.11 pl.plot hinge.loss label hinge loss pl.plot log_loss label log loss pl.plot modified_huber.loss label modified huber loss label squared loss pl.plot 2.0squared_loss.loss pl.ylim pl.legend loc upper right pl.xlabel cdot pl.ylabel pl.show figure 2.99 ordinary least squares sgd ordinary least squares sgd simple ordinary least squares example stochastic gradient descent draw linear least squares solution random set points plane 
4106: 2.1. examples scikitlearn user guide release 0.11 python source code plot_sgd_ols.py print __doc__ import pylab sklearn.linear_model import sgdregressor sklearn.datasets.samples_generator import make_regression test set straight line gaussian noise make_regression n_samples100 n_features1 n_informative1 random_state0 noise35 run classifier clf sgdregressor alpha0.1 n_iter20 clf.fit plot result pl.scatter colorblack pl.plot clf.predict colorblue linewidth3 pl.show chapter example gallery scikitlearn user guide release 0.11 figure 2.100 sgd penalties sgd penalties plot contours three penalties supported sklearn.linear_model.stochastic_gradient 
4107: python source code plot_sgd_penalties.py __future__ import division print __doc__ import numpy import pylab def 2.1. examples scikitlearn user guide release 0.11 return np.array np.sqrt np.sqrt 2.0 2.0 def return np.array np.sqrt 1.0 2.0 def return np.array def cross ext pl.plot ext ext pl.plot ext ext np.linspace alpha 0.501 0.5 division throuh zero cross 1.2 pl.plot label pl.plot 1.0 pl.plot pl.plot 1.0 pl.plot label pl.plot 1.0 pl.plot pl.plot 1.0 pl.plot alpha label elastic net pl.plot 1.0 alpha pl.plot alpha pl.plot 1.0 alpha pl.xlabel w_0 pl.ylabel w_1 pl.legend pl.axis equal pl.show sgd maximum margin separating hyperplane plot maximum margin separating hyperplane within twoclass separable dataset using linear support vector machines classier trained using sgd 
4108: chapter example gallery scikitlearn user guide release 0.11 figure 2.101 sgd maximum margin separating hyperplane python source code plot_sgd_separating_hyperplane.py print __doc__ import numpy import pylab sklearn.linear_model import sgdclassifier sklearn.datasets.samples_generator import make_blobs create separable points make_blobs n_samples50 centers2 random_state0 cluster_std0.60 fit model clf sgdclassifier loss hinge alpha0.01 n_iter200 fit_intercepttrue 2.1. examples scikitlearn user guide release 0.11 clf.fit plot line points nearest vectors plane np.linspace np.linspace np.meshgrid np.empty x1.shape val np.ndenumerate val clf.decision_function levels 1.0 0.0 1.0 linestyles dashed solid dashed colors pl.contour levels colorscolors linestyleslinestyles pl.scatter cmappl.cm.paired pl.axis tight pl.show figure 2.102 sgd separating hyperplane weighted classes sgd separating hyperplane weighted classes fit linear svms without class weighting allows handle problems unbalanced classes 
4109: chapter example gallery scikitlearn user guide release 0.11 python source code plot_sgd_weighted_classes.py print __doc__ import numpy import pylab sklearn.linear_model import sgdclassifier create separable points np.random.seed n_samples_1 n_samples_2 np.r_ 1.5 np.random.randn n_samples_1 0.5 np.random.randn n_samples_2 np.array n_samples_1 n_samples_2 dtypenp.float64 idx np.arange y.shape np.random.shuffle idx idx idx mean x.mean axis0 std x.std axis0 mean std fit model get separating hyperplane clf sgdclassifier n_iter100 alpha0.01 clf.fit 2.1. examples scikitlearn user guide release 0.11 clf.coef_.ravel np.linspace clf.intercept_ get separating hyperplane using weighted classes wclf sgdclassifier n_iter100 alpha0.01 class_weight wclf.fit wclf.coef_.ravel wyy wclf.intercept_ plot separating hyperplanes samples pl.plot labelno weights pl.plot wyy labelwith weights pl.scatter cmappl.cm.paired pl.legend pl.axis tight pl.show figure 2.103 sgd weighted samples sgd weighted samples plot decision function weighted dataset size points proportional weight 
4110: chapter example gallery scikitlearn user guide release 0.11 python source code plot_sgd_weighted_samples.py print __doc__ import numpy import pylab sklearn import linear_model create points np.random.seed np.r_ np.random.randn np.random.randn sample_weight np.abs np.random.randn assign bigger weight last samples sample_weight plot weighted data points np.meshgrid np.linspace np.linspace pl.figure pl.scatter ssample_weight alpha0.9 cmappl.cm.bone fit unweighted model clf linear_model.sgdclassifier alpha0.01 n_iter100 clf.fit clf.decision_function np.c_ xx.ravel yy.ravel 2.1. examples scikitlearn user guide release 0.11 z.reshape xx.shape no_weights pl.contour levels linestyles solid fit weighted model clf linear_model.sgdclassifier alpha0.01 n_iter100 clf.fit sample_weightsample_weight clf.decision_function np.c_ xx.ravel yy.ravel z.reshape xx.shape samples_weights pl.contour levels linestyles dashed pl.legend no_weights.collections samples_weights.collections weights weights loc lower left pl.xticks pl.yticks pl.show figure 2.104 sparse recovery feature selection sparse linear models sparse recovery feature selection sparse linear models given small number observations want recover features relevant explain sparse linear models outperform standard statistical tests true model sparse i.e small fraction features relevant detailed compressive sensing notes ability l1based approach identify relevant variables pends sparsity ground truth number samples number features conditionning design matrix signal subspace amount noise absolute value smallest nonzero coefcient wainwright2006 http statistics.berkeley.edutechreports709.pdf keep parameters constant vary conditionning design matrix wellconditionned design matrix small mutual incoherence exactly compressive sensing conditions i.i.d gaussian sensing matrix l1recovery lasso performs well illconditionned matrix high mutual incoherence regressors correlated lasso randomly selects one however randomizedlasso recover ground truth well situation rst vary alpha parameter setting sparsity estimated model look stability scores randomized lasso analysis knowing ground truth shows optimal regime relevant features stand irrelevant ones alpha chosen small nonrelevant variables enter model opposite alpha selected large lasso equivalent stepwise regression thus brings advantage univariate ftest second time set alpha compare performance different feature selection methods using area curve auc precisionrecall 
4111: chapter example gallery scikitlearn user guide release 0.11 2.1. examples scikitlearn user guide release 0.11 python source code plot_sparse_recovery.py print __doc__ author alexandre gramfort gael varoquaux license bsd import pylab import numpy scipy import linalg sklearn.linear_model import randomizedlasso lasso_stability_path sklearn.feature_selection import f_regression sklearn.preprocessing import scaler sklearn.metrics import auc precision_recall_curve sklearn.ensemble import extratreesregressor lassolarscv def mutual_incoherence x_relevant x_irelevant mutual incoherence defined formula 26a wainwright2006 projector np.dot np.dot x_irelevant.t x_relevant linalg.pinv np.dot x_relevant.t x_relevant return np.max np.abs projector .sum axis1 conditionning 1e4 simulate regression data correlated design n_features n_relevant_features noise_level coef_min donohotanner phase transition around n_samples25 completely fail recover wellconditionned case n_samples block_size n_relevant_features rng np.random.randomstate coefficients model coef np.zeros n_features coef n_relevant_features coef_min rng.rand n_relevant_features correlation design variables correlated blocs corr np.zeros n_features n_features range n_features block_size corr block_size block_size conditionning corr.flat n_features corr linalg.cholesky corr design rng.normal size n_samples n_features np.dot corr keep wainwright2006 26c constant n_relevant_features np.abs chapter example gallery scikitlearn user guide release 0.11 linalg.svdvals n_relevant_features .max scaler .fit_transform x.copy output variable np.dot coef np.std scale added noise function average correlation design output variable noise_level rng.normal sizen_samples mutual_incoherence n_relevant_features n_relevant_features plot stability selection path using high eps early stopping path save computation time alpha_grid scores_path lasso_stability_path random_state42 eps0.05 pl.figure plot path function alphaalpha_max power power scales path less brutally log enables see progression along path pl.plot alpha_grid .333 scores_path coef pl.plot alpha_grid .333 scores_path coef ymin ymax pl.ylim pl.xlabel alpha alpha_ max pl.ylabel stability score proportion times selected pl.title stability scores path mutual incoherence .1f pl.axis tight pl.legend relevant features irrelevant features locbest plot estimated stability scores given alpha use 6fold crossvalidation rather default 3fold leads better choice alpha lars_cv lassolarscv cv6 .fit run randomizedlasso use paths going .1alpha_max avoid exploring regime noisy variables enter model alphas np.linspace lars_cv.alphas_ lars_cv.alphas_ clf randomizedlasso alphaalphas random_state42 .fit trees extratreesregressor compute_importancestrue .fit compare fscore f_regression pl.figure name score ftest stability selection clf.scores_ lasso coefs np.abs lars_cv.coef_ trees trees.feature_importances_ precision recall thresholds precision_recall_curve coef score pl.semilogy np.maximum score np.max score 1e4 label auc .3f name auc recall precision 2.1. examples scikitlearn user guide release 0.11 pl.plot np.where coef 2e4 n_relevant_features label ground truth pl.xlabel features pl.ylabel score plot first coefficients pl.xlim pl.legend locbest pl.title feature selection scores mutual incoherence .1f pl.show figure 2.105 lasso dense sparse data lasso dense sparse data show linear_model.lasso linear_model.sparse.lasso provide results case sparse data linear_model.sparse.lasso improves speed python source code lasso_dense_vs_sparse_data.py print __doc__ time import time scipy import sparse scipy import linalg sklearn.datasets.samples_generator import make_regression sklearn.linear_model.sparse import lasso sparselasso sklearn.linear_model import lasso denselasso two lasso implementations dense data print dense matrices make_regression n_samples200 n_features5000 random_state0 alpha sparse_lasso sparselasso alphaalpha fit_interceptfalse max_iter1000 dense_lasso denselasso alphaalpha fit_interceptfalse max_iter1000 time sparse_lasso.fit print sparse lasso done time chapter example gallery scikitlearn user guide release 0.11 time dense_lasso.fit print dense lasso done time print distance coefficients linalg.norm sparse_lasso.coef_ dense_lasso.coef_ two lasso implementations sparse data print sparse matrices x.copy 2.5 0.0 sparse.coo_matrix xs.tocsc print matrix density xs.nnz float x.size alpha 0.1 sparse_lasso sparselasso alphaalpha fit_interceptfalse max_iter10000 dense_lasso denselasso alphaalpha fit_interceptfalse max_iter10000 time sparse_lasso.fit print sparse lasso done time time dense_lasso.fit xs.todense print dense lasso done time print distance coefficients linalg.norm sparse_lasso.coef_ dense_lasso.coef_ 2.1.11 manifold learning examples concerning sklearn.manifold package 
4112: figure 2.106 comparison manifold learning methods comparison manifold learning methods illustration dimensionality reduction scurve dataset various manifold learning methods 
4113: 2.1. examples scikitlearn user guide release 0.11 discussion comparison algorithms see manifold module page script output standard 0.2 sec ltsa 0.69 sec hessian 0.56 sec modified 0.49 sec isomap 0.85 sec python source code plot_compare_methods.py author jake vanderplas vanderplas astro.washington.edu print __doc__ time import time import pylab mpl_toolkits.mplot3d import axes3d matplotlib.ticker import nullformatter sklearn import manifold datasets next line silence pyflakes import needed axes3d n_points color datasets.samples_generator.make_s_curve n_points n_neighbors chapter example gallery scikitlearn user guide release 0.11 n_components fig pl.figure figsize pl.suptitle manifold learning points neighbors n_neighbors fontsize14 try compatibility matplotlib 1.0 fig.add_subplot projection3d ax.scatter ccolor cmappl.cm.spectral ax.view_init except fig.add_subplot projection3d pl.scatter ccolor cmappl.cm.spectral methods standard ltsa hessian modified labels lle ltsa hessian lle modified lle method enumerate methods time manifold.locallylinearembedding n_neighbors n_components eigen_solverauto methodmethod .fit_transform time print .2g sec methods fig.add_subplot pl.scatter ccolor cmappl.cm.spectral pl.title .2g sec labels ax.xaxis.set_major_formatter nullformatter ax.yaxis.set_major_formatter nullformatter pl.axis tight time manifold.isomap n_neighbors n_components .fit_transform time print isomap .2g sec fig.add_subplot pl.scatter ccolor cmappl.cm.spectral pl.title isomap .2g sec ax.xaxis.set_major_formatter nullformatter ax.yaxis.set_major_formatter nullformatter pl.axis tight pl.show figure 2.107 manifold learning handwritten digits locally linear embedding isomap.. 
4114: 2.1. examples scikitlearn user guide release 0.11 manifold learning handwritten digits locally linear embedding isomap.. 
4115: illustration various embeddings digits dataset 
4116: chapter example gallery scikitlearn user guide release 0.11 2.1. examples scikitlearn user guide release 0.11 script output computing random projection computing pca projection computing lda projection computing isomap embedding done computing lle embedding done reconstruction error 1.28548e06 computing modified lle embedding done reconstruction error 0.359911 computing hessian lle embedding done reconstruction error 0.21178 computing ltsa embedding done reconstruction error 0.212077 python source code plot_lle_digits.py authors fabian pedregosa fabian.pedregosa inria.fr license bsd inria olivier grisel olivier.grisel ensta.org mathieu blondel mathieu mblondel.org print __doc__ time import time import numpy import pylab matplotlib import offsetbox chapter example gallery scikitlearn user guide release 0.11 sklearn.utils.fixes import qr_economic sklearn import manifold datasets decomposition lda digits datasets.load_digits n_class6 digits.data digits.target n_samples n_features x.shape n_neighbors scale visualize embedding vectors def plot_embedding titlenone x_min x_max np.min np.max x_min x_max x_min pl.figure pl.subplot range digits.data.shape pl.text str digits.target colorpl.cm.set1 digits.target fontdict weight bold size hasattr offsetbox annotationbbox print thumbnails matplotlib 1.0 shown_images np.array something big range digits.data.shape dist np.sum shown_images np.min dist 4e3 dont show points close continue shown_images np.r_ shown_images imagebox offsetbox.annotationbbox offsetbox.offsetimage digits.images cmappl.cm.gray_r ax.add_artist imagebox pl.xticks pl.yticks title none pl.title title plot images digits img np.zeros range range img .reshape pl.imshow img cmappl.cm.binary pl.xticks pl.yticks pl.title selection 64dimensional digits dataset random projection using random unitary matrix 2.1. examples scikitlearn user guide release 0.11 print computing random projection rng np.random.randomstate qr_economic rng.normal size n_features x_projected np.dot q.t x.t plot_embedding x_projected random projection digits projection first principal components print computing pca projection time x_pca decomposition.randomizedpca n_components2 .fit_transform plot_embedding x_pca principal components projection digits time .2fs time projection first linear discriminant components print computing lda projection x.copy x2.flat x.shape 0.01 make invertible time x_lda lda.lda n_components2 .fit_transform plot_embedding x_lda linear discriminant projection digits time .2fs time isomap projection digits dataset print computing isomap embedding time x_iso manifold.isomap n_neighbors n_components2 .fit_transform print done plot_embedding x_iso isomap projection digits time .2fs time locally linear embedding digits dataset print computing lle embedding clf manifold.locallylinearembedding n_neighbors n_components2 methodstandard time x_lle clf.fit_transform print done reconstruction error clf.reconstruction_error_ plot_embedding x_lle locally linear embedding digits time .2fs time modified locally linear embedding digits dataset print computing modified lle embedding clf manifold.locallylinearembedding n_neighbors n_components2 chapter example gallery scikitlearn user guide release 0.11 methodmodified time x_mlle clf.fit_transform print done reconstruction error clf.reconstruction_error_ plot_embedding x_mlle modified locally linear embedding digits time .2fs time hlle embedding digits dataset print computing hessian lle embedding clf manifold.locallylinearembedding n_neighbors n_components2 methodhessian time x_hlle clf.fit_transform print done reconstruction error clf.reconstruction_error_ plot_embedding x_hlle hessian locally linear embedding digits time .2fs time ltsa embedding digits dataset print computing ltsa embedding clf manifold.locallylinearembedding n_neighbors n_components2 methodltsa time x_ltsa clf.fit_transform print done reconstruction error clf.reconstruction_error_ plot_embedding x_ltsa local tangent space alignment digits time .2fs time pl.show figure 2.108 swiss roll reduction lle swiss roll reduction lle illustration swiss roll reduction locally linear embedding 2.1. examples scikitlearn user guide release 0.11 script output computing lle embedding done reconstruction error 9.68564e08 python source code plot_swissroll.py author fabian pedregosa fabian.pedregosa inria.fr license bsd inria print __doc__ import pylab import needed modify way figure behaves mpl_toolkits.mplot3d import axes3d axes3d locally linear embedding swiss roll sklearn import manifold datasets color datasets.samples_generator.make_swiss_roll n_samples1500 print computing lle embedding x_r err manifold.locally_linear_embedding n_neighbors12 n_components2 chapter example gallery scikitlearn user guide release 0.11 print done reconstruction error err plot result fig pl.figure try compatibility matplotlib 1.0 fig.add_subplot projection3d ax.scatter ccolor cmappl.cm.spectral except fig.add_subplot ax.scatter ccolor cmappl.cm.spectral ax.set_title original data fig.add_subplot ax.scatter x_r x_r ccolor cmappl.cm.spectral pl.axis tight pl.xticks pl.yticks pl.title projected data pl.show 2.1.12 gaussian mixture models examples concerning sklearn.mixture package 
4117: figure 2.109 gaussian mixture model ellipsoids gaussian mixture model ellipsoids plot condence ellipsoids mixture two gaussians variational dirichlet process models access components data note model necessarily use components model effectively use many needed good property dirichlet process prior see model splits components arbitrarily trying many components dirichlet process model adapts number state automatically example doesnt show lowdimensional space another advantage dirichlet process model full covariance matrices effectively even less examples per cluster dimensions data due regularization properties inference algorithm 
4118: 2.1. examples scikitlearn user guide release 0.11 python source code plot_gmm.py import itertools import numpy scipy import linalg import pylab import matplotlib mpl sklearn import mixture number samples per component n_samples generate random sample two components np.random.seed np.array 0.1 1.7 np.r_ np.dot np.random.randn n_samples np.random.randn n_samples np.array fit mixture gaussians using five components gmm mixture.gmm n_components5 covariance_typefull gmm.fit fit dirichlet process mixture gaussians using five components dpgmm mixture.dpgmm n_components5 covariance_typefull chapter example gallery scikitlearn user guide release 0.11 dpgmm.fit color_iter itertools.cycle clf title enumerate gmm gmm dpgmm dirichlet process gmm splot pl.subplot clf.predict mean covar color enumerate zip clf.means_ clf._get_covars color_iter linalg.eigh covar linalg.norm use every component access unless needs shouldnt plot redundant components np.any continue pl.scatter colorcolor plot ellipse show gaussian component angle np.arctan angle angle np.pi convert degrees ell mpl.patches.ellipse mean angle colorcolor ell.set_clip_box splot.bbox ell.set_alpha 0.5 splot.add_artist ell pl.xlim pl.ylim pl.xticks pl.yticks pl.title title pl.show figure 2.110 gmm classication gmm classication demonstration gaussian mixture models classication plots predicted labels training held test data using variety gmm classiers iris dataset compares gmms spherical diagonal full tied covariance matrices increasing order performance although one would expect full covariance perform best general prone overtting small datasets generalize well held test data plots train data shown dots test data shown crosses iris dataset fourdimensional 2.1. examples scikitlearn user guide release 0.11 rst two dimensions shown thus points separated dimensions 
4119: python source code plot_gmm_classifier.py print __doc__ author ron weiss ronweiss gmail.com gael varoquaux license bsd style 
4120: import pylab import matplotlib mpl import numpy sklearn import datasets sklearn.cross_validation import stratifiedkfold sklearn.mixture import gmm chapter example gallery scikitlearn user guide release 0.11 def make_ellipses gmm color enumerate rgb np.linalg.eigh gmm._get_covars np.linalg.norm angle np.arctan2 angle angle np.pi convert degrees ell mpl.patches.ellipse gmm.means_ angle colorcolor ell.set_clip_box ax.bbox ell.set_alpha 0.5 ax.add_artist ell iris datasets.load_iris break dataset nonoverlapping training testing sets skf stratifiedkfold iris.target take first fold train_index test_index skf.__iter__ .next x_train iris.data train_index y_train iris.target train_index x_test iris.data test_index y_test iris.target test_index n_classes len np.unique y_train try gmms using different types covariances classifiers dict covar_type gmm n_componentsn_classes covariance_typecovar_type init_paramswc n_iter20 covar_type spherical diag tied full n_classifiers len classifiers pl.figure figsize n_classifiers pl.subplots_adjust bottom.01 top0.95 hspace.15 wspace.05 left.01 right.99 index name classifier enumerate classifiers.iteritems since class labels training data initialize gmm parameters supervised manner classifier.means_ np.array x_train y_train .mean axis0 xrange n_classes train parameters using algorithm classifier.fit x_train pl.subplot n_classifiers index make_ellipses classifier color enumerate rgb data iris.data iris.target pl.scatter data data 0.8 colorcolor plot test data crosses labeliris.target_names 2.1. examples scikitlearn user guide release 0.11 color enumerate rgb data x_test y_test pl.plot data data colorcolor y_train_pred classifier.predict x_train train_accuracy np.mean y_train_pred.ravel y_train.ravel pl.text 0.05 0.9 train accuracy .1f train_accuracy transformh.transaxes y_test_pred classifier.predict x_test test_accuracy np.mean y_test_pred.ravel y_test.ravel pl.text 0.05 0.8 test accuracy .1f test_accuracy transformh.transaxes pl.xticks pl.yticks pl.title name pl.legend loclower right propdict size12 pl.show figure 2.111 density estimation mixture gaussians density estimation mixture gaussians plot density estimation mixture two gaussians data generated two gaussians different centers covariance matrices 
4121: chapter example gallery scikitlearn user guide release 0.11 python source code plot_gmm_pdf.py import numpy import pylab sklearn import mixture n_samples generate random sample two components np.random.seed np.array 0.7 3.5 x_train np.r_ np.dot np.random.randn n_samples np.random.randn n_samples np.array clf mixture.gmm n_components2 covariance_typefull clf.fit x_train np.linspace 20.0 30.0 np.linspace 20.0 40.0 np.meshgrid np.c_ x.ravel y.ravel np.log clf.eval z.reshape x.shape pl.contour pl.colorbar shrink0.8 extendboth 2.1. examples scikitlearn user guide release 0.11 pl.scatter x_train x_train pl.axis tight pl.show figure 2.112 gaussian mixture model selection gaussian mixture model selection example shows model selection perfomed gaussian mixture models using informationtheoretic criteria bic model selection concerns covariance type number components model case aic also provides right result shown save time bic better suited problem identify right model unlike bayesian procedures inferences priorfree case model components full covariance corresponds true generative model selected 
4122: chapter example gallery scikitlearn user guide release 0.11 python source code plot_gmm_selection.py print __doc__ import itertools import numpy scipy import linalg import pylab import matplotlib mpl sklearn import mixture number samples per component n_samples generate random sample two components np.random.seed np.array 0.1 1.7 np.r_ np.dot np.random.randn n_samples np.random.randn n_samples np.array lowest_bic np.infty bic n_components_range range cv_types spherical tied diag full 2.1. examples scikitlearn user guide release 0.11 cv_type cv_types n_components n_components_range fit mixture gaussians gmm mixture.gmm n_componentsn_components covariance_typecv_type gmm.fit bic.append gmm.bic bic lowest_bic lowest_bic bic best_gmm gmm bic np.array bic color_iter itertools.cycle clf best_gmm bars plot bic scores spl pl.subplot cv_type color enumerate zip cv_types color_iter xpos np.array n_components_range bars.append pl.bar xpos bic len n_components_range len n_components_range width.2 colorcolor pl.xticks n_components_range pl.ylim bic.min 1.01 .01 bic.max bic.max pl.title bic score per model xpos np.mod bic.argmin len n_components_range .65 np.floor bic.argmin len n_components_range pl.text xpos bic.min 0.97 .03 bic.max fontsize14 spl.set_xlabel number components spl.legend bars cv_types plot winner splot pl.subplot clf.predict mean covar color enumerate zip clf.means_ clf.covars_ color_iter linalg.eigh covar np.any continue pl.scatter colorcolor plot ellipse show gaussian component angle np.arctan2 angle angle np.pi convert degrees ell mpl.patches.ellipse mean angle colorcolor ell.set_clip_box splot.bbox ell.set_alpha splot.add_artist ell pl.xlim pl.ylim pl.xticks pl.yticks pl.title selected gmm full model components pl.subplots_adjust hspace.35 bottom.02 pl.show chapter example gallery scikitlearn user guide release 0.11 figure 2.113 gaussian mixture model sine curve gaussian mixture model sine curve example highlights advantages dirichlet process complexity control dealing sparse data dataset formed points loosely spaced following noisy sine curve gmm class using expectationmaximization algorithm mixture gaussian components nds toosmall components little structure dirichlet process however show model either learn global structure data small alpha easily interpolate nding relevant local structure large alpha never falling problems shown gmm class 
4123: python source code plot_gmm_sin.py import itertools import numpy 2.1. examples scikitlearn user guide release 0.11 scipy import linalg import pylab import matplotlib mpl sklearn import mixture number samples per component n_samples generate random sample following sine curve np.random.seed np.zeros n_samples step np.pi n_samples xrange x.shape step np.random.normal 0.1 np.sin np.random.normal color_iter itertools.cycle clf title enumerate mixture.gmm n_components10 covariance_typefull n_iter100 expectationmaximization mixture.dpgmm n_components10 covariance_typefull dirichlet process alpha0.01 alpha0.01 n_iter100 mixture.dpgmm n_components10 covariance_typediag dirichlet process alpha100 alpha100. n_iter100 clf.fit splot pl.subplot clf.predict mean covar color enumerate zip clf.means_ clf._get_covars color_iter linalg.eigh covar linalg.norm use every component access unless needs shouldnt plot redundant components np.any continue pl.scatter colorcolor plot ellipse show gaussian component angle np.arctan angle angle np.pi convert degrees ell mpl.patches.ellipse mean angle colorcolor ell.set_clip_box splot.bbox ell.set_alpha 0.5 splot.add_artist ell pl.xlim np.pi pl.ylim chapter example gallery scikitlearn user guide release 0.11 pl.title title pl.xticks pl.yticks pl.show 2.1.13 nearest neighbors examples concerning sklearn.neighbors package 
4124: figure 2.114 nearest neighbors classication nearest neighbors classication sample usage nearest neighbors classication plot decision boundaries class 
4125: 2.1. examples scikitlearn user guide release 0.11 python source code plot_classification.py print __doc__ import numpy import pylab matplotlib.colors import listedcolormap sklearn import neighbors datasets n_neighbors import data play iris datasets.load_iris iris.data take first two features could avoid ugly slicing using twodim dataset iris.target .02 step size mesh create color maps cmap_light listedcolormap ffaaaa aaffaa aaaaff cmap_bold listedcolormap ff0000 00ff00 0000ff weights uniform distance create instance neighbours classifier fit data clf neighbors.kneighborsclassifier n_neighbors weightsweights clf.fit plot decision boundary asign color point mesh x_min m_max y_min y_max x_min x_max .min .max y_min y_max .min .max np.meshgrid np.arange x_min x_max np.arange y_min y_max clf.predict np.c_ xx.ravel yy.ravel put result color plot z.reshape xx.shape pl.figure pl.pcolormesh cmapcmap_light plot also training points pl.scatter cmapcmap_bold pl.title 3class classification weights n_neighbors weights pl.axis tight pl.show nearest centroid classication sample usage nearest centroid classication plot decision boundaries class 
4126: chapter example gallery scikitlearn user guide release 0.11 figure 2.115 nearest centroid classication script output none 0.813333333333 0.1 0.826666666667 python source code plot_nearest_centroid.py print __doc__ import numpy import pylab matplotlib.colors import listedcolormap sklearn import datasets sklearn.neighbors import nearestcentroid n_neighbors import data play 2.1. examples scikitlearn user guide release 0.11 iris datasets.load_iris iris.data take first two features could avoid ugly slicing using twodim dataset iris.target .02 step size mesh create color maps cmap_light listedcolormap ffaaaa aaffaa aaaaff cmap_bold listedcolormap ff0000 00ff00 0000ff shrinkage none 0.1 create instance neighbours classifier fit data clf nearestcentroid shrink_thresholdshrinkage clf.fit y_pred clf.predict print shrinkage np.mean y_pred plot decision boundary asign color point mesh x_min m_max y_min y_max x_min x_max .min .max y_min y_max .min .max np.meshgrid np.arange x_min x_max np.arange y_min y_max clf.predict np.c_ xx.ravel yy.ravel put result color plot z.reshape xx.shape pl.figure pl.pcolormesh cmapcmap_light plot also training points pl.scatter cmapcmap_bold pl.title 3class classification shrink_threshold shrinkage pl.axis tight pl.show figure 2.116 nearest neighbors regression nearest neighbors regression demonstrate resolution regression problem using knearest neighbor interpolation target using barycenter constant weights 
4127: chapter example gallery scikitlearn user guide release 0.11 python source code plot_regression.py print __doc__ author alexandre gramfort alexandre.gramfort inria.fr license bsd inria fabian pedregosa fabian.pedregosa inria.fr generate sample data import numpy import pylab sklearn import neighbors np.random.seed np.sort np.random.rand axis0 np.linspace np.newaxis np.sin .ravel add noise targets 0.5 np.random.rand fit regression model 2.1. examples scikitlearn user guide release 0.11 n_neighbors weights enumerate uniform distance knn neighbors.kneighborsregressor n_neighbors weightsweights knn.fit .predict pl.subplot pl.scatter labeldata pl.plot labelprediction pl.axis tight pl.legend pl.title kneighborsregressor weights n_neighbors weights pl.show 2.1.14 semi supervised classication examples concerning sklearn.semi_supervised package 
4128: figure 2.117 label propagation digits demonstrating performance label propagation digits demonstrating performance example demonstrates power semisupervised learning training label spreading model classify handwritten digits sets labels handwritten digit dataset total points model trained using points labeled results form confusion matrix series metrics class good end top uncertain predictions shown 
4129: chapter example gallery scikitlearn user guide release 0.11 script output label spreading model labeled unlabeled points total precision recall f1score support avg total 1.00 0.58 0.96 0.00 0.91 0.96 0.97 0.89 0.48 0.54 0.73 1.00 0.54 0.93 0.00 0.80 0.79 0.97 1.00 0.83 0.77 0.77 1.00 0.56 0.95 0.00 0.85 0.87 0.97 0.94 0.61 0.64 0.74 confusion matrix 2.1. examples scikitlearn user guide release 0.11 python source code plot_label_propagation_digits.py print __doc__ authors clay woolam clay woolam.org licence bsd import numpy import pylab scipy import stats sklearn import datasets sklearn.semi_supervised import label_propagation sklearn.metrics import metrics sklearn.metrics.metrics import confusion_matrix digits datasets.load_digits rng np.random.randomstate indices np.arange len digits.data rng.shuffle indices digits.data indices digits.target indices images digits.images indices n_total_samples len n_labeled_points indices np.arange n_total_samples unlabeled_set indices n_labeled_points shuffle everything around y_train np.copy y_train unlabeled_set learn labelspreading lp_model label_propagation.labelspreading gamma0.25 max_iters5 lp_model.fit y_train predicted_labels lp_model.transduction_ unlabeled_set true_labels unlabeled_set confusion_matrix true_labels predicted_labels labelslp_model.classes_ print label spreading model labeled unlabeled points total n_labeled_points n_total_samples n_labeled_points n_total_samples print metrics.classification_report true_labels predicted_labels print confusion matrix print calculate uncertainty values transduced distribution pred_entropies stats.distributions.entropy lp_model.label_distributions_.t chapter example gallery scikitlearn user guide release 0.11 pick top uncertain labels uncertainty_index np.argsort pred_entropies plot pl.figure figsize index image_index enumerate uncertainty_index image images image_index sub f.add_subplot index sub.imshow image cmappl.cm.gray_r pl.xticks pl.yticks sub.set_title predict intrue lp_model.transduction_ image_index image_index f.suptitle learning small amount labeled data pl.show figure 2.118 label propagation digits active learning label propagation digits active learning demonstrates active learning technique learn handwritten digits using label propagation start training label propagation model labeled points select top uncertain points label next train labeled points original new ones repeat process four times model trained labeled examples plot appear showing top uncertain digits iteration training may may contain mistakes train next model true labels 
4130: 2.1. examples scikitlearn user guide release 0.11 script output iteration ______________________________________________________________________ label spreading model labeled unlabeled total precision recall f1score support avg total 0.00 0.49 0.88 0.00 0.00 0.89 0.86 0.75 0.54 0.41 0.52 0.00 0.90 0.97 0.00 0.00 0.49 0.95 0.92 0.79 0.86 0.63 0.00 0.63 0.92 0.00 0.00 0.63 0.90 0.83 0.64 0.56 0.55 confusion matrix chapter example gallery scikitlearn user guide release 0.11 iteration ______________________________________________________________________ label spreading model labeled unlabeled total precision recall f1score support avg total 1.00 0.61 0.91 1.00 0.79 0.89 0.86 0.97 0.54 0.70 0.82 1.00 0.59 0.97 0.56 0.88 0.46 0.95 0.92 0.84 0.81 0.80 1.00 0.60 0.94 0.71 0.84 0.60 0.90 0.94 0.66 0.75 0.79 confusion matrix iteration ______________________________________________________________________ label spreading model labeled unlabeled total precision recall f1score support avg total 1.00 0.68 0.91 0.96 0.81 0.89 0.86 0.97 0.68 0.75 0.85 1.00 0.59 0.97 1.00 1.00 0.46 0.95 0.92 0.84 0.81 0.84 1.00 0.63 0.94 0.98 0.89 0.60 0.90 0.94 0.75 0.78 0.83 confusion matrix iteration ______________________________________________________________________ 2.1. examples scikitlearn user guide release 0.11 label spreading model labeled unlabeled total precision recall f1score support avg total 1.00 0.70 1.00 1.00 1.00 0.96 1.00 0.90 0.83 0.75 0.91 1.00 0.85 0.90 1.00 1.00 0.74 0.95 1.00 0.81 0.83 0.90 1.00 0.77 0.95 1.00 1.00 0.83 0.97 0.95 0.82 0.79 0.90 confusion matrix iteration ______________________________________________________________________ label spreading model labeled unlabeled total precision recall f1score support avg total 1.00 0.77 1.00 1.00 1.00 0.94 1.00 0.90 0.89 0.94 0.94 1.00 0.88 0.90 1.00 1.00 0.97 0.97 1.00 0.81 0.89 0.94 1.00 0.82 0.95 1.00 1.00 0.95 0.99 0.95 0.85 0.91 0.94 confusion matrix python source code plot_label_propagation_digits_active_learning.py chapter example gallery scikitlearn user guide release 0.11 print __doc__ authors clay woolam clay woolam.org licence bsd import numpy import pylab scipy import stats sklearn import datasets sklearn.semi_supervised import label_propagation sklearn.metrics import classification_report confusion_matrix digits datasets.load_digits rng np.random.randomstate indices np.arange len digits.data rng.shuffle indices digits.data indices digits.target indices images digits.images indices n_total_samples len n_labeled_points unlabeled_indices np.arange n_total_samples n_labeled_points pl.figure range y_train np.copy y_train unlabeled_indices lp_model label_propagation.labelspreading gamma0.25 max_iters5 lp_model.fit y_train predicted_labels lp_model.transduction_ unlabeled_indices true_labels unlabeled_indices confusion_matrix true_labels predicted_labels labelslp_model.classes_ print iteration print label spreading model labeled unlabeled total n_labeled_points n_total_samples n_labeled_points n_total_samples print classification_report true_labels predicted_labels print confusion matrix print compute entropies transduced label distributions pred_entropies stats.distributions.entropy lp_model.label_distributions_.t select five digit examples classifier uncertain uncertainty_index uncertainty_index np.argsort pred_entropies keep track indicies get labels 2.1. examples scikitlearn user guide release 0.11 delete_indices np.array f.text .05 .183 model dnnfit withn labels size10 index image_index enumerate uncertainty_index image images image_index sub f.add_subplot index sub.imshow image cmappl.cm.gray_r sub.set_title predict intrue lp_model.transduction_ image_index image_index size10 sub.axis labeling points remote labeled set delete_index np.where unlabeled_indices image_index delete_indices np.concatenate delete_indices delete_index unlabeled_indices np.delete unlabeled_indices delete_indices n_labeled_points f.suptitle active learning label propagation.nrows show uncertain labels learn next model pl.subplots_adjust 0.12 0.03 0.9 0.8 0.2 0.45 pl.show figure 2.119 label propagation learning complex structure label propagation learning complex structure example labelpropagation learning complex internal structure demonstrate manifold learning outer circle labeled red inner circle blue label groups lie inside distinct shape see labels propagate correctly around circle 
4131: chapter example gallery scikitlearn user guide release 0.11 python source code plot_label_propagation_structure.py print __doc__ authors clay woolam clay woolam.org licence bsd andreas mueller amueller ais.unibonn.de import numpy import pylab sklearn.semi_supervised import label_propagation sklearn.datasets import make_circles generate ring inner box n_samples make_circles n_samplesn_samples shufflefalse outer inner labels np.ones n_samples labels outer labels inner learn labelspreading label_spread label_propagation.labelspreading kernelknn alpha1.0 label_spread.fit labels plot output labels output_labels label_spread.transduction_ pl.figure figsize 8.5 pl.subplot plot_outer_labeled pl.plot labels outer plot_unlabeled pl.plot labels labels plot_inner_labeled pl.plot labels inner labels outer pl.legend plot_outer_labeled plot_inner_labeled plot_unlabeled outer labeled inner labeled unlabeled upper left labels inner 2.1. examples scikitlearn user guide release 0.11 numpoints1 shadowfalse pl.title raw data classesred blue pl.subplot output_label_array np.asarray output_labels outer_numbers np.where output_label_array outer inner_numbers np.where output_label_array inner plot_outer pl.plot outer_numbers outer_numbers plot_inner pl.plot inner_numbers inner_numbers pl.legend plot_outer plot_inner outer learned inner learned upper left numpoints1 shadowfalse pl.title labels learned label spreading knn pl.subplots_adjust left0.07 bottom0.07 right0.93 top0.92 pl.show figure 2.120 decision boundary label propagation versus svm iris dataset decision boundary label propagation versus svm iris dataset comparison decision boundary generated iris dataset label propagation svm demonstrates label propagation learning good boundary even small amount labeled data 
4132: chapter example gallery scikitlearn user guide release 0.11 python source code plot_label_propagation_versus_svm_iris.py print __doc__ authors clay woolam clay woolam.org licence bsd import numpy import pylab sklearn import datasets sklearn import svm sklearn.semi_supervised import label_propagation rng np.random.randomstate iris datasets.load_iris iris.data iris.target step size mesh .02 y_30 np.copy y_30 rng.rand len 0.3 y_50 np.copy 2.1. examples scikitlearn user guide release 0.11 y_50 rng.rand len 0.5 create instance svm fit data scale data since want plot support vectors ls30 label_propagation.labelspreading .fit y_30 y_30 ls50 label_propagation.labelspreading .fit y_50 y_50 ls100 label_propagation.labelspreading .fit rbf_svc svm.svc kernelrbf .fit create mesh plot x_min x_max .min .max y_min y_max .min .max np.meshgrid np.arange x_min x_max np.arange y_min y_max title plots titles label spreading data label spreading data label spreading data svc rbf kernel color_map clf y_train enumerate ls30 ls50 ls100 rbf_svc plot decision boundary asign color point mesh x_min m_max y_min y_max pl.subplot clf.predict np.c_ xx.ravel yy.ravel put result color plot z.reshape xx.shape pl.contourf cmappl.cm.paired pl.axis plot also training points colors color_map y_train pl.scatter ccolors cmappl.cm.paired pl.title titles pl.text .90 unlabeled points colored white pl.show 2.1.15 support vector machines examples concerning sklearn.svm package 
4133: svm custom kernel simple usage support vector machines classify sample plot decision surface support vectors 
4134: chapter example gallery scikitlearn user guide release 0.11 figure 2.121 svm custom kernel python source code plot_custom_kernel.py print __doc__ import numpy import pylab sklearn import svm datasets import data play iris datasets.load_iris iris.data take first two features could avoid ugly slicing using twodim dataset iris.target 2.1. examples scikitlearn user guide release 0.11 def my_kernel create custom kernel y.t np.array 1.0 return np.dot np.dot y.t .02 step size mesh create instance svm fit data clf svm.svc kernelmy_kernel clf.fit plot decision boundary asign color point mesh x_min m_max y_min y_max x_min x_max .min .max y_min y_max .min .max np.meshgrid np.arange x_min x_max np.arange y_min y_max clf.predict np.c_ xx.ravel yy.ravel put result color plot z.reshape xx.shape pl.pcolormesh cmappl.cm.paired plot also training points pl.scatter cmappl.cm.paired pl.title 3class classification using support vector machine custom kernel pl.axis tight pl.show figure 2.122 plot different svm classiers iris dataset plot different svm classiers iris dataset comparison different linear svm classiers iris dataset plot decision surface four different svm classiers 
4135: chapter example gallery scikitlearn user guide release 0.11 python source code plot_iris.py print __doc__ import numpy import pylab sklearn import svm datasets import data play iris datasets.load_iris iris.data take first two features could avoid ugly slicing using twodim dataset iris.target .02 step size mesh create instance svm fit data scale data since want plot support vectors 1.0 svm regularization parameter svc svm.svc kernellinear .fit rbf_svc svm.svc kernelrbf gamma0.7 .fit poly_svc svm.svc kernelpoly degree3 .fit lin_svc svm.linearsvc .fit create mesh plot x_min x_max .min .max 2.1. examples scikitlearn user guide release 0.11 y_min y_max .min .max np.meshgrid np.arange x_min x_max np.arange y_min y_max title plots titles svc linear kernel svc rbf kernel svc polynomial degree kernel linearsvc linear kernel clf enumerate svc rbf_svc poly_svc lin_svc plot decision boundary asign color point mesh x_min m_max y_min y_max pl.subplot clf.predict np.c_ xx.ravel yy.ravel put result color plot z.reshape xx.shape pl.contourf cmappl.cm.paired pl.axis plot also training points pl.scatter cmappl.cm.paired pl.title titles pl.show figure 2.123 oneclass svm nonlinear kernel rbf oneclass svm nonlinear kernel rbf oneclass svm unsupervised algorithm learns decision function novelty detection classifying new data similar different training set 
4136: chapter example gallery scikitlearn user guide release 0.11 python source code plot_oneclass.py print __doc__ import numpy import pylab import matplotlib.font_manager sklearn import svm np.meshgrid np.linspace np.linspace generate train data 0.3 np.random.randn x_train np.r_ generate regular novel observations 0.3 np.random.randn x_test np.r_ generate abnormal novel observations x_outliers np.random.uniform low4 high4 size fit model clf svm.oneclasssvm nu0.1 kernel rbf gamma0.1 clf.fit x_train y_pred_train clf.predict x_train y_pred_test clf.predict x_test y_pred_outliers clf.predict x_outliers n_error_train y_pred_train y_pred_train .size 2.1. examples scikitlearn user guide release 0.11 n_error_test y_pred_test y_pred_test .size n_error_outliers y_pred_outliers y_pred_outliers .size plot line points nearest vectors plane clf.decision_function np.c_ xx.ravel yy.ravel z.reshape xx.shape pl.title novelty detection pl.contourf levelsnp.linspace z.min cmappl.cm.blues_r pl.contour levels linewidths2 colorsred pl.contourf levels z.max colorsorange pl.scatter x_train x_train cwhite pl.scatter x_test x_test cgreen pl.scatter x_outliers x_outliers cred pl.axis tight pl.xlim pl.ylim pl.legend a.collections learned frontier training observations new regular observations new abnormal observations loc upper left propmatplotlib.font_manager.fontproperties size11 pl.xlabel error train d200 errors novel regular d20 errors novel abnormal d20 n_error_train n_error_test n_error_outliers pl.show figure 2.124 rbf svm parameters rbf svm parameters example illustrates effect parameters gamma rbf kernel svm intuitively gamma parameter denes far inuence single training example reaches low values meaning far high values meaning close parameter trades misclassication training examples simplicity decision surface low makes decision surface smooth high aims classifying training examples correctly 
4137: chapter example gallery scikitlearn user guide release 0.11 script output best classifier svc c1000000.0 cache_size200 class_weightnone coef00.0 degree3 gamma0.0001 kernelrbf probabilityfalse shrinkingtrue tol0.001 verbosefalse python source code plot_rbf_parameters.py print __doc__ import numpy import pylab sklearn import svm sklearn.datasets import load_iris sklearn.preprocessing import scaler iris load_iris iris.data take dimensions iris.target scaler scaler scaler.fit_transform 2.1. examples scikitlearn user guide release 0.11 np.meshgrid np.linspace np.linspace np.random.seed gamma_range 1e1 1e1 c_range 1e2 1e4 pl.figure c_range gamma gamma_range fit model clf svm.svc gammagamma clf.fit plot decision function datapoint grid clf.decision_function np.c_ xx.ravel yy.ravel z.reshape xx.shape pl.subplot pl.title gamma .1f .2f gamma pl.pcolormesh cmappl.cm.jet pl.scatter cmappl.cm.jet pl.xticks pl.yticks pl.axis tight pl.subplots_adjust left0.05 right0.95 bottom0.05 top0.95 pl.show figure 2.125 svm maximum margin separating hyperplane svm maximum margin separating hyperplane plot maximum margin separating hyperplane within twoclass separable dataset using support vector machines classier linear kernel 
4138: chapter example gallery scikitlearn user guide release 0.11 python source code plot_separating_hyperplane.py print __doc__ import numpy import pylab sklearn import svm create separable points np.random.seed np.r_ np.random.randn np.random.randn fit model clf svm.svc kernellinear clf.fit get separating hyperplane clf.coef_ np.linspace clf.intercept_ plot parallels separating hyperplane pass support vectors clf.support_vectors_ 2.1. examples scikitlearn user guide release 0.11 yy_down clf.support_vectors_ yy_up plot line points nearest vectors plane pl.plot pl.plot yy_down pl.plot yy_up pl.scatter clf.support_vectors_ clf.support_vectors_ s80 facecolorsnone pl.scatter cmappl.cm.paired pl.axis tight pl.show figure 2.126 svm separating hyperplane unbalanced classes svm separating hyperplane unbalanced classes find optimal separating hyperplane using svc classes unbalanced rst separating plane plain svc plot dashed separating hyperplane automatically correction unbalanced classes 
4139: chapter example gallery scikitlearn user guide release 0.11 python source code plot_separating_hyperplane_unbalanced.py print __doc__ import numpy import pylab sklearn import svm create separable points np.random.seed n_samples_1 n_samples_2 np.r_ 1.5 np.random.randn n_samples_1 0.5 np.random.randn n_samples_2 n_samples_1 n_samples_2 fit model get separating hyperplane clf svm.svc kernellinear c1.0 clf.fit clf.coef_ np.linspace clf.intercept_ 2.1. examples scikitlearn user guide release 0.11 get separating hyperplane using weighted classes wclf svm.svc kernellinear class_weight wclf.fit wclf.coef_ wyy wclf.intercept_ plot separating hyperplanes samples pl.plot labelno weights pl.plot wyy labelwith weights pl.scatter cmappl.cm.paired pl.legend pl.axis tight pl.show figure 2.127 svmanova svm univariate feature selection svmanova svm univariate feature selection example shows perform univariate feature running svc support vector classier improve classication scores 
4140: chapter example gallery scikitlearn user guide release 0.11 python source code plot_svm_anova.py print __doc__ import numpy import pylab sklearn import svm datasets feature_selection cross_validation sklearn.pipeline import pipeline import data play digits datasets.load_digits digits.target throw away data curse dimension settings digits.data n_samples len x.reshape n_samples add noninformative features np.hstack np.random.random n_samples create featureselection transform instance svm combine together fullblown estimator transform feature_selection.selectpercentile feature_selection.f_classif 2.1. examples scikitlearn user guide release 0.11 clf pipeline anova transform svc svm.svc c1.0 plot crossvalidation score function percentile features score_means list score_stds list percentiles percentile percentiles clf.set_params anova__percentilepercentile compute crossvalidation score using cpus this_scores cross_validation.cross_val_score clf n_jobs1 score_means.append this_scores.mean score_stds.append this_scores.std pl.errorbar percentiles score_means np.array score_stds pl.title performance svmanova varying percentile features selected pl.xlabel percentile pl.ylabel prediction rate pl.axis tight pl.show figure 2.128 svmsvc support vector classication svmsvc support vector classication classication application svm used iris dataset used example decision boundaries shown points trainingset 
4141: chapter example gallery scikitlearn user guide release 0.11 python source code plot_svm_iris.py print __doc__ code source gael varoqueux modified documentation merge jaques grobler license bsd import numpy import pylab sklearn import svm datasets import data play iris datasets.load_iris iris.data take first two features iris.target .02 step size mesh clf svm.svc c1.0 kernellinear create instance svm classifier fit data clf.fit plot decision boundary asign color point mesh x_min m_max y_min y_max x_min x_max .min .max y_min y_max .min .max np.meshgrid np.arange x_min x_max np.arange y_min y_max clf.predict np.c_ xx.ravel yy.ravel put result color plot z.reshape xx.shape pl.figure figsize pl.pcolormesh cmappl.cm.paired plot also training points 2.1. examples scikitlearn user guide release 0.11 pl.scatter cmappl.cm.paired pl.xlabel sepal length pl.ylabel sepal width pl.xlim xx.min xx.max pl.ylim yy.min yy.max pl.xticks pl.yticks pl.show figure 2.129 svmkernels svmkernels three different types svmkernels displayed polynomial rbf especially useful datapoints linearly seperable 
4142: python source code plot_svm_kernels.py chapter example gallery scikitlearn user guide release 0.11 print __doc__ code source gael varoqueux license bsd import numpy import pylab sklearn import svm dataset targets np.c_ 1.5 1.4 1.3 1.2 1.1 1.2 1.2 1.5 2.1 1.3 1.2 2.4 2.3 2.7 1.3 2.1 figure number fignum fit model kernel linear poly rbf clf svm.svc kernelkernel gamma2 clf.fit plot line points nearest vectors plane pl.figure fignum figsize pl.clf pl.scatter clf.support_vectors_ clf.support_vectors_ s80 facecolorsnone zorder10 pl.scatter zorder10 cmappl.cm.paired pl.axis tight x_min x_max y_min y_max np.mgrid x_min x_max200j y_min y_max200j clf.decision_function np.c_ xx.ravel yy.ravel put result color plot 2.1. examples scikitlearn user guide release 0.11 z.reshape xx.shape pl.figure fignum figsize pl.pcolormesh cmappl.cm.paired pl.contour colors linestyles levels pl.xlim x_min x_max pl.ylim y_min y_max pl.xticks pl.yticks fignum fignum pl.show figure 2.130 svm margins example svm margins example plots illustrate effect parameter seperation line large value basically tells model much faith datas distrubution consider points close line seperation small value includes moreall observations allowing margins calculated using data area 
4143: python source code plot_svm_margin.py print __doc__ code source gael varoqueux chapter example gallery scikitlearn user guide release 0.11 modified documentation merge jaques grobler license bsd import numpy import pylab sklearn import svm create separable points np.random.seed np.r_ np.random.randn np.random.randn figure number fignum fit model name penality unreg reg 0.05 clf svm.svc kernellinear cpenality clf.fit get separating hyperplane clf.coef_ np.linspace clf.intercept_ plot parallels separating hyperplane pass support vectors margin np.sqrt np.sum clf.coef_ yy_down margin yy_up margin plot line points nearest vectors plane pl.figure fignum figsize pl.clf pl.plot pl.plot yy_down pl.plot yy_up pl.scatter clf.support_vectors_ clf.support_vectors_ s80 facecolorsnone zorder10 pl.scatter zorder10 cmappl.cm.paired pl.axis tight x_min 4.8 x_max 4.2 y_min y_max np.mgrid x_min x_max200j y_min y_max200j clf.predict np.c_ xx.ravel yy.ravel put result color plot z.reshape xx.shape pl.figure fignum figsize pl.pcolormesh cmappl.cm.paired 2.1. examples scikitlearn user guide release 0.11 pl.xlim x_min x_max pl.ylim y_min y_max pl.xticks pl.yticks fignum fignum pl.show figure 2.131 nonlinear svm nonlinear svm perform binary classication using nonlinear svc rbf kernel target predict xor inputs color map illustrates decision function learn svc 
4144: chapter example gallery scikitlearn user guide release 0.11 python source code plot_svm_nonlinear.py print __doc__ import numpy import pylab sklearn import svm np.meshgrid np.linspace np.linspace np.random.seed np.random.randn np.logical_xor fit model clf svm.nusvc clf.fit plot decision function datapoint grid clf.decision_function np.c_ xx.ravel yy.ravel z.reshape xx.shape pl.imshow interpolationnearest extent xx.min xx.max yy.min yy.max aspectauto originlower cmappl.cm.puor_r contours pl.contour levels linewidths2 2.1. examples scikitlearn user guide release 0.11 linetypes pl.scatter s30 cmappl.cm.paired pl.xticks pl.yticks pl.axis pl.show figure 2.132 seleting hyperparameter gamma rbfkernel svm seleting hyperparameter gamma rbfkernel svm svms particular kernelized svms setting hyperparameter crucial nontrivial practice usually set using holdout validation set using cross validation example shows use stratied kfold crossvalidation set gamma rbfkernel svm use logarithmic grid parameters 
4145: chapter example gallery scikitlearn user guide release 0.11 script output best classifier svc c1000000.0 cache_size200 class_weightnone coef00.0 degree3 gamma0.0001 kernelrbf probabilityfalse shrinkingtrue tol0.001 verbosefalse python source code plot_svm_parameters_selection.py print __doc__ import numpy import pylab sklearn.svm import svc sklearn.preprocessing import scaler sklearn.datasets import load_iris sklearn.cross_validation import stratifiedkfold sklearn.grid_search import gridsearchcv iris_dataset load_iris iris_dataset.data iris_dataset.target usually good idea scale data svm training cheating bit example scaling data instead fitting transformation trainingset 2.1. examples scikitlearn user guide release 0.11 applying test set 
4146: scaler scaler scaler.fit_transform initial search logarithmic grid basis often helpful using basis finer tuning achieved much higher cost 
4147: c_range np.arange gamma_range np.arange param_grid dict gammagamma_range cc_range grid gridsearchcv svc param_gridparam_grid cvstratifiedkfold grid.fit print best classifier grid.best_estimator_ plot scores grid grid_scores_ contains parameter settings scores score_dict grid.grid_scores_ extract scores scores score_dict scores np.array scores .reshape len c_range len gamma_range make nice figure pl.figure figsize pl.subplots_adjust left0.15 right0.95 bottom0.15 top0.95 pl.imshow scores interpolationnearest cmappl.cm.spectral pl.xlabel gamma pl.ylabel pl.colorbar pl.xticks np.arange len gamma_range gamma_range rotation45 pl.yticks np.arange len c_range c_range pl.show figure 2.133 support vector regression svr using linear nonlinear kernels support vector regression svr using linear nonlinear kernels toy example regression using linear polynominial rbf kernels 
4148: chapter example gallery scikitlearn user guide release 0.11 python source code plot_svm_regression.py print __doc__ generate sample data import numpy np.sort np.random.rand axis0 np.sin .ravel add noise targets 0.5 np.random.rand fit regression model sklearn.svm import svr svr_rbf svr kernelrbf c1e3 gamma0.1 svr_lin svr kernellinear c1e3 svr_poly svr kernelpoly c1e3 degree2 y_rbf svr_rbf.fit .predict y_lin svr_lin.fit .predict y_poly svr_poly.fit .predict 2.1. examples scikitlearn user guide release 0.11 look results import pylab pl.scatter labeldata pl.hold pl.plot y_rbf labelrbf model pl.plot y_lin labellinear model pl.plot y_poly labelpolynomial model pl.xlabel data pl.ylabel target pl.title support vector regression pl.legend pl.show figure 2.134 svm weighted samples svm weighted samples plot decision function weighted dataset size points proportional weight 
4149: chapter example gallery scikitlearn user guide release 0.11 python source code plot_weighted_samples.py print __doc__ import numpy import pylab sklearn import svm create points np.random.seed np.r_ np.random.randn np.random.randn sample_weight np.abs np.random.randn assign bigger weight last samples sample_weight fit model clf svm.svc clf.fit sample_weightsample_weight plot decision function np.meshgrid np.linspace np.linspace clf.decision_function np.c_ xx.ravel yy.ravel z.reshape xx.shape 2.1. examples scikitlearn user guide release 0.11 plot line points nearest vectors plane pl.contourf alpha0.75 pl.scatter ssample_weight alpha0.9 cmappl.cm.bone pl.axis pl.show 2.1.16 decision trees examples concerning sklearn.tree package 
4150: figure 2.135 plot decision surface decision tree iris dataset plot decision surface decision tree iris dataset plot decision surface decision tree trained pairs features iris dataset pair iris features decision tree learns decision boundaries made combinations simple thresholding rules inferred training samples 
4151: chapter example gallery scikitlearn user guide release 0.11 python source code plot_iris.py print __doc__ import numpy import pylab sklearn.datasets import load_iris sklearn.tree import decisiontreeclassifier parameters n_classes plot_colors bry plot_step 0.02 load data iris load_iris pairidx pair enumerate take two corresponding features iris.data pair iris.target shuffle idx np.arange x.shape 2.1. examples scikitlearn user guide release 0.11 np.random.seed np.random.shuffle idx idx idx standardize mean x.mean axis0 std x.std axis0 mean std train clf decisiontreeclassifier .fit plot decision boundary pl.subplot pairidx x_min x_max .min .max y_min y_max .min .max np.meshgrid np.arange x_min x_max plot_step np.arange y_min y_max plot_step clf.predict np.c_ xx.ravel yy.ravel z.reshape xx.shape pl.contourf cmappl.cm.paired pl.xlabel iris.feature_names pair pl.ylabel iris.feature_names pair pl.axis tight plot training points color zip xrange n_classes plot_colors idx np.where pl.scatter idx idx ccolor labeliris.target_names cmappl.cm.paired pl.axis tight pl.suptitle decision surface decision tree using paired features pl.legend pl.show figure 2.136 decision tree regression decision tree regression regression decision trees decision tree used sine curve addition noisy observation result learns local linear regressions approximating sine curve 
4152: chapter example gallery see maximum depth tree controled max_depth parameter set high decision trees learn details training data learn noise i.e overt 
4153: scikitlearn user guide release 0.11 python source code plot_tree_regression.py print __doc__ import numpy create random dataset rng np.random.randomstate np.sort rng.rand axis0 np.sin .ravel 0.5 rng.rand fit regression model sklearn.tree import decisiontreeregressor clf_1 decisiontreeregressor max_depth2 clf_2 decisiontreeregressor max_depth5 clf_1.fit clf_2.fit predict x_test np.arange 0.0 5.0 0.01 np.newaxis y_1 clf_1.predict x_test 2.1. examples scikitlearn user guide release 0.11 y_2 clf_2.predict x_test plot results import pylab pl.figure pl.scatter label data pl.plot x_test y_1 label max_depth2 linewidth2 pl.plot x_test y_2 label max_depth5 linewidth2 pl.xlabel data pl.ylabel target pl.title decision tree regression pl.legend pl.show chapter example gallery chapter three development 3.1 contributing project community effort everyone welcome contribute project hosted http github.comscikitlearnscikitlearn 3.1.1 submitting bug report case experience issues using package hesitate submit ticket bug tracker also welcome post feature requests links pull requests 
4154: 3.1.2 retrieving latest code use git version control github hosting main repository check latest sources command git clone git github.comscikitlearnscikitlearn.git write privileges git clone git github.com scikitlearnscikitlearn.git run development version cumbersome reinstall package time update sources thus preferred add scikitlearn directory pythonpath build extension place python setup.py build_ext inplace unixlike systems simply type make toplevel folder build inplace launch tests look makefile additional utilities 
4155: 3.1.3 contributing code note avoid duplicating work highly advised contact developers mailing list starting work nontrivial feature https lists.sourceforge.netlistslistinfoscikitlearngeneral scikitlearn user guide release 0.11 contribute preferred way contribute scikitlearn fork main repository github create account github already one fork project repository click fork button near top page creates copy code account github server 
4156: clone copy local disk git clone git github.com yourloginscikitlearn.git work copy computer using git version control git add modified_files git commit git push origin master 
4157: changes trivial xes better directly work branch name feature working case replace step step create branch host changes publish public repo git checkout myfeature git add modified_files git commit git push origin myfeature ready pushed changes github repo web page repo click pull request send pull request send email committers might also send email mailing list order get visibility 
4158: setup origin remote repository points yourloginscikitlearn.git 
4159: note wish fetchmerge main repository instead forked one need add another remote use instead origin choose name upstream command git remote add upstream git github.com scikitlearnscikitlearn.git seems like magic look git documentation web recommended check contribution complies following rules submitting pull request follow codingguidelines see applicable use validation tools code sklearn.utils submodule list utility routines available developers found utilities developers page 
4160: public methods informative docstrings sample usage presented doctests appropri ate 
4161: tests pass everything rebuilt scratch unixlike systems check toplevel source folder make adding additional functionality provide least one example script examples folder look examples reference examples demonstrate new functionality useful practice possible compare methods available scikitlearn 
4162: chapter development scikitlearn user guide release 0.11 least one paragraph narrative documentation links references literature pdf links possible example documentation also include expected time space complexity algorithm scalability e.g algorithm scale large number samples scale dimensionality n_features expected lower 100. build documentation see documentation section 
4163: also check common programming errors following tools code good unittest coverage least check pip install nose coverage nosetests withcoverage pathtotests_for_package pyakes warnings check pip install pyflakes pyflakes pathtomodule.py pep8 warnings check pip install pep8 pep8 pathtomodule.py autopep8 help easy redundant errors pip install autopep8 autopep8 pathtopep8.py bonus points contributions include performance analysis benchmark script proling output please report mailing list github wiki also check optimize speed guide details proling cython optimizations 
4164: note current state scikitlearn code base compliant guidelines expect enforcing constraints new contributions get overall code base quality right direction 
4165: easyfix issues great way start contributing scikitlearn pick item list easyfix issues issue tracker resolving issues allow start contributing project without much prior knowledge assistance area greatly appreciated experienced developers helps free time concentrate issues 
4166: documentation glad accept sort documentation function docstrings restructuredtext documents like one tutorials etc restructuredtext documents live source code repository doc directory edit documentation using text editor generate html output typing make html doc directory alternatively make htmlnoplot used quickly generate documentation without example gallery resulting html les placed _buildhtml viewable web browser see readme doc directory information building documentation need sphinx matplotlib 
4167: 3.1. contributing scikitlearn user guide release 0.11 writing documentation important keep good compromise mathematical algorith mic details give intuition reader algorithm best always start small paragraph handwaiving explanation method data gure coming example illustrat ing 
4168: warning sphinx version best documentation build many version sphinx possible different versions tend behave slightly differently get best results use version 1.0 
4169: developers web site information found developers wiki 
4170: 3.1.4 ways contribute code way contribute scikitlearn instance documentation also important part project often doesnt get much attention deserves typo documentation made improvements hesitate send email mailing list submit github pull request full documentation found doc directory also helps spread word reference project blog articles link website simply say use 3.1.5 coding guidelines following guidelines new code written course special cases exceptions rules however following rules submitting new code makes review easier new code integrated less time uniformly formatted code makes easier share code ownership scikitlearn project tries closely follow ofcial python guidelines detailed pep8 detail code formatted indented please read follow addition add following guidelines use underscores separate words non class names n_samples rather nsamples avoid multiple statements one line prefer line return control statement iffor use relative imports references inside scikitlearn please dont use import case considered harmful ofcial python recommendations makes code harder read origin symbols longer explicitly referenced important prevents using static analysis tool like pyakes automatically bugs scikitlearn 
4171: use numpy docstring standard docstrings 
4172: good example code like found 
4173: input validation module sklearn.utils contains various functions input validation conversion sometimes np.asarray sufces validation use np.asanyarray np.atleast_2d since let numpys np.matrix different api e.g. means dot product np.matrix hadamard product np.ndarray 
4174: chapter development scikitlearn user guide release 0.11 cases sure call safe_asarray atleast2d_or_csr as_float_array array2d arraylike argument passed scikitlearn api function exact function use depends mainly whether scipy.sparse matrices must accepted information refer utilities developers page 
4175: random numbers code depends random number generator use numpy.random.random similar routines ensure repeatability error checking routine accept keyword random_state use con struct numpy.random.randomstate object see sklearn.utils.check_random_state utilities developers heres simple example code using guidelines sklearn.utils import array2d check_random_state def choose_random_sample random_state0 choose random point parameters arraylike shape n_samples n_features array representing data random_state randomstate int seed default random number generator instance define state random permutations generator 
4176: returns numpy array shape n_features random point selected array2d random_state check_random_state random_state random_state.randint x.shape return 3.1.6 apis scikitlearn objects uniform api try common basic api objects addition avoid proliferation framework code try adopt simple conventions limit minimum number methods object must implement 
4177: different objects main objects scikitlearn one class implement multiple interfaces estimator base object implements estimator obj.fit data predictor supervised learning unsupervised problems implements 3.1. contributing scikitlearn user guide release 0.11 prediction obj.predict data transformer ltering modifying data supervised unsupervised way implements new_data obj.transform data tting transforming performed much efciently together separately implements new_data obj.fit_transform data model model give goodness likelihood unseen data implements higher better score obj.score data estimators api one predominant object estimator estimator object model based training data capable inferring properties new data instance classier regressor estimators implement method estimator.fit builtin estimators also set_params method sets dataindependent parameters overriding previ ous parameter values passed __init__ method required object estimator estimators inherit sklearn.base.baseestimator 
4178: instantiation concerns creation object objects __init__ method might accept constants arguments determine estimators behavior like constant svms however take actual training data argument left fit method clf2 svc c2.3 clf3 svc wrong arguments accepted __init__ keyword arguments default value words user able instantiate estimator without passing arguments arguments correspond hyperparameters describing model optimisation problem estimator tries solve addition every keyword argument accepted __init__ correspond attribute instance scikitlearn relies relevant attributes set estimator model selection summarize __init__ look like def __init__ self param11 param22 self.param1 param1 self.param2 param2 logic parameters changed corresponding logic put parameters used following wrong def __init__ self param11 param22 param33 wrong parameters modified param1 chapter development scikitlearn user guide release 0.11 param2 self.param1 param1 wrong objects attributes exactly name argument constructor self.param3 param2 scikitlearn relies mechanism introspect objects set parameters crossvalidation 
4179: fitting next thing probably want estimate parameters model implemented fit method fit method takes training data arguments one array case unsupervised learning two arrays case supervised learning note model tted using object holds reference however exceptions case precomputed kernels data must stored use predict method 
4180: parameters kwargs arraylike shape number samples number features array shape number samples optional datadependent parameters 
4181: x.shape y.shape requisite met exception type valueerror raised might ignored case unsupervised learning however make possible use estimator part pipeline mix supervised unsupervised transformers even unsupervised estimators kindly asked accept ynone keyword argument second position ignored estimator method return object self pattern useful able implement quick one liners ipython session y_predicted svc c100 .fit x_train y_train .predict x_test depending nature algorithm fit sometimes also accept additional keywords arguments however parameter value assigned prior access data __init__ keyword argument parameters restricted directly data dependent variables instance gram matrix afnity matrix precomputed data matrix data dependent tolerance stopping criterion tol directly data dependent although optimal value according scoring function probably attribute ends expected overridden call fit second time without taking previous value account idempotent 
4182: optional arguments iterative algorithms number iterations specied integer called n_iter 
4183: unresolved api issues things must still decided happen predict called fit exception raised shape arrays match fit 3.1. contributing scikitlearn user guide release 0.11 working notes unresolved issues todos remarks ongoing work developers advised maintain notes github wiki 
4184: specic models linear models coefcients stored array called coef_ independent term stored intercept_ 
4185: 3.2 optimize speed following gives practical guidelines help write efcient code scikitlearn project 
4186: note always useful prole code check performance assumptions also highly recommended review literature ensure implemented algorithm state art task investing costly implementation optimization times times hours efforts invested optimizing complicated implementation details rended irrele vant late discovery simple algorithmic tricks using another algorithm altogether better suited problem section sample algorithmic trick warm restarts cross validation gives example trick 
4187: 3.2.1 python cython general scikitlearn project emphasizes readability source code make easy project users dive source code understand algorithm behaves data also ease maintanability developers implementing new algorithm thus recommended start implementing python using numpy scipy taking care avoiding looping code using vectorized idioms libraries practice means trying replace nested loops calls equivalent numpy array methods goal avoid cpu wasting time python interpreter rather crunching numbers statistical model sometimes however algorithm expressed efciently simple vectorized numpy code case recommended strategy following prole python implementation main bottleneck isolate dedicated module level func tion function reimplemented compiled extension module 
4188: exists well maintained bsd mit implementation algorithm big write cython wrapper include copy source code library scikitlearn source tree strategy used classes svm.linearsvc svm.svc linear_model.logisticregression wrappers liblinear libsvm 
4189: otherwise write optimized version python function using cython directly strategy used linear_model.elasticnet linear_model.sgdclassifier classes instance 
4190: move python version function tests use check results compiled extension consistent gold standard easy debug python version 
4191: code optimized simple bottleneck spottable proling check whether possible coarse grained parallelism amenable multiprocessing using joblib.parallel class 
4192: chapter development scikitlearn user guide release 0.11 using cython include generated source code alongside cython source code goal make possible install scikit machine python numpy scipy compiler 
4193: 3.2.2 proling python code order prole python code recommend write script loads prepare data use ipython integrated proler interactively exploring relevant part code suppose want prole non negative matrix factorization module scikit let setup new ipython session load digits dataset recognizing handwritten digits example sklearn.decomposition import nmf sklearn.datasets import load_digits load_digits .data starting proling session engaging tentative optimization iterations important measure total execution time function want optimize without kind proler overhead save somewhere later reference timeit nmf n_components16 tol1e2 .fit loops best 1.7 per loop look overall performance prole using prun magic command prun nmf.py nmf n_components16 tol1e2 .fit function calls 1.682 cpu seconds ordered internal time list reduced due restriction nmf.py ncalls tottime 0.609 0.157 0.053 0.008 0.006 0.001 0.001 0.000 0.000 percall cumtime percall filename lineno function 0.017 0.000 0.053 0.000 0.006 0.000 0.000 0.000 0.000 1.499 0.157 1.681 0.057 0.047 0.010 0.001 0.000 1.681 0.042 nmf.py151 _nls_subproblem 0.000 nmf.py18 _pos 1.681 nmf.py352 fit_transform 0.000 nmf.py28 norm 0.047 nmf.py42 _initialize_nmf 0.000 nmf.py36 _sparseness 0.000 nmf.py23 _neg 0.000 nmf.py337 __init__ 1.681 nmf.py461 fit totime columns interesting gives total time spent executing code given function ignoring time spent executing subfunctions real total time local code subfunction calls given cumtime column note use nmf.py restricts output lines contains nmf.py string useful quick look hotspot nmf python module ignoring anything else begining output command without nmf.py lter prun nmf n_components16 tol1e2 .fit function calls 1.840 cpu seconds ordered internal time ncalls tottime 0.653 percall cumtime percall filename lineno function 0.000 numpy.core._dotblas.dot 0.000 0.653 3.2. optimize speed scikitlearn user guide release 0.11 0.651 0.171 0.167 0.064 0.043 0.019 0.011 0.010 0.009 0.014 0.000 0.000 0.064 0.000 0.000 0.000 0.005 0.000 1.636 0.171 0.167 1.840 0.043 0.019 0.181 0.010 0.065 0.036 nmf.py151 _nls_subproblem 0.000 nmf.py18 _pos 0.000 method sum numpy.ndarray objects 1.840 nmf.py352 fit_transform 0.000 method flatten numpy.ndarray objects 0.000 method numpy.ndarray objects 0.000 fromnumeric.py1185 sum 0.005 numpy.linalg.lapack_lite.dgesdd 0.000 nmf.py28 norm 
4194: results show execution largely dominated dot products operations delegated blas hence probably huge gain expect rewriting code cython case 1.7s total execution time almost 0.7s spent compiled code consider optimal rewriting rest python code assuming could achieve boost portion highly unlikely given shallowness python loops would gain 2.4x speedup globally hence major improvements achieved algorithmic improvements particular example e.g trying operation costly useless avoid computing rather trying optimize implementation however still interesting check whats happening inside _nls_subproblem function hotspot consider python code takes around cumulated time module order better understand prole specic function let install lineprof wire ipython pip install lineprofiler ipython 0.10 edit .ipythonipy_user_conf.py ensure following lines present import ipython.ipapi ipython.ipapi.get towards end dene lprun magic import line_profiler ip.expose_magic lprun line_profiler.magic_lprun ipython 0.11 rst create conguration prole ipython profile create create named .ipythonextensionsline_profiler_ext.py following con tent import line_profiler def load_ipython_extension ip.define_magic lprun line_profiler.magic_lprun register .ipythonprofile_defaultipython_config.py c.terminalipythonapp.extensions line_profiler_ext c.interactiveshellapp.extensions line_profiler_ext chapter development register lprun magic command ipython terminal application frontends qtconsole notebook 
4195: scikitlearn user guide release 0.11 restart ipython let use new toy sklearn.datasets import load_digits sklearn.decomposition.nmf import _nls_subproblem nmf load_digits .data lprun _nls_subproblem nmf n_components16 tol1e2 .fit timer unit 1e06 file sklearndecompositionnmf.py function _nls_subproblem line total time 1.73153 line time line contents per hit hits time ... 
4196: def _nls_subproblem h_init tol max_iter nonnegative least square solver 122.1 0.3 h_init .any raise valueerror negative values h_init passed nls solver 2.9 2336.3 336.3 3.0 2.4 2.9 305.9 777.1 3.8 2.7 3.0 56.9 131.8 33.1 102.0 349.7 0.0 5.8 0.8 0.0 0.0 0.1 10.2 25.9 0.1 0.0 0.2 4.4 10.1 2.5 7.8 26.9 h_init wtv np.dot w.t wtw np.dot w.t values justified paper alpha beta 0.1 n_iter xrange max_iter grad np.dot wtw wtv proj_gradient norm grad np.logical_or grad proj_gradient tol break inner_iter xrange alpha grad np.where _pos gradd np.sum grad dqd np.sum np.dot wtw looking top values time column really easy pinpoint expensive expressions would deserve additional care 
4197: 3.2.3 performance tips cython developer proling python code reveals python interpreter overhead larger one order magnitude cost actual numerical computation e.g loops vector components nested evaluation conditional expression scalar arithmetics ... probably adequate extract hotspot portion code 3.2. optimize speed scikitlearn user guide release 0.11 standalone function .pyx add static type declarations use cython generate program suitable compiled python extension module ofcial documentation available http docs.cython.org contains tutorial reference guide developing module following highlight couple tricks found important practice existing cython codebase scikitlearn project todo html report type declarations bound checks division zero checks memory alignement direct blas calls.. 
4198: http www.euroscipy.orgle3696 viddownload http conference.scipy.orgproceedingsscipy2009paper_1 http conference.scipy.orgproceedingsscipy2009paper_2 3.2.4 proling compiled extensions working compiled extensions written wrapper directly cython extension default python proler useless need dedicated tool instrospect whats happening inside compiled extension self order prole compiled python extensions one could use gprof recompiled project gcc using pythondbg variant interpreter debian ubuntu however approach requires also numpy scipy recompiled rather complicated get working fortunately exist two alternative prolers dont require recompile everything 
4199: using googleperftools todo https github.comfabianpyep http fseoane.netblog2011aprolerforpythonextensions note googleperftools provides nice line line report mode triggered lines option however seem work correctly time writing issue tracked project issue tracker 
4200: using valgrind callgrind kcachegrind todo 3.2.5 multicore parallelism using joblib.parallel todo give simple teaser example checkout ofcial joblib documentation http packages.python.orgjoblib 3.2.6 sample algorithmic trick warm restarts cross validation todo demonstrate warm restart tricks cross validation linear regression coordinate descent 
4201: chapter development scikitlearn user guide release 0.11 3.3 utilities developers scikitlearn contains number utilities help development located sklearn.utils include tools number categories following functions classes module sklearn.utils 
4202: warning utilities meant used internally within scikitlearn package guaran teed stable versions scikitlearn backports particular removed scikitlearn dependencies evolve 
4203: 3.3.1 validation tools tools used check validate input write function accepts arrays matrices sparse matrices arguments following used applicable 
4204: assert_all_finite throw error array contains nans infs safe_asarray convert input array sparse matrix equivalent np.asarray sparse matrices passed 
4205: as_float_array convert input array oats sparse matrix passed sparse matrix returned 
4206: array2d equivalent np.atleast_2d order dtype input maintained atleast2d_or_csr equivalent array2d sparse matrix passed convert csr format 
4207: also calls assert_all_finite 
4208: check_arrays check input arrays consistent rst dimensions work arbitrary number arrays 
4209: warn_if_not_float warn input oatingpoint value input assumed x.dtype code relies random number generator never use functions like numpy.random.random numpy.random.normal instead numpy.random.randomstate object used built random_state argument passed class function function check_random_state used create random number generator object 
4210: approach lead repeatability issues unit tests 
4211: check_random_state create np.random.randomstate object parameter random_state random_state none np.random randomlyinitialized randomstate object turned 
4212: random_state integer used seed new randomstate object random_state randomstate object passed 
4213: example sklearn.utils import check_random_state random_state random_state check_random_state random_state random_state.rand array 0.5488135 0.71518937 0.60276338 0.54488318 3.3. utilities developers scikitlearn user guide release 0.11 3.3.2 efcient linear algebra array operations extmath.randomized_range_finder construct orthonormal matrix whose range approximates range input used extmath.randomized_svd 
4214: extmath.randomized_svd compute ktruncated randomized svd algorithm nds exact truncated singular values decomposition using randomization speed computations particularly fast large matrices wish extract small number components 
4215: arrayfuncs.cholesky_delete used sklearn.linear_model.least_angle.lars_path remove item cholesky factorization 
4216: arrayfuncs.min_pos used sklearn.linear_model.least_angle find minimum positive values within array 
4217: extmath.norm computes euclidean vector norm directly calling blas nrm2 function stable scipy.linalg.norm see fabians blog post discussion 
4218: extmath.fast_logdet efciently compute log determinant matrix extmath.density efciently compute density sparse vector extmath.safe_sparse_dot dot product correctly handle scipy.sparse inputs inputs dense equivalent numpy.dot 
4219: extmath.logsumexp compute sum assuming log domain equivalent calling np.log np.sum np.exp robust overowunderow errors note similar functionality np.logaddexp.reduce pairwise nature routine slower large arrays scipy similar routine scipy.misc.logsumexp scipy versions 0.10 found scipy.maxentropy.logsumexp scipy version accept axis keyword 
4220: extmath.weighted_mode extension scipy.stats.mode allows item real valued weight 
4221: resample resample arrays sparse matrices consistent way used shuffle shuffle shufe arrays sparse matrices consistent way used sklearn.cluster.k_means 
4222: 3.3.3 efcient routines sparse matrices sklearn.utils.sparsefuncs cython module hosts compiled extensions efciently process scipy.sparse data 
4223: sparsefuncs.mean_variance_axis0 compute means variances along axis csr matrix 
4224: used normalizing tolerance stopping criterion sklearn.cluster.k_means_.kmeans 
4225: sparsefuncs.inplace_csr_row_normalize_l1 sparsefuncs.inplace_csr_row_normalize_l2 unit norm done used normalize sklearn.preprocessing.normalizer 
4226: individual sparse samples sparsefuncs.inplace_csr_column_scale used multiply columns csr trix constant scale one scale per column used scaling features unit standard deviation sklearn.preprocessing.scaler 
4227: 3.3.4 graph routines graph.single_source_shortest_path_length currently used scikitlearn return shortest path single source connected nodes graph code adapted networkx 
4228: chapter development scikitlearn user guide release 0.11 ever needed would far faster use single iteration dijkstras algorithm graph_shortest_path 
4229: graph.graph_laplacian used sklearn.cluster.spectral.spectral_embedding turn laplacian given graph specialized code dense sparse connectivity matrices graph_shortest_path.graph_shortest_path used class sklearn.manifold.isomap return shortest path pairs connected points directed undirected graph floyd warshall algorithm dijkstras algorithm available algorithm efcient connectivity matrix scipy.sparse.csr_matrix 
4230: 3.3.5 backports fixes.counter partial backport collections.counter python 2.7 used sklearn.feature_extraction.text 
4231: fixes.unique backport np.unique numpy 1.4 find unique entries array numpy versions 1.4 np.unique less exible used sklearn.cross_validation 
4232: fixes.copysign backport np.copysign numpy 1.4 change sign elementwise 
4233: fixes.in1d backport np.in1d numpy 1.4 
4234: element used sklearn.datasets.twenty_newsgroups test whether array sklearn.feature_extraction.image 
4235: second array 
4236: fixes.savemat backport scipy.io.savemat scipy 0.7.2 save array matlabformat 
4237: earlier versions keyword oned_as available 
4238: fixes.count_nonzero backport np.count_nonzero numpy 1.6 count nonzero ele ments matrix used tests sklearn.linear_model 
4239: arrayfuncs.solve_triangular sklearn.linear_model.omp sklearn.gaussian_process 
4240: backported independent backports used scipy sklearn.mixture.gmm v0.9 sparsetools.cs_graph_components backported scipy.sparse.cs_graph_components sklearn.cluster.hierarchical well tests scipy sklearn.feature_extraction 
4241: used 0.9 
4242: arpack arpack.eigs backported scipy.sparse.linalg.eigs scipy 0.10 sparse nonsymmetric eigenvalue decomposition using arnoldi method limited version eigs available earlier scipy versions 
4243: arpack.eigsh backported scipy.sparse.linalg.eigsh scipy 0.10 sparse nonsymmetric eigenvalue decomposition using arnoldi method limited version eigsh available earlier scipy versions 
4244: arpack.svds backported scipy.sparse.linalg.svds scipy 0.10 sparse nonsymmetric eigenvalue decomposition using arnoldi method limited version svds available earlier scipy versions 
4245: 3.3. utilities developers scikitlearn user guide release 0.11 benchmarking bench.total_seconds backported timedelta.total_seconds python 2.7 used benchmarksbench_glm.py 
4246: 3.3.6 testing functions testing.assert_in testing.assert_not_in assertions container membership designed forward compatibility nose 1.0 
4247: mock_urllib2 object mocks urllib2 module fake requests mldata used tests sklearn.datasets 
4248: 3.3.7 helper functions gen_even_slices generator create npacks slices going 
4249: sklearn.decomposition.dict_learning sklearn.cluster.k_means 
4250: used arraybuilder.arraybuilder helper class incrementally build numpy.ndarray currently used sklearn.datasets._svmlight_format.pyx 
4251: safe_mask helper function convert mask format expected numpy array scipy sparse matrix use sparse matrices support integer indices numpy arrays support boolean masks integer indices 
4252: 3.3.8 hash functions murmurhash3_32 provides python wrapper murmurhash3_x86_32 non cryptographic hash function hash function suitable implementing lookup tables bloom lters count min sketch feature hashing implicitly dened sparse random projections sklearn.utils import murmurhash3_32 murmurhash3_32 feature seed0 murmurhash3_32 feature seed0 positivetrue 3910350737l sklearn.utils.murmurhash module also cimported cython modules benet high performance murmurhash skipping overhead python interpreter 
4253: 3.3.9 warnings exceptions deprecated decorator mark function class deprecated convergencewarning custom warning catch sklearn.covariance.graph_lasso 
4254: convergence problems 
4255: used chapter development scikitlearn user guide release 0.11 3.4 developers tips debugging 3.4.1 memory errors debugging cython valgrind pythonnumpys builtin memory management relatively robust lead performance penalties routines reason much highperformance code scikitlearn written cython performance gain comes tradeoff however easy memory bugs crop cython code especially situations code relies heavily pointer arithmetic memory errors manifest number ways easiest ones debug often segmentation faults related glibc errors uninitialized variables lead unexpected behavior difcult track useful tool debugging sorts errors valgrind valgrind commandline tool trace memory errors variety code follow steps install valgrind system download python valgrind suppression valgrindpython.supp follow directions readme.valgrind customize python suppressions dont spurious output coming related python interpreter instead code 
4256: run valgrind follows valgrind suppressionsvalgrindpython.supp python my_test_script.py result list memoryrelated errors reference lines ccode generated cython .pyx examine referenced lines see comments indicate corresponding location .pyx source hopefully output give clues source memory error information valgrind array options see tutorials documentation valgrind web site 
4257: 3.5 community effort many people contributed years 
4258: 3.5.1 history project started google summer code project david cournapeau later year matthieu brucher started work project part thesis fabian pedregosa gael varoquaux alexandre gramfort vincent michel inria took leadership project made rst public release february 1st 2010. since several releases appeard following month cycle striving international community leading development 
4259: 3.5.2 people david cournapeau fred mailhot david cooke david huard dave morrill 3.4. developers tips debugging scikitlearn user guide release 0.11 schoeld eric jones jarrod millman matthieu brucher travis oliphant pearu peterson fabian pedregosa maintainer gael varoquaux jake vanderplas alexandre gramfort olivier grisel bertrand thirion vincent michel chris filo gorgolewski angel soler gollonet yaroslav halchenko ron weiss virgile fritsch mathieu blondel peter prettenhofer vincent dubourg alexandre passos vlad niculae edouard duchesnay thouis ray jones lars buitinck paolo losi nelle varoquaux brian holt robert layton gilles louppe andreas mller satra ghosh forgot anyone hesitate send email fabian.pedregosa inria.fr ill include list 
4260: 3.5.3 citing scikitlearn use scikitlearn scientic publication would appreciate citations following paper scikitlearn machine learning python pedregosa al. jmlr 2011. bibtex entry article scikitlearn title scikitlearn machine learning python author pedregosa varoquaux gramfort michel 
4261: thirion grisel blondel prettenhofer weiss dubourg vanderplas passos cournapeau brucher perrot duchesnay journal journal machine learning research volume pages year chapter development scikitlearn user guide release 0.11 3.5.4 funding inria actively supports project provided funding fabian pedregosa work project full time period 20102012. also hosts coding sprints events 
4262: google sponsored david cournapeau summer code scholarship summer vlad niculae 2011. would like participate next google summer code program please see page neurodebian project providing debian packaging contributions supported dr. james haxby dart mouth college 
4263: 3.6 support several ways get touch developers 
4264: 3.6.1 mailing list main mailing list scikitlearngeneral also commit list scikitlearncommits updates main repository get notied 
4265: 3.6.2 bug tracker think youve encoutered bug please report issue tracker https github.comscikitlearnscikitlearnissues 3.6.3 irc developers like hang channel scikitlearn irc.freenode.net irc client behind rewall web client works http webchat.freenode.net 3.6.4 documentation resources documentation relative 0.11. documentation versions found development version 0.10 0.9 0.8 0.7 0.6 0.5 printable pdf documentation versions found 
4266: 3.6. support scikitlearn user guide release 0.11 3.7 0.11 3.7.1 changelog highlights hofer scott white dictbased simple gradient boosted regression trees gradient tree boosting classication regression peter pretten feature_extraction.dictvectorizer lars buitinck 
4267: feature loader support categorical variables added matthews correlation coefcient metrics.matthews_corrcoef added macro micro erage options metrics.precision_score metrics.recall_score metrics.f1_score satrajit ghosh 
4268: bag estimates generalization error ensemble methods andreas mller randomized sparse models randomized sparse linear models feature selection alexandre gramfort gael varoquaux label propagation semisupervised learning clay woolam note semisupervised api still work progress may change 
4269: added bicaic model selection classical gaussian mixture models unied api remainder scikitlearn bertrand thirion added sklearn.cross_validation.stratifiedshufflesplit sklearn.cross_validation.shufflesplit balanced splits yannick schwartz 
4270: sklearn.neighbors.nearestcentroid classier added along shrink_threshold param eter implements shrunken centroid classication robert layton 
4271: changes merged dense sparse implementations stochastic gradient descent module exposed utility extension types sequential datasets seq_dataset weight vectors weight_vector peter prettenhofer 
4272: added partial_t support onlineminibatch learning warm_start stochastic gradient descent module mathieu blondel 
4273: dense sparse implementations support vector machines classes linear_model.logisticregression merged lars buitinck 
4274: regressors used base estimator multiclass multilabel algorithms module mathieu blondel 
4275: added n_jobs option metrics.pairwise.pairwise_kernels parallel computation mathieu blondel 
4276: metrics.pairwise.pairwise_distances kmeans run parallel using n_jobs argument either kmeans kmeans robert layton improved crossvalidation evaluating estimator performance grid search setting estimator parame ters documentation introduced new cross_validation.train_test_split helper function olivier grisel svm.svc members coef_ intercept_ changed sign consistency decision_function kernellinear coef_ xed onevsone case andreas mller 
4277: chapter development scikitlearn user guide release 0.11 performance improvements efcient leaveoneout crossvalidated ridge regression esp n_samples n_features case linear_model.ridgecv reuben fletchercostin 
4278: refactoring simplication text feature extraction api xed bug caused possible negative idf olivier grisel 
4279: beam pruning option _basehmm module removed since difcult cythonize interested contributing cython version use python version git history reference 
4280: classes nearest neighbors support arbitrary minkowski metric nearest neighbors searches metric specied argument 
4281: 3.7.2 api changes summary covariance.ellipticenvelop deprecated please use covariance.ellipticenvelope instead 
4282: neighborsclassier neighborsregressor gone module nearest neighbors 
4283: use classes kneighborsclassifier radiusneighborsclassifier kneighborsregressor andor radiusneighborsregressor instead 
4284: sparse classes stochastic gradient descent module deprecated mixture.gmm mixture.dpgmm mixture.vbgmm parameters must passed object initialising fit fit accept data input parameter 
4285: methods rvs decode gmm module deprecated sample score predict used instead attribute _scores _pvalues univariate feature selection objects deprecated scores_ pvalues_ used instead 
4286: logisticregression linearsvc svc nusvc class_weight parameter initializa tion parameter parameter makes grid searches parameter possible 
4287: lfw data always shape n_samples n_features consistent olivetti faces dataset use images pairs attribute access natural images shapes instead 
4288: svm.linearsvc meaning multi_class parameter changed options ovr cram mer_singer ovr default change default behavior hopefully less confusing 
4289: classs feature_selection.text.vectorizer feature_selection.text.tfidfvectorizer 
4290: deprecated replaced preprocessor analyzer nested structure text feature extraction removed features directly passed constructor arguments feature_selection.text.tfidfvectorizer feature_selection.text.countvectorizer particular following parameters used analyzer word char switch default analysis scheme use specic python callable previously 
4291: tokenizer preprocessor introduced make still possible customize steps new api 
4292: input explicitly control interpret sequence passed fit predict lenames objects direct byte unicode strings 
4293: charset decoding explicit strict default vocabulary tted stored vocabulary_ attribute consistent project conventions 
4294: 3.7 0.11 scikitlearn user guide release 0.11 class feature_selection.text.tfidfvectorizer feature_selection.text.countvectorizer make grid search trivial 
4295: derives directly methods rvs _basehmm module deprecated sample used instead beam pruning option _basehmm module removed since difcult cythonized inter ested look history codes git 
4296: svmlight format loader supports les zerobased onebased column indices since occur wild 
4297: arguments class shufflesplit consistent stratifiedshufflesplit arguments test_fraction train_fraction deprecated renamed test_size train_size accept float int 
4298: arguments class bootstrap consistent stratifiedshufflesplit arguments n_test n_train deprecated renamed test_size train_size accept float int 
4299: argument added classes nearest neighbors specify arbitrary minkowski metric nearest neigh bors searches 
4300: 3.7.3 people andreas mller peter prettenhofer gael varoquaux olivier grisel mathieu blondel clay woolam lars buitinck jaques grobler alexandre gramfort bertrand thirion robert layton yingimmidev jake vanderplas shiqiao satrajit ghosh david marek gilles louppe vlad niculae yannick schwartz fabian pedregosa fcostin nick wilson chapter development scikitlearn user guide release 0.11 adrien gaidon nicolas pinto david wardefarley nelle varoquaux emmanuelle gouillart joonas sillanp paolo losi charles mccarthy roy hyunjin han scott white ibayer brandyn white carlos scheidegger claire revillet conrad lee edouard duchesnay jan hendrik metzen meng xinfan rob zinkov shiqiao udi weinsberg virgile fritsch xinfan meng yaroslav halchenko jansoe leon palafox 3.8 0.10 3.8.1 changelog python 2.5 compatibility dropped minimum python version needed use scikitlearn 2.6. sparse inverse covariance estimation using graph lasso associated crossvalidated estimator gael varoquaux new tree module brian holt peter prettenhofer satrajit ghosh gilles louppe module comes complete documentation examples 
4301: fixed bug rfe module gilles louppe issue 
4302: 3.8 0.10 scikitlearn user guide release 0.11 fixed memory leak support vector machines module brian holt issue faster tests fabian pedregosa others silhouette coefcient cluster analysis evaluation metric added sklearn.metrics.silhouette_score robert layton 
4303: fixed bug kmeans handling n_init parameter clustering algorithm used run n_init times last solution retained instead best solution olivier grisel 
4304: minor refactoring stochastic gradient descent module consolidated dense sparse predict methods hanced test time performance converting model paramters fortranstyle arrays tting multi class 
4305: adjusted mutual information metric added sklearn.metrics.adjusted_mutual_info_score robert layton 
4306: models like svcsvrlinearsvclogisticregression libsvmliblinear support scaling regular ization parameter number samples alexandre gramfort 
4307: new ensemble methods module gilles louppe brian holt module comes random forest algorithm extratrees method along documentation examples novelty outlier detection outlier novelty detection virgile fritsch kernel approximation transform implementing kernel approximation fast sgd nonlinear kernels andreas mller 
4308: fixed bug due atom swapping orthogonal matching pursuit omp vlad niculae sparse coding precomputed dictionary vlad niculae mini batch kmeans performance improvements olivier grisel kmeans support sparse matrices mathieu blondel improved documentation developers sklearn.utils module jake vanderplas vectorized 20newsgroups dataset loader sklearn.datasets.fetch_20newsgroups_vectorized mathieu blondel 
4309: multiclass multilabel algorithms lars buitinck utilities fast computation mean variance sparse matrices mathieu blondel make sklearn.preprocessing.scale sklearn.preprocessing.scaler work sparse matrices olivier grisel feature importances using decision trees andor forest trees gilles louppe parallel implementation forests randomized trees gilles louppe sklearn.cross_validation.shufflesplit subsample train sets well test sets olivier grisel 
4310: errors build documentation xed andreas mller 
4311: 3.8.2 api changes summary code migration instructions updgrading scikitlearn version 0.9 estimators may overwrite inputs save memory previously overwrite_ parameters replaced copy_ parameters exactly opposite meaning 
4312: chapter development scikitlearn user guide release 0.11 particularly affects estimators linear_model default behavior still copy everything passed 
4313: svmlight dataset loader sklearn.datasets.load_svmlight_file longer supports loading two les use load_svmlight_files instead also unused buffer_mb parameter gone sparse estimators stochastic gradient descent module use dense parameter vector coef_ instead sparse_coef_ signicantly improves test time performance 
4314: covariance estimation module robust estimator covariance minimum covariance deter minant estimator 
4315: cluster evaluation metrics metrics.cluster refactored changes back moved metrics.cluster.supervised along wards compatible metrics.cluster.unsupervised contains silhouette coefcient 
4316: permutation_test_score function behaves way cross_val_score i.e uses mean score across folds cross validation generators use integer indices indicestrue default instead boolean masks 
4317: make intuitive use sparse matrix data 
4318: functions used sparse coding sparse_encode sparse_encode_parallel com bined sklearn.decomposition.sparse_encode shapes arrays trans posed consistency matrix factorization setting opposed regression setting 
4319: fixed offbyone error svmlightlibsvm format handling les generated using sklearn.datasets.dump_svmlight_file regenerated continue work accidentally one extra column zeros prepended basedictionarylearning class replaced sparsecodingmixin sklearn.utils.extmath.fast_svd renamed sklearn.utils.extmath.randomized_svd default oversampling xed additional random vectors instead doubling number components extract new behavior follows reference paper 
4320: 3.8.3 people following people contributed scikitlearn since last release andreas mller olivier grisel gilles louppe brian holt gael varoquaux lars buitinck vlad niculae peter prettenhofer fabian pedregosa robert layton mathieu blondel jake vanderplas noel dawe 3.8 0.10 scikitlearn user guide release 0.11 alexandre gramfort virgile fritsch satrajit ghosh jan hendrik metzen kenneth arnold shiqiao tim sheermanchase yaroslav halchenko bala subrahmanyam varanasi draxus michael eickenberg bogdan trach flixantoine fortin juan manuel caicedo carvajal nelle varoquaux nicolas pinto tiziano zito xinfan meng 3.9 0.9 scikitlearn 0.9 released september three months 0.8 release includes new modules manifold learning dirichlet process well several new algorithms documentation improvements release also includes dictionarylearning work developed vlad niculae part google summer code program 
4321: 3.9.1 changelog new manifold learning module jake vanderplas fabian pedregosa new dirichlet process gaussian mixture model alexandre passos chapter development scikitlearn user guide release 0.11 nearest neighbors module refactoring jake vanderplas general refactoring support sparse matrices input speed documentation improvements see next section full list api changes 
4322: improvements feature selection module gilles louppe refactoring rfe classes documenta tion rewrite increased efciency minor api changes 
4323: sparse principal components analysis sparsepca minibatchsparsepca vlad niculae gael varo quaux alexandre gramfort printing estimator behaves independently architectures python version thanks jean kossai loader libsvmsvmlight format mathieu blondel lars buitinck documentation improvements thumbnails example gallery fabian pedregosa important bugxes support vector machines module segfaults bad performance fabian pedregosa added multinomial naive bayes bernoulli naive bayes lars buitinck text feature extraction optimizations lars buitinck chisquare feature selection feature_selection.univariate_selection.chi2 lars buit inck 
4324: sample generators module refactoring gilles louppe multiclass multilabel algorithms mathieu blondel ball tree rewrite jake vanderplas implementation dbscan algorithm robert layton kmeans predict transform robert layton preprocessing module refactoring olivier grisel faster mean shift conrad lee new bootstrapping crossvalidation random permutations crossvalidation a.k.a shufe split various improvements cross validation schemes olivier grisel gael varoquaux adjusted rand index vmeasure clustering evaluation metrics olivier grisel added orthogonal matching pursuit vlad niculae added 2dpatch extractor utilites feature extraction module vlad niculae implementation linear_model.lassolarscv crossvalidated lasso solver using lars algorithm linear_model.lassolarsic bicaic model selection lars gael varoquaux alexandre gramfort scalability improvements metrics.roc_curve olivier hervieu distance functions metrics.pairwise.pairwise_kernels robert layton helper metrics.pairwise.pairwise_distances minibatch kmeans nelle varoquaux peter prettenhofer downloading datasets mldata.org repository utilities pietro berkes olivetti faces dataset david wardefarley 
4325: 3.9.2 api changes summary code migration instructions updgrading scikitlearn version 0.8 3.9 0.9 scikitlearn user guide release 0.11 scikits.learn package renamed sklearn still scikits.learn package alias backward compatibility thirdparty projects dependency scikitlearn 0.9 upgrade codebase instance linux macosx run make backup rst find name .py xargs sed sbscikits.learnbsklearng estimators longer accept model parameters fit arguments instead parameters must passed constructor arguments using public set_params method inhereted base.baseestimator estimators still accept keyword arguments fit restricted datadependent values e.g gram matrix afnity matrix precomputed data matrix 
4326: cross_val package renamed cross_validation although also cross_val package alias place backward compatibility thirdparty projects dependency scikitlearn 0.9 upgrade codebase instance linux macosx run make backup rst find name .py xargs sed sbcross_valbcross_validationg score_func argument sklearn.cross_validation.cross_val_score function expected accept y_test y_predicted arguments classication regression tasks x_test unsupervised estimators 
4327: gamma parameter support vector machine algorithms set n_features default instead n_samples 
4328: sklearn.hmm marked orphaned removed scikitlearn version 0.11 unless someone steps contribute documentation examples lurking numerical stability issues 
4329: sklearn.neighbors made submodule 
4330: two previously available estimators neighborsclassifier neighborsregressor marked deprecated function ality divided among new classes nearestneighbors unsupervised neighbors searches kneighborsclassifier radiusneighborsclassifier supervised classication problems kneighborsregressor radiusneighborsregressor supervised regression problems 
4331: sklearn.ball_tree.balltree moved sklearn.neighbors.balltree using former generate warning 
4332: sklearn.linear_model.lars related classes lassolars lassolarscv etc named sklearn.linear_model.lars 
4333: distance metrics kernels sklearn.metrics.pairwise parameter default none given result distance kernel similarity sample given result pairwise distance kernel similarity samples 
4334: sklearn.metrics.pairwise.l1_distance called manhattan_distance default returns pairwise distance component wise distance set parameter sum_over_features false 
4335: backward compatibilty package aliases deprecated classes functions removed version 0.11 
4336: 3.9.3 people people contributed release 
4337: vlad niculae chapter development scikitlearn user guide release 0.11 olivier grisel lars buitinck gael varoquaux fabian pedregosa inria parietal team jake vanderplas mathieu blondel alexandre passos alexandre gramfort peter prettenhofer gilles louppe robert layton nelle varoquaux jean kossai conrad lee pietro berkes andy david wardefarley brian holt robert amit aides virgile fritsch yaroslav halchenko salvatore masecchia paolo losi vincent schut alexis metaireau bryan silverthorn andreas mller minwoo jake lee emmanuelle gouillart keith goodman lucas wiman nicolas pinto thouis ray jones tim sheermanchase 3.9 0.9 scikitlearn user guide release 0.11 3.10 0.8 scikitlearn 0.8 released may one month rst international scikitlearn coding sprint marked inclusion important modules hierarchical clustering partial least squares nonnegative matrix factorization nmf nnmf initial support python important enhacements bug xes 
4338: 3.10.1 changelog several new modules introduced release new hierarchical clustering module vincent michel bertrand thirion alexandre gramfort gael varo quaux 
4339: kernel pca implementation mathieu blondel labeled faces wild face recognition dataset olivier grisel new partial least squares module edouard duchesnay nonnegative matrix factorization nmf nnmf module vlad niculae implementation oracle approximating shrinkage algorithm virgile fritsch covariance estima tion module 
4340: modules beneted signicant improvements cleanups 
4341: initial support python builds imports cleanly modules usable others failing tests fabian pedregosa 
4342: decomposition.pca usable pipeline object olivier grisel guide optimize speed olivier grisel fixes memory leaks libsvm bindings 64bit safer balltree lars buitinck bug style xing kmeans algorithm jan schlter add attribute coverged gaussian mixture models vincent schut implement transform predict_log_proba lda.lda mathieu blondel refactoring support vector machines module bug xes fabian pedregosa gael varoquaux amit aides 
4343: refactored sgd module removed code duplication better variable naming added interface sample weight peter prettenhofer 
4344: wrapped balltree cython thouis ray jones added function svm.l1_min_c paolo losi typos doc style etc yaroslav halchenko gael varoquaux olivier grisel yann malet nicolas pinto lars buitinck fabian pedregosa 
4345: 3.10.2 people people made release possible preceeded number commits olivier grisel gael varoquaux vlad niculae chapter development scikitlearn user guide release 0.11 fabian pedregosa alexandre gramfort paolo losi edouard duchesnay mathieu blondel peter prettenhofer nicolas pinto virgile fritsch lars buitinck vincent michel bertrand thirion thouis ray jones vincent schut jan schlter julien miotte matthieu perrot yann malet yaroslav halchenko amit aides andreas mller feth arezki meng xinfan 3.11 0.7 scikitlearn 0.7 released march roughly three months 0.6 release release marked speed improvements existing algorithms like knearest neighbors kmeans algorithm inclusion efcient algorithm computing ridge generalized cross validation solution unlike preceding release new modules added release 
4346: 3.11.1 changelog performance improvements gaussian mixture model sampling jan schlter implementation efcient leaveoneout crossvalidated ridge linear_model.ridgecv mathieu blondel better handling collinearity early stopping linear_model.lars_path alexandre gramfort fabian pedregosa 
4347: fixes liblinear ordering labels sign coefcients dan yamins paolo losi mathieu blondel fabian pedregosa 
4348: 3.11 0.7 scikitlearn user guide release 0.11 performance improvements nearest neighbors algorithm highdimensional spaces fabian pedregosa performance improvements cluster.kmeans gael varoquaux james bergstra sanity checks svmbased classes mathieu blondel refactoring neighbors.neighborsclassifier neighbors.kneighbors_graph added different algorithms knearest neighbor search implemented stable algorithm nding barycenter weigths also added developer documentation module see notes_neighbors information fabian pedregosa 
4349: documentation improvements added pca.randomizedpca linear_model.logisticregression class reference also added references matrices used clustering xes gael varoquaux fabian pedregosa mathieu blondel olivier grisel virgile fritsch emmanuelle gouillart binded decision_function classes make use liblinear dense sparse variants svm.linearsvc linear_model.logisticregression fabian pedregosa 
4350: like performance pca.randomizedpca james bergstra 
4351: api improvements metrics.euclidean_distances fix compilation issues netbsd kamel ibn hassen derouiche allow input sequences different lengths hmm.gaussianhmm ron weiss fix bug afnity propagation caused incorrect indexing xinfan meng 3.11.2 people people made release possible preceeded number commits fabian pedregosa mathieu blondel alexandre gramfort james bergstra dan yamins olivier grisel gael varoquaux edouard duchesnay ron weiss satrajit ghosh vincent dubourg emmanuelle gouillart kamel ibn hassen derouiche paolo losi virgilefritsch yaroslav halchenko xinfan meng chapter development scikitlearn user guide release 0.11 3.12 0.6 scikitlearn 0.6 released december 2010. marked inclusion several new modules general renaming old ones also marked inclusion new example including applications realworld datasets 
4352: 3.12.1 changelog new stochastic gradient descent module peter prettenhofer module comes complete documentation examples 
4353: improved svm module memory consumption reduced heuristic automatically set class weights possibility assign weights samples see svm weighted samples example 
4354: new gaussian processes module vincent dubourg module also great documentation neat examples see gaussian processes regression basic introductory example gaussian processes classication example exploiting probabilistic output taste done 
4355: possible use liblinears multiclass svc option multi_class svm.linearsvc new features performance improvements text feature extraction improved sparse matrix support main classes grid_search.gridsearchcv modules sklearn.svm.sparse sklearn.linear_model.sparse 
4356: lots cool new examples new section uses realworld datasets created include faces recognition example using eigenfaces svms species distribution modeling libsvm gui wikipedia princi pal eigenvector others 
4357: faster least angle regression algorithm faster version worst case 10x times faster cases 
4358: faster full linear_model.lasso_path 200x times faster 
4359: coordinate algorithm 
4360: descent particular path version lasso possible get probability estimates linear_model.logisticregression model module renaming glm module renamed linear_model gmm module included general mixture model sgd module included linear_model 
4361: lots bug xes documentation improvements 
4362: 3.12.2 people people made release possible preceeded number commits olivier grisel fabian pedregosa peter prettenhofer alexandre gramfort mathieu blondel gael varoquaux vincent dubourg ron weiss bertrand thirion 3.12 0.6 scikitlearn user guide release 0.11 alexandre passos annelaure fouque ronan amicel christian osendorfer 3.13 0.5 3.13.1 changelog 3.13.2 new classes support sparse matrices classiers modules svm linear_model see svm.sparse.svc linear_model.sparse.lasso svm.sparse.linearsvc svm.sparse.svr linear_model.sparse.elasticnet new pipeline.pipeline object compose different estimators recursive feature elimination routines module feature selection addition capable classes various cross validation linear_model module linear_model.lassocv linear_model.elasticnetcv etc 
4363: new efcient lars algorithm implementation lasso variant algorithm also implemented 
4364: see linear_model.lars_path linear_model.lars linear_model.lassolars 
4365: new hidden markov models module see classes hmm.gaussianhmm hmm.multinomialhmm hmm.gmmhmm new module feature_extraction see class reference new fastica algorithm module sklearn.fastica 3.13.3 documentation improved documentation many modules separating narrative documentation class reference 
4366: example see documentation svm module complete class reference 
4367: 3.13.4 fixes api changes adhere variable names pep8 give meaningful names fixes svm module run shared memory context multiprocessing possible generate latex thus pdf sphinx docs 
4368: 3.13.5 examples new examples using mlcomp datasets classication text documents using mlcomp dataset classication text documents using sparse features many examaples see full list examples 
4369: chapter development scikitlearn user guide release 0.11 3.13.6 external dependencies joblib dependencie package although shipped sklearn.externals.joblib 
4370: 3.13.7 removed modules module ann articial neural networks removed distribution users wanting sort algorithms take look pybrain 
4371: 3.13.8 misc new sphinx theme web page 
4372: 3.13.9 authors following list authors release preceeded number commits fabian pedregosa gael varoquaux alexandre gramfort olivier grisel vincent michel ron weiss matthieu perrot bertrand thirion yaroslav halchenko virgilefritsch edouard duchesnay mathieu blondel ariel rokem matthieu brucher 3.14 0.4 3.14.1 changelog major changes release include coordinate descent algorithm lasso elasticnet refactoring speed improvements roughly 100x times faster 
4373: coordinate descent refactoring bug xing consistency package glmnet new metrics module 
4374: 3.14 0.4 scikitlearn user guide release 0.11 new gmm module contributed ron weiss implementation lars algorithm without lasso variant feature_selection module redesign migration git content management system removal obsolete attrselect module rename private compiled extensions aded underscore removal legacy unmaintained code documentation improvements docstring rst improvement build system optionally link mkl also provide lite blas implementation case systemwide blas found 
4375: lots new examples many many bug xes 
4376: 3.14.2 authors committer list release following preceded number commits fabian pedregosa alexandre gramfort olivier grisel gael varoquaux yaroslav halchenko vincent michel chris filo gorgolewski 3.15 presentations tutorials scikitlearn written tutorials see tutorial section documentation 
4377: 3.15.1 videos introduction scikitlearn gael varoquaux icml three minute video early stage scikit explaining basic idea approach following 
4378: introduction statistical learning scikit learn gael varoquaux scipy extensive tutorial consisting four sessions one hour tutorial covers basics machine learning many algorithms apply using scikitlearn material corresponding scikitlearn documentation section tutorial statisticallearning scientic data processing 
4379: statistical learning text classication scikitlearn nltk slides olivier grisel pycon chapter development scikitlearn user guide release 0.11 thirty minute introduction text classication explains use nltk scikitlearn solve realworld text classication tasks compares cloudbased solutions 
4380: introduction interactive predictive analytics python scikitlearn olivier grisel pycon 3hours long introduction prediction tasks using scikitlearn 
4381: scikitlearn machine learning python jake vanderplas pydata workshop google interactive demonstration scikitlearn features minutes 
4382: 3.15. presentations tutorials scikitlearn scikitlearn user guide release 0.11 chapter development bibliography b2001 leo breiman random forests machine learning b1998 leo breiman arcing classiers annals statistics gew2006 pierre geurts damien ernst. louis wehenkel extremely randomized trees machine learning 
4383: f2001 friedman greedy function approximation gradient boosting machine annals statistics vol 
4384: ridgeway generalized boosted models guide gbm package f1999 10. friedman stochastic gradient boosting htf2009 20. hastie tibshirani friedman elements statistical learning springer r2007 mrl09 online dictionary learning sparse coding mairal bach ponce sapiro jen09 structured sparse principal component analysis jenatton obozinski bach rd1999 rousseeuw p.j. van driessen fast algorithm minimum covariance determinant estimator technometrics r59 12. breiman random forests machine learning r60 12. breiman random forests machine learning r57 geurts ernst. wehenkel extremely randomized trees machine learning r58 geurts ernst. wehenkel extremely randomized trees machine learning rr2007 random features largescale kernel machines rahimi recht advances neural infor mation processing ls2010 random fourier approximations skewed multiplicative histogram kernels random fourier approxi mations skewed multiplicative histogram kernels lecture notes computer sciencd dagm vz2010 efcient additive kernels via explicit feature maps vedaldi zisserman computer vision pattern recognition vvz2010 generalized rbf feature maps efcient detection vempati vedaldi zisserman 
4385: jawahar rouseeuw1984 rousseeuw least median squares regression stat ass rouseeuw1999 fast algorithm minimum covariance determinant estimator american statistical association american society quality technometrics scikitlearn user guide release 0.11 butler1993 butler davies jhun asymptotics minimum covariance determinant esti mator annals statistics vol r48 guyon design experiments nips variable selection benchmark r49 friedman multivariate adaptive regression splines annals statistics pages r50 breiman bagging predictors machine learning pages r51 friedman multivariate adaptive regression splines annals statistics pages r52 breiman bagging predictors machine learning pages r53 friedman multivariate adaptive regression splines annals statistics pages r54 breiman bagging predictors machine learning pages r55 celeux anbari j.m. marin robert regularization regression comparing bayesian frequentist methods poorly informative situation 
4386: r56 marsland machine learning algorithmic perpsective chapter 2009. http www ist.massey.ac.nzsmarslandcode10lle.py halko2009 finding structure randomness stochastic algorithms constructing approximate matrix decom positions halko al. arxiv909 mrt randomized algorithm decomposition matrices pergunnar martinsson vladimir rokhlin mark tygert r59 12. breiman random forests machine learning r60 12. breiman random forests machine learning r57 geurts ernst. wehenkel extremely randomized trees machine learning r58 geurts ernst. wehenkel extremely randomized trees machine learning yates2011 baezayates ribeironeto modern information retrieval addison wesley msr2008 c.d manning schtze raghavan introduction information retrieval cambridge university press 
4387: r61 guyon weston barnhill vapnik gene selection cancer classication using support vector machines mach learn. 
4388: r62 guyon weston barnhill vapnik gene selection cancer classication using support vector machines mach learn. 
4389: nlns2002 h.b nielsen s.n lophaven nielsen sondergaard dace matlab kriging toolbox 
4390: http www2.imm.dtu.dkhbndacedace.pdf wbswm1992 w.j welch r.j. buck sacks h.p wynn t.j. mitchell m.d morris screening pre dicting computer experiments technometrics 1525. http www.jstor.orgpss1269548 r63 roweis saul nonlinear dimensionality reduction locally linear embedding science 
4391: r64 donoho grimes hessian eigenmaps locally linear embedding techniques highdimensional data 
4392: proc natl acad sci 
4393: r65 zhang wang mlle modied locally linear embedding using multiple weights 
4394: http citeseerx.ist.psu.eduviewdocsummary doi10.1.1.70.382 r66 zhang zha principal manifolds nonlinear dimensionality reduction via tangent space alignment 
4395: journal shanghai univ bibliography scikitlearn user guide release 0.11 r67 roweis saul nonlinear dimensionality reduction locally linear embedding science 
4396: r68 donoho grimes hessian eigenmaps locally linear embedding techniques highdimensional data 
4397: proc natl acad sci 
4398: r69 zhang wang mlle modied locally linear embedding using multiple weights 
4399: http citeseerx.ist.psu.eduviewdocsummary doi10.1.1.70.382 r70 zhang zha principal manifolds nonlinear dimensionality reduction via tangent space alignment 
4400: journal shanghai univ hubert1985 hubert arabie comparing partitions journal classication http www.springerlink.comcontentx64124718341j1j0 http en.wikipedia.orgwikirand_index adjusted_rand_index r72 andrew rosenberg julia hirschberg vmeasure conditional entropybased external cluster evaluation measure http acl.ldc.upenn.edudd07d071043.pdf r71 andrew rosenberg julia hirschberg vmeasure conditional entropybased external cluster evaluation measure http acl.ldc.upenn.edudd07d071043.pdf rosenberg2007 vmeasure conditional entropybased external cluster evaluation measure andrew rosenberg julia hirschberg http acl.ldc.upenn.edudd07d071043.pdf r73 solving multiclass learning problems via errorcorrecting output codes dietterich bakiri journal articial intelligence research 
4401: r74 error coding method picts james hastie journal computational graphical statistics 
4402: r75 elements statistical learning hastie tibshirani friedman page secondedition r76 http en.wikipedia.orgwikidecision_tree_learning r77 breiman friedman olshen stone classication regression trees wadsworth belmont 
4403: r78 hastie tibshirani friedman elements statistical learning springer r79 breiman cutler random forests http www.stat.berkeley.edubreimanrandomforestscc_home.htm r80 http en.wikipedia.orgwikidecision_tree_learning r81 breiman friedman olshen stone classication regression trees wadsworth belmont 
4404: r82 hastie tibshirani friedman elements statistical learning springer r83 breiman cutler random forests http www.stat.berkeley.edubreimanrandomforestscc_home.htm r84 geurts ernst. wehenkel extremely randomized trees machine learning r85 geurts ernst. wehenkel extremely randomized trees machine learning 
4405: bibliography scikitlearn user guide release 0.11 bibliography python module index sklearn.cluster sklearn.covariance sklearn.cross_validation sklearn.datasets sklearn.decomposition sklearn.ensemble sklearn.feature_extraction sklearn.feature_extraction.image sklearn.feature_extraction.text sklearn.feature_selection sklearn.gaussian_process sklearn.grid_search sklearn.hmm sklearn.kernel_approximation sklearn.lda sklearn.linear_model sklearn.linear_model.sparse sklearn.manifold sklearn.metrics sklearn.metrics.cluster sklearn.metrics.pairwise sklearn.mixture sklearn.multiclass sklearn.naive_bayes sklearn.neighbors sklearn.pipeline sklearn.pls sklearn.preprocessing sklearn.qda sklearn.semi_supervised sklearn.svm sklearn.tree sklearn.utils scikitlearn user guide release 0.11 python module index python module index sklearn.cluster sklearn.covariance sklearn.cross_validation sklearn.datasets sklearn.decomposition sklearn.ensemble sklearn.feature_extraction sklearn.feature_extraction.image sklearn.feature_extraction.text sklearn.feature_selection sklearn.gaussian_process sklearn.grid_search sklearn.hmm sklearn.kernel_approximation sklearn.lda sklearn.linear_model sklearn.linear_model.sparse sklearn.manifold sklearn.metrics sklearn.metrics.cluster sklearn.metrics.pairwise sklearn.mixture sklearn.multiclass sklearn.naive_bayes sklearn.neighbors sklearn.pipeline sklearn.pls sklearn.preprocessing sklearn.qda sklearn.semi_supervised sklearn.svm sklearn.tree sklearn.utils scikitlearn user guide release 0.11 python module index index symbols __init__ sklearn.cluster.afnitypropagation method __init__ sklearn.cluster.dbscan method __init__ sklearn.cluster.kmeans method __init__ sklearn.cluster.meanshift method __init__ sklearn.cluster.minibatchkmeans method __init__ sklearn.cluster.spectralclustering method __init__ sklearn.cluster.ward method __init__ sklearn.covariance.ellipticenvelope method __init__ sklearn.decomposition.dictionarylearning __init__ sklearn.decomposition.fastica method __init__ sklearn.decomposition.kernelpca method __init__ sklearn.decomposition.minibatchdictionarylearning __init__ sklearn.decomposition.minibatchsparsepca __init__ sklearn.decomposition.nmf method __init__ sklearn.decomposition.pca method __init__ sklearn.decomposition.probabilisticpca __init__ sklearn.covariance.empiricalcovariance __init__ sklearn.decomposition.projectedgradientnmf method method method method method method method __init__ sklearn.covariance.graphlasso method __init__ sklearn.covariance.graphlassocv method __init__ sklearn.covariance.ledoitwolf method __init__ sklearn.covariance.mincovdet method __init__ sklearn.covariance.oas method __init__ sklearn.covariance.shrunkcovariance method __init__ sklearn.cross_validation.bootstrap method __init__ sklearn.cross_validation.kfold method sklearn.cross_validation.leaveonelabelout __init__ __init__ __init__ method method method sklearn.cross_validation.leaveoneout sklearn.cross_validation.leaveplabelout __init__ sklearn.cross_validation.leavepout method __init__ sklearn.cross_validation.shufesplit method __init__ sklearn.cross_validation.stratiedkfold __init__ sklearn.cross_validation.stratiedshufesplit method method __init__ sklearn.decomposition.randomizedpca __init__ sklearn.decomposition.sparsecoder method __init__ sklearn.decomposition.sparsepca method __init__ __init__ sklearn.ensemble.extratreesclassier sklearn.ensemble.extratreesregressor __init__ sklearn.ensemble.gradientboostingclassier __init__ sklearn.ensemble.gradientboostingregressor __init__ __init__ __init__ sklearn.ensemble.randomforestclassier sklearn.ensemble.randomforestregressor sklearn.feature_extraction.dictvectorizer method method method method method method method method method method __init__ sklearn.feature_extraction.text.tdftransformer __init__ sklearn.feature_extraction.image.patchextractor __init__ sklearn.feature_extraction.text.countvectorizer scikitlearn user guide release 0.11 __init__ sklearn.feature_extraction.text.tdfvectorizer sklearn.linear_model.perceptron method __init__ sklearn.feature_selection.rfe method __init__ sklearn.feature_selection.rfecv method method sklearn.linear_model.randomizedlasso __init__ sklearn.linear_model.randomizedlogisticregression __init__ sklearn.feature_selection.selectfdr method method __init__ sklearn.feature_selection.selectfpr method __init__ sklearn.linear_model.ridge method __init__ sklearn.linear_model.ridgecv method __init__ __init__ __init__ sklearn.feature_selection.selectfwe method sklearn.linear_model.ridgeclassier sklearn.feature_selection.selectkbest sklearn.linear_model.ridgeclassiercv sklearn.feature_selection.selectpercentile __init__ sklearn.linear_model.sgdclassier method sklearn.gaussian_process.gaussianprocess __init__ sklearn.linear_model.sgdregressor method __init__ __init__ method method method __init__ __init__ __init__ method method method __init__ sklearn.grid_search.gridsearchcv method __init__ sklearn.linear_model.sparse.elasticnet __init__ sklearn.kernel_approximation.rbfsampler method __init__ sklearn.kernel_approximation.skewedchi2sampler __init__ sklearn.grid_search.itergrid method __init__ sklearn.hmm.gmmhmm method __init__ sklearn.hmm.multinomialhmm method __init__ sklearn.kernel_approximation.additivechi2sampler method method method method method __init__ sklearn.lda.lda method __init__ sklearn.linear_model.ardregression __init__ sklearn.linear_model.bayesianridge __init__ sklearn.linear_model.elasticnet method __init__ sklearn.linear_model.elasticnetcv method __init__ sklearn.linear_model.lars method __init__ sklearn.linear_model.larscv method __init__ sklearn.linear_model.lasso method __init__ sklearn.linear_model.lassocv method __init__ sklearn.linear_model.lassolars method __init__ sklearn.linear_model.lassolarscv method method method __init__ sklearn.linear_model.sparse.lasso method __init__ sklearn.linear_model.sparse.sgdclassier __init__ sklearn.linear_model.sparse.sgdregressor __init__ sklearn.manifold.isomap method __init__ sklearn.manifold.locallylinearembedding method __init__ sklearn.mixture.dpgmm method __init__ sklearn.mixture.gmm method __init__ sklearn.mixture.vbgmm method __init__ sklearn.multiclass.onevsoneclassier __init__ __init__ __init__ __init__ method method method sklearn.multiclass.onevsrestclassier sklearn.multiclass.outputcodeclassier sklearn.naive_bayes.bernoullinb method sklearn.naive_bayes.gaussiannb method __init__ sklearn.naive_bayes.multinomialnb method __init__ sklearn.neighbors.balltree method __init__ sklearn.neighbors.kneighborsclassier __init__ sklearn.neighbors.kneighborsregressor __init__ sklearn.neighbors.nearestcentroid method __init__ sklearn.neighbors.nearestneighbors method __init__ sklearn.neighbors.radiusneighborsclassier method __init__ sklearn.linear_model.lassolarsic method method sklearn.linear_model.linearregression method __init__ __init__ method method method sklearn.linear_model.logisticregression __init__ sklearn.linear_model.orthogonalmatchingpursuit index scikitlearn user guide release 0.11 __init__ sklearn.neighbors.radiusneighborsregressor method __init__ sklearn.pipeline.pipeline method __init__ sklearn.pls.cca method __init__ sklearn.pls.plscanonical method __init__ sklearn.pls.plsregression method __init__ sklearn.pls.plssvd method __init__ sklearn.preprocessing.binarizer method __init__ sklearn.preprocessing.kernelcenterer method method __init__ __init__ sklearn.preprocessing.labelbinarizer sklearn.preprocessing.normalizer method __init__ sklearn.preprocessing.scaler method __init__ sklearn.qda.qda method __init__ sklearn.semi_supervised.labelpropagation auc module sklearn.metrics balltree class sklearn.neighbors bayesianridge class sklearn.linear_model bernoullinb class sklearn.naive_bayes best_estimator sklearn.grid_search.gridsearchcv best_score sklearn.grid_search.gridsearchcv attribute tribute bic sklearn.mixture.dpgmm method bic sklearn.mixture.gmm method bic sklearn.mixture.vbgmm method binarize module sklearn.preprocessing binarizer class sklearn.preprocessing bootstrap class sklearn.cross_validation build_analyzer sklearn.feature_extraction.text.countvectorizer __init__ sklearn.semi_supervised.labelspreading build_analyzer sklearn.feature_extraction.text.tdfvectorizer method method method method method method method method build_preprocessor sklearn.feature_extraction.text.countvectorizer build_preprocessor sklearn.feature_extraction.text.tdfvectorizer build_tokenizer sklearn.feature_extraction.text.countvectorizer build_tokenizer sklearn.feature_extraction.text.tdfvectorizer cca class sklearn.pls check_cv module sklearn.cross_validation check_random_state module sklearn.utils chi2 module sklearn.feature_selection class_prior sklearn.naive_bayes.gaussiannb attribute __init__ sklearn.svm.linearsvc method __init__ sklearn.svm.nusvc method __init__ sklearn.svm.nusvr method __init__ sklearn.svm.oneclasssvm method __init__ sklearn.svm.svc method __init__ sklearn.svm.svr method __init__ sklearn.tree.decisiontreeclassier method __init__ sklearn.tree.decisiontreeregressor method __init__ sklearn.tree.extratreeclassier method __init__ sklearn.tree.extratreeregressor method absolute_exponential module sklearn.gaussian_process.correlation_models additivechi2sampler sklearn.kernel_approximation class adjusted_mutual_info_score sklearn.metrics adjusted_rand_score module sklearn.metrics afnity_propagation module sklearn.cluster afnitypropagation class sklearn.cluster aic sklearn.mixture.dpgmm method aic sklearn.mixture.gmm method aic sklearn.mixture.vbgmm method algorithm sklearn.hmm.gmmhmm attribute algorithm sklearn.hmm.multinomialhmm attribute ardregression class sklearn.linear_model arg_max_reduced_likelihood_function sklearn.gaussian_process.gaussianprocess method classes sklearn.linear_model.perceptron attribute classes sklearn.linear_model.sgdclassier attribute module classes sklearn.linear_model.sparse.sgdclassier tribute classication_report module sklearn.metrics completeness_score module sklearn.metrics confusion_matrix module sklearn.metrics constant module sklearn.gaussian_process.regression_models correct_covariance sklearn.covariance.ellipticenvelope method method correct_covariance sklearn.covariance.mincovdet countvectorizer class sklearn.feature_extraction.text covariance_type sklearn.hmm.gmmhmm attribute index scikitlearn user guide release 0.11 cross_val_score module sklearn.cross_validation decision_function sklearn.linear_model.sparse.lasso cross_validation module sklearn.svm.libsvm cubic module sklearn.gaussian_process.correlation_models decision_function sklearn.linear_model.sparse.sgdclassier decision_function sklearn.linear_model.sparse.sgdregressor method method method method dbscan class sklearn.cluster dbscan module sklearn.cluster decision_function module sklearn.svm.libsvm decision_function sklearn.covariance.ellipticenvelope decision_function sklearn.lda.lda method decision_function sklearn.linear_model.ardregression decision_function sklearn.linear_model.bayesianridge decision_function sklearn.linear_model.elasticnet decision_function sklearn.linear_model.elasticnetcv method decision_function sklearn.linear_model.larscv decision_function sklearn.linear_model.lasso decision_function sklearn.linear_model.lassocv decision_function sklearn.linear_model.lassolars decision_function sklearn.linear_model.lassolarscv decision_function sklearn.linear_model.lassolarsic method method method method method method method method method method method decision_function sklearn.pipeline.pipeline method decision_function sklearn.qda.qda method decision_function sklearn.svm.linearsvc method decision_function sklearn.svm.nusvc method decision_function sklearn.svm.nusvr method decision_function sklearn.svm.oneclasssvm method decision_function sklearn.svm.svc method decision_function sklearn.svm.svr method decisiontreeclassier class sklearn.tree decisiontreeregressor class sklearn.tree decode sklearn.feature_extraction.text.countvectorizer decode sklearn.hmm.gmmhmm method decode sklearn.hmm.multinomialhmm method decode sklearn.mixture.dpgmm method decode sklearn.mixture.gmm method decode sklearn.mixture.vbgmm method dict_learning module sklearn.decomposition dict_learning_online module sklearn.decomposition dictionarylearning class sklearn.decomposition dictvectorizer class sklearn.feature_extraction distance_metrics module sklearn.metrics.pairwise decision_function sklearn.linear_model.lars method decode sklearn.feature_extraction.text.tdfvectorizer method dpgmm class sklearn.mixture decision_function sklearn.linear_model.linearregression decision_function sklearn.linear_model.logisticregression elasticnet class sklearn.linear_model elasticnet class sklearn.linear_model.sparse elasticnetcv class sklearn.linear_model ellipticenvelope class sklearn.covariance emissionprob_ decision_function sklearn.linear_model.orthogonalmatchingpursuit sklearn.linear_model.perceptron sklearn.hmm.multinomialhmm decision_function method method method decision_function sklearn.linear_model.ridge tribute decision_function sklearn.linear_model.ridgecv decision_function sklearn.linear_model.sgdclassier empirical_covariance module sklearn.covariance empiricalcovariance class sklearn.covariance error_norm sklearn.covariance.ellipticenvelope decision_function sklearn.linear_model.sgdregressor error_norm sklearn.covariance.empiricalcovariance decision_function sklearn.linear_model.sparse.elasticnet sklearn.covariance.graphlasso method method method method method method method method error_norm index scikitlearn user guide release 0.11 error_norm sklearn.covariance.graphlassocv sklearn.decomposition.dictionarylearning method method sklearn.covariance.ledoitwolf method module error_norm error_norm sklearn.covariance.mincovdet method error_norm sklearn.covariance.oas method error_norm sklearn.covariance.shrunkcovariance method estimate_bandwidth module sklearn.cluster euclidean_distances module sklearn.metrics.pairwise eval sklearn.hmm.gmmhmm method eval sklearn.hmm.multinomialhmm method eval sklearn.mixture.dpgmm method eval sklearn.mixture.gmm method eval sklearn.mixture.vbgmm method export_graphviz module sklearn.tree extract_patches_2d sklearn.feature_extraction.image extratreeclassier class sklearn.tree extratreeregressor class sklearn.tree extratreesclassier class sklearn.ensemble extratreesregressor class sklearn.ensemble f1_score module sklearn.metrics f_classif module sklearn.feature_selection f_regression module sklearn.feature_selection fastica class sklearn.decomposition fastica module sklearn.decomposition fbeta_score module sklearn.metrics fetch_20newsgroups module sklearn.datasets fetch_20newsgroups_vectorized module sklearn.datasets fetch_lfw_pairs module sklearn.datasets fetch_lfw_people module sklearn.datasets fetch_olivetti_faces module sklearn.datasets module sklearn.svm.libsvm sklearn.cluster.afnitypropagation method sklearn.cluster.dbscan method sklearn.cluster.kmeans method sklearn.cluster.meanshift method sklearn.cluster.minibatchkmeans method sklearn.cluster.spectralclustering method sklearn.cluster.ward method sklearn.covariance.ellipticenvelope method sklearn.covariance.empiricalcovariance method sklearn.covariance.ledoitwolf method sklearn.covariance.mincovdet method sklearn.covariance.oas method sklearn.covariance.shrunkcovariance method sklearn.decomposition.kernelpca method sklearn.decomposition.minibatchdictionarylearning method sklearn.decomposition.minibatchsparsepca method sklearn.decomposition.nmf method sklearn.decomposition.pca method sklearn.decomposition.probabilisticpca method sklearn.decomposition.projectedgradientnmf method sklearn.decomposition.randomizedpca method sklearn.decomposition.sparsecoder method sklearn.decomposition.sparsepca method sklearn.ensemble.extratreesclassier method sklearn.ensemble.extratreesregressor method sklearn.ensemble.gradientboostingclassier method sklearn.ensemble.gradientboostingregressor method sklearn.ensemble.randomforestclassier method sklearn.ensemble.randomforestregressor method sklearn.feature_extraction.dictvectorizer method sklearn.feature_extraction.image.patchextractor method sklearn.feature_extraction.text.countvectorizer method sklearn.feature_extraction.text.tdftransformer method sklearn.feature_extraction.text.tdfvectorizer method sklearn.feature_selection.rfe method sklearn.feature_selection.rfecv method sklearn.feature_selection.selectfdr method sklearn.feature_selection.selectfpr method sklearn.feature_selection.selectfwe method sklearn.feature_selection.selectkbest method sklearn.feature_selection.selectpercentile method sklearn.gaussian_process.gaussianprocess method sklearn.grid_search.gridsearchcv method sklearn.hmm.gmmhmm method sklearn.hmm.multinomialhmm method sklearn.kernel_approximation.additivechi2sampler method index scikitlearn user guide release 0.11 sklearn.kernel_approximation.rbfsampler sklearn.multiclass.outputcodeclassier method method method sklearn.kernel_approximation.skewedchi2sampler sklearn.lda.lda method sklearn.linear_model.ardregression method sklearn.linear_model.bayesianridge method sklearn.linear_model.elasticnet method sklearn.linear_model.elasticnetcv method sklearn.linear_model.lars method sklearn.linear_model.larscv method sklearn.linear_model.lasso method sklearn.linear_model.lassocv method sklearn.linear_model.lassolars method sklearn.linear_model.lassolarscv method sklearn.linear_model.lassolarsic method sklearn.linear_model.linearregression method sklearn.linear_model.logisticregression method method sklearn.linear_model.orthogonalmatchingpursuit sklearn.linear_model.perceptron method sklearn.linear_model.randomizedlasso method sklearn.linear_model.randomizedlogisticregression method sklearn.linear_model.ridge method sklearn.linear_model.ridgeclassier method sklearn.linear_model.ridgeclassiercv method sklearn.linear_model.ridgecv method sklearn.linear_model.sgdclassier method sklearn.linear_model.sgdregressor method sklearn.linear_model.sparse.elasticnet method sklearn.linear_model.sparse.lasso method sklearn.linear_model.sparse.sgdclassier method sklearn.naive_bayes.bernoullinb method sklearn.naive_bayes.gaussiannb method sklearn.naive_bayes.multinomialnb method sklearn.neighbors.kneighborsclassier method sklearn.neighbors.kneighborsregressor method sklearn.neighbors.nearestcentroid method sklearn.neighbors.nearestneighbors method sklearn.neighbors.radiusneighborsclassier method sklearn.neighbors.radiusneighborsregressor method sklearn.pipeline.pipeline method sklearn.preprocessing.binarizer method sklearn.preprocessing.kernelcenterer method sklearn.preprocessing.labelbinarizer method sklearn.preprocessing.normalizer method sklearn.preprocessing.scaler method sklearn.qda.qda method sklearn.semi_supervised.labelpropagation method sklearn.semi_supervised.labelspreading method sklearn.svm.linearsvc method sklearn.svm.nusvc method sklearn.svm.nusvr method sklearn.svm.oneclasssvm method sklearn.svm.svc method sklearn.svm.svr method sklearn.tree.decisiontreeclassier method sklearn.tree.decisiontreeregressor method sklearn.tree.extratreeclassier method sklearn.tree.extratreeregressor method t_ecoc module sklearn.multiclass t_ovo module sklearn.multiclass t_ovr module sklearn.multiclass t_predict sklearn.cluster.kmeans method t_predict sklearn.cluster.minibatchkmeans method sklearn.linear_model.sparse.sgdregressor t_stage sklearn.ensemble.gradientboostingclassier method method sklearn.manifold.isomap method sklearn.manifold.locallylinearembedding sklearn.mixture.dpgmm method sklearn.mixture.gmm method sklearn.mixture.vbgmm method sklearn.multiclass.onevsoneclassier method t_stage sklearn.ensemble.gradientboostingregressor t_transform sklearn.decomposition.dictionarylearning t_transform sklearn.decomposition.kernelpca t_transform sklearn.decomposition.minibatchdictionarylearning sklearn.multiclass.onevsrestclassier method t_transform sklearn.decomposition.minibatchsparsepca method method method method method method index scikitlearn user guide release 0.11 method method method method method t_transform t_transform sklearn.decomposition.nmf method sklearn.decomposition.pca method t_transform sklearn.linear_model.randomizedlogisticregression t_transform sklearn.linear_model.sgdclassier t_transform sklearn.decomposition.probabilisticpca t_transform sklearn.linear_model.sgdregressor method method method method method method method method method t_transform sklearn.decomposition.projectedgradientnmf t_transform sklearn.linear_model.sparse.sgdclassier t_transform sklearn.decomposition.randomizedpca t_transform sklearn.linear_model.sparse.sgdregressor t_transform sklearn.decomposition.sparsecoder t_transform sklearn.decomposition.sparsepca method t_transform sklearn.ensemble.extratreesclassier t_transform sklearn.manifold.isomap method t_transform sklearn.manifold.locallylinearembedding t_transform sklearn.pipeline.pipeline method t_transform sklearn.preprocessing.binarizer method t_transform method method method t_transform t_transform sklearn.ensemble.extratreesregressor sklearn.preprocessing.kernelcenterer t_transform sklearn.ensemble.randomforestclassier t_transform sklearn.preprocessing.labelbinarizer t_transform sklearn.ensemble.randomforestregressor t_transform sklearn.preprocessing.normalizer t_transform sklearn.feature_extraction.dictvectorizer sklearn.preprocessing.scaler method t_transform sklearn.feature_extraction.text.countvectorizer t_transform sklearn.svm.linearsvc method t_transform sklearn.tree.decisiontreeclassier t_transform sklearn.feature_extraction.text.tdftransformer method t_transform sklearn.feature_extraction.text.tdfvectorizer method t_transform sklearn.feature_selection.selectfdr t_transform sklearn.tree.decisiontreeregressor t_transform sklearn.tree.extratreeclassier method t_transform sklearn.tree.extratreeregressor t_transform sklearn.feature_selection.selectfpr method method method method method method method method method method method method method t_transform sklearn.feature_selection.selectfwe t_transform sklearn.feature_selection.selectkbest t_transform sklearn.feature_selection.selectpercentile t_transform sklearn.kernel_approximation.additivechi2sampler t_transform sklearn.kernel_approximation.rbfsampler t_transform sklearn.kernel_approximation.skewedchi2sampler t_transform sklearn.lda.lda method t_transform sklearn.linear_model.logisticregression gaussianhmm class sklearn.hmm gaussiannb class sklearn.naive_bayes gaussianprocess class sklearn.gaussian_process generalized_exponential module sklearn.gaussian_process.correlation_models get_feature_names sklearn.feature_extraction.dictvectorizer get_feature_names sklearn.feature_extraction.text.countvectorizer get_feature_names sklearn.feature_extraction.text.tdfvectorizer get_mixing_matrix sklearn.decomposition.fastica t_transform sklearn.linear_model.perceptron get_params sklearn.cluster.afnitypropagation method method method method method method method method t_transform sklearn.linear_model.randomizedlasso get_params sklearn.cluster.dbscan method get_params sklearn.cluster.kmeans method get_params sklearn.cluster.meanshift method index scikitlearn user guide release 0.11 get_params sklearn.cluster.minibatchkmeans get_params sklearn.feature_extraction.dictvectorizer get_params sklearn.cluster.spectralclustering get_params sklearn.feature_extraction.image.patchextractor method method method get_params sklearn.feature_extraction.text.countvectorizer get_params sklearn.feature_extraction.text.tdftransformer get_params sklearn.feature_extraction.text.tdfvectorizer get_params sklearn.cluster.ward method get_params sklearn.covariance.ellipticenvelope method method method method method method method method method method method method method method method get_params sklearn.covariance.empiricalcovariance method get_params sklearn.covariance.graphlasso method method sklearn.covariance.graphlassocv sklearn.covariance.ledoitwolf method method sklearn.covariance.mincovdet method method get_params get_params sklearn.feature_selection.rfe method sklearn.feature_selection.rfecv get_params sklearn.feature_selection.selectfdr get_params sklearn.feature_selection.selectfpr get_params sklearn.feature_selection.selectfwe get_params sklearn.covariance.oas method get_params sklearn.covariance.shrunkcovariance get_params sklearn.decomposition.dictionarylearning get_params sklearn.feature_selection.selectkbest get_params sklearn.decomposition.fastica method get_params sklearn.feature_selection.selectpercentile get_params get_params get_params get_params sklearn.decomposition.kernelpca get_params sklearn.gaussian_process.gaussianprocess get_params sklearn.decomposition.minibatchdictionarylearning get_params sklearn.grid_search.gridsearchcv get_params sklearn.decomposition.minibatchsparsepca get_params sklearn.hmm.gmmhmm method get_params sklearn.hmm.multinomialhmm method method method method method method method method get_params sklearn.decomposition.nmf method get_params sklearn.decomposition.pca method get_params sklearn.decomposition.probabilisticpca get_params sklearn.decomposition.projectedgradientnmf method get_params sklearn.decomposition.randomizedpca method get_params sklearn.decomposition.sparsecoder get_params sklearn.lda.lda method get_params sklearn.linear_model.ardregression get_params sklearn.decomposition.sparsepca get_params sklearn.linear_model.bayesianridge get_params sklearn.ensemble.extratreesclassier get_params sklearn.linear_model.elasticnet method method method get_params get_params sklearn.ensemble.extratreesregressor sklearn.linear_model.elasticnetcv get_params sklearn.ensemble.gradientboostingclassier get_params sklearn.ensemble.gradientboostingregressor get_params sklearn.ensemble.randomforestclassier method get_params sklearn.linear_model.lars method get_params sklearn.linear_model.larscv method get_params sklearn.linear_model.lasso method get_params sklearn.linear_model.lassocv method get_params sklearn.ensemble.randomforestregressor get_params sklearn.linear_model.lassolars method method method method method method method index get_params sklearn.kernel_approximation.additivechi2sampler get_params sklearn.kernel_approximation.rbfsampler get_params sklearn.kernel_approximation.skewedchi2sampler method method method method method method method method method method method method method method method method method method method method get_params sklearn.linear_model.randomizedlasso get_params sklearn.linear_model.randomizedlogisticregression get_params sklearn.linear_model.ridge method get_params sklearn.linear_model.ridgeclassier get_params sklearn.linear_model.ridgeclassiercv method get_params sklearn.linear_model.ridgecv method method get_params sklearn.linear_model.sgdclassier method get_params sklearn.linear_model.sgdregressor get_params sklearn.linear_model.sparse.elasticnet method get_params sklearn.linear_model.sparse.lasso method get_params sklearn.linear_model.sparse.sgdclassier get_params sklearn.linear_model.sparse.sgdregressor get_params sklearn.manifold.isomap method get_params sklearn.manifold.locallylinearembedding get_params sklearn.mixture.dpgmm method get_params sklearn.mixture.gmm method get_params sklearn.mixture.vbgmm method get_params sklearn.multiclass.onevsoneclassier get_params sklearn.multiclass.onevsrestclassier scikitlearn user guide release 0.11 get_params sklearn.linear_model.lassolarscv get_params sklearn.neighbors.kneighborsclassier get_params sklearn.linear_model.lassolarsic get_params sklearn.neighbors.kneighborsregressor get_params sklearn.linear_model.linearregression get_params sklearn.neighbors.nearestcentroid get_params sklearn.linear_model.logisticregression get_params sklearn.neighbors.nearestneighbors get_params sklearn.linear_model.orthogonalmatchingpursuit get_params sklearn.neighbors.radiusneighborsclassier get_params sklearn.linear_model.perceptron method get_params sklearn.neighbors.radiusneighborsregressor method method method method method method get_params sklearn.pls.cca method get_params sklearn.pls.plscanonical method get_params sklearn.pls.plsregression method get_params sklearn.pls.plssvd method get_params sklearn.preprocessing.binarizer method get_params sklearn.preprocessing.kernelcenterer get_params sklearn.preprocessing.labelbinarizer get_params sklearn.preprocessing.normalizer get_params sklearn.preprocessing.scaler method get_params sklearn.qda.qda method get_params sklearn.semi_supervised.labelpropagation get_params sklearn.semi_supervised.labelspreading get_params sklearn.svm.linearsvc method get_params sklearn.svm.nusvc method get_params sklearn.svm.nusvr method get_params sklearn.svm.oneclasssvm method get_params sklearn.svm.svc method get_params sklearn.svm.svr method get_params sklearn.tree.decisiontreeclassier get_params sklearn.tree.decisiontreeregressor get_params sklearn.tree.extratreeclassier method get_params sklearn.tree.extratreeregressor method method method get_stop_words sklearn.feature_extraction.text.countvectorizer get_stop_words sklearn.feature_extraction.text.tdfvectorizer get_params sklearn.multiclass.outputcodeclassier method get_params sklearn.naive_bayes.bernoullinb method get_params sklearn.naive_bayes.gaussiannb method method sklearn.naive_bayes.multinomialnb method get_params method get_support sklearn.feature_selection.selectfdr get_support sklearn.feature_selection.selectfpr get_support sklearn.feature_selection.selectfwe method index get_support sklearn.feature_selection.selectkbest inverse_transform sklearn.linear_model.randomizedlasso get_support sklearn.feature_selection.selectpercentile inverse_transform sklearn.linear_model.randomizedlogisticregression get_support sklearn.linear_model.randomizedlasso inverse_transform sklearn.preprocessing.labelbinarizer get_support sklearn.linear_model.randomizedlogisticregression inverse_transform sklearn.preprocessing.scaler scikitlearn user guide release 0.11 method method method method gmm class sklearn.mixture gmmhmm class sklearn.hmm gradientboostingclassier class sklearn.ensemble gradientboostingregressor class sklearn.ensemble graph_lasso module sklearn.covariance graphlasso class sklearn.covariance graphlassocv class sklearn.covariance grid_to_graph sklearn.feature_extraction.image gridsearchcv class sklearn.grid_search hinge_loss module sklearn.metrics homogeneity_completeness_v_measure sklearn.metrics homogeneity_score module sklearn.metrics img_to_graph module sklearn.feature_extraction.image inverse_transform sklearn.decomposition.kernelpca method method method method method method method method method method method isomap class sklearn.manifold itergrid class sklearn.grid_search k_means module sklearn.cluster kernel_metrics module sklearn.metrics.pairwise module kernelcenterer class sklearn.preprocessing kernelpca class sklearn.decomposition kfold class sklearn.cross_validation kmeans class sklearn.cluster kneighbors sklearn.neighbors.kneighborsclassier kneighbors sklearn.neighbors.kneighborsregressor module kneighbors sklearn.neighbors.nearestneighbors kneighbors_graph module sklearn.neighbors kneighbors_graph sklearn.neighbors.kneighborsclassier kneighbors_graph sklearn.neighbors.kneighborsregressor kneighbors_graph sklearn.neighbors.nearestneighbors method method method inverse_transform sklearn.decomposition.pca inverse_transform sklearn.feature_extraction.dictvectorizer kneighborsclassier class sklearn.neighbors kneighborsregressor class sklearn.neighbors inverse_transform sklearn.decomposition.probabilisticpca inverse_transform sklearn.decomposition.randomizedpca l1_min_c module sklearn.svm labelbinarizer class sklearn.preprocessing labelpropagation class sklearn.semi_supervised labelspreading class sklearn.semi_supervised lars class sklearn.linear_model lars_path module sklearn.linear_model larscv class sklearn.linear_model lasso class sklearn.linear_model lasso class sklearn.linear_model.sparse lasso_path module sklearn.linear_model lasso_stability_path module sklearn.linear_model inverse_transform sklearn.feature_extraction.text.countvectorizer inverse_transform sklearn.feature_extraction.text.tdfvectorizer sklearn.feature_selection.selectfdr sklearn.feature_selection.selectfpr inverse_transform inverse_transform method method method method inverse_transform sklearn.feature_selection.selectfwe inverse_transform sklearn.feature_selection.selectkbest inverse_transform sklearn.feature_selection.selectpercentile method method method method lassocv class sklearn.linear_model lassolars class sklearn.linear_model lassolarscv class sklearn.linear_model lassolarsic class sklearn.linear_model lda class sklearn.lda index leaveonelabelout class sklearn.cross_validation leaveoneout class sklearn.cross_validation leaveplabelout class sklearn.cross_validation leavepout class sklearn.cross_validation ledoit_wolf module sklearn.covariance ledoitwolf class sklearn.covariance linear module sklearn.gaussian_process.correlation_models linear module sklearn.gaussian_process.regression_models scikitlearn user guide release 0.11 make_friedman2 module sklearn.datasets make_friedman3 module sklearn.datasets make_hastie_10_2 module sklearn.datasets make_low_rank_matrix module sklearn.datasets make_moons module sklearn.datasets make_multilabel_classication sklearn.datasets module make_regression module sklearn.datasets make_s_curve module sklearn.datasets make_sparse_coded_signal module sklearn.datasets make_sparse_spd_matrix module sklearn.datasets make_sparse_uncorrelated module sklearn.datasets make_spd_matrix module sklearn.datasets make_swiss_roll module sklearn.datasets manhattan_distances module sklearn.metrics.pairwise mean_shift module sklearn.cluster mean_squared_error module sklearn.metrics meanshift class sklearn.cluster mincovdet class sklearn.covariance minibatchdictionarylearning class sklearn.decomposition minibatchkmeans class sklearn.cluster minibatchsparsepca class sklearn.decomposition multilabel_ sklearn.multiclass.onevsrestclassier tribute multinomialhmm class sklearn.hmm multinomialnb class sklearn.naive_bayes nearestcentroid class sklearn.neighbors nearestneighbors class sklearn.neighbors nmf class sklearn.decomposition normalize module sklearn.preprocessing normalizer class sklearn.preprocessing nusvc class sklearn.svm nusvr class sklearn.svm oas class sklearn.covariance oas module sklearn.covariance oneclasssvm class sklearn.svm onevsoneclassier class sklearn.multiclass onevsrestclassier class sklearn.multiclass orthogonal_mp module sklearn.linear_model orthogonal_mp_gram module sklearn.linear_model orthogonalmatchingpursuit sklearn.linear_model class linear_kernel module sklearn.metrics.pairwise linearregression class sklearn.linear_model linearsvc class sklearn.svm load_20newsgroups module sklearn.datasets load_boston module sklearn.datasets load_diabetes module sklearn.datasets load_digits module sklearn.datasets load_les module sklearn.datasets load_iris module sklearn.datasets load_lfw_pairs module sklearn.datasets load_lfw_people module sklearn.datasets load_linnerud module sklearn.datasets load_sample_image module sklearn.datasets load_sample_images module sklearn.datasets load_svmlight_le module sklearn.datasets locally_linear_embedding module sklearn.manifold locallylinearembedding class sklearn.manifold logisticregression class sklearn.linear_model lower_bound sklearn.mixture.dpgmm method lower_bound sklearn.mixture.vbgmm method mahalanobis sklearn.covariance.ellipticenvelope mahalanobis sklearn.covariance.empiricalcovariance mahalanobis sklearn.covariance.graphlasso method method method mahalanobis method sklearn.covariance.graphlassocv mahalanobis sklearn.covariance.ledoitwolf method mahalanobis sklearn.covariance.mincovdet method mahalanobis sklearn.covariance.oas method mahalanobis sklearn.covariance.shrunkcovariance method make_blobs module sklearn.datasets make_circles module sklearn.datasets make_classication module sklearn.datasets make_friedman1 module sklearn.datasets index scikitlearn user guide release 0.11 outputcodeclassier class sklearn.multiclass pairwise_distances module sklearn.metrics.pairwise pairwise_kernels module sklearn.metrics.pairwise predict predict predict sklearn.ensemble.gradientboostingregressor method sklearn.ensemble.randomforestclassier method method sklearn.ensemble.randomforestregressor partial_t sklearn.cluster.minibatchkmeans method partial_t sklearn.decomposition.minibatchdictionarylearning method partial_t sklearn.linear_model.perceptron method partial_t sklearn.linear_model.sgdclassier predict sklearn.feature_selection.rfe method predict sklearn.feature_selection.rfecv method predict sklearn.gaussian_process.gaussianprocess predict sklearn.hmm.gmmhmm method predict sklearn.hmm.multinomialhmm method predict sklearn.lda.lda method predict sklearn.linear_model.ardregression method method method method method method index partial_t sklearn.linear_model.sgdregressor predict sklearn.linear_model.bayesianridge method partial_t sklearn.linear_model.sparse.sgdclassier partial_t sklearn.linear_model.sparse.sgdregressor patchextractor class sklearn.feature_extraction.image sklearn.linear_model.elasticnetcv method sklearn.linear_model.lassocv static method path path static pca class sklearn.decomposition perceptron class sklearn.linear_model permutation_test_score sklearn.cross_validation pipeline class sklearn.pipeline plscanonical class sklearn.pls plsregression class sklearn.pls plssvd class sklearn.pls polynomial_kernel sklearn.metrics.pairwise predict sklearn.linear_model.elasticnet method sklearn.linear_model.elasticnetcv method predict predict sklearn.linear_model.lars method predict sklearn.linear_model.larscv method predict sklearn.linear_model.lasso method predict sklearn.linear_model.lassocv method predict sklearn.linear_model.lassolars method predict sklearn.linear_model.lassolarscv method sklearn.linear_model.lassolarsic method sklearn.linear_model.linearregression sklearn.linear_model.logisticregression module predict predict predict module predict sklearn.linear_model.orthogonalmatchingpursuit method method method precision_recall_curve module sklearn.metrics precision_recall_fscore_support module sklearn.metrics precision_score module sklearn.metrics predict module sklearn.svm.libsvm predict sklearn.cluster.kmeans method predict sklearn.cluster.minibatchkmeans method predict sklearn.covariance.ellipticenvelope method predict sklearn.linear_model.perceptron method predict sklearn.linear_model.ridge method predict sklearn.linear_model.ridgeclassier method predict sklearn.linear_model.ridgeclassiercv method predict sklearn.linear_model.ridgecv method predict sklearn.linear_model.sgdclassier method sklearn.ensemble.extratreesclassier predict sklearn.linear_model.sgdregressor method method sklearn.ensemble.extratreesregressor method sklearn.ensemble.gradientboostingclassier method predict predict sklearn.linear_model.sparse.elasticnet method sklearn.linear_model.sparse.lasso method predict predict predict predict predict method method sklearn.linear_model.sparse.sgdclassier predict_log_proba sklearn.linear_model.logisticregression sklearn.linear_model.sparse.sgdregressor predict_log_proba sklearn.naive_bayes.bernoullinb predict sklearn.naive_bayes.bernoullinb method predict sklearn.naive_bayes.gaussiannb method predict sklearn.naive_bayes.multinomialnb method predict_log_proba sklearn.svm.svc method predict_log_proba sklearn.svm.svr method predict_log_proba sklearn.tree.decisiontreeclassier sklearn.neighbors.kneighborsclassier predict_log_proba sklearn.tree.extratreeclassier predict sklearn.mixture.dpgmm method predict sklearn.mixture.gmm method predict sklearn.mixture.vbgmm method predict sklearn.multiclass.onevsoneclassier predict predict method method method sklearn.multiclass.onevsrestclassier sklearn.multiclass.outputcodeclassier method predict predict predict predict predict sklearn.neighbors.kneighborsregressor method sklearn.neighbors.nearestcentroid method sklearn.neighbors.radiusneighborsclassier method sklearn.neighbors.radiusneighborsregressor method predict sklearn.pipeline.pipeline method predict sklearn.pls.cca method predict sklearn.pls.plscanonical method predict sklearn.pls.plsregression method predict sklearn.qda.qda method predict sklearn.semi_supervised.labelpropagation scikitlearn user guide release 0.11 method method method method predict_log_proba sklearn.naive_bayes.gaussiannb predict_log_proba sklearn.naive_bayes.multinomialnb predict_log_proba sklearn.qda.qda method predict_log_proba sklearn.svm.nusvc method predict_log_proba sklearn.svm.nusvr method predict_log_proba sklearn.svm.oneclasssvm method method method method method method method predict_ovo module sklearn.multiclass predict_ovr module sklearn.multiclass predict_proba module sklearn.svm.libsvm predict_proba sklearn.ensemble.extratreesclassier predict_proba sklearn.ensemble.gradientboostingclassier predict_proba sklearn.ensemble.randomforestclassier predict_proba sklearn.hmm.gmmhmm method predict_proba sklearn.hmm.multinomialhmm predict_proba sklearn.lda.lda method predict_proba sklearn.linear_model.logisticregression method method method predict sklearn.semi_supervised.labelspreading predict_proba sklearn.linear_model.perceptron predict sklearn.svm.linearsvc method predict sklearn.svm.nusvc method predict sklearn.svm.nusvr method predict sklearn.svm.oneclasssvm method predict sklearn.svm.svc method predict sklearn.svm.svr method predict sklearn.tree.decisiontreeclassier method sklearn.tree.decisiontreeregressor method predict predict sklearn.tree.extratreeclassier method predict sklearn.tree.extratreeregressor method predict_ecoc module sklearn.multiclass predict_log_proba sklearn.ensemble.extratreesclassier predict_log_proba sklearn.ensemble.randomforestclassier method method predict_log_proba sklearn.lda.lda method predict_proba sklearn.linear_model.sgdclassier predict_proba sklearn.linear_model.sparse.sgdclassier predict_proba sklearn.mixture.dpgmm method predict_proba sklearn.mixture.gmm method predict_proba sklearn.mixture.vbgmm method sklearn.naive_bayes.bernoullinb predict_proba predict_proba sklearn.naive_bayes.gaussiannb predict_proba sklearn.naive_bayes.multinomialnb predict_proba sklearn.pipeline.pipeline method predict_proba sklearn.qda.qda method predict_proba sklearn.semi_supervised.labelpropagation method method method method method method method index scikitlearn user guide release 0.11 predict_proba sklearn.semi_supervised.labelspreading method predict_proba sklearn.svm.nusvc method predict_proba sklearn.svm.nusvr method predict_proba sklearn.svm.oneclasssvm method predict_proba sklearn.svm.svc method predict_proba sklearn.svm.svr method predict_proba sklearn.tree.decisiontreeclassier predict_proba sklearn.tree.extratreeclassier method method probabilisticpca class sklearn.decomposition projectedgradientnmf class sklearn.decomposition pure_nugget module sklearn.gaussian_process.correlation_models qda class sklearn.qda quadratic module sklearn.gaussian_process.regression_models query sklearn.neighbors.balltree method query_radius sklearn.neighbors.balltree method r2_score module sklearn.metrics radius_neighbors sklearn.neighbors.nearestneighbors radius_neighbors sklearn.neighbors.radiusneighborsclassier radius_neighbors sklearn.neighbors.radiusneighborsregressor method method method radius_neighbors_graph module sklearn.neighbors radius_neighbors_graph sklearn.neighbors.nearestneighbors method radius_neighbors_graph sklearn.neighbors.radiusneighborsclassier method radius_neighbors_graph sklearn.neighbors.radiusneighborsregressor method radiusneighborsclassier class sklearn.neighbors randomizedlasso class sklearn.linear_model randomizedlogisticregression class sklearn.linear_model randomizedpca class sklearn.decomposition rbf_kernel module sklearn.metrics.pairwise rbfsampler class sklearn.kernel_approximation recall_score module sklearn.metrics reconstruct_from_patches_2d sklearn.feature_extraction.image module sklearn.manifold.isomap reconstruction_error method reduced_likelihood_function sklearn.gaussian_process.gaussianprocess method resample module sklearn.utils restrict sklearn.feature_extraction.dictvectorizer reweight_covariance sklearn.covariance.ellipticenvelope method method reweight_covariance method sklearn.covariance.mincovdet rfe class sklearn.feature_selection rfecv class sklearn.feature_selection ridge class sklearn.linear_model ridgeclassier class sklearn.linear_model ridgeclassiercv class sklearn.linear_model ridgecv class sklearn.linear_model roc_curve module sklearn.metrics rvs sklearn.hmm.gmmhmm method rvs sklearn.hmm.multinomialhmm method rvs sklearn.mixture.dpgmm method rvs sklearn.mixture.gmm method rvs sklearn.mixture.vbgmm method sample sklearn.hmm.gmmhmm method sample sklearn.hmm.multinomialhmm method sample sklearn.mixture.dpgmm method sample sklearn.mixture.gmm method sample sklearn.mixture.vbgmm method scale module sklearn.preprocessing scaler class sklearn.preprocessing score sklearn.cluster.kmeans method score sklearn.cluster.minibatchkmeans method score sklearn.covariance.ellipticenvelope method method score sklearn.covariance.graphlasso method score sklearn.covariance.graphlassocv method score sklearn.covariance.ledoitwolf method score sklearn.covariance.mincovdet method radiusneighborsregressor class sklearn.neighbors score sklearn.covariance.empiricalcovariance randomforestclassier class sklearn.ensemble randomforestregressor class sklearn.ensemble index scikitlearn user guide release 0.11 score sklearn.covariance.oas method score sklearn.covariance.shrunkcovariance method score sklearn.decomposition.probabilisticpca score sklearn.ensemble.extratreesclassier method method score sklearn.linear_model.ridgecv method score score sklearn.linear_model.sgdclassier method sklearn.linear_model.sgdregressor method score sklearn.linear_model.sparse.elasticnet method score sklearn.ensemble.extratreesregressor method score score score score sklearn.ensemble.gradientboostingclassier method sklearn.ensemble.gradientboostingregressor method sklearn.ensemble.randomforestclassier method method sklearn.ensemble.randomforestregressor score sklearn.linear_model.sparse.lasso method score sklearn.linear_model.sparse.sgdclassier score sklearn.linear_model.sparse.sgdregressor method method score sklearn.mixture.dpgmm method score sklearn.mixture.gmm method score sklearn.mixture.vbgmm method score sklearn.multiclass.onevsoneclassier method score sklearn.feature_selection.rfe method score sklearn.feature_selection.rfecv method score sklearn.gaussian_process.gaussianprocess method score sklearn.multiclass.outputcodeclassier method score sklearn.hmm.gmmhmm method score sklearn.hmm.multinomialhmm method score sklearn.lda.lda method score sklearn.linear_model.ardregression method score sklearn.linear_model.bayesianridge method score sklearn.linear_model.elasticnet method score sklearn.linear_model.elasticnetcv method score sklearn.linear_model.lars method score sklearn.linear_model.larscv method score sklearn.linear_model.lasso method score sklearn.linear_model.lassocv method score sklearn.linear_model.lassolars method score sklearn.linear_model.lassolarscv method score sklearn.linear_model.lassolarsic method score sklearn.linear_model.linearregression method score sklearn.linear_model.logisticregression method method score sklearn.linear_model.orthogonalmatchingpursuit score sklearn.linear_model.perceptron method score sklearn.linear_model.ridge method score sklearn.linear_model.ridgeclassier method sklearn.linear_model.ridgeclassiercv method score index score sklearn.naive_bayes.bernoullinb method score sklearn.naive_bayes.gaussiannb method score sklearn.naive_bayes.multinomialnb method sklearn.neighbors.kneighborsclassier sklearn.neighbors.kneighborsregressor score score method method score sklearn.neighbors.nearestcentroid method score sklearn.neighbors.radiusneighborsclassier method sklearn.neighbors.radiusneighborsregressor method score score sklearn.pipeline.pipeline method score sklearn.qda.qda method score sklearn.semi_supervised.labelpropagation score sklearn.semi_supervised.labelspreading method method score sklearn.svm.linearsvc method score sklearn.svm.nusvc method score sklearn.svm.nusvr method score sklearn.svm.svc method score sklearn.svm.svr method score sklearn.tree.decisiontreeclassier method sklearn.tree.decisiontreeregressor method score score sklearn.tree.extratreeclassier method score sklearn.tree.extratreeregressor method selectfdr class sklearn.feature_selection selectfpr class sklearn.feature_selection selectfwe class sklearn.feature_selection selectkbest class sklearn.feature_selection selectpercentile class sklearn.feature_selection scikitlearn user guide release 0.11 set_params sklearn.cluster.afnitypropagation set_params sklearn.ensemble.gradientboostingregressor method set_params sklearn.cluster.dbscan method set_params sklearn.cluster.kmeans method set_params sklearn.cluster.meanshift method set_params sklearn.cluster.minibatchkmeans method method method set_params sklearn.ensemble.randomforestclassier set_params sklearn.ensemble.randomforestregressor set_params sklearn.feature_extraction.dictvectorizer set_params sklearn.cluster.spectralclustering method method set_params sklearn.cluster.ward method set_params sklearn.covariance.ellipticenvelope set_params sklearn.feature_extraction.image.patchextractor set_params sklearn.feature_extraction.text.countvectorizer set_params sklearn.covariance.empiricalcovariance set_params sklearn.feature_extraction.text.tdftransformer set_params set_params set_params set_params sklearn.covariance.graphlasso method set_params sklearn.feature_extraction.text.tdfvectorizer sklearn.covariance.graphlassocv sklearn.feature_selection.rfe method sklearn.covariance.ledoitwolf method set_params sklearn.feature_selection.rfecv method sklearn.covariance.mincovdet method sklearn.feature_selection.selectfdr set_params set_params set_params sklearn.covariance.oas method set_params sklearn.covariance.shrunkcovariance method method set_params sklearn.feature_selection.selectfpr set_params sklearn.feature_selection.selectfwe set_params sklearn.feature_selection.selectkbest method method method method set_params sklearn.decomposition.dictionarylearning method set_params sklearn.decomposition.fastica method method set_params sklearn.decomposition.kernelpca method set_params sklearn.feature_selection.selectpercentile set_params sklearn.gaussian_process.gaussianprocess set_params sklearn.decomposition.minibatchdictionarylearning method set_params sklearn.decomposition.minibatchsparsepca method set_params sklearn.decomposition.nmf method set_params sklearn.decomposition.pca method set_params sklearn.decomposition.probabilisticpca set_params sklearn.grid_search.gridsearchcv set_params sklearn.hmm.gmmhmm method set_params sklearn.hmm.multinomialhmm method set_params sklearn.kernel_approximation.additivechi2sampler set_params sklearn.decomposition.projectedgradientnmf set_params sklearn.kernel_approximation.rbfsampler set_params sklearn.decomposition.randomizedpca set_params sklearn.kernel_approximation.skewedchi2sampler method method method method method method method method method method method method method method method method method set_params sklearn.decomposition.sparsecoder set_params sklearn.decomposition.sparsepca method set_params sklearn.ensemble.extratreesclassier method set_params sklearn.ensemble.extratreesregressor set_params sklearn.lda.lda method set_params sklearn.linear_model.ardregression set_params sklearn.linear_model.bayesianridge set_params sklearn.linear_model.elasticnet method set_params sklearn.linear_model.elasticnetcv set_params sklearn.linear_model.lars method method method method set_params sklearn.ensemble.gradientboostingclassier method index scikitlearn user guide release 0.11 set_params sklearn.linear_model.larscv method set_params sklearn.naive_bayes.bernoullinb method set_params sklearn.linear_model.lasso method set_params sklearn.linear_model.lassocv method set_params sklearn.naive_bayes.gaussiannb method set_params sklearn.linear_model.lassolars method method set_params sklearn.linear_model.lassolarscv method set_params sklearn.neighbors.kneighborsclassier set_params sklearn.neighbors.kneighborsregressor set_params sklearn.linear_model.lassolarsic method sklearn.naive_bayes.multinomialnb set_params set_params sklearn.linear_model.linearregression method set_params sklearn.linear_model.logisticregression method set_params sklearn.linear_model.orthogonalmatchingpursuit method set_params sklearn.linear_model.perceptron method method set_params sklearn.linear_model.randomizedlasso set_params sklearn.neighbors.nearestcentroid set_params sklearn.neighbors.nearestneighbors set_params sklearn.neighbors.radiusneighborsclassier set_params sklearn.neighbors.radiusneighborsregressor set_params sklearn.linear_model.randomizedlogisticregression set_params sklearn.linear_model.ridge method set_params sklearn.linear_model.ridgeclassier set_params sklearn.pipeline.pipeline method set_params sklearn.pls.cca method set_params sklearn.pls.plscanonical method set_params sklearn.pls.plsregression method set_params sklearn.pls.plssvd method set_params sklearn.preprocessing.binarizer method set_params sklearn.preprocessing.kernelcenterer set_params sklearn.preprocessing.labelbinarizer set_params sklearn.preprocessing.normalizer method set_params sklearn.preprocessing.scaler method set_params sklearn.qda.qda method set_params sklearn.semi_supervised.labelpropagation set_params sklearn.semi_supervised.labelspreading set_params sklearn.svm.linearsvc method set_params sklearn.svm.nusvc method set_params sklearn.svm.nusvr method set_params sklearn.svm.oneclasssvm method set_params sklearn.svm.svc method set_params sklearn.svm.svr method set_params sklearn.tree.decisiontreeclassier set_params sklearn.tree.decisiontreeregressor set_params sklearn.tree.extratreeclassier method set_params sklearn.tree.extratreeregressor method method method sgdclassier class sklearn.linear_model sgdclassier class sklearn.linear_model.sparse sgdregressor class sklearn.linear_model method method method method method method method method method method method method method method method method method method method set_params sklearn.linear_model.ridgeclassiercv method set_params sklearn.linear_model.ridgecv method method set_params sklearn.linear_model.sgdclassier set_params sklearn.linear_model.sgdregressor set_params sklearn.linear_model.sparse.elasticnet method set_params sklearn.linear_model.sparse.lasso method set_params sklearn.linear_model.sparse.sgdclassier set_params sklearn.linear_model.sparse.sgdregressor set_params sklearn.manifold.isomap method set_params sklearn.manifold.locallylinearembedding set_params sklearn.mixture.dpgmm method set_params sklearn.mixture.gmm method set_params sklearn.mixture.vbgmm method set_params sklearn.multiclass.onevsoneclassier set_params sklearn.multiclass.onevsrestclassier set_params sklearn.multiclass.outputcodeclassier index scikitlearn user guide release 0.11 sgdregressor class sklearn.linear_model.sparse staged_decision_function shrunk_covariance module sklearn.covariance shrunkcovariance class sklearn.covariance shufe module sklearn.utils shufesplit class sklearn.cross_validation sigma sklearn.naive_bayes.gaussiannb attribute silhouette_score module sklearn.metrics skewedchi2sampler class sklearn.kernel_approximation sklearn.cluster module sklearn.covariance module sklearn.cross_validation module sklearn.datasets module sklearn.decomposition module sklearn.ensemble module sklearn.feature_extraction module sklearn.feature_extraction.image module sklearn.feature_extraction.text module sklearn.feature_selection module sklearn.gaussian_process module sklearn.grid_search module sklearn.hmm module sklearn.kernel_approximation module sklearn.lda module sklearn.linear_model module sklearn.linear_model.sparse module sklearn.manifold module sklearn.metrics module sklearn.metrics.cluster module sklearn.metrics.pairwise module sklearn.mixture module sklearn.multiclass module sklearn.naive_bayes module sklearn.neighbors module sklearn.pipeline module sklearn.pls module sklearn.preprocessing module sklearn.qda module sklearn.semi_supervised module sklearn.svm module sklearn.tree module sklearn.utils module sparse_encode module sklearn.decomposition sparsecoder class sklearn.decomposition sparsepca class sklearn.decomposition spectral_clustering module sklearn.cluster spectralclustering class sklearn.cluster squared_exponential module sklearn.gaussian_process.correlation_models staged_decision_function sklearn.ensemble.gradientboostingclassier method sklearn.ensemble.gradientboostingregressor method staged_predict sklearn.ensemble.gradientboostingregressor method startprob_ sklearn.hmm.gmmhmm attribute startprob_ sklearn.hmm.multinomialhmm attribute stratiedkfold class sklearn.cross_validation stratiedshufesplit class sklearn.cross_validation svc class sklearn.svm svr class sklearn.svm tdftransformer sklearn.feature_extraction.text tdfvectorizer class sklearn.feature_extraction.text class theta sklearn.naive_bayes.gaussiannb attribute train_test_split module sklearn.cross_validation transform sklearn.cluster.kmeans method transform sklearn.cluster.minibatchkmeans method transform sklearn.decomposition.dictionarylearning method transform sklearn.decomposition.fastica method transform sklearn.decomposition.kernelpca method transform sklearn.decomposition.minibatchdictionarylearning method method method method method method transform sklearn.decomposition.minibatchsparsepca transform sklearn.decomposition.nmf method transform sklearn.decomposition.pca method transform sklearn.decomposition.probabilisticpca transform sklearn.decomposition.projectedgradientnmf transform sklearn.decomposition.randomizedpca transform sklearn.decomposition.sparsecoder transform sklearn.decomposition.sparsepca method transform sklearn.ensemble.extratreesclassier transform sklearn.ensemble.extratreesregressor transform sklearn.ensemble.randomforestclassier method method method index scikitlearn user guide release 0.11 transform sklearn.ensemble.randomforestregressor method transform sklearn.feature_extraction.dictvectorizer transform sklearn.pls.plsregression method transform sklearn.pls.plssvd method transform sklearn.preprocessing.binarizer method transform sklearn.feature_extraction.image.patchextractor transform sklearn.preprocessing.kernelcenterer transform sklearn.feature_extraction.text.countvectorizer transform sklearn.preprocessing.labelbinarizer method method transform sklearn.feature_extraction.text.tdftransformer transform sklearn.preprocessing.normalizer method method method method method method transform sklearn.feature_extraction.text.tdfvectorizer transform sklearn.feature_selection.rfe method transform sklearn.feature_selection.rfecv method transform sklearn.feature_selection.selectfdr method method transform sklearn.preprocessing.scaler method transform sklearn.svm.linearsvc method transform sklearn.tree.decisiontreeclassier method transform sklearn.tree.decisiontreeregressor transform sklearn.tree.extratreeclassier method transform sklearn.tree.extratreeregressor method transmat_ sklearn.hmm.gmmhmm attribute transmat_ sklearn.hmm.multinomialhmm attribute transform sklearn.feature_selection.selectfpr method transform sklearn.feature_selection.selectfwe transform sklearn.feature_selection.selectkbest method method transform sklearn.feature_selection.selectpercentile transform method method method sklearn.kernel_approximation.rbfsampler transform sklearn.kernel_approximation.additivechi2sampler v_measure_score module sklearn.metrics vbgmm class sklearn.mixture transform sklearn.kernel_approximation.skewedchi2sampler ward class sklearn.cluster ward_tree module sklearn.cluster zero_one module sklearn.metrics zero_one_score module sklearn.metrics transform sklearn.lda.lda method transform sklearn.linear_model.perceptron method sklearn.linear_model.logisticregression method method transform transform sklearn.linear_model.randomizedlasso transform sklearn.linear_model.randomizedlogisticregression method method method method method method method transform sklearn.linear_model.sgdclassier transform sklearn.linear_model.sgdregressor transform sklearn.linear_model.sparse.sgdclassier transform sklearn.linear_model.sparse.sgdregressor transform sklearn.manifold.isomap method transform sklearn.manifold.locallylinearembedding transform sklearn.pipeline.pipeline method transform sklearn.pls.cca method transform sklearn.pls.plscanonical method index 
