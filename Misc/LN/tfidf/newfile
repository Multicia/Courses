0: scikit learn user guide release scikit learn developers june contents user guide 
1:  
2:  
3:  
4:  
5:  
6:  
7: installing scikit learn tutorials bottom scikit learn supervised learning unsupervised learning model selection dataset transformations dataset loading utilities 
8: reference 
9:  
10:  
11:  
12:  
13:  
14:  
15:  
16:  
17:  
18:  
19:  
20: example gallery examples 
21:  
22:  
23:  
24:  
25:  
26:  
27:  
28:  
29:  
30:  
31:  
32:  
33:  
34:  
35: development 
36:  
37:  
38:  
39:  
40:  
41:  
42: contributing 
43: optimize speed utilities developers developers tips debugging support presentations tutorials scikit learn 
44:  
45:  
46:  
47:  
48:  
49:  
50:  
51:  
52:  
53:  
54:  
55: bibliography python module index python module index index scikit learn user guide release scikit learn python module integrating classic machine learning algorithms tightly knit sci entic python world numpy scipy matplotlib aims provide simple efcient solutions learning problems accessible everybody reusable various contexts machine learning versatile tool science engineering 
56: license open source commercially usable bsd license clause documentation scikit learn version versions printable format see documentation resources 
57: contents scikit learn user guide release contents chapter one user guide installing scikit learn different ways get scikit learn installed install version scikit learn provided operating system distribution quickest option operating systems distribute scikit learn 
58: install ofcial release best approach users want stable version number arent concerned running slightly older version scikit learn 
59: install latest development version best users want latest greatest features arent afraid running brand new code 
60: note wish contribute project recommended install latest development version 
61: installing ofcial release installing source installing source requires installed python numpy scipy setuptools python development headers working compiler debian based systems get executing root privileges sudo apt get install python dev python numpy python numpy dev python setuptools python numpy dev python scipy libatlas dev order build documentation run example code contains documentation need note matplotlib sudo apt get install python matplotlib note ubuntu lts package libatlas dev called libatlas headers easy install usually fastest way install latest stable release pip easy_install install update command scikit learn user guide release pip install scikit learn easy_install scikit learn easy_install note might need root privileges run commands 
62: source package download package http pypi python org pypi scikit learn unpack sources archive packages uses distutils default way installing python modules install command python setup install windows installer download windows installer downloads projects web page note must also installed packages numpy setuptools package also expected work python 
63: installing windows 64bit http www lfd uci edu gohlke pythonlibs scikit learn note numpy scipy matplotlib easiest option also download url 
64: require compatible version download 64bit version install binaries scikit building windows build scikit learn windows need compiler addition numpy scipy setuptools least mingw port gcc windows microsoft visual work box force use particular compiler write named setup cfg source directory content build_ext compiler my_compiler build compiler my_compiler my_compiler one mingw32 msvc appropriate compiler set assuming python path see python faq windows details installation done executing command python setup install build precompiled package like ones distributed downloads section command execute python setup bdist_wininst doc logos scikit learn logo bmp create installable binary directory dist 
65: chapter user guide scikit learn user guide release third party distributions scikit learn third party distributions providing versions scikit learn integrated package management systems make installation upgrading much easier users since integration includes ability automat ically install dependencies numpy scipy scikit learn requires following list linux distributions provide version scikit learn debian derivatives ubuntu debian package named python sklearn formerly python scikits learn installed using following commands root privileges apt get install python sklearn additionally backport builds recent release scikit learn existing releases debian ubuntu available neurodebian repository 
66: python python distributes scikit learn additional plugin found additional plugins page 
67: enthought python distribution enthought python distribution already ships recent version 
68: macports macports package named py26 sklearn py27 sklearn depending version python installed typing following command sudo port install py26 scikits learn sudo port install py27 scikits learn depending version python want use 
69: netbsd scikit learn available via pkgsrc wip http pkgsrc wip scikit_learn bleeding edge see section retrieving latest code get development version 
70:  
71: installing scikit learn scikit learn user guide release testing testing requires nose library installation package tested executing outside source directory nosetests sklearn exe give lot output warnings eventually nish text similar ran tests 920s skip otherwise please consider posting issue bug tracker mailing list 
72: note alternative testing method reason recommended method failing please try alternate method python import sklearn sklearn test method might display doctest failures nosetests issues 
73: scikit learn also tested without package installed must compile sources inplace source directory python setup build_ext inplace test run using nosetests nosetests sklearn automated commands make make test tutorials bottom scikit learn quick start section introduce machine learning vocabulary use scikit learn give simple learning example 
74: introduction machine learning scikit learn section contents section introduce machine learning vocabulary use scikit learn give simple learning example 
75: chapter user guide scikit learn user guide release machine learning problem setting general learning problem considers set samples data try predict properties unknown data sample single number instance multi dimensional entry aka multivariate data said several attributes features separate learning problems large categories supervised learning data comes additional attributes want predict click scikit learn supervised learning page problem either classication samples belong two classes want learn already labeled data predict class unlabeled data example classication problem would digit recognition example aim assign input vector one nite number discrete categories 
76: regression desired output consists one continuous variables task called regression example regression problem would prediction length salmon function age weight 
77: unsupervised learning training data consists set input vectors without corresponding target values goal problems may discover groups similar examples within data called clustering determine distribution data within input space known density estima tion project data high dimensional space two thee dimensions purpose visualization click scikit learn unsupervised learning page 
78: training set testing set machine learning learning properties data set applying new data common practice machine learning evaluate algorithm split data hand two sets one call training set learn data properties one call testing set test properties 
79: loading example dataset scikit learn comes standard datasets instance iris digits datasets classication boston house prices dataset regression sklearn import datasets iris datasets load_iris digits datasets load_digits dataset dictionary like object holds data metadata data data stored data member n_samples n_features array case supervised problem explanatory variables stored target member details different datasets found dedicated section instance case digits dataset digits data gives access features used classify digits samples 
80: print digits data 
81:  
82:  
83:  
84:  
85:  
86:  
87: tutorials bottom scikit learn scikit learn user guide release digits target gives ground truth digit dataset number corresponding digit image trying learn digits target array shape data arrays data always array n_samples n_features although original data may different shape case digits original sample image shape accessed using digits images array simple example dataset illustrates starting original problem one shape data consumption scikit learn 
88: learning predicting case digits dataset task predict value hand written digit image given samples possible classes estimator able predict labels corresponding new data scikit learn estimator plain python class implements methods predict example estimator class sklearn svm svc implements support vector classication con structor estimator takes arguments parameters model time consider estimator black box sklearn import svm clf svm svc gamma choosing parameters model example set value gamma manually possible automatically good values parameters using tools grid search cross validation 
89: call estimator instance clf classier must tted model must learn model done passing training set fit method training set let use images dataset apart last one clf fit digits data digits target svc cache_size class_weight none coef0 degree gamma kernel rbf probability false shrinking true tol verbose false predict new values particular ask classier digit last image digits dataset used train classier chapter user guide clf predict digits data array scikit learn user guide release corresponding image following images poor resolution agree classier complete example classication problem available example run study recognizing hand written digits 
90: see challenging task model persistence possible save model scikit using pythons built persistence model namely pickle sklearn import svm sklearn import datasets clf svm svc iris datasets load_iris iris data iris target clf fit svc cache_size class_weight none coef0 degree gamma kernel rbf probability false shrinking true tol verbose false import pickle pickle dumps clf clf2 pickle loads clf2 predict array specic case scikit may interesting use joblibs replacement pickle joblib dump joblib load efcient big data pickle disk string sklearn externals import joblib joblib dump clf filename pkl statistical learning tutorial tutorial covers models tools available data processing scikit learn learn data 
91: tutorial statistical learning scientic data processing tutorials bottom scikit learn scikit learn user guide release statistical learning machine learning technique growing importance size datasets experimental sciences facing rapidly growing problems tackles range building prediction function linking different observations classifying observations learning structure unlabeled dataset tutorial explore statistical learning use machine learning techniques goal statistical inference drawing conclusions data hand sklearn python module integrating classic machine learning algorithms tightly knit world scien tic python packages numpy scipy matplotlib 
92: warning cross version compatibility use scikit learn release import path changed scikits learn sklearn import try sklearn import something except importerror scikits learn import something statistical learning setting estimator object scikit learn datasets scikit learn deals learning information one datasets represented arrays understood list multi dimensional observations say rst axis arrays samples axis second features axis 
93: simple example shipped scikit iris dataset sklearn import datasets iris datasets load_iris data iris data data shape made observations irises described features sepal petal length width detailed iris descr 
94: data intially n_samples n_features shape needs preprocessed used scikit 
95: chapter user guide example reshaping data digits dataset scikit learn user guide release digits dataset made 8x8 images hand written digits digits datasets load_digits digits images shape import pylab imshow digits images cmap gray_r matplotlib image axesimage object use dataset scikit transform 8x8 image feature vector length data digits images reshape digits images shape estimators objects fitting data core object scikit learn estimator object estimator objects expose method takes dataset array estimator fit data estimator parameters parameters estimator set instanciated modifying corresponding attribute estimator estimator param1 param2 estimator param1 estimated parameters data tted estimator parameters estimated data hand estimated parameters attributes estimator object ending underscore estimator estimated_param_ supervised learning predicting output variable high dimensional observations problem solved supervised learning supervised learning consists learning link two datasets observed data external variable trying predict usually called target labels often array length n_samples supervised estimators scikit learn implement method model predict method given unlabeled observations returns predicted labels 
96: tutorials bottom scikit learn scikit learn user guide release vocabulary classication regression prediction task classify observations set nite labels words name objects observed task said classication task opposite goal predict continous target variable said regression task scikit learn classication tasks vector integers note see introduction machine learning scikit learn tutorial quick run basic machine learning vocabulary used within scikit learn 
97: nearest neighbor curse dimensionality classifying irises different types irises setosa versicolour virginica petal sepal length width iris dataset classication task consisting identifying import numpy sklearn import datasets iris datasets load_iris iris_x iris data iris_y iris target unique iris_y array nearest neighbors classier simplest possible classier nearest neighbor given new observation x_test training set data used train estimator observation closest feature vector please see nearest neighbors section online scikit learn documentation information type classier training set testing set experimenting learning algorithm important test prediction estimator data used estimator would evaluating performance estimator new data datasets often split train test data 
98: chapter user guide scikit learn user guide release knn nearest neighbors classication example split iris data train test data random permutation split data randomly random seed indices random permutation len iris_x iris_x_train iris_x indices iris_y_train iris_y indices iris_x_test iris_x indices iris_y_test iris_y indices create fit nearest neighbor classifier sklearn neighbors import kneighborsclassifier knn kneighborsclassifier knn fit iris_x_train iris_y_train kneighborsclassifier algorithm auto leaf_size n_neighbors warn_on_equidistant true weights uniform knn predict iris_x_test array iris_y_test array curse dimensionality estimator effective need distance neighboring points less value depends problem one dimension requires average points context knn example data described one feature values ranging training observations new data thus away therefore nearest neighbor decision rule efcient soon small compared scale class feature variations number features require points lets say require points one dimension points required dimensions pave space becomes large number training points required good estimator grows exponentially example point single number bytes effective knn estimator paltry mensions would require training data current estimated size entire internet exabytes called curse dimensionality core problem machine learning addresses 
99: tutorials bottom scikit learn scikit learn user guide release linear model regression sparsity diabetes dataset diabetes dataset consists physiological variables age sex weight blood pressure measure patients indication disease progression one year diabetes datasets load_diabetes diabetes_x_train diabetes data diabetes_x_test diabetes data diabetes_y_train diabetes target diabetes_y_test diabetes target task hand predict disease progression physiological variables 
100: linear regression linearregression simplest form linear model data set adjust ing set parameters order make sum squared residuals model small possilbe 
101: linear models data target variable coefcients observation noise sklearn import linear_model regr linear_model linearregression regr fit diabetes_x_train diabetes_y_train linearregression copy_x true fit_intercept true normalize false print regr coef_ mean square error mean regr predict diabetes_x_test diabetes_y_test 
102: explained variance score perfect prediction means linear relationship regr score diabetes_x_test diabetes_y_test 
103: chapter user guide shrinkage data points per dimension noise observations induces high variance scikit learn user guide release test regr linear_model linearregression import pylab figure random seed range 
104: this_x random normal size regr fit this_x plot test regr predict test scatter this_x solution two randomly chosen set observations likely uncorrelated 
105: high dimensional statistical learning shrink regression coefcients zero called ridge regression regr linear_model ridge alpha figure random seed range 
106: this_x random normal size regr fit this_x plot test regr predict test scatter this_x tutorials bottom scikit learn scikit learn user guide release example bias variance tradeoff larger ridge alpha parameter higher bias lower variance choose alpha minimize left error time using diabetes dataset rather synthetic data alphas logspace print regr set_params alpha alpha fit diabetes_x_train diabetes_y_train score diabetes_x_test diabetes_y_test alpha alphas note capturing tted parameters noise prevents model generalize new data called overtting bias introduced ridge regression called regularization 
107: sparsity fitting features note representation full diabetes dataset would involve dimensions feature dimensions one target variable hard develop intuition representation may useful keep mind would fairly empty space 
108: see although feature strong coefcient full model conveys little information considered feature improve conditioning problem mitigate curse dimensionality would interesting select informative features set non informative ones like feature ridge regression decrease contribution set zero another penalization approach called lasso least absolute shrinkage selection operator set coefcients zero methods called sparse method sparsity seen application occams razor prefer simpler models 
109: chapter user guide scikit learn user guide release fit diabetes_x_train diabetes_y_train score diabetes_x_test diabetes_y_test regr linear_model lasso scores regr set_params alpha alpha best_alpha alphas scores index max scores regr alpha best_alpha regr fit diabetes_x_train diabetes_y_train lasso alpha copy_x true fit_intercept true alpha alphas max_iter normalize false positive false precompute auto tol warm_start false print regr coef_ 
110:  
111: different algorithms problem different algorithms used solve mathematical problem instance lasso object scikit learn solves lasso regression using coordinate decent method efcient large datasets however scikit learn also provides lassolars object using lars efcient problems weight vector estimated sparse problems observations 
112: classication classication labeling iris task linear regression right approach give much weight data far decision frontier linear approach sigmoid function logistic function sigmoid offset exp offset logistic linear_model logisticregression 1e5 logistic fit iris_x_train iris_y_train logisticregression class_weight none dual false fit_intercept true intercept_scaling penalty tol tutorials bottom scikit learn scikit learn user guide release known logisticregression 
113: multiclass classication several classes predict option often used one versus classiers use voting heuristic nal decision 
114: shrinkage sparsity logistic regression parameter controls amount regularization logisticregression object large value results less regularization penalty gives shrinkage non sparse coefcients penalty gives sparsity 
115: exercise try classifying digits dataset nearest neighbors linear model leave last test prediction performance observations 
116: sklearn import datasets neighbors linear_model digits datasets load_digits x_digits digits data y_digits digits target solution auto_examples exercises plot_digits_classification_exercise support vector machines svms linear svms support vector machines belong discrimant model family try combination samples build plane maximizing margin two classes regularization set parameter small value means margin calculated using many observations around separating line regularization large value means margin calculated observations close separating line less regularization 
117: chapter user guide unregularized svm regularized svm default scikit learn user guide release regression classication svc support vector classication 
118: svms used regression svr support vector sklearn import svm svc svm svc kernel linear svc fit iris_x_train iris_y_train svc cache_size class_weight none coef0 degree gamma kernel linear probability false shrinking true tol verbose false warning normalizing data many estimators including svms datasets unit standard deviation feature important get good prediction 
119: using kernels classes always linearly separable feature space solution build decision function linear may instance polynomial done using kernel trick seen creating decision energy positioning kernels observations tutorials bottom scikit learn scikit learn user guide release linear kernel polynomial kernel svc svm svc kernel linear svc svm svc kernel poly degree polynomial degree degree rbf kernel radial basis function svc svm svc kernel rbf gamma inverse size radial kernel interactive example see svm gui download svm_gui add data points classes right left button model change parameters data 
120: chapter user guide scikit learn user guide release exercise try classifying classes iris dataset svms rst features leave class test prediction performance observations warning classes ordered leave last would testing one class hint use decision_function method grid get intuitions 
121: iris datasets load_iris iris data iris target solution auto_examples exercises plot_iris_exercise model selection choosing estimators parameters score cross validated scores seen every estimator exposes score method judge quality prediction new data bigger better sklearn import datasets svm digits datasets load_digits x_digits digits data y_digits digits target svc svm svc kernel linear svc fit x_digits y_digits score x_digits y_digits get better measure prediction accuracy use proxy goodness model successively split data folds use training testing import numpy x_folds array_split x_digits y_folds array_split y_digits scores list range print scores use list copy order pop later x_train list x_folds x_test x_train concatenate x_train y_train list y_folds y_test y_train concatenate y_train scores append svc fit x_train y_train score x_test y_test x_train pop y_train pop called kfold cross validation cross validation generators code split data train test sets tedious write sklearn exposes cross validation generators generate list indices purpose tutorials bottom scikit learn scikit learn user guide release sklearn import cross_validation k_fold cross_validation kfold indices true train_indices test_indices k_fold train test train test train test print train test train_indices test_indices cross validation implemented easily kfold cross_validation kfold len x_digits svc fit x_digits train y_digits train score x_digits test y_digits test train test kfold compute score method estimator sklearn exposes helper function cross_validation cross_val_score svc x_digits y_digits kfold n_jobs array n_jobs means computation dispatched cpus computer 
122: cross validation generators kfold stratifiedkfold split folds train test left make sure classes even accross folds leaveoneout leave one observation leaveonelabelout labels takes label array group observations chapter user guide scikit learn user guide release exercise digits dataset plot cross validation score svc estimator rbf kernel function parameter use logarithmic grid points 
123: sklearn import cross_validation datasets svm digits datasets load_digits digits data digits target svc svm svc c_s logspace scores list scores_std list solution auto_examples exercises plot_cv_digits grid search cross validated estimators grid search sklearn provides object given data computes score estimator parameter grid chooses parameters maximize cross validation score object takes estimator construction exposes estimator api sklearn grid_search import gridsearchcv gammas logspace clf gridsearchcv estimator svc param_grid dict gamma gammas clf fit x_digits y_digits gridsearchcv none clf best_score_ clf best_estimator_ gamma 9999999999999995e n_jobs prediction performance test set good train set clf score x_digits y_digits default gridsearchcv uses fold cross validation however detects classier passed rather regressor uses stratied fold 
124: nested cross validation cross_validation cross_val_score clf x_digits y_digits array two cross validation loops performed parallel one gridsearchcv estimator set gamma one cross_val_score measure prediction performance estimator resulting scores unbiased estimates prediction score new data 
125: warning cannot nest objects parallel computing n_jobs different 
126: tutorials bottom scikit learn scikit learn user guide release cross validated estimators cross validation set parameter done efciently algorithm algorithm basis certain estimators sklearn exposes cross validation evaluating estimator per formance estimators set parameter automatically cross validation sklearn import linear_model datasets lasso linear_model lassocv diabetes datasets load_diabetes x_diabetes diabetes data y_diabetes diabetes target lasso fit x_diabetes y_diabetes lassocv alphas array copy_x true none eps fit_intercept true max_iter n_alphas normalize false precompute auto tol verbose false estimator chose automatically lambda lasso alpha 
127: estimators called similarly counterparts appended name 
128: exercise diabetes dataset optimal regularization parameter alpha bonus much trust selection alpha import numpy import pylab sklearn import cross_validation datasets linear_model diabetes datasets load_diabetes diabetes data diabetes target lasso linear_model lasso alphas logspace solution auto_examples exercises plot_cv_diabetes unsupervised learning seeking representations data clustering grouping observations together problem solved clustering given iris dataset knew types iris access taxonomist label could try clustering task split observations well separated group called clusters 
129: chapter user guide means clustering note exists lot different clustering criteria associated algorithms sim scikit learn user guide release plest clustering algorithm means 
130: sklearn import cluster datasets iris datasets load_iris x_iris iris data y_iris iris target k_means cluster kmeans k_means fit x_iris kmeans copy_x true init means max_iter print k_means labels_ print y_iris warning absolutely guarantee recovering ground truth first choosing right number clusters hard second algorithm sensitive initialization fall local minima although sklearn package play many tricks mitigate issue 
131: bad initialization dont interpret clustering results clusters ground truth tutorials bottom scikit learn scikit learn user guide release application example vector quantization clustering general kmeans particular seen way choosing small number examplars compress information problem sometimes known vector quantization instance used posterize image lena lena scipy import misc lena misc lena import scipy try except attributeerror lena reshape need n_sample n_feature array k_means cluster kmeans n_init k_means fit kmeans copy_x true init means values k_means cluster_centers_ squeeze labels k_means labels_ lena_compressed choose labels values lena_compressed shape lena shape raw image means quantization equal bins image histogram hierarchical agglomerative clustering ward hierarchical clustering method type cluster analysis aims build hierarchy clusters general various approaches technique either agglomerative bottom approaches divisive top approaches 
132: estimating large number clusters top approaches statisticaly ill posed slow due starting observations one cluster splits recursively agglomerative hierarchical clustering bottom approach successively merges observations together particularly useful clusters interest made observations ward clustering minimizes criterion similar means bottom approach number clusters large much computationally efcient means 
133: connectivity constrained clustering ward clustering possible specify samples clus tered together giving connectivity graph graphs scikit represented adjacency matrix ten sparse matrix used useful instance retrieve connect regions clustering image chapter user guide scikit learn user guide release generate data lena misc lena downsample image factor lena lena lena lena lena reshape lena define structure data pixels connected neighbors connectivity grid_to_graph lena shape compute clustering print compute structured hierarchical clustering time time n_clusters number regions ward ward n_clusters n_clusters connectivity connectivity fit label reshape ward labels_ lena shape print elaspsed time time time print number pixels label size print number clusters unique label size feature agglomeration seen sparsity could used mitigate curse dimensionality insufcience observations compared number features another approach merge together similar features feature agglomeration approach implementing clustering feature direction words clustering transposed data 
134: digits datasets load_digits images digits images reshape images len images connectivity grid_to_graph images shape agglo cluster wardagglomeration connectivity connectivity agglo fit wardagglomeration connectivity x_reduced agglo transform n_clusters x_approx agglo inverse_transform x_reduced images_approx reshape x_approx images shape tutorials bottom scikit learn scikit learn user guide release transform inverse_transform methods estimators expose transform method instance reduce dimensionality dataset 
135: decompositions signal components loadings components loadings multivariate data problem trying solve rewrite different observation basis want learn loadings set components different criteria exist choose components principal component analysis pca principal component analysis pca selects successive components explain maximum variance signal 
136: point cloud spanned observations one direction one univariate features almost exactly computed using pca nds directions data used transform data pca reduce dimensionality data projecting principal subspace 
137: create signal useful dimensions random normal size random normal size sklearn import decomposition pca decomposition pca pca fit pca copy true n_components none whiten false print pca explained_variance_ 18565811e 19346747e 43026679e see first components useful pca n_components x_reduced pca fit_transform x_reduced shape chapter user guide independent component analysis ica independent component analysis ica selects components able recover non distribution loadings carries maximum amount independent information 
138: scikit learn user guide release gaussian independent signals generate sample data time linspace sin time sign sin time random normal size shape std axis mix data array mixing matrix dot generate observations standardize data signal sinusoidal signal signal square signal add noise compute ica ica decomposition fastica ica fit transform get estimated sources ica get_mixing_matrix get estimated mixing matrix allclose dot true tutorials bottom scikit learn scikit learn user guide release putting together pipelining seen estimators transform data estimators predict variables create combined estimators import pylab sklearn import linear_model decomposition datasets cross_validation logistic linear_model logisticregression pca decomposition pca sklearn pipeline import pipeline pipe pipeline steps pca pca logistic logistic digits datasets load_digits x_digits digits data y_digits digits target plot pca spectrum pca fit x_digits figure figsize clf axes plot pca explained_variance_ linewidth axis tight xlabel n_components ylabel explained_variance_ prediction sklearn grid_search import gridsearchcv n_components logspace parameters pipelines set using separated parameter names estimator gridsearchcv pipe dict pca__n_components n_components logistic__c chapter user guide scikit learn user guide release estimator fit x_digits y_digits axvline estimator best_estimator_ named_steps pca n_components linestyle label n_components chosen legend prop dict size face recognition eigenfaces dataset used example preprocessed excerpt labeled faces wild aka lfw http vis www umass edu lfw lfw funneled tgz 233mb faces recognition example using eigenfaces svms dataset used example preprocessed excerpt labeled faces wild aka lfw_ http vis www umass edu lfw lfw funneled tgz 233mb _lfw http vis www umass edu lfw expected results top represented people dataset precision recall score support gerhard_schroeder donald_rumsfeld tony_blair colin_powell george_w_bush avg total print __doc__ time import time import logging import pylab sklearn cross_validation import train_test_split sklearn datasets import fetch_lfw_people sklearn grid_search import gridsearchcv sklearn metrics import classification_report sklearn metrics import confusion_matrix sklearn decomposition import randomizedpca sklearn svm import svc display progress logs stdout logging basicconfig level logging info format asctime message tutorials bottom scikit learn scikit learn user guide release download data already disk load numpy arrays lfw_people fetch_lfw_people min_faces_per_person resize introspect images arrays find shapes plotting n_samples lfw_people images shape fot machine learning use data directly relative pixel positions info ignored model lfw_people data n_features shape label predict person lfw_people target target_names lfw_people target_names n_classes target_names shape print total dataset size print n_samples n_samples print n_features n_features print n_classes n_classes split training set test set using stratified fold split training testing set x_train x_test y_train y_test train_test_split test_fraction compute pca eigenfaces face dataset treated unlabeled dataset unsupervised feature extraction dimensionality reduction n_components print extracting top eigenfaces faces n_components x_train shape time pca randomizedpca n_components n_components whiten true fit x_train print done 3fs time eigenfaces pca components_ reshape n_components print projecting input data eigenfaces orthonormal basis time x_train_pca pca transform x_train x_test_pca pca transform x_test print done 3fs time train svm classification model print fitting classifier training set time param_grid chapter user guide scikit learn user guide release 1e3 5e3 1e4 5e4 1e5 gamma clf gridsearchcv svc kernel rbf class_weight auto param_grid clf clf fit x_train_pca y_train print done 3fs time print best estimator found grid search print clf best_estimator_ quantitative evaluation model quality test set print predicting people names testing set time y_pred clf predict x_test_pca print done 3fs time print classification_report y_test y_pred target_names target_names print confusion_matrix y_test y_pred labels range n_classes qualitative evaluation predictions using matplotlib def plot_gallery images titles n_row n_col helper function plot gallery portraits figure figsize n_col n_row subplots_adjust bottom left right top hspace range n_row n_col subplot n_row n_col imshow images reshape cmap gray title titles size xticks yticks plot result prediction portion test set def title y_pred y_test target_names pred_name target_names y_pred rsplit true_name target_names y_test rsplit return predicted ntrue pred_name true_name prediction_titles title y_pred y_test target_names range y_pred shape plot_gallery x_test prediction_titles plot gallery significative eigenfaces eigenface_titles eigenface range eigenfaces shape plot_gallery eigenfaces eigenface_titles show tutorials bottom scikit learn scikit learn user guide release prediction expected results top represented people dataset eigenfaces precision recall score support gerhard_schroeder donald_rumsfeld tony_blair colin_powell george_w_bush avg total open problem stock market structure predict variation stock prices google visualizing stock market structure finding help project mailing list encounter bug scikit learn something needs clarication docstring online documentation please feel free ask mailing list communities machine learning practictioners metaoptimize forum machine learning natural language processing data analytics discussions similar stackoverow developers http metaoptimize com good starting point discussion good freely available textbooks machine learning quora com quora topic machine learning related questions also features interesting discussions http quora com machine learning look best questions section good resources learning machine learning 
139: note videos videos tutorials also found videos section 
140: chapter user guide scikit learn user guide release supervised learning generalized linear models following set methods intended regression target value expected linear combi nation input variables mathematical notion predicted value 
141: w1x1 wpxp across module designate vector coef_ intercept_ perform classication generalized linear models see logisitic regression 
142: ordinary least squares linearregression linear model coefcients minimize residual sum squares observed responses dataset responses predicted linear approximation mathemati cally solves problem form min linearregression take method arrays store coefcients linear model coef_ member sklearn import linear_model clf linear_model linearregression clf fit linearregression copy_x true fit_intercept true normalize false clf coef_ array supervised learning scikit learn user guide release however coefcient estimates ordinary least squares rely independence model terms terms correlated columns design matrix approximate linear dependence design matrix becomes close singular result least squares estimate becomes highly sensitive random errors observed response producing large variance situation multicollinearity arise example data collected without experimental design 
143: examples linear regression example ordinary least squares complexity method computes least squares solution using singular value decomposition matrix size method cost np2 assuming 
144: ridge regression ridge regression addresses problems ordinary least squares imposing penalty size coefcients ridge coefcients minimize penalized residual sum squares complexity parameter controls amount shrinkage larger value greater amount shrinkage thus coefcients become robust collinearity 
145: min linear models ridge take method arrays store coefcients linear model coef_ member sklearn import linear_model clf linear_model ridge alpha clf fit ridge alpha copy_x true fit_intercept true normalize false tol clf coef_ array clf intercept_ 
146: chapter user guide scikit learn user guide release examples plot ridge coefcients function regularization classication text documents using sparse features ridge complexity method order complexity ordinary least squares 
147: setting regularization parameter generalized cross validation ridgecv implements ridge regression built cross validation alpha parameter object works way gridsearchcv except defaults generalized cross validation gcv efcient form leave one cross validation sklearn import linear_model clf linear_model ridgecv alphas clf fit ridgecv alphas none fit_intercept true loss_func none normalize false score_func none clf best_alpha references notes regularized least squares rifkin lippert technical report course slides 
148: lasso lasso linear model estimates sparse coefcients useful contexts due tendency prefer solutions fewer parameter values effectively reducing number variables upon given solution dependent reason lasso variants fundamental eld compressed sensing certain conditions recover exact set non zero weights see compressive sensing tomography reconstruction prior lasso mathematically consists linear model trained cid prior regularizer objective function minimize min 2nsamples lasso estimate thus solves minimization least squares penalty added constant cid norm parameter vector implementation class lasso uses coordinate descent algorithm coefcients see least angle regression another implementation clf linear_model lasso alpha clf fit lasso alpha copy_x true fit_intercept true max_iter supervised learning scikit learn user guide release normalize false positive false precompute auto tol warm_start false clf predict array also useful lower level tasks function lasso_path computes coefcients along full path possible values 
149: examples lasso elastic net sparse signals compressive sensing tomography reconstruction prior lasso note feature selection lasso lasso regression yields sparse models thus used perform feature selection detailed based feature selection 
150: setting regularization parameter alpha parameter control degree sparsity coefcients estimated 
151: using cross validation scikit learn exposes objects set lasso alpha parameter cross validation lassocv lassolarscv lassolarscv based least angle regression algorithm explained high dimensional datasets many collinear regressors lassocv often preferrable lassolarscv advantage exploring relevant values alpha parameter number samples small compared number observations often faster lassocv 
152: information criteria based model selection alternatively estimator lassolarsic proposes use akaike information criterion aic bayes information criterion bic computationally cheaper ternative optimal value alpha regularization path computed instead times using fold cross validation however criteria needs proper estimation degrees freedom solution derived large samples asymptotic results assume model correct data chapter user guide actually generated model also tend break problem badly conditioned features samples 
153: scikit learn user guide release examples lasso model selection cross validation aic bic elastic net elasticnet linear model trained prior regularizer objective function minimize case min 2nsamples class elasticnetcv used set parameters alpha rho cross validation 
154: supervised learning scikit learn user guide release examples lasso elastic net sparse signals lasso elastic net least angle regression least angle regression lars regression algorithm high dimensional data developed bradley efron trevor hastie iain johnstone robert tibshirani advantages lars numerically efcient contexts number dimensions signicantly greater number points computationally fast forward selection order complexity ordinary least squares 
155: produces full piecewise linear solution path useful cross validation similar attempts tune model 
156: two variables almost equally correlated response coefcients increase proximately rate algorithm thus behaves intuition would expect also stable 
157: easily modied produce solutions estimators like lasso 
158: disadvantages lars method include lars based upon iterative retting residuals would appear especially sensitive effects noise problem discussed detail weisberg discussion section efron annals statistics article 
159: lars model used using estimator lars low level implementation lars_path 
160: lars lasso lassolars lasso model implemented using lars algorithm unlike implementation based coordinate_descent yields exact solution piecewise linear function norm coefcients 
161: chapter user guide scikit learn user guide release sklearn import linear_model clf linear_model lassolars alpha clf fit lassolars alpha copy_x true eps fit_intercept true max_iter normalize true precompute auto verbose false clf coef_ array 
162: examples lasso path using lars lars algorithm provides full path coefcients along regularization parameter almost free thus common operation consist retrieving path function lars_path mathematical formulation algorithm similar forward stepwise regression instead including variables step estimated parameters increased direction equiangular ones correlations residual instead giving vector result lars solution consists curve denoting solution value norm parameter vector full coefents path stored array coef_path_ size n_features max_features rst column always zero 
163: references original algorithm detailed paper least angle regression hastie 
164: orthogonal matching pursuit omp orthogonalmatchingpursuit orthogonal_mp implements omp algorithm approximating linear model constraints imposed number non zero coefcients pseudo norm forward feature selection method like least angle regression orthogonal matching pursuit approximate optimum solution vector xed number non zero elements arg min subject nnonzerocoef alternatively orthogonal matching pursuit target specic error instead specic number non zero coef cients expressed arg min subject tol omp based greedy algorithm includes step atom highly correlated current residual similar simpler matching pursuit method better iteration residual recomputed using orthogonal projection space previously chosen dictionary elements 
165: examples orthogonal matching pursuit supervised learning scikit learn user guide release references http www technion ronrubin publications ksvd omp pdf matching pursuits time frequency dictionaries mallat zhang bayesian regression bayesian regression techniques used include regularization parameters estimation procedure regularization parameter set hard sense tuned data hand done introducing uninformative priors hyper parameters model cid regularization used ridge regression equivalent nding maximum postiori solution gaussian prior parameters precision instead setting lambda manually possible treat random variable estimated data obtain fully probabilistic model output assumed gaussian distributed around alpha treated random variable estimated data advantages bayesian regression adapts data hand used include regularization parameters estimation procedure 
166: disadvantages bayesian regression include inference model time consuming 
167: references good introduction bayesian methods given bishop pattern recognition machine learning original algorithm detailed book bayesian learning neural networks radford neal bayesian ridge regression bayesianridge estimates probabilistic model regression problem described prior parameter given spherical gaussian 1ip priors choosen gamma distributions conjugate prior precision gaussian resulting model called bayesian ridge regression similar classical ridge parameters estimated jointly model remaining hyperparameters parameters gamma priors usually choosen non informative parameters estimated maximizing marginal log likelihood default bayesian ridge regression used regression chapter user guide scikit learn user guide release sklearn import linear_model clf linear_model bayesianridge clf fit bayesianridge alpha_1 alpha_2 compute_score false copy_x true fit_intercept true lambda_1 lambda_2 n_iter normalize false tol verbose false tted model used predict new values clf predict array weights model access clf coef_ array due bayesian framework weights found slightly different ones found ordinary least squares however bayesian ridge regression robust ill posed problem 
168: examples bayesian ridge regression references details found article bayesian interpolation mackay david 
169: automatic relevance determination ard ardregression similar bayesian ridge regression lead sparser weights ardregression poses different prior dropping assuption gaussian spherical 
170: david wipf srikantan nagarajan new view automatic relevance determination 
171: supervised learning scikit learn user guide release instead distribution assumed axis parallel elliptical gaussian distribution means weight drawn gaussian distribution centered zero precision diag constrast bayesian ridge regression coordinate standard deviation prior choosen gamma distribution given hyperparameters 
172: examples automatic relevance determination regression ard references logisitic regression task hand choose class sample belongs given nite hopefuly small set choices learning problem classication rather regression linear models used decision best use called logistic regression doesnt try minimize sum square residuals regression rather hit miss cost logisticregression class used penalized logistic regression penalization yields sparse predicting weights penalization sklearn svm l1_min_c allows calculate lower bound order get non null feature weights zero model 
173: examples penalty sparsity logistic regression path logistic regression chapter user guide scikit learn user guide release note feature selection sparse logistic regression logistic regression penalty yields sparse models thus used perform feature selection detailed based feature selection 
174: stochastic gradient descent sgd stochastic gradient descent simple yet efcient approach linear models particulary useful number samples number features large classes sgdclassifier sgdregressor provide functionality linear models classication regression using different convex loss functions different penalties 
175: references stochastic gradient descent perceptron perceptron another simple algorithm suitable large scale learning default require learning rate regularized penalized updates model mistakes 
176: last characteristic implies perceptron slightly faster train sgd hinge loss resulting models sparser 
177: support vector machines support vector machines svms set supervised learning methods used classication regression outliers detection advantages support vector machines effective high dimensional spaces still effective cases number dimensions greater number samples uses subset training points decision function called support vectors also memory efcient versatile different kernel functions specied decision function common kernels provided also possible specify custom kernels disadvantages support vector machines include number features much greater number samples method likely give poor perfor mances 
178: svms directly provide probability estimates calculated using fold cross validation thus performance suffer 
179: supervised learning scikit learn user guide release support vector machines scikit learn support dens numpy ndarray convertible numpy asarray sparse scipy sparse sample vectors input however use svm make pre dictions sparse data must data optimal performance use ordered numpy ndarray dense scipy sparse csr_matrix sparse dtype float64 previous versions scikit learn sparse input support existed sklearn svm sparse module duplicated sklearn svm interface module still exists backward compatibility deprecated removed scikit learn 
180: classication svc nusvc linearsvc classes capable performing multi class classication dataset 
181: svc nusvc similar methods accept slightly different sets parameters different mathematical formulations see section mathematical formulation hand linearsvc another implementation support vector classication case linear kernel note linearsvc accept keyword kernel assumed linear also lacks members svc nusvc like support_ classiers svc nusvc linearsvc take input two arrays array size n_samples n_features holding training samples array integer values size n_samples holding class labels training samples sklearn import svm chapter user guide scikit learn user guide release clf svm svc clf fit svc cache_size class_weight none coef0 degree gamma kernel rbf probability false shrinking true tol verbose false tted model used predict new values clf predict array svms decision function depends subset training data called support vectors properties support vectors found members support_vectors_ support_ n_support get support vectors clf support_vectors_ array get indices support vectors clf support_ array get number support vectors class clf n_support_ array multi class classication svc nusvc implement one one approach knerr multi class classication n_class number classes n_class n_class classiers constructed one trains data two classes clf svm svc clf fit svc cache_size class_weight none coef0 degree gamma kernel rbf probability false shrinking true tol verbose false dec clf decision_function dec shape classes hand linearsvc implements one rest multi class strategy thus training n_class models two classes one model trained lin_clf svm linearsvc lin_clf fit linearsvc class_weight none dual true fit_intercept true intercept_scaling loss multi_class ovr penalty tol verbose dec lin_clf decision_function dec shape see mathematical formulation complete description decision function 
182: supervised learning scikit learn user guide release note linearsvc also implements alternative multi class strategy called multi class svm formu lated crammer singer using option multi_class crammer_singer method consistent true one rest classication practice rest classication usually preferred since results mostly similar runtime signicantly less one rest linearsvc attributes coef_ intercept_ shape n_class n_features n_class respectively row coefcients corresponds one n_class many one rest classiers simliar interecepts order one class case one one svc layout attributes little involved case linear kernel layout coef_ intercept_ similar one described linearsvc described except shape coef_ n_class n_class corresponding many binary clas siers order classes shape dual_coef_ n_class n_sv somewhat hard grasp layout columns corre spond support vectors involved n_class n_class one one classiers support vectors used n_class classiers n_class entries row correspond dual coefcients classiers might made clear example consider three class problem class support vectors support vectors coefcient support vector class two dual coefcients lets call dual_coef_ looks like respectively support vector classier classes 
183: coefcients svs class coefcients svs class coefcients svs class unbalanced problems problems desired give importance certain classes certain individual samples keywords class_weight sample_weight used svc nusvc implement keyword class_weight method dictionary form class_label value value oating point number sets parameter class class_label value svc nusvc svr nusvr oneclasssvm implement also weights individual samples method fit keyword sample_weight 
184: examples plot different svm classiers iris dataset svm maximum margin separating hyperplane svm separating hyperplane unbalanced classes svm anova svm univariate feature selection non linear svm svm weighted samples chapter user guide scikit learn user guide release supervised learning scikit learn user guide release regression method support vector classication extended solve regression problems method called support vector regression model produced support vector classication described depends subset training data cost function building model care training points lie beyond margin analogously model produced support vector regression depends subset training data cost function building model ignores training data close model prediction two avors support vector regression svr nusvr classication classes method take argument vectors case expected oating point values instead integer values sklearn import svm clf svm svr clf fit svr cache_size coef0 degree epsilon gamma kernel rbf probability false shrinking true tol verbose false clf predict array examples support vector regression svr using linear non linear kernels density estimation novelty detection one class svm used novelty detection given set samples detect soft boundary set classify new points belonging set class implements called oneclasssvm case type unsupervised learning method take input array class labels see section novelty outlier detection details usage 
185: examples one class svm non linear kernel rbf species distribution modeling complexity support vector machines powerful tools compute storage requirements increase rapidly number training vectors core svm quadratic programming problem separating support vectors rest training data solver used libsvm based implementation scales eatures samples depending efciently libsvm cache used practice dataset dependent data sparse eatures replaced average number non zero features sample vector 
186: samples eatures chapter user guide scikit learn user guide release also note linear case algorithm used linearsvc liblinear implementation much efcient libsvm based svc counterpart scale almost linearly millions samples features 
187: tips practical use avoiding data copy svc svr nusvc nusvr data passed certain methods ordered contiguous double precision copied calling underlying implementation check whether give numpy array contiguous inspecting ags attribute linearsvc logisticregression input passed numpy array copied converted liblinear internal sparse data representation double precision oats int32 indices non zero components want large scale linear classier without copying dense numpy contiguous double precision array input suggest use sgdclassier class instead objective function congured almost linearsvc model 
188: kernel cache size svc svr nusvc nusvr size kernel cache strong impact run times larger problems enough ram available recommended set cache_size higher value default 
189: setting constrast scaling libsvm liblinear parameter sklearn svm per sample penalty commonly good values often large seldom 
190: support vector machine algorithms scale invariant highly recommended scale data example scale attribute input vector standardize mean variance note scaling must applied test vector obtain meaningful results see section preprocessing data details scaling normalization 
191: parameter nusvc oneclasssvm nusvr approximates fraction training errors support vec tors 
192: svc data classication unbalanced many positive negative set class_weight auto try different penalty parameters 
193: supervised learning scikit learn user guide release underlying linearsvc implementation uses random number generator select features tting model thus uncommon slightly different results input data happens try smaller tol parameter 
194: using penalization provided linearsvc loss penalty dual false yields sparse solution subset feature weights different zero contribute decision function increasing yields complex model feature selected value yields null model weights equal zero calculated using l1_min_c 
195: kernel functions kernel function following 
196: linear cid polynomial cid specied keyword degree coef0 rbf exp cid specied keyword gamma sigmoid tanh specied coef0 
197: different kernels specied keyword kernel initialization linear_svc svm svc kernel linear linear_svc kernel linear rbf_svc svm svc kernel rbf rbf_svc kernel rbf custom kernels dene kernels either giving kernel python function precomputing gram matrix classiers custom kernels behave way classiers except field support_vectors_ empty indices support vectors stored support_ reference copy rst argument method stored future reference array changes use predict unexpected results 
198: using python functions kernels also use dened kernels passing function keyword kernel constructor kernel must take arguments two matrices return third matrix following code denes linear kernel creates classier instance use kernel import numpy sklearn import svm def my_kernel clf svm svc kernel my_kernel return dot examples svm custom kernel 
199: chapter user guide using gram matrix set kernel precomputed pass gram matrix instead method moment kernel values training vectors test vectors must provided 
200: scikit learn user guide release import numpy sklearn import svm array clf svm svc kernel precomputed linear kernel computation gram dot clf fit gram svc cache_size class_weight none coef0 degree gamma kernel precomputed probability false shrinking true tol verbose false predict training examples clf predict gram array mathematical formulation support vector machine constructs hyper plane set hyper planes high innite dimensional space used classication regression tasks intuitively good separation achieved hyper plane largest distance nearest training data points class called functional margin since general larger margin lower generalization error classier 
201: supervised learning scikit learn user guide release svc given training vectors two classes vector svc solves following primal problem min cid dual subject min subject vector ones upper bound positive semidenite matrix qij kernel training vectors mapped higher maybe innite dimensional space function decision function cid sgn yiik note svm models derived libsvm liblinear use regularization parameter estimators use alpha relation nsamples alpha 
202: parameters accessed members dual_coef_ holds product yii support_vectors_ holds support vectors intercept_ holds independent term references automatic capacity tuning large dimension classiers guyon boser vapnik advances neural information processing support vector networks cortes vapnik machine leaming nusvc introduce new parameter controls number support vectors training errors parameter upper bound fraction training errors lower bound fraction support vectors shown svc formulation reparametrization svc therefore mathematically equivalent 
203: chapter user guide scikit learn user guide release implementation details internally use libsvm liblinear handle computations libraries wrapped using cython 
204: references description implementation details algorithms used please refer libsvm library support vector machines liblinear library large linear classication stochastic gradient descent stochastic gradient descent sgd simple yet efcient approach discriminative learning linear clas siers convex loss functions linear support vector machines logistic regression even though sgd around machine learning community long time received considerable amount attention recently context large scale learning sgd successfully applied large scale sparse machine learning problems often encountered text classication natural language processing given data sparse classiers module easily scale problems training examples features advantages stochastic gradient descent efciency ease implementation lots opportunities code tuning 
205: disadvantages stochastic gradient descent include sgd requires number hyperparameters regularization parameter number iterations sgd sensitive feature scaling 
206: classication warning make sure permute shufe training data tting model use shufe true shufe iterations 
207: class sgdclassifier implements plain stochastic gradient descent learning routine supports different loss functions penalties classication classiers sgd tted two arrays array size n_samples n_features holding training samples array size n_samples holding target values class labels training samples sklearn linear_model import sgdclassifier clf sgdclassifier loss hinge penalty clf fit sgdclassifier alpha class_weight none eta0 fit_intercept true learning_rate optimal loss hinge n_iter n_jobs penalty power_t rho seed shuffle false verbose warm_start false tted model used predict new values supervised learning scikit learn user guide release clf predict array sgd linear model training data member coef_ holds model parameters clf coef_ array member intercept_ holds intercept aka offset bias clf intercept_ array whether model use intercept biased hyperplane controlled parameter t_intercept get signed distance hyperplane use decision_function clf decision_function array concrete loss function set via loss parameter sgdclassifier supports following loss functions loss hinge soft margin linear support vector machine loss modied_huber smoothed hinge loss loss log logistic regression rst two loss functions lazy update model parameters example violates margin con straint makes training efcient log loss hand provides probability estimates case binary classication loss log get probability estimate using predict_proba largest class label chapter user guide scikit learn user guide release clf sgdclassifier loss log fit clf predict_proba array concrete penalty set via penalty parameter sgd supports following penalties penalty norm penalty coef_ penalty norm penalty coef_ penalty elasticnet convex combination rho rho 
208: default setting penalty penalty leads sparse solutions driving coefcients zero elastic net solves deciencies penalty presence highly correlated attributes parameter rho specied user sgdclassifier supports multi class classication combining multiple binary classiers one versus ova scheme classes binary classier learned discriminates classes testing time compute condence score signed distances hyperplane classier choose class highest condence figure illustrates ova approach iris dataset dashed lines represent three ova classiers background colors show decision surface induced three classiers 
209: case multi class classication coef_ two dimensionaly array shape n_classes n_features tercept_ one dimensional array shape n_classes row coef_ holds weight vector ova classier class classes indexed ascending order see attribute classes sgdclassifier supports weighted classes weighted instances via parameters class_weight sample_weight see examples doc string sgdclassifier fit information 
210: supervised learning scikit learn user guide release examples sgd maximum margin separating hyperplane plot multi class sgd iris dataset sgd separating hyperplane weighted classes sgd weighted samples regression class sgdregressor implements plain stochastic gradient descent learning routine supports different loss functions penalties linear regression models sgdregressor well suited regression prob lems large number training samples problems recommend ridge lasso elasticnet 
211: concrete loss function set via loss parameter sgdregressor supports following loss functions loss squared_loss ordinary least squares loss huber huber loss robust regression 
212: huber loss function epsilon insensitive loss function robust regression width insensitive region specied via parameter epsilon 
213: examples ordinary least squares sgd stochastic gradient descent sparse data chapter user guide scikit learn user guide release note sparse implementation produces slightly different results dense implementation due shrunk learning rate intercept 
214: built support sparse data given matrix format supported scipy sparse maximum efciency however use csr matrix format dened scipy sparse csr_matrix 
215: examples classication text documents using sparse features complexity major advantage sgd efciency basically linear number training examples matrix size training cost knp number iterations epochs average number non zero attributes per sample recent theoretical results however show runtime get desired optimization accuracy increase training set size increases 
216: tips practical use stochastic gradient descent sensitive feature scaling highly recommended scale data example scale attribute input vector standardize mean variance note scaling must applied test vector obtain meaningful results easily done using scaler sklearn preprocessing import scaler scaler scaler scaler fit x_train dont cheat fit training data x_train scaler transform x_train x_test scaler transform x_test apply transformation test data attributes intrinsic scale word frequencies indicator features scaling needed 
217: finding reasonable regularization term best done using gridsearchcv usually range arange 
218: empirically found sgd converges observing approx training samples thus reasonable rst guess number iterations n_iter ceil size training set 
219: apply sgd features extracted using pca found often wise scale feature values constant average norm training data equals one 
220: references efcient backprop lecun bottou orr mller neural networks tricks trade 
221: mathematical formulation given set training examples goal learn linear scoring function model parameters intercept order make predictions supervised learning scikit learn user guide release simply look sign common choice model parameters minimizing regularized training error given cid loss function measures model mis regularization term aka penalty penalizes model complexity non negative hyperparameter different choices entail different classiers hinge soft margin support vector machines log logistic regression least squares ridge regression 
222: loss functions regarded upper bound misclassication error zero one loss shown figure 
223: popular choices regularization term include cid norm cid cid leads sparse solutions 
224: norm cid elastic net convex combination 
225: figure shows contours different regularization terms parameter space 
226: sgd stochastic gradient descent optimization method unconstrained optimization problems contrast batch gradient descent sgd approximates true gradient considering single training example time 
227: chapter user guide scikit learn user guide release class sgdclassifier implements rst order sgd learning routine algorithm iterates training examples example updates model parameters according update rule given learning rate controls step size parameter space intercept updated similarly without regularization learning rate either constant gradually decaying classication default learning rate schedule learning_rate optimal given time step total n_samples epochs time steps determined based heuristic proposed lon bottou expected initial updates comparable expected size weights assuming norm training samples approx see tradeoffs large scale machine learning lon bottou details regression default learning rate schedule inverse scaling learning_rate invscaling given eta0 tpower_t eta0 power_t hyperparameters choosen user via eta0 power_t resp constant learning rate use learning_rate constant use eta0 specify learning rate model parameters accessed members coef_ intercept_ member coef_ holds weights member intercept_ holds supervised learning scikit learn user guide release references solving large scale linear prediction problems using stochastic gradient descent algorithms zhang regularization variable selection via elastic net zou hastie journal royal statis proceedings icml 
228: tical society series 
229: implementation details implementation sgd inuenced stochastic gradient svm lon bottou similar svmsgd weight vector represented product scalar vector allows efcient weight update case regularization case sparse feature vectors intercept updated smaller learning rate multiplied account fact updated frequently training examples picked sequentially learning rate lowered observed example adopted learning rate schedule shalev shwartz multi class classication one versus approach used use truncated gradient algorithm proposed tsuruoka regularization elastic net code written cython 
230: references stochastic gradient descent bottou website tradeoffs large scale machine learning bottou website pegasos primal estimated sub gradient solver svm shalev shwartz singer srebro proceedings icml 
231: stochastic gradient descent training regularized log linear models cumulative penalty 
232: tsuruoka tsujii ananiadou proceedings afnlp acl 
233: nearest neighbors sklearn neighbors provides functionality unsupervised supervised neighbors based learning methods unsupervised nearest neighbors foundation many learning methods notably manifold learning spectral clustering supervised neighbors based learning comes two avors classication data discrete labels regression data continuous labels principle behind nearest neighbor methods predened number training samples closest distance new point predict label number samples user dened constant nearest neighbor learning vary based local density points radius based neighbor learning distance general metric measure standard euclidean distance common choice neighbors based meth ods known non generalizing machine learning methods since simply remember training data possibly transformed fast indexing structure ball tree tree despite simplicity nearest neighbors successful large number classication regression prob lems including handwritten digits satellite image scenes often successful classication situations decision boundary irregular classes sklearn neighbors handle either numpy arrays scipy sparse matrices input arbitrary minkowski metrics supported searches 
234: unsupervised nearest neighbors nearestneighbors implements unsupervised nearest neighbors learning 
235: acts uniform interface chapter user guide scikit learn user guide release three different nearest neighbors algorithms balltree scipy spatial ckdtree brute force algo rithm based routines sklearn metrics pairwise choice neighbors search algorithm con trolled keyword algorithm must one auto ball_tree kd_tree brute default value auto passed algorithm attempts determine best approach training data discussion strengths weaknesses option see nearest neighbor algorithms 
236: nearest neighbors classication neighbors based classication type instance based learning non generalizing learning attempt construct general internal model simply stores instances training data classication computed simple majority vote nearest neighbors point query point assigned data class representatives within nearest neighbors point scikit learn implements two different nearest neighbors classiers kneighborsclassifier implements learn ing based nearest neighbors query point integer value specied user radiusneighborsclassifier implements learning based number neighbors within xed radius training point oating point value specied user neighbors classication kneighborsclassifier commonly used two techniques optimal choice value highly data dependent general larger suppresses effects noise makes classication boundaries less distinct radiusneighborsclassifier better choice user species xed radius points sparser neighborhoods use fewer nearest neighbors classication high dimensional parameter spaces method becomes less effective due called curse dimensionality basic nearest neighbors classication uses uniform weights value assigned query point computed simple majority vote nearest neighbors circumstances better weight neighbors nearer neighbors contribute accomplished weights keyword default value weights uniform assigns uniform weights neighbor weights distance assigns weights proportional inverse distance query point alternatively user dened function distance supplied used compute weights 
237: cases classication radius based uniformly sampled neighbors data supervised learning scikit learn user guide release examples nearest neighbors classication example classication using nearest neighbors 
238: nearest neighbors regression neighbors based regression used cases data labels continuous rather discrete variables label assigned query point computed based mean labels nearest neighbors scikit learn implements two different neighbors regressors kneighborsregressor implements learning based nearest neighbors query point integer value specied user radiusneighborsregressor implements learning based neighbors within xed radius query point oating point value specied user basic nearest neighbors regression uses uniform weights point local neighborhood contributes uniformly classication query point circumstances advantageous weight points nearby points contribute regression faraway points accomplished weights keyword default value weights uniform assigns equal weights points weights distance assigns weights proportional inverse distance query point alternatively user dened function distance supplied used compute weights 
239: examples nearest neighbors regression example regression using nearest neighbors 
240: nearest neighbor algorithms brute force fast computation nearest neighbors active area research machine learning naive neighbor search implementation involves brute force computation distances pairs points dataset samples dimensions approach scales efcient brute force neighbors searches competetive small data samples however number samples grows brute force proach quickly becomes infeasible classes within sklearn neighbors brute force neighbors searches chapter user guide scikit learn user guide release specied using keyword algorithm brute computed using routines available sklearn metrics pairwise 
241: tree address computational inefciencies brute force approach variety tree based data structures invented general structures attempt reduce required number distance calculations efciently encoding aggregate distance information sample basic idea point distant point point close point know points distant without explicitly calculate distance way computational cost nearest neighbors search reduced log better signicant improvement brute force large early approach taking advantage aggregate information tree data structure short dimensional tree generalizes two dimensional quad trees dimensional oct trees arbitrary number dimensions tree tree structure recursively partitions parameter space along data axes deviding nested orthotopic regions data points led construction tree fast partitioning performed along data axes dimensional distances need computed constructed nearest neighbor query point determined log distance computations though tree approach fast low dimensional neighbors searches becomes inefcient grows large one manifestation called curse dimensionality scikit learn tree neighbors searches specied using keyword algorithm kd_tree computed using class scipy spatial ckdtree 
242: references multidimensional binary search trees used associative searching bentley communications acm supervised learning scikit learn user guide release ball tree address inefciencies trees higher dimensions ball tree data structure developed trees partition data along cartesian axes ball trees partition data series nesting hyper spheres makes tree construction costly tree results data structure allows efcient neighbors searches even high dimensions ball tree recursively divides data nodes dened centroid radius point node lies within hyper sphere dened number candidate points neighbor search reduced use triangle inequality setup single distance calculation test point centroid sufcient determine lower upper bound distance points within node spherical geometry ball tree nodes performance degrade high dimensions scikit learn ball tree based neigh bors searches specied using keyword algorithm ball_tree computed using class sklearn neighbors balltree alternatively user work balltree class directly 
243: references five balltree construction algorithms omohundro international computer science institute tech nical report choice nearest neighbors algorithm optimal algorithm given dataset complicated choice depends number factors number samples n_samples dimensionality n_features 
244: brute force query time grows ball tree query time grows approximately log tree query time changes way difcult precisely characterise small less cost approximately log tree query efcient larger cost increases nearly overhead due tree structure lead queries slower brute force 
245: small data sets less log comparable brute force algorithms efcient tree based approach ckdtree balltree address providing leaf size parameter controls number samples query switches brute force allows algorithms approach efciency brute force computation small 
246: data structure intrinsic dimensionality data sparsity data intrinsic dimensionality refers dimension manifold data lies linearly nonlinearly embedded parameter space sparsity refers degree data lls parameter space distinguished concept used sparse matrices data matrix may zero entries structure still sparse sense 
247: brute force query time unchanged data structure ball tree tree query times greatly inuenced data structure general sparser data smaller intrinsic dimensionality leads faster query times tree internal representation aligned parameter axes generally show much improvement ball tree arbitrarily structured data 
248: chapter user guide scikit learn user guide release datasets used machine learning tend structured well suited tree based queries 
249: number neighbors requested query point 
250: brute force query time largely unaffected value ball tree tree query time become slower increases due two effects rst larger leads necessity search larger portion parameter space second using requires internal queueing results tree traversed 
251: becomes large compared ability prune branches tree based query reduced situation brute force queries efcient 
252: number query points ball tree tree require construction phase cost construction becomes negligible amortized many queries small number queries performed however construction make signicant fraction total cost query points required brute force better tree based method 
253: currently algorithm auto selects ball_tree brute otherwise choice based assumption number query points least order number training points leaf_size close default value 
254: effect leaf_size noted small sample sizes brute force search efcient tree based query fact accounted ball tree tree internally switching brute force searches within leaf nodes level switch specied parameter leaf_size parameter choice many effects construction time larger leaf_size leads faster tree construction time fewer nodes need created query time large small leaf_size lead suboptimal query cost leaf_size approaching overhead involved traversing nodes signicantly slow query times leaf_size approach ing size training set queries become essentially brute force good compromise leaf_size default value parameter 
255: memory leaf_size increases memory required store tree structure decreases especially important case ball tree stores dimensional centroid node required storage space balltree approximately leaf_size times size training set 
256: leaf_size referenced brute force queries 
257: nearest centroid classier nearestcentroid classier simple algorithm represents class centroid members effect makes similar label updating phase sklearn kmeans algorithm also parameters choose making good baseline classier however suffer non convex classes well classes drastically different variances equal variance dimensions assumed see linear discriminant analysis sklearn lda lda quadratic discriminant analysis sklearn qda qda complex methods make assumption usage default nearestcentroid simple sklearn neighbors nearest_centroid import nearestcentroid import numpy array array clf nearestcentroid clf fit nearestcentroid metric euclidean shrink_threshold none supervised learning scikit learn user guide release print clf predict nearest shrunken centroid nearestcentroid classier shrink_threshold parameter implements nearest shrunken cen troid classier effect value feature centroid divided within class variance feature feature values reduced shrink_threshold notably particular feature value crosses zero set zero effect removes feature affecting classication useful example removing noisy features example using small shrink threshold increases accuracy model 
258: examples nearest centroid classication example classication using nearest centroid different shrink thresholds 
259: chapter user guide scikit learn user guide release gaussian processes gaussian processes machine learning gpml generic supervised learning method primarily designed solve regression problems also extended probabilistic classication present implementation post processing regression exercise advantages gaussian processes machine learning prediction interpolates observations least regular correlation models prediction probabilistic gaussian one compute empirical condence intervals excee dence probabilities might used ret online tting adaptive tting prediction region interest 
260: versatile different linear regression models correlation models specied common models provided also possible specify custom models provided stationary 
261: disadvantages gaussian processes machine learning include sparse uses whole samples features information perform prediction loses efciency high dimensional spaces namely number features exceeds dozens might indeed give poor performance loses computational efciency 
262: classication post processing meaning one rst need solve regression problem providing complete scalar oat precision output experiment one attempt model 
263: thanks gaussian property prediction given varied applications global optimization probabilistic classication 
264: examples introductory regression example say want surrogate function sin function evaluated onto design experi ments dene gaussianprocess model whose regression correlation models might specied using additional kwargs ask model tted data depending number parameters provided instanciation tting procedure may recourse maximum likelihood estimation parameters alternatively uses given parameters 
265: return sin import numpy sklearn import gaussian_process def atleast_2d ravel atleast_2d linspace gaussian_process gaussianprocess theta0 thetal thetau fit gaussianprocess beta0 none corr function squared_exponential normalize true nugget array optimizer fmin_cobyla random_start random_state regr function constant storage_mode full theta0 array thetal array thetau array verbose false y_pred sigma2_pred predict eval_mse true supervised learning scikit learn user guide release chapter user guide fitting noisy data scikit learn user guide release data includes noise gaussian process model used specifying variance noise point gaussianprocess takes parameter nugget added diagonal correlation matrix training points general type tikhonov regularization special case squared exponential correlation function normalization equivalent specifying fractional variance input cid cid nuggeti nugget corr properly set gaussian processes used robustly recover underlying function noisy data examples gaussian processes classication example exploiting probabilistic output supervised learning scikit learn user guide release mathematical formulation initial assumption suppose one wants model output computer experiment say mathematical function rnfeatures cid gpml starts assumption function conditional sample path gaussian process additionally assumed read follows linear regression model zero mean gaussian process fully stationary covari ance function cid cid variance correlation function solely depends absolute relative distance sample possibly featurewise stationarity assumption basic formulation note gpml nothing extension basic least squares linear regression problem except additionaly assume spatial coherence correlation samples dictated correlation function indeed ordinary least squares assumes correlation model cid one cid zero otherwise dirac correlation model sometimes referred nugget correlation model kriging literature 
266: best linear unbiased prediction blup derive best linear unbiased prediction sample path conditioned observations ynsamples xnsamples derived given properties linear linear combination observations unbiased best mean squared error sense arg min optimal weight vector solution following equality constrained optimization problem arg min chapter user guide scikit learn user guide release rewriting constrained optimization problem form lagrangian looking rst order optimality conditions satised one ends closed form expression sought predictor see references complete proof end blup shown gaussian random variate mean variance introduced correlation matrix whose terms dened wrt autocorrelation function built parameters vector cross correlations point prediction made points doe regression matrix vandermonde matrix polynomial basis generalized least square regression weights vectors important notice probabilistic response gaussian process predictor fully analytic mostly relies basic linear algebra operations precisely mean prediction sum two simple linear combinations dot products variance requires two matrix inversions correlation matrix decomposed using cholesky decomposition algorithm 
267: empirical best linear unbiased predictor eblup autocorrelation regression models assumed given practice however never known advance one make motivated empirical choices models correlation models provided choices made one estimate remaining unknown parameters involved blup one uses set provided observations conjunction inference technique present implemen tation based daces matlab toolbox uses maximum likelihood estimation technique see dace manual references complete equations maximum likelihood estimation problem turned global optimization problem onto autocorrelation parameters present implementation global optimization solved means fmin_cobyla optimization function scipy optimize case anisotropy however provide implementation welchs componentwise optimization algorithm see references comprehensive description theoretical aspects gaussian processes machine learning please refer references supervised learning scikit learn user guide release references dace matlab kriging toolbox lophaven nielsen sondergaard screening predicting computer experiments welch buck sacks wynn mitchell morris technometrics gaussian processes machine learning rasmussen cki williams mit press diet trich design analysis computer experiments santner williams notz springer correlation models common correlation models matches famous svms kernels mostly built equivalent sumptions must fulll mercers conditions additionaly remain stationary note however choice correlation model made agreement known properties original experiment observations come instance original experiment known innitely differentiable smooth one use squared exponential correlation model 
268: one rather use exponential correlation model note also exists correlation model takes degree derivability input matern correlation model implemented todo 
269: detailed discussion selection appropriate correlation models see book rasmussen williams references 
270: regression models common linear regression models involve zero constant rst second order polynomials one may specify form python function takes features input returns vector containing values functional set constraint number functions must exceed number available observations underlying regression problem underdetermined 
271: implementation details present implementation based translation dace matlab toolbox 
272: references dace matlab kriging toolbox lophaven nielsen sondergaard welch buck sacks wynn mitchell morris screening predicting computer experiments technometrics 
273: partial least squares partial least squares pls models useful linear relations two multivariate datasets pls arguments method arrays pls nds fundamental relations two matrices latent variable approach modeling covariance structures two spaces pls model try multidimensional direction chapter user guide scikit learn user guide release space explains maximum multidimensional variance direction space pls regression particularly suited matrix predictors variables observations multicollinearity among values contrast standard regression fail cases classes included module plsregression plscanonical cca plssvd reference wegelin survey partial least squares pls methods emphasis two block case examples pls partial least squares naive bayes naive bayes methods set supervised learning algorithms based applying bayes theorem naive assumption independence every pair features given class variable dependent feature vector bayes theorem states following relationship using naive independence assumption xi1 supervised learning scikit learn user guide release relationship simplied cid since constant given input use following classication rule cid arg max cid use maximum posteriori map estimation estimate former relative frequency class training set different naive bayes classiers differ mainly assumptions make regarding distribution spite apparently simplied assumptions naive bayes classiers worked quite well many real world situations famously document classication spam ltering requires small amount training data estimate necessary parameters theoretical reasons naive bayes works well types data see references naive bayes learners classiers extremely fast compared sophisticated methods decoupling class conditional feature distributions means distribution independently estimated one dimensional distribution turn helps alleviate problems stemming curse dimensionality side although naive bayes known decent classier known bad estimator probability outputs predict_proba taken seriously 
274: references zhang optimality naive bayes proc flairs 
275: gaussian naive bayes gaussiannb implements gaussian naive bayes algorithm classication likelihood features assumed gaussian cid cid cid exp parameters estimated using maximum likelihood sklearn import datasets iris datasets load_iris sklearn naive_bayes import gaussiannb gnb gaussiannb y_pred gnb fit iris data iris target predict iris data print number mislabeled points iris target y_pred sum number mislabeled points chapter user guide scikit learn user guide release multinomial naive bayes multinomialnb implements naive bayes algorithm multinomially distributed data one two classic naive bayes variants used text classication data typically represented word vector counts although idf vectors also known work well practice distribution parametrized vectors class number features text classication size vocabulary probability feature appearing sample belonging class parameters estimated smoothed version maximum likelihood relative frequency counting nyi nyi cid cid number times feature appears sample class training set nyi total count features class 
276: smoothing priors accounts features present learning samples prevents zero probabilities computations setting called laplace smoothing called lidstone smoothing 
277: bernoulli naive bayes bernoullinb implements naive bayes training classication algorithms data distributed cording multivariate bernoulli distributions may multiple features one assumed binary valued bernoulli boolean variable therefore class requires samples represented binary valued feature vectors handed kind data bernoullinb instance may binarizes input depending binarize parameter decision rule bernoulli naive bayes based differs multinomial nbs rule explicitly penalizes non occurrence feature indicator class multinomial variant would simply ignore non occurring feature case text classication word occurrence vectors rather word count vectors may used train use classier bernoullinb might perform better datasets especially shorter documents advisable evaluate models time permits 
278: references manning raghavan schtze introduction information retrieval cambridge university press 
279: mccallum nigam comparison event models naive bayes text classication 
280: proc aaai icml workshop learning text categorization 
281: metsis androutsopoulos paliouras spam ltering naive bayes naive bayes 3rd conf email anti spam ceas 
282: decision trees decision trees dts non parametric supervised learning method used classication regression goal create model predicts value target variable learning simple decision rules inferred data features instance example decision trees learn data approximate sine curve set else decision rules deeper tree complex decision rules tter model 
283: supervised learning scikit learn user guide release advantages decision trees simple understand interpret trees visualised requires little data preparation techniques often require data normalisation dummy variables need created blank values removed note however module support missing values 
284: cost using tree predicting data logarithmic number data points used train tree able handle numerical categorical data techniques usually specialised analysing datasets one type variable see algorithms information 
285: uses white box model given situation observable model explanation condition easily explained boolean logic constrast black box model articial neural network results may difcult interpret 
286: possible validate model using statistical tests makes possible account reliability model 
287: performs well even assumptions somewhat violated true model data generated 
288: disadvantages decision trees include decision tree learners create complex trees generalise data well called overt ting mechanisms pruning currently supported setting minimum number samples required leaf node setting maximum depth tree necessary avoid problem 
289: decision trees unstable small variations data might result completely different tree generated problem mitigated using decision trees within ensemble 
290: problem learning optimal decision tree known complete several aspects optimality even simple concepts consequently practical decision tree learning algorithms based heuristic algorithms greedy algorithm locally optimal decisions made node algorithms cannot guarantee return globally optimal decision tree mitigated training multiple trees ensemble learner features samples randomly sampled replacement 
291: chapter user guide scikit learn user guide release concepts hard learn decision trees express easily xor parity multiplexer problems 
292: decision tree learners create biased trees classes dominate therefore recommended balance dataset prior tting decision tree 
293: classication decisiontreeclassifier class capable performing multi class classication dataset classiers decisiontreeclassifier take input two arrays array size n_samples n_features holding training samples array integer values size n_samples holding class bels training samples sklearn import tree clf tree decisiontreeclassifier clf clf fit tted model used predict new values clf predict array decisiontreeclassifier capable binary labels classication multiclass labels classication using iris dataset construct tree follows sklearn datasets import load_iris sklearn import tree iris load_iris clf tree decisiontreeclassifier clf clf fit iris data iris target trained export tree graphviz format using export_graphviz exporter example export tree trained entire iris dataset stringio import stringio stringio tree export_graphviz clf out_file tted model used predict new values clf predict iris data array examples plot decision surface decision tree iris dataset regression decision trees also applied regression problems using decisiontreeregressor class 
294: supervised learning scikit learn user guide release chapter user guide petal length 45000004768error 666666686535samples 150value error 0samples 50value petal width 75error 5samples 100value petal length 94999980927error 168038412929samples 54value petal length 85000038147error 0425330810249samples 46value petal width 65000009537error 040798611939samples 48value petal width 54999995232error 444444447756samples 6value sepal length 94999980927error 444444447756samples 3value error 0samples 43value error 0samples 47value error 0samples 1value error 0samples 3value sepal length 94999980927error 444444447756samples 3value error 0samples 2value error 0samples 1value error 0samples 1value error 0samples 2value scikit learn user guide release classication setting method take argument arrays case expected oating point values instead integer values sklearn import tree clf tree decisiontreeregressor clf clf fit clf predict array examples decision tree regression complexity general run time cost construct balanced binary tree nsamplesnf eatureslog nsamples query time log nsamples although tree construction algorithm attempts generate balanced trees always balanced assuming subtrees remain approximately balanced cost node consists searching eatures feature offers largest reduction entropy cost eaturesnsampleslog nsamples node leading total cost entire trees summing cost node eaturesn2 scikit learn offers efcient implementation construction decision trees naive implementation would recompute class label histograms classication means regression new split point along given feature presorting feature relevant samples retaining running label count reduce complexity node eatureslog nsamples results total cost eaturesnsampleslog nsamples 
295: sampleslog nsamples 
296: supervised learning scikit learn user guide release implementation also offers parameter min_density control optimization heuristic sample mask used mask data points inactive given node avoids copying data important large datasets training trees within ensemble density dened ratio active data samples total samples given node minimum density parameter species level fancy indexing therefore data copied sample mask reset min_density fancy indexing always used data partitioning tree building phase case size memory proportion input data required node depth approximated using geometric series size 1rn ratio samples used node best case analysis shows lowest memory requirement innitely deep tree partition divides data half worst case analysis shows memory requirement increase practise usually requires times setting min_density always use sample mask select subset samples node results little additional memory allocated making appropriate massive datasets within ensemble learners default value min_density empirically leads fast training many problems typically high values min_density lead excessive reallocation slowing algorithm signicantly 
297: tips practical use decision trees tend overt data large number features getting right ratio samples number features important since tree samples high dimensional space likely overt 
298: consider performing dimensionality reduction pca ica feature selection beforehand give tree better chance nding features discriminative 
299: visualise tree training using export function use max_depth initial tree depth get feel tree tting data increase depth 
300: remember number samples required populate tree doubles additional level tree grows use max_depth control size tree prevent overtting 
301: use min_samples_split min_samples_leaf control number samples leaf node small number usually mean tree overt whereas large number prevent tree learning data try min_samples_leaf initial value main difference two min_samples_leaf guarantees minimum number samples leaf min_samples_split create arbitrary small leaves though min_samples_split common literature 
302: balance dataset training prevent tree creating tree biased toward classes dominant 
303: decision trees use fortran ordered float32 arrays internally training data format copy dataset made 
304: tree algorithms id3 cart various decision tree algorithms differ one implemented scikit learn id3 iterative dichotomiser developed ross quinlan algorithm creates multiway tree nding node greedy manner categorical feature yield largest information gain categorical targets trees grown maximum size pruning step usually applied improve ability tree generalise unseen data successor id3 removed restriction features must categorical dynamically dening discrete attribute based numerical variables partitions continuous attribute value discrete set intervals converts trained trees output id3 algorithm sets rules accuracy rule evaluated determine order applied pruning done removing rules precondition accuracy rule improves without 
305: chapter user guide scikit learn user guide release quinlans latest version release proprietary license uses less memory builds smaller rulesets accurate cart classication regression trees similar differs supports numerical target variables regression compute rule sets cart constructs binary trees using feature threshold yield largest information gain node scikit learn uses optimised version cart algorithm 
306: mathematical formulation given training vectors label vector decision tree recursively partitions space samples labels grouped together let data node represented candidate split consisting feature threshold partition data qlef qright subsets qlef qright qlef impurity computed using impurity function choice depends task solved classication regression nlef qlef nright qright select parameters minimises impurity argming recurse subsets qlef qright maximum allowable depth reached min_samples 
307: classication criteria target classication outcome taking values node representing region observations let cid xirm pmk proportion class observations node common measures impurity gini cross entropy misclassication pmk pmk cid cid pmklog pmk max pmk supervised learning scikit learn user guide release regression criteria target continuous value node representing region observations common criterion minimise mean squared error cid cid inm inm references http wikipedia org wiki decision_tree_learning http wikipedia org wiki predictive_analytics breiman friedman olshen stone classication regression trees wadsworth belmont 
308: quinlan programs machine learning morgan kaufmann hastie tibshirani friedman elements statistical learning springer 
309: ensemble methods goal ensemble methods combine predictions several models built given learning algorithm order improve generalizability robustness single model two families ensemble methods usually distinguished averaging methods driving principle build several models independently average predictions average combined model usually better single model variance reduced examples bagging methods forests randomized trees 
310: contrast boosting methods models built sequentially one tries reduce bias combined model motivation combine several weak models produce powerful ensemble examples adaboost least squares boosting gradient tree boosting 
311: forests randomized trees sklearn ensemble module includes two averaging algorithms based randomized decision trees ran domforest algorithm extra trees method algorithms perturb combine techniques b1998 specically designed trees means diverse set classiers created introducing randomness classier construction prediction ensemble given averaged prediction individual classiers classiers forest classiers tted two arrays array size n_samples n_features holding training samples array size n_samples holding target values class labels training samples sklearn ensemble import randomforestclassifier clf randomforestclassifier n_estimators clf clf fit chapter user guide scikit learn user guide release random forests random forests see randomforestclassifier randomforestregressor classes tree ensemble built sample drawn replacement bootstrap sample training set addition splitting node construction tree split chosen longer best split among features instead split picked best split among random subset features result randomness bias forest usually slightly increases respect bias single non random tree due averaging variance also decreases usually compensating increase bias hence yielding overall better model contrast original publication b2001 scikit learn implementation combines classiers averaging probabilistic prediction instead letting classier vote single class 
312: extremely randomized trees extremely randomized trees see extratreesclassifier extratreesregressor classes random ness goes one step way splits computed random forests random subset candidate features used instead looking discriminative thresholds thresholds drawn random candi date feature best randomly generated thresholds picked splitting rule usually allows reduce variance model bit expense slightly greater increase bias sklearn cross_validation import cross_val_score sklearn datasets import make_blobs sklearn ensemble import randomforestclassifier sklearn ensemble import extratreesclassifier sklearn tree import decisiontreeclassifier make_blobs n_samples n_features centers 
313: random_state random_state clf decisiontreeclassifier max_depth none min_samples_split scores cross_val_score clf scores mean 
314: min_samples_split random_state clf randomforestclassifier n_estimators max_depth none scores cross_val_score clf scores mean 
315: min_samples_split random_state clf extratreesclassifier n_estimators max_depth none scores cross_val_score clf scores mean true parameters main parameters adjust using methods n_estimators max_features former number trees forest larger better also longer take compute addition note results stop getting signicantly better beyond critical number trees latter size random subsets features consider splitting node lower greater reduction variance also greater increase bias empiricial good default values max_features n_features regression supervised learning scikit learn user guide release problems max_features sqrt n_features classication tasks n_features number features data best results also usually reached setting max_depth none combination min_samples_split fully developping trees bear mind though values usually optimal best parameter values always cross validated addition note bootstrap samples used default random forests bootstrap true default strategy use original dataset building extra trees bootstrap false training large datasets runtime memory requirements important might also benecial adjust min_density parameter controls heuristic speeding computations tree see complexity trees details 
316: parallelization finally module also features parallel construction trees parallel computation predictions n_jobs parameter n_jobs computations partitioned jobs run cores machine n_jobs cores available machine used note inter process communication overhead speedup might linear using jobs unfortunately times fast signicant speedup still achieved though building large number trees building single tree requires fair amount time large datasets 
317: examples plot decision surfaces ensembles trees iris dataset pixel importances parallel forest trees references chapter user guide scikit learn user guide release gradient tree boosting gradient tree boosting gradient boosted regression trees gbrt generalization boosting arbitrary differentiable loss functions gbrt accurate effective shelf procedure used regression classication problems gradient tree boosting models used variety areas including web search ranking ecology advantages gbrt natural handling data mixed type heterogeneous features predictive power robustness outliers input space via robust loss functions disadvantages gbrt scalability due sequential nature boosting hardly parallelized 
318: module sklearn ensemble provides methods classication regression via gradient boosted regression trees 
319: classication gradientboostingclassifier supports binary multi class classication via deviance loss func tion loss deviance following example shows gradient boosting classier decision stumps weak learners sklearn datasets import make_hastie_10_2 sklearn ensemble import gradientboostingclassifier make_hastie_10_2 random_state x_train x_test y_train y_test clf gradientboostingclassifier n_estimators learn_rate clf score x_test y_test 
320: max_depth random_state fit x_train y_train number weak learners regression trees controlled parameter n_estimators maximum depth tree controlled via max_depth learn_rate hyper parameter range controls overtting via shrinkage 
321: regression gradientboostingregressor supports number different loss functions regression spec ied via argument loss currently supported least squares loss least absolute deviation loss lad robust outliers see f2001 detailed information 
322: import numpy sklearn metrics import mean_squared_error sklearn datasets import make_friedman1 sklearn ensemble import gradientboostingregressor make_friedman1 n_samples random_state noise x_train x_test y_train y_test supervised learning scikit learn user guide release clf gradientboostingregressor n_estimators learn_rate max_depth random_state loss fit x_train y_train mean_squared_error y_test clf predict x_test 
323: gure shows results applying gradientboostingregressor least squares loss base learners boston house price dataset see sklearn datasets load_boston plot left shows train test error iteration plots like often used early stopping plot right shows feature importances optained via feature_importance property 
324: mathematical formulation gbrt considers additive models following form cid mhm basis functions usually called weak learners context boosting gradient tree boosting uses decision trees xed size weak learners decision trees number abilities make valuable boosting namely ability handle data mixed type ability model complex functions similar boosting algorithms gbrt builds additive model forward stagewise fashion fm1 mhm stage decision tree choosen minimizes loss function given current model fm1 fm1 fm1 arg min cid fm1 chapter user guide scikit learn user guide release initial model problem specic least squares regression one usually chooses mean target values 
325: note initial model also specied via init argument passed object implement fit predict 
326: gradient boosting attempts solve minimization problem numerically via steepest descent steepest descent direction negative gradient loss function evaluated current model fm1 calculated differentiable loss function fm1 cid fm1 step length choosen using line search cid arg min fm1 fm1 fm1 algorithms regression classication differ concrete loss function used 
327: loss functions following loss functions supported specied using parameter loss regression least squares natural choice regression due superior computational properties initial model given mean target values 
328: least absolute deviation lad robust loss function regression initial model given median target values 
329: classication binomial deviance deviance negative binomial log likelihood loss function binary classi cation provides probability estimates initial model given log odds ratio 
330: multinomial deviance deviance negative multinomial log likelihood loss function multi class classication n_classes mutually exclusive classes provides probability estimates initial model given prior probability class iteration n_classes regression trees constructed makes gbrt rather inefcient data sets large number classes 
331: regularization shrinkage f2001 proposed simple regularization strategy scales contribution weak learner factor parameter also called learning rate scales step length gradient descent procedure set via learn_rate parameter 
332: fm1 mhm supervised learning scikit learn user guide release parameter learn_rate strongly interacts parameter n_estimators number weak learners smaller values learn_rate require larger numbers weak learners maintain constant training error empirical evidence suggests small values learn_rate favor better test error htf2009 recommend set learning rate small constant learn_rate choose n_estimators early stopping detailed discussion interaction learn_rate n_estimators see r2007 
333: subsampling f1999 proposed stochastic gradient boosting combines gradient boosting bootstrap averaging bagging iteration base classier trained fraction subsample available training data subsample drawn without replacement typical value subsample gure illustrates effect shrinkage subsampling goodness model clearly see shrinkage outperforms shrinkage subsampling shrinkage increase accuracy model subsampling without shrinkage hand poorly 
334: examples gradient boosting regression gradient boosting regularization references multiclass multilabel algorithms module implements multiclass multilabel learning algorithms chapter user guide scikit learn user guide release one rest one one one error correcting output codes multiclass classication means classication two classes multilabel classication different task classier used predict set target labels instance set target classes assumed disjoint ordinary binary multiclass classication also called classication estimators provided module meta estimators require base estimator provided constructor example possible use estimators turn binary classier regressor multiclass classier also possible use estimators multiclass estimators hope accuracy runtime performance improves 
335: note dont need use estimators unless want experiment different multiclass strategies classiers scikit learn support multiclass classication box summary classiers supported scikit learn grouped strategy used 
336: inherently multiclass naive bayes sklearn lda lda decision trees random forests one one sklearn svm svc one sklearn svm linearsvc sklearn linear_model logisticregression sklearn linear_model sgdclassifier sklearn linear_model ridgeclassifier 
337: note moment evaluation metrics implemented multilabel learnings 
338: one rest strategy also known one implemented onevsrestclassifier strategy consists tting one classier per class classier class tted classes addition computational efciency n_classes classiers needed one advantage approach interpretability since class represented one one classier possible gain knowledge class inspecting corresponding classier commonly used strategy fair default choice example sklearn import datasets sklearn multiclass import onevsrestclassifier sklearn svm import linearsvc iris datasets load_iris iris data iris target onevsrestclassifier linearsvc fit predict array multilabel learning ovr onevsrestclassifier also supports multilabel classication use feature feed classier list tuples containing target labels like example 
339: supervised learning scikit learn user guide release examples multilabel classication one one onevsoneclassifier constructs one classier per pair classes prediction time class received votes selected since requires n_classes n_classes classiers method usually slower one rest due n_classes complexity however method may advantageous algorithms kernel algorithms dont scale well n_samples individual learning problem involves small subset data whereas one rest complete dataset used n_classes times example sklearn import datasets sklearn multiclass import onevsoneclassifier sklearn svm import linearsvc iris datasets load_iris iris data iris target onevsoneclassifier linearsvc fit predict array chapter user guide scikit learn user guide release error correcting output codes output code based strategies fairly different one rest one one strategies class represented euclidean space dimension another way put class represented binary code array matrix keeps track location code class called code book code size dimensionality aforementioned space intuitively class represented code unique possible good code book designed optimize classication accuracy implementation simply use randomly generated code book advocated although elaborate methods may added future tting time one binary classier per bit code book tted prediction time classiers used project new points class space class closest points chosen outputcodeclassifier code_size attribute allows user control number classiers used percentage total number classes number require fewer classiers one rest theory log2 n_classes n_classes sufcient represent class unambiguously however practice may lead good accuracy since log2 n_classes much smaller n_classes number greater require classiers one rest case classiers theory correct mistakes made classiers hence name error correcting practice however may happen classier mistakes typically correlated error correcting output codes similar effect bagging example sklearn import datasets sklearn multiclass import outputcodeclassifier sklearn svm import linearsvc iris datasets load_iris iris data iris target outputcodeclassifier linearsvc code_size random_state fit predict array references feature selection classes sklearn feature_selection module used feature selection dimensionality duction sample sets either improve estimators accuracy scores boost performance high dimensional datasets 
340: univariate feature selection univariate feature selection works selecting best features based univariate statistical tests seen preprocessing step estimator scikit learn exposes feature selection routines objects implement error coding method picts james hastie journal computational graphical statistics 
341: supervised learning scikit learn user guide release transform method selecting best features selectkbest setting percentile features keep selectpercentile using common univariate statistical tests feature false positive rate selectfpr false discovery rate selectfdr family wise error selectfwe 
342: objects take input scoring function returns univariate values regression f_regression classication chi2 f_classif feature selection sparse data use sparse data data represented sparse matrices chi2 deal data without making dense 
343: warning beware use regression scoring function classication problem get useless results 
344: examples univariate feature selection recursive feature elimination given external estimator assigns weights features coefcients linear model recursive feature elimination rfe select features recursively considering smaller smaller sets features first estimator trained initial set features weights assigned one features whose absolute weights smallest pruned current set features procedure recursively repeated pruned set desired number features select eventually reached 
345: examples recursive feature elimination recursive feature elimination example showing relevance pixels digit classication task 
346: recursive feature elimination cross validation recursive feature elimination example auto matic tuning number features selected cross validation 
347: based feature selection selecting non zero coefcients linear models penalized norm sparse solutions many estimated coefcients zero goal reduce dimensionality data use another classier expose transform method lect non zero coefcient particular sparse estimators useful purpose linear_model lasso regression linear_model logisticregression svm linearsvc classication chapter user guide scikit learn user guide release sklearn svm import linearsvc sklearn datasets import load_iris iris load_iris iris data iris target shape x_new linearsvc penalty dual false fit_transform x_new shape svms logistic regression parameter controls sparsity smaller fewer features selected lasso higher alpha parameter fewer features selected 
348: examples classication text documents using sparse features comparison different algorithms document classication including based feature selection 
349: recovery compressive sensing good choice alpha lasso fully recover exact set non zero variables using observations provided certain specic conditions met paraticular number samples sufciently large models perform random sufciently large depends number non zero coefcients logarithm number features amount noise smallest absolute value non zero coefcients structure design matrix addition design matrix must display certain specic properties correlated general rule select alpha parameter recovery non zero coefcients set cross validation lassocv lassolarscv though may lead penalized models including small number non relevant variables detrimental prediction score bic lassolarsic tends opposite set high values alpha reference richard baraniuk compressive sensing ieee signal processing magazine july http dsp rice edu les baraniukcslecture07 pdf randomized sparse models limitation based sparse models faced group correlated features select one mitigate problem possible use randomization techniques reestimating sparse model many times perturbing design matrix sub sampling data counting many times given regressor selected randomizedlasso implements lasso randomizedlogisticregression uses logistic regression suitable classication tasks get full path stability scores use lasso_stability_path note randomized sparse models powerful standard statistics detecting non zero features ground truth model sparse words small fraction features non zero 
350: regression settings using strategy examples sparse recovery feature selection sparse linear models example comparing different feature selection approaches discussing situation approach favored 
351: supervised learning scikit learn user guide release references meinshausen buhlmann stability selection journal royal statistical society http arxiv org pdf bach model consistent sparse estimation bootstrap http hal inria hal tree based feature selection tree based estimators see sklearn tree module forest trees sklearn ensemble module used compute feature importances turn used discard irrelevant features sklearn ensemble import extratreesclassifier sklearn datasets import load_iris iris load_iris iris data iris target shape clf extratreesclassifier compute_importances true random_state x_new clf fit transform x_new shape examples feature importances forests trees example synthetic data showing recovery actually meaningful features 
352: pixel importances parallel forest trees example face recognition data 
353: chapter user guide scikit learn user guide release semi supervised semi supervised learning situation training data samples labeled semi supervised estimators sklean semi_supervised able make use addition unlabeled data capture better shape underlying data distribution generalize better new samples algorithms perform well small amount labeled points large amount unlabeled points 
354: unlabeled entries important assign identier unlabeled points along labeled data training model method identier implementation uses integer value 
355: label propagation label propagation denote variations semi supervised graph inference algorithms features available model used classication regression tasks kernel methods project data alternate dimensional spaces scikit learn provides two label propagation models labelpropagation labelspreading work constructing similarity graph items input dataset 
356: figure illustration label propagation structure unlabeled observations consistent class structure thus class label propagated unlabeled observations training set 
357: labelpropagation labelspreading differ modications similarity matrix graph clamping effect label distributions clamping allows algorithm change weight true ground labeled data degree labelpropagation algorithm performs hard clamping input labels means clamping factor relaxed say means always retain percent original label distribution algorithm gets change condence distribution within percent labelpropagation uses raw similarity matrix constructed data modications contrast labelspreading minimizes loss function regularization properties often robust noise algorithm iterates modied version original graph normalizes edge weights computing normalized graph laplacian matrix procedure also used spectral clustering label propagation models two built kernel methods choice kernel effects scalability performance algorithms following available rbf exp specied keyword gamma 
358: supervised learning scikit learn user guide release knn cid specied keyword n_neighbors 
359: rbf kernel produce fully connected graph represented memory dense matrix matrix may large combined cost performing full matrix multiplication calculation iteration algorithm lead prohibitively long running times hand knn kernel produce much memory friendly sparse matrix drastically reduce running times 
360: examples decision boundary label propagation versus svm iris dataset label propagation learning complex structure decision boundary label propagation versus svm iris dataset label propagation digits active learning references yoshua bengio olivier delalleau nicolas roux semi supervised learning olivier delalleau yoshua bengio nicolas roux efcient non parametric function induction semi supervised learning aistat http research microsoft com people nicolasl efcient_ssl pdf linear quadratic discriminant analysis linear discriminant analysis lda lda quadratic discriminant analysis qda qda two classic classi ers names suggest linear quadratic decision surface respectively classiers attractive closed form solutions easily computed inherently multi class proven work well practice also parameters tune algorithms 
361: chapter user guide scikit learn user guide release plot shows decision boundaries lda qda bottom row demonstrates lda learn linear boundaries qda learn quadratic boundaries therefore exible 
362: examples linear quadratic discriminant analysis condence ellipsoid comparison lda qda syn thetic data 
363: references dimensionality reduction using lda lda lda used perform supervised dimensionality reduction projecting input data subspace con sisting discriminant directions implemented lda lda transform desired dimension ality set using n_components constructor parameter parameter inuence lda lda fit lda lda predict 
364: mathematical idea methods work modeling class conditional distribution data class predictions obtained using bayes rule cid cid cid cid linear quadratic discriminant analysis modeled gaussian distribution case lda gaussians class assumed share covariance matrix leads linear decision surface seen comparing log probability rations log case qda assumptions covariance matrices gaussians leading quadratic decision surface 
365: unsupervised learning gaussian mixture models sklearn mixture package enables one learn gaussian mixture models diagonal spherical tied full covariance matrices supported sample estimate data facilities help determine appropriate number components also provided 
366: gaussian mixture model probabilistic model assumes data points generated mixture nite number gaussian distributions unknown parameters one think mixture models generalizing means clustering incorporate information covariance structure data well centers latent gaussians scikit learn implements different classes estimate gaussian mixture models correspond different esti mation strategies detailed 
367: unsupervised learning scikit learn user guide release figure two component gaussian mixture model data points equi probability surfaces model 
368: gmm classier gmm object implements expectation maximization algorithm tting mixture gaussian models also draw condence ellipsoids multivariate models compute bayesian information criterion assess number clusters data gmm fit method provided learns gaussian mixture model train data given test data assign sample class gaussian mostly probably belong using gmm predict method gmm comes different options constrain covariance difference classes estimated spherical diagonal tied full covariance 
369: examples see gmm classication example using gmm classier iris dataset see density estimation mixture gaussians example plotting density estimation 
370: pros cons class gmm expectation maximization inference pros cons speed fastest algorithm learning mixture models agnostic algorithm maximizes likelihood bias means towards zero bias cluster sizes specic structures might might apply 
371: singularities one insufciently many points per mixture estimating covariance matrices becomes difcult algorithm known diverge solutions innite likelihood unless one regularizes covariances articially 
372: number components algorithm always use components access needing held data information theoretical criteria decide many components use absence external cues 
373: chapter user guide scikit learn user guide release selecting number components classical gmm bic criterion used select number components gmm efcient way theory recovers true number components asymptotic regime much data available note using dpgmm avoids specication number components gaussian mixture model 
374: examples see gaussian mixture model selection example model selection performed classical gmm 
375: estimation algorithm expectation maximization main difculty learning gaussian mixture models unlabeled data one usually doesnt know points came latent component one access information gets easy separate gaussian distribution set points expectation maximization well fundamented statistical algorithm get around problem iterative process first one assumes random components randomly centered data points learned means even normally distributed around origin computes point probability generated component model one tweaks parameters maximize likelihood data given assignments repeating process guaranteed always converge local optimum 
376: unsupervised learning scikit learn user guide release vbgmm classier variational gaussian mixtures vbgmm object implements variant gaussian mixture model variational inference algorithms api identical gmm essentially middle ground gmm dpgmm properties dirichlet process 
377: pros cons class vbgmm variational inference regularization due incorporation prior information variational solutions less pathological special cases expectation maximization solutions one use full covariance matrices high dimensions cases components might centered around single point without risking divergence 
378: pros cons bias regularize model one add biases variational algorithm bias means towards origin part prior information adds ghost point origin every mixture component bias covariances spherical also depending concentration parameter bias cluster structure either towards uniformity towards rich get richer scenario 
379: hyperparameters algorithm needs extra hyperparameter might need experimental tuning via cross validation 
380: estimation algorithm variational inference variational inference extension expectation maximization maximizes lower bound model evidence including priors instead data likelihood principle behind variational methods expectation maximization iterative algorithms alternate nding probabilities point generated mixture tting mixtures assigned points variational methods add regular ization integrating information prior distributions avoids singularities often found expectation maximization solutions introduces subtle biases model inference often notably slower usually much render usage unpractical 
381: chapter user guide scikit learn user guide release due bayesian nature variational algorithm needs hyper parameters expectation maximization important concentration parameter alpha specifying high value alpha leads often uniformly sized mixture components specifying small values lead mixture components getting almost points mixture components centered remaining points 
382: dpgmm classier innite gaussian mixtures dpgmm object implements variant gaussian mixture model variable bounded number components using dirichlet process api identical gmm class doesnt require user choose number components expense extra computational time user needs specify loose upper bound number concentration parameter 
383: examples compare gaussian mixture models xed number components dpgmm models left gmm tted components dataset composed clusters see dpgmm able limit components whereas gmm data many components note little observations dpgmm take conservative stand one component right tting dataset well depicted mixture gaussian adjusting alpha parameter dpgmm controls number components used data 
384: examples see gaussian mixture model ellipsoids example plotting condence ellipsoids gmm dpgmm 
385: gaussian mixture model sine curve shows using gmm dpgmm sine wave pros cons class dpgmm diriclet process mixture model pros less sensitivity number parameters unlike nite models almost always use components much hence produce wildly different solutions different numbers components dirichlet process solution wont change much changes parameters leading stability less tuning 
386: need specify number components upper bound number needs pro vided note however dpmm formal model selection procedure thus provides guarantee result 
387: unsupervised learning scikit learn user guide release cons speed extra parametrization necessary variational inference structure dirichlet process make inference slower although much 
388: bias variational techniques many implicit biases dirichlet process inference algorithms whenever mismatch biases data might possible better models using nite mixture 
389: dirichlet process describe variational inference algorithms dirichlet process mixtures dirichlet process prior probability distribution clusterings innite unbounded number partitions variational techniques let incorporate prior structure gaussian mixture models almost penalty inference time comparing nite gaussian mixture model important question dirichlet process use innite unbounded number clusters still consistent full explanation doesnt manual one think chinese restaurant process analogy help understanding chinese restaurant process generative story dirichlet process imagine chinese restaurant innite number tables rst empty rst customer day arrives sits rst table every following customer either sit occupied table probability proportional number customers table sit entirely new table probability proportional concentration parameter alpha nite number customers sat easy see nitely many innite tables ever used higher value alpha total tables used dirichlet process clustering unbounded number mixture components assuming asymmetrical prior structure assignments points components concentrated property known rich get richer full tables chinese restaurant process tend get fuller simulation progresses variational inference techniques dirichlet process still work nite approximation innite mixture model instead specify priori many components one wants use one species concen tration parameter upper bound number mixture components upper bound assuming higher true number components affects algorithmic complexity actual number components used 
390: derivation see full derivation algorithm 
391: variational gaussian mixture models api identical gmm class main difference offers access precision matrices well covariance matrices inference algorithm one following paper variational inference dirichlet process mixtures david blei michael jordan bayesian analysis paper presents parts inference algorithm concerned structure dirichlet pro cess detail mixture modeling part complex even reason present full derivation inference algorithm update lower bound equations youre interested learning derive similar algorithms youre interested changing debugging implementation scikit document complexity implementation linear number mixture components data points regards dimensionality linear using spherical diag quadratic cubic using tied full spherical diag n_states n_points dimension tied full n_states n_points dimension chapter user guide scikit learn user guide release n_states dimension necessary invert covariance precision matrices compute determinant hence cubic term implementation expected scale least well mixture gaussians 
392: update rules inference full mathematical derivation variational bayes update rules gaussian mixture models given main parameters model dened class class proportion mean parameters covariance parameters characterized variational wishart density ishart degrees freedom scale matrix depending covariance parameterization positive scalar positive vector symmetric positive denite matrix 
393: spherical model model variational distribution well use beta ormal gamma sbp ormal beta ormal gamma discrete bound variational bound log cid cid cid cid cid log log log log log log log log log bound log beta log beta log log log log log bound cid log cid log log log log log bound ill use inverse scale parametrization gamma distribution 
394: log log log log unsupervised learning scikit learn user guide release bound cid cid cid cid log log cid log cid bound recall need bound log cid cid log log 2bk cid log simplicity ill later call term inside parenthesis log updates updating cid cid cid 
395: updating updates essentially weighted expectations regularized prior see taking gradient bound setting zero gradient update cid cid cid updating odd reason doesnt really work derive updates using gradients lower bound terms involving cid function show hard isolate however use formula log cid log const terms involving get folded constant get two terms prior probability gives log gamma distribution log cid cid log cid cid 
396: verify normalizing previous term updating log log cid 
397: chapter user guide scikit learn user guide release diagonal model model tha variational distribution well use beta ormal gamma sbp ormal beta ormal gamma discrete lower bound changes lower bound previous model distributions lot bound bound safelly ommited bound main difference precision matrix scales norm extra term computing expectation log cid cid cid cid log cid log cid log updates updates chance weight new change folded term variables update cid cid cid cid cid cid updates well something similar spheric model main difference controls one dimension bound log log hence cid cid cid cid unsupervised learning scikit learn user guide release tied model model tha variational distribution well use beta ormal ishart sbp ormal beta ormal ishart discrete lower bound two changes lower bound bound log log cid log cid cid log log cid cid cid cid cid cid cid cid log log atr cid atr log log cid log log cid bound log cid updates last setting changes trivial update update update update cid cid cid cid cid cid update distribution far complicated even going try going gradient way 
398: log log log cid non trivially seeing quadratic form middle expressed trace something reduces cid log log log hence bit squinting looks like wishart parameters cid cid cid cid chapter user guide cid cid cid cid cid cid cid akbk akbk cid cid cid cid cid cid scikit learn user guide release full model model variational distribution well use beta ormal ishart sbp ormal beta ormal ishart discrete lower bound changes lower bound comparison previous one priors different precision matrices correct indices bound 
399: updates changes updates update uses proper sigma updates dont sum cid cid cid cid cid cid manifold learning look bare necessities simple bare necessities forget worries strife mean bare necessities old mother natures recipes bring bare necessities life baloos song jungle book manifold learning approach nonlinear dimensionality reduction algorithms task based idea dimensionality many data sets articially high 
400: unsupervised learning scikit learn user guide release introduction high dimensional datasets difcult visualize data two three dimensions plotted show inherent structure data equivalent high dimensional plots much less intuitive aid visualization structure dataset dimension must reduced way simplest way accomplish dimensionality reduction taking random projection data though allows degree visualization data structure randomness choice leaves much desired random projection likely interesting structure within data lost 
401: chapter user guide scikit learn user guide release address concern number supervised unsupervised linear dimensionality reduction frameworks designed principal component analysis pca independent component analysis linear discriminant analysis others algorithms dene specic rubrics choose interesting linear projection data methods powerful often miss important nonlinear structure data 
402: manifold learning thought attempt generalize linear frameworks like pca sensitive non linear structure data though supervised variants exist typical manifold learning problem unsupervised learns high dimensional structure data data without use predetermined classications 
403: unsupervised learning scikit learn user guide release examples curve dataset 
404: see manifold learning handwritten digits locally linear embedding isomap example dimensionality reduction handwritten digits 
405: see comparison manifold learning methods example dimensionality reduction toy manifold learning implementations available sklearn summarized isomap one earliest approaches manifold learning isomap algorithm short isometric mapping isomap viewed extension multi dimensional scaling mds kernel pca isomap seeks lower dimensional embedding maintains geodesic distances points isomap performed object isomap 
406: complexity isomap algorithm comprises three stages nearest neighbor search isomap uses sklearn neighbors balltree efcient neighbor search 
407: cost approximately log log nearest neighbors points dimensions 
408: shortest path graph search efcient known algorithms dijkstras algorithm approximately log floyd warshall algorithm algorithm selected user path_method keyword isomap unspecied code attempts choose best algorithm input data 
409: partial eigenvalue decomposition embedding encoded eigenvectors corresponding largest eigenvalues isomap kernel dense solver cost approximately cost often improved using arpack solver eigensolver specied user path_method keyword isomap unspecied code attempts choose best algorithm input data 
410: overall complexity isomap log log log 
411: number training data points chapter user guide scikit learn user guide release input dimension number nearest neighbors output dimension references global geometric framework nonlinear dimensionality reduction tenenbaum silva langford science locally linear embedding locally linear embedding lle seeks lower dimensional projection data preserves distances within local neighborhoods thought series local principal component analyses globally compared best nonlinear embedding locally linear embedding performed function locally_linear_embedding object oriented counterpart locallylinearembedding 
412: complexity standard lle algorithm comprises three stages nearest neighbors search see discussion isomap weight matrix construction construction lle weight matrix involves solution linear equation local neighborhoods partial eigenvalue decomposition see discussion isomap 
413: overall complexity standard lle log log 
414: number training data points input dimension number nearest neighbors output dimension unsupervised learning scikit learn user guide release references nonlinear dimensionality reduction locally linear embedding roweis saul science modied locally linear embedding one well known issue lle regularization problem number neighbors greater number input dimensions matrix dening local neighborhood rank decient address standard lle applies arbitrary regularization parameter chosen relative trace local weight matrix though shown formally solution coverges desired embedding guarantee optimal solution found problem manifests embeddings distort underlying geometry manifold one method address regularization problem use multiple weight vectors neighborhood essence modied locally linear embedding mlle mlle performed function locally_linear_embedding object oriented counterpart locallylinearembedding key word method modified requires n_neighbors n_components 
415: complexity mlle algorithm comprises three stages nearest neighbors search standard lle weight matrix construction approximately rst term exactly equivalent standard lle second term constructing weight matrix multiple weights practice added cost constructing mlle weight matrix relatively small compared cost steps 
416: partial eigenvalue decomposition standard lle overall complexity mlle log log 
417: number training data points input dimension chapter user guide scikit learn user guide release number nearest neighbors output dimension references mlle modied locally linear embedding using multiple weights zhang wang 
418: hessian eigenmapping hessian eigenmapping also known hessian based lle hlle another method solving regularization problem lle revolves around hessian based quadratic form neighborhood used recover locally linear structure though implementations note poor scaling data size sklearn imple ments algorithmic improvements make cost comparable lle variants small output dimension hlle performed function locally_linear_embedding object oriented counter part locallylinearembedding keyword method hessian requires n_neighbors n_components n_components 
419: complexity hlle algorithm comprises three stages nearest neighbors search standard lle weight matrix construction approximately rst term reects similar cost standard lle second term comes decomposition local hessian estimator 
420: partial eigenvalue decomposition standard lle overall complexity standard hlle log log 
421: number training data points input dimension number nearest neighbors output dimension unsupervised learning scikit learn user guide release references hessian eigenmaps locally linear embedding techniques high dimensional data donoho grimes proc natl acad sci usa local tangent space alignment though technically variant lle local tangent space alignment ltsa algorithmically similar enough lle put category rather focusing preserving neighborhood distances lle ltsa seeks characterize local geometry neighborhood via tangent space performs global optimization align local tangent spaces learn embedding ltsa performed function locally_linear_embedding object oriented counterpart locallylinearembedding key word method ltsa 
422: complexity ltsa algorithm comprises three stages nearest neighbors search standard lle weight matrix construction approximately k2d rst term reects similar cost standard lle 
423: partial eigenvalue decomposition standard lle overall complexity standard ltsa log log k2d 
424: number training data points input dimension number nearest neighbors output dimension chapter user guide scikit learn user guide release references principal manifolds nonlinear dimensionality reduction via tangent space alignment zhang zha journal shanghai univ tips practical use make sure scale used features manifold learning methods based nearest neighbor search algorithm may perform poorly otherwise see scaler convenient ways scaling het erogeneous data 
425: reconstruction error computed routine used choose optimal output dimension dimensional manifold embedded dimensional parameter space reconstruction error decrease n_components increased n_components 
426: note noisy data short circuit manifold essence acting bridge parts manifold would otherwise well separated manifold learning noisy incomplete data active area research 
427: certain input congurations lead singular weight matrices example two points dataset identical data split disjointed groups case method arpack fail null space easiest way address use method dense work singular matrix though may slow depending number input points alternatively one attempt understand source singularity due disjoint sets increasing n_neighbors may help due identical points dataset removing points may help 
428: clustering clustering unlabeled data performed module sklearn cluster clustering algorithm comes two variants class implements method learn clusters train data function given train data returns array integer labels corresponding different clusters class labels training data found labels_ attribute 
429: input data one important thing note algorithms implemented module take different kinds trix input one hand meanshift kmeans take data matrices shape n_samples n_features obtained classes sklearn feature_extraction module hand affinitypropagation spectralclustering take similarity matrices shape n_samples n_samples obtained functions sklearn metrics pairwise module words meanshift kmeans work points vector space whereas affinitypropagation spectralclustering work arbitrary objects long simi larity measure exists objects 
430: unsupervised learning scikit learn user guide release figure comparison clustering algorithms scikit learn overview clustering methods method name means afnity propaga tion mean shift spectral clustering hierar chical clustering dbscan gaussian mixtures parame ters number clusters damping sample preference bandwidth number clusters scalability usecase large n_samples medium n_clusters minibatch code scalable n_samples general purpose even cluster size geometry many clusters many clusters uneven cluster size non geometry scalable n_samples medium n_samples small n_clusters many clusters uneven cluster size non geometry clusters even cluster size non geometry number clusters large n_samples n_clusters many clusters possibly connectivity constraints geometry metric used distances points graph distance nearest neighbor graph distances points graph distance nearest neighbor graph distances points neighbor hood size many large n_samples medium n_clusters non geometry uneven cluster sizes distances nearest points scalable flat geometry good density estimation mahalanobis distances centers non geometry clustering useful clusters specic shape non manifold standard euclidean distance right metric case arises two top rows gure gaussian mixture models useful clustering described another chapter documentation dedicated mixture models kmeans seen special case gaussian mixture model equal covariance per component 
431: means kmeans algorithm clusters data trying separate samples groups equal variance minimizing criterion known inertia groups algorithm requires number cluster specied scales well chapter user guide scikit learn user guide release large number samples however results may dependent initialisation result computation often done several times different initialisation centroids means often referred lloyds algorithm initialization means consists looping two major steps first voronoi diagram points calculated using current centroids segment voronoi diagram becomes separate cluster secondly centroids updated mean segment algorithm repeats stopping criteria fullled usually implementation algorithm stops relative increment results iterations less given tolerance value parameter given allow means run parallel called n_jobs giving parameter positive value uses many processors default value uses processors using one less parallelization generally speeds computation cost memory case multiple copies centroids need stored one job means used vector quantization achieved using transform method trained model kmeans 
432: examples demo means clustering handwritten digits data clustering handwritten digits mini batch means minibatchkmeans variant kmeans algorithm using mini batches random subset dataset compute centroids althought minibatchkmeans converge faster kmeans version quality results measured inertia sum distance points nearest centroid good kmeans algorithm 
433: examples demo means clustering algorithm comparison kmeans minibatchkmeans clustering text documents using means document clustering using sparse minibatchkmeans unsupervised learning scikit learn user guide release references web scale means clustering sculley proceedings 19th international conference world wide web afnity propagation affinitypropagation clusters data diffusion similarity matrix algorithm automatically sets numbers cluster difculties scaling thousands samples 
434: examples demo afnity propagation clustering algorithm afnity propagation synthetic datasets visualizing stock market structure afnity propagation financial time series groups classes 
435: companies mean shift meanshift clusters data estimating blobs smooth density points matrix algorithm automati cally sets numbers cluster difculties scaling thousands samples utility function estimate_bandwidth used guess optimal bandwidth meanshift data 
436: examples demo mean shift clustering algorithm mean shift clustering synthetic datasets classes 
437: spectral clustering spectralclustering low dimension embedding afnity matrix samples followed kmeans low dimensional space especially efcient afnity matrix sparse pyamg module chapter user guide scikit learn user guide release installed spectralclustering requires number clusters specied works well small number clusters advised using many clusters two clusters solves convex relaxation normalised cuts problem similarity graph cutting graph two weight edges cut small compared weights edges inside cluster criteria especially interesting working images graph vertices pixels edges similarity graph function gradient image 
438: warning shapeless isotropic data data really shapeless generated random distribution clusters spectral clustering problem ill conditioned different choices almost equivalent spectral clustering solver chooses arbitrary one putting rst sample alone one bin 
439: examples spectral clustering image segmentation segmenting objects noisy background using spectral clustering 
440: segmenting picture lena regions spectral clustering split image lena regions 
441: unsupervised learning scikit learn user guide release references tutorial spectral clustering ulrike von luxburg normalized cuts image segmentation jianbo shi jitendra malik random walks view spectral segmentation marina meila jianbo shi spectral clustering analysis algorithm andrew michael jordan yair weiss hierarchical clustering hierarchical clustering general family clustering algorithms build nested clusters merging succes sively hierarchy clusters represented tree dendrogram root tree unique cluster gathers samples leaves clusters one sample see wikipedia page details ward object performs hierarchical clustering based ward algorithm variance minimizing proach step minimizes sum squared differences within clusters inertia criterion algorithm scale large number samples used jointly connectivity matrix computationally expensive connectivity constraints added samples considers step possible merges 
442: adding connectivity constraints interesting aspect ward object connectivity constraints added algorithm adjacent clusters merged together connectivity matrix denes sample neighboring samples following given structure data instance swiss roll example connectivity constraints forbid merging points adjacent swiss roll thus avoid forming clusters extend across overlapping folds roll 
443: connectivity constraints imposed via connectivity matrix scipy sparse matrix elements intersection row column indices dataset connected trix constructed apriori information instance whish cluster web pages merg ing pages link pointing one another also learned data instance using sklearn neighbors kneighbors_graph restrict merging nearest neighbors swiss roll exam ple using sklearn feature_extraction image grid_to_graph enable merging neigh boring pixels image lena example 
444: chapter user guide scikit learn user guide release examples demo structured ward hierarchical clustering lena image ward clustering split image lena regions 
445: hierarchical clustering structured unstructured ward example ward algorithm swiss roll comparison structured approaches versus unstructured approaches 
446: feature agglomeration univariate selection example dimensionality reduction feature glomeration based ward hierarchical clustering 
447: dbscan dbscan algorithm clusters data nding core points many neighbours within given radius core point found cluster expanded adding neighbours current cluster recusively checking core points formally point considered core point min_points points similarity greater given threshold eps shown gure color indicates cluster membership large circles indicate core points found algorithm moreover algorithm detect outliers indicated black points outliers dened points belong current cluster enough close neighbours start new cluster 
448: examples demo dbscan clustering algorithm clustering synthetic data dbscan references density based algorithm discovering clusters large spatial databases noise ester kriegel sander proceedings 2nd international conference knowledge discovery data mining portland aaai press clustering performance evaluation evaluating performance clustering algorithm trivial counting number errors precision recall supervised classication algorithm particular evaluation metric take absolute unsupervised learning scikit learn user guide release values cluster labels account rather clustering dene separations data similar ground truth set classes satisfying assumption members belong class similar members different classes according similarity metric 
449: inertia presentation usage todo factorize inertia computation kmeans write advantages need ground truth knowledge real classes 
450: drawbacks inertia makes assumption clusters convex isotropic always case especially clusters manifolds weird shapes instance inertia useless metrics evaluate clustering algorithm tries identify nested circles plane 
451: inertia normalized metrics know lower values better bounded zero one potential solution would adjust inertia random clustering assuming number ground truth classes known 
452: adjusted rand index presentation usage given knowledge ground truth class assignments labels_true clus tering algorithm assignments samples labels_pred adjusted rand index function mea sures similarity two assignements ignoring permutations chance normalization sklearn import metrics labels_true labels_pred metrics adjusted_rand_score labels_true labels_pred 
453: one permute predicted labels rename get score labels_pred metrics adjusted_rand_score labels_true labels_pred 
454: furthermore adjusted_rand_score symmetric swapping argument change score thus used consensus measure metrics adjusted_rand_score labels_pred labels_true 
455: perfect labeling scored labels_pred labels_true metrics adjusted_rand_score labels_true labels_pred bad independent labelings negative close scores chapter user guide scikit learn user guide release labels_true labels_pred metrics adjusted_rand_score labels_true labels_pred 
456: advantages random uniform label assignements ari score close value n_clusters n_samples case raw rand index measure instance 
457: bounded range negative values bad independent labelings similar clusterings positve ari perfect match score 
458: assumption made cluster structure used compare clustering algorithms means assumes isotropic blob shapes results spectral clustering algorithms cluster folded shapes 
459: drawbacks contrary inertia ari requires knowlege ground truth classes almost never available practice requires manual assignment human annotators supervised learning setting however ari also useful purely unsupervised setting building block consensus index used clustering model selection todo 
460: examples adjustment chance clustering performance evaluation analysis impact dataset size value clustering measures random assignements 
461: mathematical formulation ground truth class assignement clustering let dene number pairs elements set set number pairs elements different sets different sets raw unadjusted rand index given nsamples total number possible pairs dataset without ordering 
462: nsamples however score guarantee random label assignements get value close zero esp number clusters order magnitude number samples counter effect discount expected random labelings dening adjusted rand index follows ari expected_ri max expected_ri unsupervised learning scikit learn user guide release references comparing partitions hubert arabie journal classication wikipedia entry adjusted rand index adjusted mutual information presentation usage given knowledge ground truth class assignments labels_true clus tering algorithm assignments samples labels_pred adjusted mutual information function measures agreement two assignements ignoring permutations chance normalization sklearn import metrics labels_true labels_pred metrics adjusted_mutual_info_score labels_true labels_pred 
463: one permute predicted labels rename get score labels_pred metrics adjusted_mutual_info_score labels_true labels_pred 
464: furthermore adjusted_mutual_info_score symmetric swapping argument change score thus used consensus measure metrics adjusted_mutual_info_score labels_pred labels_true 
465: perfect labeling scored labels_pred labels_true metrics adjusted_mutual_info_score labels_true labels_pred bad independent labelings non positive scores labels_true labels_pred metrics adjusted_mutual_info_score labels_true labels_pred 
466: advantages random uniform label assignements ami score close value n_clusters n_samples case raw mutual information measure instance 
467: bounded range values close zero indicate two label assignments largely independent values close one indicate signicant agreement values exactly indicate purely independent label assignments ami exactly indicates two label assignments equal without permutation 
468: assumption made cluster structure used compare clustering algorithms means assumes isotropic blob shapes results spectral clustering algorithms cluster folded shapes 
469: chapter user guide scikit learn user guide release drawbacks contrary inertia ami requires knowlege ground truth classes almost never available practice requires manual assignment human annotators supervised learning setting however ami also useful purely unsupervised setting building block consensus index used clustering model selection 
470: examples adjustment chance clustering performance evaluation analysis impact dataset size value clustering measures random assignements example also includes adjusted rand index 
471: mathematical formulation assume two label assignments data classes classes entropy either amount uncertaintly array calculated log cid log cid cid cid cid cid number instances class likewise number instances class non adjusted mutual information calculated log cid number instances label also label value mutual information adjusted cfor chance tend increase number different labels clusters increases regardless actual amount mutual information label assignments expected value mutual information calculated using following equation vinh epps bailey equation number instances label number instances label 
472: cid cid min cid nij bjn nij log nij aibj nij nij nij nij using expected value adjusted mutual information calculated using similar form adjusted rand index expected_m max expected_m unsupervised learning scikit learn user guide release references vinh epps bailey 26th annual information theoretic measures clusterings comparison icml 
473: international conference machine learning proceedings doi isbn 
474: information theoretic measures correction vinh epps comparison http jmlr csail mit edu papers volume11 vinh10a vinh10a pdf properties normalization bailey variants 
475: wikipedia entry adjusted mutual information clusterings jmlr chance homogeneity completeness measure presentation usage given knowledge ground truth class assignments samples possible dene intuitive metric using conditional entropy analysis particular rosenberg hirschberg dene following two desirable objectives cluster assign ment homogeneity cluster contains members single class completeness members given class assigned cluster 
476: turn concept scores homogeneity_score completeness_score bounded higher better sklearn import metrics labels_true labels_pred metrics homogeneity_score labels_true labels_pred 
477: metrics completeness_score labels_true labels_pred 
478: harmonic mean called measure computed v_measure_score metrics v_measure_score labels_true labels_pred 
479: three metrics computed using homogeneity_completeness_v_measure follows metrics homogeneity_completeness_v_measure labels_true labels_pred following clustering assignment slighlty better since homogeneous complete labels_pred metrics homogeneity_completeness_v_measure labels_true labels_pred note v_measure_score symmetric used evaluate agreement two independent assigne ments dataset case completeness_score homogeneity_score bound relationship chapter user guide scikit learn user guide release homogeneity_score completeness_score advantages bounded scores bad perfect score intuitive interpretation clustering bad measure qualitatively analyzed terms homogeneity completeness better feel kind mistakes done assigmenent 
480: assumption made cluster structure used compare clustering algorithms means assumes isotropic blob shapes results spectral clustering algorithms cluster folded shapes 
481: drawbacks previously introduced metrics normalized random labeling means depending number samples clusters ground truth classes completely random labeling always yield values homogeneity completeness hence measure particular random labeling wont yield zero scores especially number clusters large problem safely ignored number samples thousand number clusters less smaller sample sizes larger number clusters safer use adjusted index adjusted rand index ari 
482: unsupervised learning scikit learn user guide release metrics require knowlege ground truth classes almost never available practice requires manual assignment human annotators supervised learning setting 
483: examples adjustment chance clustering performance evaluation analysis impact dataset size value clustering measures random assignements 
484: mathematical formulation homogeneity completeness scores formally given conditional entropy classes given cluster assignments given log cid cid cid entropy classes given log total number samples number samples respectively belonging class cluster nally number samples class assigned cluster conditional entropy clusters given class entropy clusters dened sym metric manner rosenberg hirschberg dene measure harmonic mean homogeneity completeness references measure conditional entropy based external cluster evaluation measure andrew rosenberg julia hirschberg silhouette coefcient chapter user guide scikit learn user guide release presentation usage ground truth labels known evaluation must performed using model self silhouette coefcient sklearn metrics silhouette_score example evaluation higher silhouette coefcient score relates model better dened clusters silhouette coefcient dened sample composed two scores mean distance sample points class mean distance sample points next nearest cluster 
485: silhoeutte coefcient single sample given max silhouette coefcient set samples given mean silhouette coefcient sample 
486: sklearn import metrics sklearn metrics import pairwise_distances sklearn import datasets dataset datasets load_iris dataset data dataset target normal usage silhouette coefcient applied results cluster analysis 
487: import numpy sklearn cluster import kmeans kmeans_model kmeans random_state fit labels kmeans_model labels_ metrics silhouette_score labels metric euclidean 
488: references peter rousseeuw silhouettes graphical aid interpretation validation cluster analysis computational applied mathematics doi 
489: advantages score bounded incorrect clustering highly dense clustering scores around zero indicate overlapping clusters 
490: score higher clusters dense well separated relates standard concept cluster 
491: drawbacks silhouette coefcient generally higher convex clusters concepts clusters density based clusters like obtained dbscan 
492: unsupervised learning scikit learn user guide release decomposing signals components matrix factorization problems principal component analysis pca exact pca probabilistic interpretation pca used decompose multivariate dataset set successive orthogonal components explain maximum amount variance scikit learn pca implemented transformer object learns components method used new data project components optional parameter whiten true parameter make possible project data onto singular space scaling component unit variance often useful models stream make strong assumptions isotropy signal example case support vector machines rbf kernel means clustering algorithm however case inverse transform longer exact since information lost forward transforming addition probabilisticpca object provides probabilistic interpretation pca give like lihood data based amount variance explains implements score method used cross validation example iris dataset comprised features projected dimensions explain variance examples comparison lda pca projection iris dataset chapter user guide scikit learn user guide release approximate pca often interested projecting data onto lower dimensional space preserves variance dropping singular vector components associated lower singular values instance face recognition work 64x64 gray level pixel pictures dimensionality data slow train rbf support vector machine wide data furthermore know intrinsic dimensionality data much lower since faces pictures look alike samples lie manifold much lower dimension say around instance pca algorithm used linearly transform data reducing dimensionality preserve explained variance time class randomizedpca useful case since going drop singular vectors much efcient limit computation approximated estimate singular vectors keep actually perform transform instance following shows sample portraits centered around olivetti dataset right hand side rst singular vectors reshaped portraits since require top singular vectors dataset size nsamples eatures computation time less randomizedpca hence used drop replacement pca minor exception need give size lower dimensional space n_components mandatory input parameter note nmax max nsamples eatures nmin min nsamples eatures time complexity unsupervised learning scikit learn user guide release max ncomponents instead max nmin exact method implemented pca randomizedpca memory footprint randomizedpca also proportional nmax ncomponents instead nmax nmin exact method furthermore randomizedpca able work scipy sparse matrices input make suitable reducing dimensionality features extracted text documents instance note implementation inverse_transform randomizedpca exact inverse transform transform even whiten false default 
493: examples faces recognition example using eigenfaces svms faces dataset decompositions references finding structure randomness stochastic algorithms constructing approximate matrix decom positions halko kernel pca kernelpca extension pca achieves non linear dimensionality reduction use kernels many applications including denoising compression structured prediction kernel dependency estimation kernelpca supports transform inverse_transform 
494: chapter user guide scikit learn user guide release examples kernel pca sparse principal components analysis sparsepca minibatchsparsepca sparsepca variant pca goal extracting set sparse components best reconstruct data mini batch sparse pca minibatchsparsepca variant sparsepca faster less accurate increased speed reached iterating small chunks set features given number iterations principal component analysis pca disadvantage components extracted method exclu sively dense expressions non zero coefcients expressed linear combinations original variables make interpretation difcult many cases real underlying components naturally imagined sparse vectors example face recognition components might naturally map parts faces sparse principal components yields parsimonious interpretable representation clearly emphasizing original features contribute differences samples following example illustrates components extracted using sparse pca olivetti faces dataset seen regularization term induces many zeros furthermore natural structure data causes non zero coefcients vertically adjacent model enforce mathematically component vector r4096 notion vertical adjacency except human friendly visualization 64x64 pixel images fact components shown appear local effect inherent structure data makes local patterns minimize reconstruction error exist sparsity inducing norms take account adjacency different kinds structure see see jen09 review methods details use sparse pca see examples section 
495: unsupervised learning scikit learn user guide release note many different formulations sparse pca problem one implemented based mrl09 optimization problem solved pca problem dictionary learning cid penalty components arg min subject ncomponents sparsity inducing cid norm also prevents learning components noise training samples available degree penalization thus sparsity adjusted hyperparameter alpha small values lead gently regularized factorization larger values shrink many coefcients zero 
496: note spirit online algorithm class minibatchsparsepca implement partial_t algorithm online along features direction samples direction 
497: examples faces dataset decompositions references dictionary learning sparse coding precomputed dictionary sparsecoder object estimator used transform signals sparse linear combination atoms xed precomputed dictionary discrete wavelet basis object therefore implement method transformation amounts sparse coding problem nding representation data linear combination dictionary atoms possible variations dictionary learning implement following transform methods controllable via transform_method initialization parameter chapter user guide scikit learn user guide release orthogonal matching pursuit orthogonal matching pursuit omp least angle regression least angle regression lasso computed least angle regression lasso using coordinate descent lasso thresholding thresholding fast yield accurate reconstructions shown useful literature classication tasks image reconstruction tasks orthogonal matching pursuit yields accurate unbiased reconstruction dictionary learning objects offer via split_code parameter possibility separate positive negative values results sparse coding useful dictionary learning used extracting features used supervised learning allows learning algorithm assign different weights negative loadings particular atom corresponding positive loading split code single sample length n_atoms constructed using following rule first regular code length n_atoms computed rst n_atoms entries split_code lled positive part regular code vector second half split code lled negative part code vector positive sign therefore split_code non negative 
498: examples sparse coding precomputed dictionary generic dictionary learning dictionary learning dictionarylearning matrix factorization problem amounts nding usually overcomplete dictionary perform good sparsely encoding tted data representing data sparse combinations atoms overcomplete dictionary suggested way mammal primary visual cortex works consequently dictionary learning applied image patches shown give good results image processing tasks image completion inpainting denoising well supervised recognition tasks dictionary learning optimization problem solved alternatively updating sparse code solution multiple lasso problems considering dictionary xed updating dictionary best sparse code 
499: arg min subject natoms unsupervised learning scikit learn user guide release using procedure dictionary transform simply sparse coding step shares implementation dictionary learning objects see sparse coding precomputed dictionary following image shows dictionary learned 4x4 pixel image patches extracted part image lena looks like 
500: chapter user guide scikit learn user guide release examples image denoising using dictionary learning references online dictionary learning sparse coding mairal bach ponce sapiro mini batch dictionary learning minibatchdictionarylearning implements faster less accurate version dictionary learning algo rithm better suited large datasets default minibatchdictionarylearning divides data mini batches optimizes online manner cycling mini batches specied number iterations however moment implement stopping condition estimator also implements partial_t updates dictionary iterating mini batch used online learning data readily available start data memory 
501: independent component analysis ica independent component analysis separates multivariate signal additive subcomponents maximally inde pendent implemented scikit learn using fast ica algorithm classically used separate mixed signals problem known blind source separation example ica also used yet another non linear decomposition nds components sparsity unsupervised learning scikit learn user guide release examples blind source separation using fastica fastica point clouds faces dataset decompositions non negative matrix factorization nmf nnmf nmf alternative approach decomposition assumes data components non negative nmf plugged instead pca variants cases data matrix contain negative values unlike pca representation vector obtained additive fashion superimposing components without substracting additive models efcient representing images text observed hoyer carefully constrained nmf produce parts based representation dataset resulting interpretable models following example displays sparse components found nmf images olivetti faces dataset comparison pca eigenfaces 
502: chapter user guide scikit learn user guide release init attribute determines initialization method applied great impact performance method nmf implements method nonnegative double singular value decomposition nndsvd based two svd processes one approximating data matrix approximating positive sections resulting partial svd factors utilizing algebraic property unit rank matrices basic nndsvd algorithm better sparse factorization variants nndsvda zeros set equal mean elements data nndsvdar zeros set random perturbations less mean data divided recommended dense case nmf also initialized random non negative matrices passing integer seed randomstate init nmf sparseness enforced setting attribute sparseness data components sparse components lead localized features sparse data leads efcient representation data 
503: examples faces dataset decompositions topics extraction non negative matrix factorization unsupervised learning scikit learn user guide release references learning parts objects non negative matrix factorization lee seung non negative matrix factorization sparseness constraints hoyer projected gradient methods non negative matrix factorization lin svd based initialization head start nonnegative matrix factorization boutsidis gallopoulos covariance estimation many statistical problems require point estimation populations covariance matrix seen estimation data set scatter plot shape time estimation done sample whose properties size structure homogeneity large inuence estimations quality sklearn covariance package aims providing tools affording accurate estimation populations covariance matrix various settings assume observations independent identically distributed 
504: empirical covariance covariance matrix data set known well approximated classical maximum likelihood estimator empirical covariance provided number observations large enough compared number features variables describing observations precisely maximum likelihood estimator sample unbiased estimator corresponding population covariance matrix empirical covariance matrix sample computed using empirical_covariance func tion data sample empiricalcovariance fit method careful depending whether data centered sult different one may want use assume_centered parameter accurately 
505: package tting empiricalcovariance object examples see ledoit wolf empiricalcovariance object data 
506: covariance simple estimation example shrunk covariance basic shrinkage despite unbiased estimator covariance matrix maximum likelihood estimator good esti mator eigenvalues covariance matrix precision matrix obtained inversion accurate sometimes even occurs empirical covariance matrix cannot inverted numerical reasons avoid inversion problem transformation empirical covariance matrix introduced shrinkage consists reducing ratio smallest largest eigenvalue empirical covariance matrix done simply shifting every eigenvalue according given offset equivalent nding penalized maximum likelihood estimator covariance matrix reducing highest eigenvalue increasing smallest help convex transformation shrunk latter approach implemented scikit learn convex transformation user dened shrinkage coefcient directly applied pre computed covari ance shrunk_covariance method also shrunk estimator covariance tted data chapter user guide scikit learn user guide release shrunkcovariance object shrunkcovariance fit method depending whether data centered result different one may want use assume_centered parameter accurately 
507: examples see ledoit wolf covariance simple estimation example shrunkcovariance object data 
508: ledoit wolf shrinkage paper ledoit wolf propose formula compute optimal shrinkage coefcient minimizes mean squared error estimated real covariance matrix terms frobenius norm ledoit wolf estimator covariance matrix computed sample ledoit_wolf function sklearn covariance package otherwise obtained tting ledoitwolf object sample ledoit wolf well conditioned estimator large dimensional covariance matrices jour nal multivariate analysis volume issue february pages 
509: examples see ledoit wolf covariance simple estimation example ledoitwolf object data visualizing performances ledoit wolf estimator terms likelihood 
510: oracle approximating shrinkage assumption data gaussian distributed chen derived formula aimed choosing shrinkage coefcient yields smaller mean squared error one given ledoit wolfs formula resulting estimator known oracle shrinkage approximating estimator covariance 
511: unsupervised learning scikit learn user guide release oas estimator covariance matrix computed sample oas function sklearn covariance package otherwise obtained tting oas object sample mula used implement oas correspond one given article taken matlab program available authors webpage https tbayes eecs umich edu yilun covestimation chen shrinkage algorithms mmse covariance estimation ieee trans sign proc volume issue october 
512: examples see ledoit wolf covariance simple estimation example oas object data see ledoit wolf oas estimation visualize mean squared error difference ledoitwolf oas estimator covariance 
513: sparse inverse covariance matrix inverse covariance matrix often called precision matrix proportional partial correlation matrix gives partial independence relationship words two features independent conditionally others corresponding coefcient precision matrix zero makes sense estimate sparse precision matrix learning independence relations data estimation covariance matrix better conditioned known covariance selection small samples situation n_samples order magnitude n_features smaller sparse inverse covariance estimators tend work better shrunk covariance estimators however opposite situation correlated data numerically unstable addition unlike shrinkage estimators sparse estimators able recover diagonal structure higher alpha graphlasso estimator uses penalty enforce sparsity precision matrix parameter sparse precision matrix corresponding graphlassocv object uses cross validation automatically set alpha parameter 
514: chapter user guide scikit learn user guide release figure comparison maximum likelihood shrinkage sparse estimates covariance precision matrix small samples settings 
515: note structure recovery recovering graphical structure correlations data challenging thing interested recovery keep mind recovery easier correlation matrix covariance matrix standardize observations running graphlasso underlying graph nodes much connections average node algorithm miss connections 
516: number observations large compared number edges underlying graph recover 
517: even favorable recovery conditions alpha parameter chosen cross validation using graphlassocv object lead selecting many edges however relevant edges heavier weights irrelevant ones 
518: mathematical formulation following argmink cid trsk logdetk cid cid cid precision matrix estimated sample covariance matrix cid cid sum absolute values diagonal coefcients algorithm employed solve problem glasso algorithm friedman biostatistics paper algorithm glasso package 
519: examples sparse inverse covariance estimation example synthetic data showing recovery structure comparing covariance estimators 
520: visualizing stock market structure example real stock market data nding symbols linked 
521: unsupervised learning scikit learn user guide release references friedman sparse inverse covariance estimation graphical lasso biostatistics robust covariance estimation real data set often subjects measurement recording errors regular uncommon observations may also appear variety reason every observation uncommon called outlier empirical covari ance estimator shrunk covariance estimators presented sensitive presence outlying observations data therefore one use robust covariance estimators estimate covariance real data sets alternatively robust covariance estimators used perform outlier detection discard downweight observations according processing data sklearn covariance package implements robust estimator covariance minimum covariance determinant 
522: minimum covariance determinant minimum covariance determinant estimator robust estimator data sets covariance introduced rousseuw idea given proportion good observations outliers com pute empirical covariance matrix empirical covariance matrix rescaled compensate performed selection observations consistency step computed minimum covariance determinant estimator one give weights observations according mahalanobis distance leading reweighted estimate covariance matrix data set reweighting step rousseuw van driessen developed fastmcd algorithm order compute minimum covariance determinant algorithm used scikit learn tting mcd object data fastmcd algorithm also computes robust estimate data set location time raw estimates accessed raw_location_ raw_covariance_ attributes mincovdet robust covariance estimator object rousseeuw least median squares regression 
523: stat ass 
524: fast algorithm minimum covariance determinant estimator american statistical associa tion american society quality technometrics 
525: examples see robust empirical covariance estimate example mincovdet object data see estimate remains accurate despite presence outliers 
526: see robust covariance estimation mahalanobis distances relevance visualize difference tween empiricalcovariance mincovdet covariance estimators terms mahalanobis dis tance get better estimate precision matrix 
527: chapter user guide inuence outliers location covariance estimates separating inliers outliers using mahalonis distance scikit learn user guide release novelty outlier detection many applications require able decide whether new observation belongs distribution exiting observations inlier considered different outlier often ability used clean real data sets two important distinction must made novelty detection training data polluted outliers interested detecting anoma lies new observations 
528: outlier detection training data contains outliers need central mode training data ignoring deviant observations 
529: scikit learn project provides set machine learning tools used novelty outliers detection strategy implemented objects learning unsupervised way data estimor fit x_train new observations sorted inliers outliers predict method estimator predict x_test inliers labeled outliers labeled 
530: novelty detection consider data set observations distribution described features consider add one observation data set new observation different others doubt regular come distribution contrary similar cannot distinguish original observations question adressed novelty detection tools methods general learn rough close frontier delimiting contour initial observations distribution plotted embedding dimensional space observations lay within frontier delimited subspace considered coming population initial observations otherwise lay outside frontier say abnormal given condence assessment one class svm introduced purpose implemented support vector machines module svm oneclasssvm object requires choice kernel scalar parameter dene frontier rbf kernel usually chosen although exist exact formula algorithm set bandwith parameter default scikit learn implementation parameter also known margin one class svm corresponds probability nding new regular observation outside frontier 
531: examples see one class svm non linear kernel rbf vizualizing frontier learned around data svm oneclasssvm object 
532: unsupervised learning scikit learn user guide release outlier detection outlier detection similar novelty detection sense goal separate core regular observations polutting ones called outliers yet case outlier detection dont clean data set representing population regular observations used train tool 
533: fitting elliptic envelop one common way performing outlier detection assume regular data come known distribution data gaussian distributed assumption generaly try dene shape data dene outlying observations observations stand far enough shape scikit learn provides object covariance ellipticenvelope robust covariance estimate data thus ellipse central data points ignoring points outside central mode instance assuming inlier data gaussian distributed estimate inlier location covariance robust way whithout inuenced outliers mahalanobis distances obtained estimate used derive measure outlyingness strategy illustrated 
534: examples see robust covariance estimation mahalanobis distances relevance illustration dif ference using standard covariance empiricalcovariance robust estimate covariance mincovdet location covariance assess degree outlyingness servation 
535: references chapter user guide scikit learn user guide release one class svm versus elliptic envelop strictly speaking one class svm outlier detection method novelty detection method training set contaminated outliers may said outlier detection high dimension without assumptions distribution inlying data challenging one class svm gives useful results situations examples illustrate performance covariance ellipticenvelope degrades data less less unimodal svm oneclasssvm works better data multiple modes 
536: table comparing one class svm approach elliptic envelopp inlier mode well centered elliptic svm oneclasssvm able benet rotational symmetry inlier population addition bit outliers present training set opposite decision rule based tting covariance ellipticenvelope learns ellipse well inlier distribution 
537: inlier distribution becomes bimodal covariance ellipticenvelope well inliers however see svm oneclasssvm tends overt model inliers interprets region chance outliers clustered inliers 
538: inlier distribution strongly non gaussian svm oneclasssvm able recover reasonable approximation whereas covariance ellipticenvelope completely fails 
539: unsupervised learning scikit learn user guide release examples see outlier detection several methods 
540: svm oneclasssvm tuned perform like outlier detection method covariance based outlier detection covariance mincovdet 
541: comparison hidden markov models sklearn hmm implements algorithms hidden markov model hmm hmm generative probabilistic model sequence observable variable generated sequence internal hidden state hidden states observed directly transition hidden states aussumed rst order markov chain specied start probability vector transition probability matrix emission probability observable distribution parameters conditioned current hidden state index multinomial gaussian thus hmm completely determined three fundamental problems hmm given model parameters observed data estimate optimal sequence hidden states given model parameters observed data calculate likelihood data given observed data estimate model parameters 
542: rst second problem solved dynamic programing algorithms known viterbi algorithm forward backward algorithm respectively last one solved expectation maximization iterative algorithm known baum welch algorithm see ref listed detailed information 
543: references rabiner89 tutorial hidden markov models selected applications speech recognition lawrence rabiner using hmm classes module include multinomalhmm gaussianhmm gmmhmm implement hmm emis sion probability multimomial distribution gaussian distribution mixture gaussian distributions 
544: building hmm generating samples build hmm instance passing parameters described constructor generate samples hmm calling sample import numpy sklearn import hmm startprob array transmat array means array covars tile identity model hmm gaussianhmm full startprob transmat model means_ means chapter user guide model covars_ covars model sample scikit learn user guide release examples demonstration sampling hmm training hmm parameters infering hidden states train hmm calling method input list sequence observed value note since algorithm gradient based optimization method generally stuck local optimal try run various initialization select highest scored model score model calculated score method infered optimal hidden states obtained calling predict method predict method specied decoder algorithm currently viterbi algorithm viterbi maximum posteriori estimation map supported time input single sequence observed values model2 hmm gaussianhmm full model2 fit gaussianhmm algorithm viterbi covariance_type full covars_prior covars_weight means_prior none means_weight n_components random_state none startprob none startprob_prior transmat none transmat_prior model predict examples gaussian hmm stock data unsupervised learning scikit learn user guide release implementing hmms emission probabilities want implement emission probability poisson make hmm class inheriting _basehmm override necessary methods __init__ _compute_log_likelihood _set _get addiitional parameters _initialize_sufcient_statistics _accumulate_sufcient_statistics _do_mstep 
545: model selection cross validation evaluating estimator performance learning parameters prediction function testing data methodological mistake model would repeat labels samples seen would perfect score would fail predict anything useful yet unseen data avoid tting dene two different sets training set x_train y_train used learning parameters predictive model testing set x_test y_test used evaluating tted predictive model scikit learn random split quickly computed train_test_split helper function let load iris data set linear support vector machine model import numpy sklearn import cross_validation sklearn import datasets sklearn import svm iris datasets load_iris iris data shape iris target shape quickly sample training set holding data testing evaluating classier x_train x_test y_train y_test cross_validation train_test_split 
546: iris data iris target test_size random_state x_train shape y_train shape x_test shape y_test shape clf svm svc kernel linear fit x_train y_train clf score x_test y_test 
547: however dening two sets drastically reduce number samples used learning model results depend particular random choice pair train test sets solution split whole data several consecutive times different train set test set return averaged value prediction scores obtained different sets procedure called cross validation approach computationally expensive waste much data case xing arbitrary test set major advantage problem inverse inference number samples small 
548: chapter user guide scikit learn user guide release computing cross validated metrics simplest way use perform cross validation call cross_val_score helper function estimator dataset following example demonstrates estimate accuracy linear kernel support vector machine iris dataset splitting data tting model computing score consecutive times different splits time clf svm svc kernel linear scores cross_validation cross_val_score scores array 
549: clf iris data iris target 
550: mean score standard deviation score estimate hence given print accuracy scores mean scores std accuracy default score computed iteration score method estimator possible change passing custom scoring function metrics module sklearn import metrics cross_validation cross_val_score clf iris data iris target array 
551: score_func metrics f1_score 
552: case iris dataset samples balanced across target classes hence accuracy score almost equal argument integer cross_val_score uses kfold stratifiedkfold strategies default depending absence presence target array also possible use othe cross validation strategies passing cross validation iterator instead instance n_samples iris data shape cross_validation shufflesplit n_samples n_iterations 
553: test_size random_state cross_validation cross_val_score clf iris data iris target array 
554: available cross validation iterators introduced following 
555: examples receiver operating characteristic roc cross validation recursive feature elimination cross validation parameter estimation using grid search nested cross validation sample pipeline text feature extraction evaluation model selection scikit learn user guide release cross validation iterators following sections list utilities generate boolean masks indices used generate dataset splits according different cross validation strategies 
556: boolean mask integer indices cross validators support generating boolean masks integer indices select samples given fold data matrix sparse integer indices work expected integer indexing hence default behavior since version explicitly pass indices false constructor object supported use boolean mask method instead 
557: fold kfold divides samples math groups samples called folds equivalent leave one strategy equal sizes possible prediction function learned using folds fold left used test example fold import numpy sklearn cross_validation import kfold array array kfold len indices false print sklearn cross_validation kfold train test false false true print train test true true false false false false true true true false false true true fold constituted two arrays rst one related training set second one test set thus one create training test sets using x_train x_test y_train y_test train test train test scipy sparse matrices train test need integer indices obtained setting parameter indices true creating cross validation procedure array array kfold len indices true train test print train test chapter user guide scikit learn user guide release stratied fold stratifiedkfold variation fold returns stratied folds creates folds preserving percentage target class complete set example stratied fold sklearn cross_validation import stratifiedkfold skf stratifiedkfold print skf sklearn cross_validation stratifiedkfold labels train test skf print train test leave one loo leaveoneout loo simple cross validation learning set created taking samples except one test set sample left thus samples different learning sets different tests set cross validation procedure waste much data one sample removed learning set sklearn cross_validation import leaveoneout array array loo leaveoneout len print loo sklearn cross_validation leaveoneout print train test train test loo leave lpo leavepout similar leave one creates possible training test sets removing samples complete set example leave sklearn cross_validation import leavepout model selection scikit learn user guide release lpo leavepout len print lpo sklearn cross_validation leavepout print train test train test lpo leave one label lolo leaveonelabelout lolo cross validation scheme holds samples according third party provided label label information used encode arbitrary domain specic stratications samples integers training set thus constituted samples except ones related specic label example cases multiple experiments lolo used create cross validation based different experiments create training set using samples experiments except one sklearn cross_validation import leaveonelabelout labels lolo leaveonelabelout labels print lolo sklearn cross_validation leaveonelabelout labels train test lolo print train test another common application use time information instance labels could year collection samples thus allow cross validation time based splits 
558: leave label leaveplabelout similar leave one label removes samples related labels training test set example leave label sklearn cross_validation import leaveplabelout labels lplo leaveplabelout labels chapter user guide scikit learn user guide release print lplo sklearn cross_validation leaveplabelout labels print train test train test lplo random permutations cross validation shufe split shufflesplit shufflesplit iterator generate user dened number independent train test dataset splits samples rst shufed splitted pair train test sets possible control randomness reproducibility results explicitly seeding random_state pseudo random number generator usage example random_state cross_validation shufflesplit n_iterations test_size len print shufflesplit n_iterations test_size indices true print train_index test_index train_index test_index shufflesplit thus good alternative kfold cross validation allows ner control number iterations proportion samples side train test split 
559: see also stratifiedshufflesplit variation shufesplit returns stratied splits creates splits preserving percentage target class complete set 
560: bootstrapping cross validation bootstrap bootstrapping general statistics technique iterates computation estimator resampled dataset bootstrap iterator generate user dened number independent train test dataset splits samples drawn replacement side split furthermore possible control size train test subset make union smaller total dataset large 
561: note contrary cross validation strategies bootstrapping allow samples occur several times splits 
562: model selection scikit learn user guide release cross_validation bootstrap random_state len print bootstrap n_bootstraps train_size test_size random_state print train_index test_index train_index test_index cross validation model selection cross validation iterators also used directly perform model selection using grid search optimal hyperparameters model topic next section grid search setting estimator parameters 
563: grid search setting estimator parameters grid search used optimize parameters model kernel gamma support vector classier alpha lasso etc using internal cross validation evaluating estimator performance scheme 
564: gridsearchcv main class implementing hyperparameters grid search scikit learn grid_search gridsearchcv class passed base model instance example sklearn svm svc along grid potential hyper parameter values gamma kernel rbf kernel linear grid_search gridsearchcv instance implements usual estimator api tting dataset possible combinations hyperparameter values evaluated best combinations retained 
565: model selection development evaluation model selection gridsearchcv seen way use labeled data train hyper parameters grid evaluating resulting model important held samples seen grid search process recommended split data development set fed gridsearchcv instance evaluation set compute performance metrics done using cross_validation train_test_split utility function 
566: examples see parameter estimation using grid search nested cross validation example grid search com putation digits dataset 
567: chapter user guide scikit learn user guide release see sample pipeline text feature extraction evaluation example grid search coupling parame ters text documents feature extractor gram count vectorizer idf transformer classier linear svm trained sgd either elastic net penalty using pipeline pipeline instance 
568: note computations run parallel supports using keyword n_jobs see function signature details 
569: alternatives brute force grid search model specic cross validation models data range value parameter almost efciently tting estimator single value parameter feature leveraged perform efcient cross validation used model selection parameter common parameter amenable strategy parameter encoding strength regularizer case say compute regularization path estimator list models linear_model ridgecv alphas ridge regression built cross validation linear_model ridgeclassifiercv alphas ridge classier built cross validation cross validated least angle regression model linear_model larscv t_intercept linear_model lassolarscv t_intercept cross validated lasso using lars algorithm lasso linear model iterative tting along regularization path linear_model lassocv eps n_alphas linear_model elasticnetcv rho eps elastic net model iterative tting along regularization path sklearn linear_model ridgecv class sklearn linear_model ridgecv alphas array 
570: t_intercept true malize false score_func none loss_func none none gcv_mode none ridge regression built cross validation default performs generalized cross validation form efcient leave one cross validation 
571: parameters alphas numpy array shape n_alpha array alpha values try small positive values alpha improve conditioning problem reduce variance estimates alpha corresponds linear models logisticregression linearsvc 
572: t_intercept boolean whether calculate intercept model set false intercept used calculations data expected already centered 
573: normalize boolean optional true regressors normalized score_func callable optional function takes arguments compares order evaluate performance prediction big good none passed score estimator maximized model selection scikit learn user guide release loss_func callable optional function takes arguments compares order evaluate performance prediction small good none passed score estimator maximized cross validation generator optional none generalized cross validation efcient leave one used 
574: see also ridgeridge regression ridgeclassifierridge classier ridgecvridge regression built cross validation attributes coef_ gcv_mode methods shape n_features array n_classes n_features none auto svd eigen tional weight vector 
575: flag indicating strategy use performing generalized cross validation options auto use svd n_samples n_features otherwise use eigen svd force computation via singular value decomposition eigen force computation via eigendecomposition auto mode default intended pick cheaper tion two depending upon shape training data 
576: decision_function decision function linear model fit sample_weight get_params deep predict score set_params params fit ridge regression model get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
577: __init__ alphas array 
578: t_intercept true normalize false score_func none loss_func none none gcv_mode none decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
579: fit sample_weight fit ridge regression model chapter user guide scikit learn user guide release parameters array like shape n_samples n_features training data array like shape n_samples n_samples n_responses target values sample_weight oat array like shape n_samples sample weight returns self returns self 
580: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
581: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
582: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
583: parameters array like shape n_samples n_features training set 
584: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn linear_model ridgeclassiercv class sklearn linear_model ridgeclassifiercv alphas array t_intercept true normalize false score_func none loss_func none none class_weight none 
585: ridge classier built cross validation default performs generalized cross validation form efcient leave one cross validation currently n_features n_samples case handled efciently 
586: model selection scikit learn user guide release parameters alphas numpy array shape n_alpha array alpha values try small positive values alpha improve conditioning problem reduce variance estimates alpha corresponds linear models logisticregression linearsvc 
587: t_intercept boolean whether calculate intercept model set false intercept used calculations data expected already centered 
588: normalize boolean optional true regressors normalized score_func callable optional function takes arguments compares order evaluate performance prediction big good none passed score estimator maximized loss_func callable optional function takes arguments compares order evaluate performance prediction small good none passed score estimator maximized cross validation generator optional none generalized cross validation efcient leave one used 
589: class_weight dict optional weights associated classes form class_label weight given classes supposed weight one 
590: see also ridgeridge regression ridgeclassifierridge classier ridgecvridge regression built cross validation notes multi class classication n_class classiers trained one versus approach concretely implemented taking advantage multi variate response support ridge 
591: methods decision_function fit sample_weight class_weight get_params deep predict score set_params params fit ridge classier get parameters estimator predict target values according tted model returns coefcient determination prediction set parameters estimator 
592: __init__ alphas array 
593: t_intercept true normalize false score_func none loss_func none none class_weight none chapter user guide scikit learn user guide release fit sample_weight class_weight none fit ridge classier 
594: parameters array like shape n_samples n_features training vectors n_samples number samples n_features num ber features 
595: array like shape n_samples target values 
596: sample_weight oat numpy array shape n_samples sample weight class_weight dict optional weights associated classes form class_label weight given classes supposed weight one 
597: returns self object returns self get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
598: predict predict target values according tted model 
599: parameters array like shape n_samples n_features returns array shape n_samples score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
600: parameters array like shape n_samples n_features training set 
601: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self model selection scikit learn user guide release sklearn linear_model larscv class sklearn linear_model larscv t_intercept true verbose false max_iter normal ize true precompute auto none max_n_alphas n_jobs eps 2204460492503131e copy_x true cross validated least angle regression model parameters t_intercept boolean whether calculate intercept model set false intercept used calculations data expected already centered 
602: verbose boolean integer optional sets verbosity amount normalize boolean optional true regressors normalized copy_x boolean optional default true true copied else may overwritten 
603: precompute true false auto array like whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
604: max_iter integer optional maximum number iterations perform 
605: crossvalidation generator optional see sklearn cross_validation module none passed default fold strategy max_n_alphas integer optional maximum number points path used compute residuals cross validation n_jobs integer optional number cpus use cross validation use cpus eps oat optional machine precision regularization computation cholesky diagonal fac tors increase ill conditioned systems 
606: see also lars_path lassolars lassolarscv attributes coef_ intercept_ coef_path array shape n_features n_alpha array shape n_features oat parameter vector fomulation formula independent term decision function varying values coefcients along path chapter user guide scikit learn user guide release methods decision_function decision function linear model fit get_params deep predict score set_params params fit model using training data get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
607: __init__ t_intercept true verbose false max_iter normalize true precompute auto none max_n_alphas n_jobs eps 2204460492503131e copy_x true decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
608: fit fit model using training data 
609: parameters array like shape n_samples n_features training data 
610: array like shape n_samples target values returns self object returns instance self 
611: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
612: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
613: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
614: parameters array like shape n_samples n_features training set 
615: model selection scikit learn user guide release array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn linear_model lassolarscv class sklearn linear_model lassolarscv t_intercept true verbose false max_iter precompute auto n_jobs cross validated lasso using lars algorithm optimization objective lasso normalize true none eps 2204460492503131e copy_x true max_n_alphas n_samples 2_2 alpha parameters t_intercept boolean whether calculate intercept model set false intercept used calculations data expected already centered 
616: verbose boolean integer optional sets verbosity amount normalize boolean optional true regressors normalized precompute true false auto array like whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
617: max_iter integer optional maximum number iterations perform 
618: crossvalidation generator optional see sklearn cross_validation module none passed default fold strategy max_n_alphas integer optional maximum number points path used compute residuals cross validation n_jobs integer optional number cpus use cross validation use cpus eps oat optional machine precision regularization computation cholesky diagonal fac tors increase ill conditioned systems 
619: copy_x boolean optional default true chapter user guide scikit learn user guide release true copied else may overwritten 
620: see also lars_path lassolars larscv lassocv notes object solves problem lassocv object however unlike lassocv relevent alphas values general property stable however fragile heavily multicollinear datasets efcient lassocv small number features selected compared total number instance samples compared number features 
621: attributes coef_ intercept_ coef_path array shape n_features n_alpha alphas_ array shape n_alpha cv_alphas array shape n_cv_alphas cv_mse_path_ array shape n_folds n_cv_alphas methods array shape n_features oat parameter vector fomulation formula independent term decision function varying values coefcients along path different values alpha along path values alpha along path different folds mean square error left fold along path alpha values given cv_alphas decision_function decision function linear model fit get_params deep predict score set_params params fit model using training data get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
622: __init__ t_intercept true verbose false max_iter normalize true precompute auto none max_n_alphas n_jobs eps 2204460492503131e copy_x true decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
623: fit fit model using training data 
624: model selection scikit learn user guide release parameters array like shape n_samples n_features training data 
625: array like shape n_samples target values returns self object returns instance self 
626: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
627: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
628: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
629: parameters array like shape n_samples n_features training set 
630: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn linear_model lassocv class sklearn linear_model lassocv eps n_alphas alphas none t_intercept true max_iter precompute auto normalize false tol copy_x true none verbose false lasso linear model iterative tting along regularization path best model selected cross validation optimization objective lasso n_samples 2_2 alpha chapter user guide scikit learn user guide release parameters eps oat optional length path eps means alpha_min alpha_max 
631: n_alphas int optional number alphas along regularization path alphas numpy array optional list alphas compute models none alphas set automatically precompute true false auto array like whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
632: max_iter int optional maximum number iterations tol oat optional tolerance optimization updates smaller tol optimization code checks dual gap optimality continues smaller tol 
633: integer crossvalidation generator optional integer passed number fold default specic crossvalidation jects passed see sklearn cross_validation module list possible objects verbose bool integer amount verbosity see also lars_path lasso_path lassolars lasso lassolarscv notes see examples linear_model lasso_path_with_crossvalidation example avoid unnecessary memory duplication argument method directly passed fortran contiguous numpy array 
634: attributes alpha_ oat coef_ intercept_ mse_path_ array shape n_alphas n_folds methods array shape n_features oat amount penalization choosen cross validation parameter vector fomulation formula independent term decision function mean square error test set fold varying alpha model selection scikit learn user guide release decision_function fit get_params deep path eps n_alphas alphas compute lasso path coordinate descent predict score set_params params predict using linear model returns coefcient determination prediction set parameters estimator 
635: decision function linear model fit linear model coordinate descent along decreasing alphas get parameters estimator __init__ eps n_alphas alphas none t_intercept true normalize false precom pute auto max_iter tol copy_x true none verbose false decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
636: fit fit linear model coordinate descent along decreasing alphas using cross validation parameters numpy array shape n_samples n_features training data pass directly fortran contiguous data avoid unnecessary memory duplication numpy array shape n_samples target values get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators precompute auto none eps t_intercept true normalize false copy_x true verbose false params static path n_alphas alphas none compute lasso path coordinate descent optimization objective lasso n_samples 2_2 alpha parameters numpy array shape n_samples n_features training data pass directly fortran contiguous data avoid unnecessary memory duplication numpy array shape n_samples target values eps oat optional length path eps means alpha_min alpha_max n_alphas int optional chapter user guide scikit learn user guide release number alphas along regularization path alphas numpy array optional list alphas compute models none alphas set automatically precompute true false auto array like whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
637: array like optional dot precomputed useful gram matrix precomputed 
638: t_intercept bool fit intercept normalize boolean optional true regressors normalized copy_x boolean optional default true true copied else may overwritten 
639: verbose bool integer amount verbosity params kwargs keyword arguments passed lasso objects returns models list models along regularization path see also lars_path sklearn decomposition sparse_encode lasso lassolars lassocv lassolarscv notes see examples linear_model plot_lasso_coordinate_descent_path example avoid unnecessary memory duplication argument method directly passed fortran contiguous numpy array 
640: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
641: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
642: model selection scikit learn user guide release parameters array like shape n_samples n_features training set 
643: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn linear_model elasticnetcv class sklearn linear_model elasticnetcv rho eps n_alphas alphas none precom t_intercept true pute auto max_iter tol none copy_x true verbose n_jobs normalize false elastic net model iterative tting along regularization path best model selected cross validation 
644: parameters rho oat optional oat passed elasticnet scaling penalties rho penalty penalty rho penalty rho penalty combination parameter list case different values tested cross validation one giving best prediction score used note good choice list values rho often put values close lasso less close ridge eps oat optional length path eps means alpha_min alpha_max 
645: n_alphas int optional number alphas along regularization path alphas numpy array optional list alphas compute models none alphas set automatically precompute true false auto array like whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
646: max_iter int optional maximum number iterations tol oat optional tolerance optimization updates smaller tol optimization code checks dual gap optimality continues smaller tol 
647: integer crossvalidation generator optional integer passed number fold default specic crossvalidation jects passed see sklearn cross_validation module list possible objects chapter user guide scikit learn user guide release verbose bool integer amount verbosity n_jobs integer optional number cpus use cross validation use cpus note used multiple values rho given 
648: see also enet_path elasticnet notes see examples linear_model lasso_path_with_crossvalidation example avoid unnecessary memory duplication argument method directly passed fortran contiguous numpy array parameter rho corresponds alpha glmnet package alpha corresponds lambda param eter glmnet specically optimization objective n_samples 2_2 alpha rho alpha rho 2_2 interested controlling penalty separately keep mind equivalent alpha rho attributes alpha_ oat rho_ oat coef_ intercept_ mse_path_ array shape n_rho n_alpha n_folds methods array shape n_features oat amount penalization choosen cross validation compromise penalization choosen cross validation parameter vector fomulation formula independent term decision function mean square error test set fold varying rho alpha decision_function fit get_params deep path rho eps n_alphas alphas compute elastic net path coordinate descent predict score decision function linear model fit linear model coordinate descent along decreasing alphas get parameters estimator predict using linear model returns coefcient determination prediction 
649: model selection continued next page scikit learn user guide release set_params params table continued previous page set parameters estimator 
650: __init__ rho eps n_alphas alphas none t_intercept true normalize false tol none copy_x true verbose precompute auto max_iter n_jobs decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
651: fit fit linear model coordinate descent along decreasing alphas using cross validation parameters numpy array shape n_samples n_features training data pass directly fortran contiguous data avoid unnecessary memory duplication numpy array shape n_samples target values get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
652: static path rho eps n_alphas alphas none precompute auto none t_intercept true normalize false copy_x true verbose false params compute elastic net path coordinate descent elastic net optimization function n_samples 2_2 alpha rho alpha rho 2_2 parameters numpy array shape n_samples n_features training data pass directly fortran contiguous data avoid unnecessary memory duplication numpy array shape n_samples target values rho oat optional oat passed elasticnet scaling penalties rho corresponds lasso eps oat length path eps means alpha_min alpha_max n_alphas int optional chapter user guide scikit learn user guide release number alphas along regularization path alphas numpy array optional list alphas compute models none alphas set automatically precompute true false auto array like whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
653: array like optional dot precomputed useful gram matrix precomputed 
654: t_intercept bool fit intercept normalize boolean optional true regressors normalized copy_x boolean optional default true true copied else may overwritten 
655: verbose bool integer amount verbosity params kwargs keyword arguments passed lasso objects returns models list models along regularization path see also elasticnet elasticnetcv notes see examples plot_lasso_coordinate_descent_path example 
656: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
657: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
658: parameters array like shape n_samples n_features training set 
659: model selection scikit learn user guide release array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self information criterion models offer information theoretic closed form formula optimal estimate regularization parameter computing single regularization path instead several using cross validation list models benetting aikike information criterion aic bayesian information crite rion bic automated model selection linear_model lassolarsic criterion lasso model lars using bic aic model selection sklearn linear_model lassolarsic class sklearn linear_model lassolarsic criterion aic lasso model lars using bic aic model selection optimization objective lasso verbose false normalize true precompute auto max_iter eps 2204460492503131e copy_x true t_intercept true n_samples 2_2 alpha aic akaike information criterion bic bayes information criterion criteria useful select value regularization parameter making trade goodness complexity model good model explain well data simple 
660: parameters criterion bic aic type criterion use 
661: t_intercept boolean whether calculate intercept model set false intercept used calculations data expected already centered 
662: verbose boolean integer optional sets verbosity amount normalize boolean optional true regressors normalized copy_x boolean optional default true true copied else may overwritten 
663: precompute true false auto array like whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
664: chapter user guide scikit learn user guide release max_iter integer optional maximum number iterations perform used early stopping 
665: eps oat optional machine precision regularization computation cholesky diagonal fac tors increase ill conditioned systems unlike tol parameter iterative optimization based algorithms parameter control tolerance optimization 
666: see also lars_path lassolars lassolarscv notes estimation number degrees freedom given degrees freedom lasso hui zou trevor hastie robert tibshirani ann statist volume number http wikipedia org wiki akaike_information_criterion http wikipedia org wiki bayesian_information_criterion examples sklearn import linear_model clf linear_model lassolarsic criterion bic clf fit lassolarsic copy_x true criterion bic eps fit_intercept true max_iter normalize true precompute auto verbose false print clf coef_ 
667: attributes coef_ intercept_ alpha_ array shape n_features oat oat parameter vector fomulation formula independent term decision function alpha parameter chosen information criterion methods decision_function decision function linear model fit copy_x get_params deep predict score set_params params fit model using training data get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
668: model selection scikit learn user guide release __init__ criterion aic t_intercept true verbose false normalize true precompute auto max_iter eps 2204460492503131e copy_x true decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
669: fit copy_x true fit model using training data 
670: parameters array like shape n_samples n_features training data 
671: array like shape n_samples target values returns self object returns instance self 
672: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
673: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
674: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
675: parameters array like shape n_samples n_features training set 
676: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self chapter user guide scikit learn user guide release bag estimates using ensemble methods base upon bagging generating new training sets using sampling replacement part training set remains unused classier ensemble different part training set left left portion used estimate generalization error without rely separate validation set estimate comes free addictional data needed used model selection currently implemented following classes ensemble randomforestclassifier ensemble randomforestregressor ensemble extratreesclassifier ensemble extratreesregressor n_estimators ensemble gradientboostingclassifier loss gradient boosting classication ensemble gradientboostingregressor loss random forest classier random forest regressor extra trees classier extra trees regressor 
677: gradient boosting regression 
678: sklearn ensemble randomforestclassier class sklearn ensemble randomforestclassifier n_estimators criterion gini max_depth none min_samples_split min_samples_leaf min_density max_features auto bootstrap true com pute_importances false oob_score false n_jobs random_state none verbose random forest classier random forest meta estimator number classical decision trees various sub samples dataset use averaging improve predictive accuracy control tting 
679: parameters n_estimators integer optional default number trees forest 
680: criterion string optional default gini function measure quality split supported criteria gini gini impurity entropy information gain note parameter tree specic 
681: max_depth integer none optional default none none nodes expanded leaves maximum depth tree pure leaves contain less min_samples_split samples note parameter tree specic 
682: min_samples_split integer optional default minimum number samples required split internal node note parame ter tree specic 
683: min_samples_leaf integer optional default minimum number samples newly created leaves split discarded split one leaves would contain less min_samples_leaf samples note parameter tree specic 
684: min_density oat optional default parameter controls trade optimization heuristic controls minimum density sample_mask fraction samples mask density falls model selection scikit learn user guide release threshold mask recomputed input data packed results min_density equals one partitions always represented data copying copies original data otherwise partitions represented bit masks aka sample masks note parameter tree specic 
685: max_features int string none optional default auto number features consider looking best split max_features sqrt n_features classication tasks auto max_features n_features regression problems sqrt max_features sqrt n_features log2 max_features log2 n_features none max_features n_features 
686: note parameter tree specic 
687: bootstrap boolean optional default true whether bootstrap samples used building trees 
688: compute_importances boolean optional default true whether computed feature_importances_ attribute calling 
689: importances feature stored oob_score bool whether use bag samples estimate generalization error 
690: n_jobs integer optional default number jobs run parallel number jobs set number cores 
691: random_state int randomstate instance none optional default none int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used random 
692: verbose int optional default controlls verbosity tree building process 
693: see also decisiontreeclassifier extratreesclassifier references r59 chapter user guide attributes fea ture_importances_ oob_score_ array shape n_features oat oob_decision_function_array shape n_samples n_classes methods scikit learn user guide release feature importances higher important feature score training dataset obtained using bag estimate decision function computed bag estimate training set 
694: fit fit_transform get_params deep predict predict_log_proba predict_proba score set_params params transform threshold reduce important features 
695: build forest trees training set fit data transform get parameters estimator predict class predict class log probabilities predict class probabilities returns mean accuracy given test data labels set parameters estimator 
696: __init__ n_estimators criterion gini max_depth none min_samples_leaf min_density max_features auto bootstrap true pute_importances false oob_score false n_jobs random_state none verbose min_samples_split com fit build forest trees training set 
697: parameters array like shape n_samples n_features training input samples 
698: array like shape n_samples target values integers correspond classes classication real numbers regression 
699: returns self object returns self 
700: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
701: parameters numpy array shape n_samples n_features training set 
702: numpy array shape n_samples target values 
703: returns x_new numpy array shape n_samples n_features_new transformed array 
704: model selection scikit learn user guide release notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
705: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
706: predict predict class predicted class input sample computed majority prediction trees forest 
707: parameters array like shape n_samples n_features input samples 
708: returns array shape n_samples predicted classes 
709: predict_log_proba predict class log probabilities predicted class log probabilities input sample computed mean predicted class log probabilities trees forest 
710: parameters array like shape n_samples n_features input samples 
711: returns array shape n_samples class log probabilities input samples classes ordered arithmetical order predict_proba predict class probabilities predicted class probabilities input sample computed mean predicted class probabilities trees forest 
712: parameters array like shape n_samples n_features input samples 
713: returns array shape n_samples class probabilities input samples classes ordered arithmetical order 
714: score returns mean accuracy given test data labels 
715: parameters array like shape n_samples n_features training set 
716: array like shape n_samples labels 
717: returns oat chapter user guide scikit learn user guide release set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform threshold none reduce important features 
718: parameters array scipy sparse matrix shape n_samples n_features input samples 
719: threshold string oat none optional default none threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor mean may also used none available object attribute threshold used otherwise mean used default 
720: returns x_r array shape n_samples n_selected_features input samples selected features 
721: sklearn ensemble randomforestregressor class sklearn ensemble randomforestregressor n_estimators criterion mse min_samples_split max_depth none min_samples_leaf min_density max_features auto bootstrap true com pute_importances false oob_score false n_jobs random_state none verbose random forest regressor random forest meta estimator number classical decision trees various sub samples dataset use averaging improve predictive accuracy control tting 
722: parameters n_estimators integer optional default number trees forest 
723: criterion string optional default mse function measure quality split supported criterion mse mean squared error note parameter tree specic 
724: max_depth integer none optional default none maximum depth tree none nodes expanded leaves pure leaves contain less min_samples_split samples note parameter tree specic 
725: min_samples_split integer optional default minimum number samples required split internal node note parame ter tree specic 
726: min_samples_leaf integer optional default model selection scikit learn user guide release minimum number samples newly created leaves split discarded split one leaves would contain less min_samples_leaf samples note parameter tree specic 
727: min_density oat optional default parameter controls trade optimization heuristic controls minimum density sample_mask fraction samples mask density falls threshold mask recomputed input data packed results min_density equals one partitions always represented data copying copies original data otherwise partitions represented bit masks aka sample masks note parameter tree specic 
728: max_features int string none optional default auto number features consider looking best split max_features sqrt n_features classication tasks auto max_features n_features regression problems sqrt max_features sqrt n_features log2 max_features log2 n_features none max_features n_features 
729: note parameter tree specic 
730: bootstrap boolean optional default true whether bootstrap samples used building trees 
731: compute_importances boolean optional default true whether computed feature_importances_ attribute calling 
732: importances feature stored oob_score bool whether use bag samples estimate generalization error 
733: n_jobs integer optional default number jobs run parallel number jobs set number cores 
734: random_state int randomstate instance none optional default none int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used random 
735: verbose int optional default controlls verbosity tree building process 
736: see also decisiontreeregressor extratreesregressor references r60 chapter user guide scikit learn user guide release attributes fea ture_importances_ oob_score_ array shape n_features oat oob_prediction_ array shape n_samples feature mportances higher important feature score training dataset obtained using bag estimate prediction computed bag estimate training set 
737: methods fit fit_transform get_params deep predict score set_params params transform threshold reduce important features 
738: build forest trees training set fit data transform get parameters estimator predict regression target returns coefcient determination prediction set parameters estimator 
739: __init__ n_estimators criterion mse max_depth none min_samples_leaf min_density max_features auto bootstrap true pute_importances false oob_score false n_jobs random_state none verbose min_samples_split com fit build forest trees training set 
740: parameters array like shape n_samples n_features training input samples 
741: array like shape n_samples target values integers correspond classes classication real numbers regression 
742: returns self object returns self 
743: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
744: parameters numpy array shape n_samples n_features training set 
745: numpy array shape n_samples target values 
746: returns x_new numpy array shape n_samples n_features_new transformed array 
747: model selection scikit learn user guide release notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
748: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
749: predict predict regression target predicted regression target input sample computed mean predicted regression targets trees forest 
750: parameters array like shape n_samples n_features input samples 
751: returns array shape n_samples predicted values 
752: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
753: parameters array like shape n_samples n_features training set 
754: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform threshold none reduce important features 
755: parameters array scipy sparse matrix shape n_samples n_features input samples 
756: threshold string oat none optional default none threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor mean may also used none available object attribute threshold used otherwise mean used default 
757: chapter user guide scikit learn user guide release returns x_r array shape n_samples n_selected_features input samples selected features 
758: sklearn ensemble extratreesclassier class sklearn ensemble extratreesclassifier n_estimators max_depth none min_samples_leaf max_features auto bootstrap false pute_importances false n_jobs random_state none verbose criterion gini min_samples_split min_density com oob_score false extra trees classier class implements meta estimator number randomized decision trees extra trees various sub samples dataset use averaging improve predictive accuracy control tting 
759: parameters n_estimators integer optional default number trees forest 
760: criterion string optional default gini function measure quality split supported criteria gini gini impurity entropy information gain note parameter tree specic 
761: max_depth integer none optional default none maximum depth tree none nodes expanded leaves pure leaves contain less min_samples_split samples note parameter tree specic 
762: min_samples_split integer optional default minimum number samples required split internal node note parame ter tree specic 
763: min_samples_leaf integer optional default minimum number samples newly created leaves split discarded split one leaves would contain less min_samples_leaf samples note parameter tree specic 
764: min_density oat optional default parameter controls trade optimization heuristic controls minimum density sample_mask fraction samples mask density falls threshold mask recomputed input data packed results min_density equals one partitions always represented data copying copies original data otherwise partitions represented bit masks aka sample masks note parameter tree specic 
765: max_features int string none optional default auto number features consider looking best split 
766: max_features sqrt n_features classication tasks auto max_features n_features regression problems sqrt max_features sqrt n_features log2 max_features log2 n_features none max_features n_features 
767: model selection scikit learn user guide release note parameter tree specic 
768: bootstrap boolean optional default false whether bootstrap samples used building trees 
769: compute_importances boolean optional default true whether computed feature_importances_ attribute calling 
770: importances feature stored oob_score bool whether use bag samples estimate generalization error 
771: n_jobs integer optional default number jobs run parallel number jobs set number cores 
772: random_state int randomstate instance none optional default none int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used random 
773: verbose int optional default controlls verbosity tree building process 
774: see also sklearn tree extratreeclassifierbase classier ensemble randomforestclassifierensemble classier based trees optimal splits 
775: references r57 attributes fea ture_importances_ oob_score_ array shape n_features oat oob_decision_function_array shape n_samples n_classes feature mportances higher important feature score training dataset obtained using bag estimate decision function computed bag estimate training set 
776: methods fit fit_transform get_params deep predict predict_log_proba build forest trees training set fit data transform get parameters estimator predict class predict class log probabilities 
777: continued next page chapter user guide scikit learn user guide release table continued previous page predict_proba score set_params params transform threshold reduce important features 
778: predict class probabilities returns mean accuracy given test data labels set parameters estimator 
779: __init__ n_estimators criterion gini max_depth none min_samples_leaf min_density max_features auto bootstrap false pute_importances false oob_score false n_jobs random_state none verbose min_samples_split com fit build forest trees training set 
780: parameters array like shape n_samples n_features training input samples 
781: array like shape n_samples target values integers correspond classes classication real numbers regression 
782: returns self object returns self 
783: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
784: parameters numpy array shape n_samples n_features training set 
785: numpy array shape n_samples target values 
786: returns x_new numpy array shape n_samples n_features_new transformed array 
787: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
788: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
789: predict predict class predicted class input sample computed majority prediction trees forest 
790: parameters array like shape n_samples n_features model selection scikit learn user guide release input samples 
791: returns array shape n_samples predicted classes 
792: predict_log_proba predict class log probabilities predicted class log probabilities input sample computed mean predicted class log probabilities trees forest 
793: parameters array like shape n_samples n_features input samples 
794: returns array shape n_samples class log probabilities input samples classes ordered arithmetical order predict_proba predict class probabilities predicted class probabilities input sample computed mean predicted class probabilities trees forest 
795: parameters array like shape n_samples n_features input samples 
796: returns array shape n_samples class probabilities input samples classes ordered arithmetical order 
797: score returns mean accuracy given test data labels 
798: parameters array like shape n_samples n_features training set 
799: array like shape n_samples labels 
800: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform threshold none reduce important features 
801: parameters array scipy sparse matrix shape n_samples n_features input samples 
802: threshold string oat none optional default none chapter user guide scikit learn user guide release threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor mean may also used none available object attribute threshold used otherwise mean used default 
803: returns x_r array shape n_samples n_selected_features input samples selected features 
804: sklearn ensemble extratreesregressor class sklearn ensemble extratreesregressor n_estimators max_depth none min_samples_leaf max_features auto pute_importances false n_jobs random_state none verbose criterion mse min_samples_split min_density com oob_score false bootstrap false extra trees regressor class implements meta estimator number randomized decision trees extra trees various sub samples dataset use averaging improve predictive accuracy control tting 
805: parameters n_estimators integer optional default number trees forest 
806: criterion string optional default mse function measure quality split supported criterion mse mean squared error note parameter tree specic 
807: max_depth integer none optional default none none nodes expanded leaves maximum depth tree pure leaves contain less min_samples_split samples note parameter tree specic 
808: min_samples_split integer optional default minimum number samples required split internal node note parame ter tree specic 
809: min_samples_leaf integer optional default minimum number samples newly created leaves split discarded split one leaves would contain less min_samples_leaf samples note parameter tree specic 
810: min_density oat optional default parameter controls trade optimization heuristic controls minimum density sample_mask fraction samples mask density falls threshold mask recomputed input data packed results min_density equals one partitions always represented data copying copies original data otherwise partitions represented bit masks aka sample masks note parameter tree specic 
811: max_features int string none optional default auto number features consider looking best split model selection scikit learn user guide release max_features sqrt n_features classication tasks auto max_features n_features regression problems sqrt max_features sqrt n_features log2 max_features log2 n_features none max_features n_features 
812: note parameter tree specic 
813: bootstrap boolean optional default false whether bootstrap samples used building trees note parameter tree specic 
814: compute_importances boolean optional default true whether computed feature_importances_ attribute calling 
815: importances feature stored oob_score bool whether use bag samples estimate generalization error 
816: n_jobs integer optional default number jobs run parallel number jobs set number cores 
817: random_state int randomstate instance none optional default none int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used random 
818: verbose int optional default controlls verbosity tree building process 
819: see also sklearn tree extratreeregressorbase estimator ensemble randomforestregressorensemble regressor using trees optimal splits 
820: references r58 attributes fea ture_importances_ oob_score_ array shape n_features oat oob_prediction_ array shape n_samples feature mportances higher important feature score training dataset obtained using bag estimate prediction computed bag estimate training set 
821: chapter user guide scikit learn user guide release methods fit fit_transform get_params deep predict score set_params params transform threshold reduce important features 
822: build forest trees training set fit data transform get parameters estimator predict regression target returns coefcient determination prediction set parameters estimator 
823: __init__ n_estimators criterion mse max_depth none min_samples_leaf min_density max_features auto bootstrap false pute_importances false oob_score false n_jobs random_state none verbose min_samples_split com fit build forest trees training set 
824: parameters array like shape n_samples n_features training input samples 
825: array like shape n_samples target values integers correspond classes classication real numbers regression 
826: returns self object returns self 
827: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
828: parameters numpy array shape n_samples n_features training set 
829: numpy array shape n_samples target values 
830: returns x_new numpy array shape n_samples n_features_new transformed array 
831: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
832: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
833: model selection scikit learn user guide release predict predict regression target predicted regression target input sample computed mean predicted regression targets trees forest 
834: parameters array like shape n_samples n_features input samples 
835: returns array shape n_samples predicted values 
836: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
837: parameters array like shape n_samples n_features training set 
838: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform threshold none reduce important features 
839: parameters array scipy sparse matrix shape n_samples n_features input samples 
840: threshold string oat none optional default none threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor mean may also used none available object attribute threshold used otherwise mean used default 
841: returns x_r array shape n_samples n_selected_features input samples selected features 
842: sklearn ensemble gradientboostingclassier class sklearn ensemble gradientboostingclassifier loss deviance learn_rate subsam n_estimators min_samples_split ple min_samples_leaf max_depth init none random_state none gradient boosting classication 
843: chapter user guide scikit learn user guide release builds additive model forward stage wise fashion allows optimization arbitrary differen tiable loss functions stage n_classes_ regression trees negative gradient binomial multinomial deviance loss function binary classication special case single regression tree induced 
844: parameters loss deviance optional default deviance loss function optimized deviance refers deviance logistic regression classication probabilistic outputs refers least squares regression 
845: learn_rate oat optional default learning rate shrinks contribution tree learn_rate trade learn_rate n_estimators 
846: n_estimators int default number boosting stages perform gradient boosting fairly robust tting large number usually results better performance 
847: max_depth integer optional default maximum depth individual regression estimators maximum depth limits number nodes tree tune parameter best performance best value depends interaction input variables 
848: min_samples_split integer optional default minimum number samples required split internal node 
849: min_samples_leaf integer optional default minimum number samples required leaf node 
850: subsample oat optional default fraction samples used tting individual base learners smaller results stochastic gradient boosting subsample interacts parameter n_estimators 
851: see also sklearn tree decisiontreeclassifier randomforestclassifier references friedman greedy function approximation gradient boosting machine annals statistics vol 
852: friedman stochastic gradient boosting hastie tibshirani friedman elements statistical learning springer 
853: examples samples labels sklearn ensemble import gradientboostingclassifier gradientboostingclassifier fit samples labels print predict model selection scikit learn user guide release methods fit fit_stage x_argsorted y_pred get_params deep predict predict_proba score set_params params staged_decision_function fit gradient boosting model fit another stage n_classes_ trees boosting model get parameters estimator predict class predict class probabilities returns mean accuracy given test data labels set parameters estimator compute decision function 
854: __init__ loss deviance learn_rate n_estimators subsample min_samples_split min_samples_leaf max_depth init none random_state none fit fit gradient boosting model 
855: parameters array like shape n_samples n_features training vectors n_samples number samples n_features num ber features use fortran style avoid memory copies 
856: array like shape n_samples target values integers classication real numbers regression classication labels must correspond classes n_classes_ returns self object returns self 
857: fit_stage x_argsorted y_pred sample_mask fit another stage n_classes_ trees boosting model 
858: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
859: predict predict class 
860: parameters array like shape n_samples n_features input samples 
861: returns array shape n_samples predicted classes 
862: predict_proba predict class probabilities 
863: parameters array like shape n_samples n_features input samples 
864: returns array shape n_samples class probabilities input samples classes ordered arithmetical order 
865: chapter user guide scikit learn user guide release score returns mean accuracy given test data labels 
866: parameters array like shape n_samples n_features training set 
867: array like shape n_samples labels 
868: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self staged_decision_function compute decision function method allows monitoring determine error testing set stage 
869: parameters array like shape n_samples n_features input samples 
870: returns array shape n_samples n_classes decision function input samples classes ordered arithmetical order regression binary classication special cases n_classes 
871: sklearn ensemble gradientboostingregressor class sklearn ensemble gradientboostingregressor loss n_estimators ple min_samples_leaf init none random_state none learn_rate subsam min_samples_split max_depth gradient boosting regression builds additive model forward stage wise fashion allows optimization arbitrary differ entiable loss functions stage regression tree negative gradient given loss function 
872: parameters loss lad optional default loss function optimized refers least squares regression lad least absolute deviation highly robust loss function soley based order information input variables 
873: learn_rate oat optional default learning rate shrinks contribution tree learn_rate trade learn_rate n_estimators 
874: n_estimators int default number boosting stages perform gradient boosting fairly robust tting large number usually results better performance 
875: max_depth integer optional default model selection scikit learn user guide release maximum depth individual regression estimators maximum depth limits number nodes tree tune parameter best performance best value depends interaction input variables 
876: min_samples_split integer optional default minimum number samples required split internal node 
877: min_samples_leaf integer optional default minimum number samples required leaf node 
878: subsample oat optional default fraction samples used tting individual base learners smaller results stochastic gradient boosting subsample interacts parameter n_estimators 
879: see also sklearn tree decisiontreeregressor randomforestregressor references friedman greedy function approximation gradient boosting machine annals statistics vol 
880: friedman stochastic gradient boosting hastie tibshirani friedman elements statistical learning springer 
881: examples samples labels sklearn ensemble import gradientboostingregressor gradientboostingregressor fit samples labels print predict 32806997e attributes fea ture_importances_ array shape n_features oob_score_ array shape n_estimators train_score_ array shape n_estimators methods feature importances higher important feature 
882: score training dataset obtained using bag estimate score oob_score_ deviance loss model iteration bag sample score train_score_ deviance loss model iteration bag sample subsample deviance training data 
883: chapter user guide scikit learn user guide release fit fit_stage x_argsorted y_pred get_params deep predict score set_params params staged_decision_function staged_predict fit gradient boosting model fit another stage n_classes_ trees boosting model get parameters estimator predict regression target returns coefcient determination prediction set parameters estimator compute decision function predict regression target stage 
884: __init__ loss min_samples_leaf max_depth init none random_state none learn_rate n_estimators subsample min_samples_split fit fit gradient boosting model 
885: parameters array like shape n_samples n_features training vectors n_samples number samples n_features num ber features use fortran style avoid memory copies 
886: array like shape n_samples target values integers classication real numbers regression classication labels must correspond classes n_classes_ returns self object returns self 
887: fit_stage x_argsorted y_pred sample_mask fit another stage n_classes_ trees boosting model 
888: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
889: predict predict regression target 
890: parameters array like shape n_samples n_features input samples 
891: returns array shape n_samples predicted values 
892: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
893: parameters array like shape n_samples n_features training set 
894: array like shape n_samples model selection scikit learn user guide release returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self staged_decision_function compute decision function method allows monitoring determine error testing set stage 
895: parameters array like shape n_samples n_features input samples 
896: returns array shape n_samples n_classes decision function input samples classes ordered arithmetical order regression binary classication special cases n_classes 
897: staged_predict predict regression target stage method allows monitoring determine error testing set stage 
898: parameters array like shape n_samples n_features input samples 
899: returns array shape n_samples predicted value input samples 
900: pipeline chaining estimators pipeline used chain multiple estimators one useful often xed sequence steps processing data example feature selection normalization classication pipeline serves two purposes convenience call fit predict data whole sequence estimators joint parameter selection grid search parameters estimators pipeline 
901: estimators usable within pipeline except last one need transform function otherwise dataset passed estimator 
902: usage pipeline build using list key value pairs key string containing name want give step value estimator object sklearn pipeline import pipeline sklearn svm import svc sklearn decomposition import pca estimators reduce_dim pca svm svc clf pipeline estimators chapter user guide scikit learn user guide release clf pipeline steps reduce_dim pca copy true n_components none whiten false svm svc cache_size class_weight none coef0 degree gamma kernel rbf probability false shrinking true tol verbose false estimators pipeline stored list steps attribute clf steps reduce_dim pca copy true n_components none whiten false dict named_steps clf named_steps reduce_dim pca copy true n_components none whiten false parameters estimators pipeline accessed using estimator parameter syntax clf set_params svm__c normalize_whitespace pipeline steps reduce_dim pca copy true n_components none whiten false svm svc cache_size class_weight none coef0 degree gamma kernel rbf probability false shrinking true tol verbose false particularly important grid searches sklearn grid_search import gridsearchcv params dict reduce_dim__n_components grid_search gridsearchcv clf param_grid params svm__c examples pipeline anova svm sample pipeline text feature extraction evaluation pipelining chaining pca logistic regression explicit feature map approximation rbf kernels svm anova svm univariate feature selection notes calling fit pipeline calling fit estimator turn transform input pass next step pipeline methods last estimator pipline last estimator classier pipeline used classier last estimator transformer pipeline 
903: dataset transformations preprocessing data sklearn preprocessing package provides several common utility functions transformer classes change raw feature vectors representation suitable downstream estimators 
904: dataset transformations scikit learn user guide release standardization mean removal variance scaling standardization datasets common requirement many machine learning estimators implemented scikit might behave badly individual feature less look like standard normally distributed data gaussian zero mean unit variance practice often ignore shape distribution transform data center removing mean value feature scale dividing non constant features standard deviation instance many elements used objective function learning algorithm rbf kernel support vector machines regularizers linear models assume features centered around zero variance order feature variance orders magnitude larger others might dominate objective function make estimator unable learn features correctly expected function scale provides quick easy way perform operation single array like dataset sklearn import preprocessing x_scaled preprocessing scale x_scaled array 
905: scaled data zero mean unit variance x_scaled mean axis array x_scaled std axis array preprocessing module provides utility class scaler implements transformer api compute mean standard deviation training set able later reapply transformation testing set class hence suitable use early steps sklearn pipeline pipeline scaler preprocessing scaler fit scaler scaler copy true with_mean true with_std true scaler mean_ array scaler std_ array scaler transform array 
906: scaler instance used new data transform way training set scaler transform array chapter user guide scikit learn user guide release possible disable either centering scaling either passing with_mean false with_std false constructor scaler 
907: references discussion importance centering scaling data available faq normal ize standardize rescale data scaling whitening sometimes enough center scale features independently since downstream model make assumption linear independence features sklearn decomposition randomizedpca whiten true correlation across features 
908: remove linear sklearn decomposition pca address issue use sparse input scale scaler accept scipy sparse matrices input with_mean false explicitly passed constructor otherwise valueerror raised silently centering would break sparsity would often crash execution allocating excessive amounts memory unintentionally centered data expected small enough explicitly convert input array using toarray method sparse matrices instead data converted compressed sparse rows representation see scipy sparse csr_matrix avoid unnecessary memory copies recommended choose csr representation upstream 
909: sparse input scaling target variables regression scale scaler work box arrays useful scaling target response variables used regression 
910: normalization normalization process scaling individual samples unit norm process useful plan use quadratic form dot product kernel quantify similarity pair samples assumption base vector space model often used text classication clustering contexts function normalize provides quick easy way perform operation single array like dataset either using norms x_normalized preprocessing normalize norm x_normalized array 
911: dataset transformations scikit learn user guide release preprocessing module provides utility class normalizer implements operation using transformer api even though fit method useless case class stateless operation treats samples independently class hence suitable use early steps sklearn pipeline pipeline normalizer preprocessing normalizer fit fit nothing normalizer normalizer copy true norm normalizer instance used sample vectors transformer normalizer transform array 
912: normalizer transform array 
913: sparse input normalize normalizer accept dense array like sparse matrices scipy sparse input data converted compressed sparse rows representation see scipy sparse csr_matrix fed efcient cython routines avoid unnecessary memory copies recommended choose csr representation upstream 
914: sparse input binarization feature binarization feature binarization process thresholding numerical features get boolean values useful downsteam probabilistic estimators make assumption input data distributed according multi variate bernoulli distribution instance case common class restricted boltzmann machines yet implemented scikit also commmon among text processing community use binary feature values probably simplify probabilistic reasoning even normalized counts term frequencies idf valued features often perform slightly better practice used early stages sklearn pipeline pipeline fit method nothing sample treated independently others utility class binarizer meant normalizer binarizer preprocessing binarizer fit fit nothing binarizer binarizer copy true threshold binarizer transform array chapter user guide scikit learn user guide release possible adjust threshold binarizer binarizer preprocessing binarizer threshold binarizer transform array scaler normalizer classes preprocessing module provides companion function binarize used transformer api necessary 
915: sparse input binarize binarizer accept dense array like sparse matrices scipy sparse input data converted compressed sparse rows representation see scipy sparse csr_matrix avoid unnecessary memory copies recommended choose csr representation upstream 
916: sparse input feature extraction sklearn feature_extraction module used extract features format supported machine learning algorithms datasets consisting formats text image 
917: note feature extraction different feature selection former consists transforming arbitrary data text images numerical features usable machine learning later machine learning technique applied features 
918: loading features dicts class dictvectorizer used convert feature arrays represented lists standard python dict objects numpy scipy representation used scikit learn estimators particularly fast process pythons dict advantages convenient use sparse absent features need stored storing feature names addition values dictvectorizer implements called one one hot coding categorical aka nominal discrete features categorical features attribute value pairs value restricted list discrete possibilities without ordering topic identiers types objects tags names following city categorical attribute temperature traditional numerical feature measurements city dubai temperature city london temperature city san fransisco temperature sklearn feature_extraction import dictvectorizer vec dictvectorizer vec fit_transform measurements toarray dataset transformations scikit learn user guide release array vec get_feature_names city dubai city london city san fransisco temperature dictvectorizer also useful representation transformation training sequence classiers natural lan guage processing models typically work extracting feature windows around particular word interest example suppose rst algorithm extracts part speech pos tags want use complementary tags training sequence classier chunker following dict could window feature extracted around word sat sentence cat sat mat pos_window word pos word cat pos word pos real application one would extract many dictionaries description vectorized sparse two dimensional matrix suitable feeding classier maybe piped text tfidftransformer normalization vec dictvectorizer pos_vectorized vec fit_transform pos_window pos_vectorized 1x6 sparse matrix type type numpy float64 stored elements coordinate format pos_vectorized toarray array vec get_feature_names pos pos pos word word cat word imagine one extracts context around individual word corpus documents resulting matrix wide many one hot features valued zero time make resulting data structure able memory dictvectorizer class uses scipy sparse matrix default instead numpy ndarray 
919: text feature extraction bag words representation text analysis major application eld machine learning algorithms however raw data sequence symbols cannot fed directly algorithms expect numerical feature vectors xed size rather raw text documents variable length order address scikit learn provides utilities common ways extract numerical features text content namely tokenizing strings giving integer possible token instance using whitespaces punctuation token separators 
920: chapter user guide scikit learn user guide release counting occurrences tokens document normalizing weighting diminishing importance tokens occur majority samples docu ments 
921: scheme features samples dened follows individual token occurrence frequency normalized treated feature vector token frequencies given document considered multivariate sample 
922: corpus documents thus represented matrix one row per document one column per token word occurring corpus call vectorization general process turning collection text documents numerical feature vectors specic stragegy tokenization counting normalization called bag words bag grams repre sentation documents described word occurrences completely ignoring relative position information words document combined idf normalization bag words encoding also known vector space model 
923: sparsity documents typically use subset words used corpus resulting matrix many feature values zeros typically instance collection short text documents emails use vocabulary size order unique words total document use unique words individually order able store matrix memory also speed algebraic operations matrix vector imple mentations typically use sparse representation implementations available scipy sparse package 
924: common vectorizer usage countvectorizer implements tokenization occurrence counting single class sklearn feature_extraction text import countvectorizer model many parameters however default values quite reasonable please see reference documen tation details vectorizer countvectorizer vectorizer countvectorizer analyzer word binary false charset utf charset_error strict dtype type long input content lowercase true max_df max_features none max_n min_n preprocessor none stop_words none strip_accents none token_pattern tokenizer none vocabulary none lets use tokenize count word occurrences minimalistic corpus text documents corpus vectorizer fit_transform corpus first document second second document third one first document dataset transformations scikit learn user guide release 4x9 sparse matrix type type numpy int64 stored elements coordinate format default conguration tokenizes string extracting words least letters specic function step requested explicitly analyze vectorizer build_analyzer analyze text document analyze uthis uis utext udocument uto uanalyze term found analyzer assigned unique integer index corresponding column resulting matrix interpretation columns retrieved follows vectorizer get_feature_names uand udocument ufirst uis uone usecond uthe uthird uthis toarray array converse mapping feature name column index stored vocabulary_ attribute vectorizer vectorizer vocabulary_ get document hence words seen training corpus completely ignored future calls transform method vectorizer transform something completely new toarray array note previous corpus rst last documents exactly words hence encoded equal vectors particular lose information last document interogative form preserve local ordering information extract grams words addition grams word themselvs bigram_vectorizer countvectorizer min_n max_n analyze bigram_vectorizer build_analyzer analyze grams cool ubi ugrams uare ucool ubi grams ugrams uare cool token_pattern vocabulary extracted vectorizer hence much bigger resolve ambiguities encoded local positioning patterns x_2 bigram_vectorizer fit_transform corpus toarray x_2 array particular interogative form present last document chapter user guide scikit learn user guide release feature_index bigram_vectorizer vocabulary_ get uis x_2 feature_index array idf normalization large text corpus words present english hence carrying little meaningul information actual contents document feed direct count data directly classier frequent terms would shadow frequencies rarer yet interesting terms order weight count features oating point values suitable usage classier common use tfidf transform means term frequency tfidf means term frequency times inverse document frequency orginally term weighting scheme developed information retrieval ranking function search engines results also found good use document classication clustering normalization implemented text tfidftransformer class sklearn feature_extraction text import tfidftransformer transformer tfidftransformer transformer tfidftransformer norm smooth_idf true sublinear_tf false use_idf true please see reference documentation details parameters lets take example following counts rst term present time hence interesting two features less time hence probably representative content documents counts tfidf transformer fit_transform counts tfidf 6x3 sparse matrix type type numpy float64 stored elements compressed sparse row format tfidf toarray array row normalized unit euclidean norm weights feature computed fit method call stored model attribute transformer idf_ array tfidf often used text features also another class called tfidfvectorizer combines option countvectorizer tfidftransformer single model dataset transformations scikit learn user guide release sklearn feature_extraction text import tfidfvectorizer vectorizer tfidfvectorizer vectorizer fit_transform corpus 4x9 sparse matrix type type numpy float64 stored elements compressed sparse row format tfidf normalization often useful might cases binary occurrence markers might offer better features achieved using binary parameter countvectorizer particular estimators bernoulli naive bayes explicitly model discrete boolean random variables also short text likely noisy tfidf values binary occurrence info stable usual way best adjust feature extraction parameters use cross validated grid search instance pipelining feature extractor classier sample pipeline text feature extraction evaluation applications examples bag words representation quite simplistic surprisingly useful practice particular supervised setting successfully combined fast scalable linear models train document classicers instance classication text documents using sparse features unsupervised setting used group similar documents together applying clustering algorithms means clustering text documents using means finally possible discover main topics corpus relaxing hard assignement constraint clustering instance using non negative matrix factorization nmf nnmf topics extraction non negative matrix factorization limitations bag words representation local positioning information preserved extracting grams instead individual words bag words bag grams destroy inner structure document hence meaning carried internal structure order address wider task natural language understanding local structure sentences paragraphs thus taken account many models thus casted structured output problems currently outside scope scikit learn 
925: customizing vectorizer classes possible customize behavior passing callable parameters vectorizer def my_tokenizer return split vectorizer countvectorizer tokenizer my_tokenizer vectorizer build_analyzer punctuation usome upunctuation chapter user guide scikit learn user guide release particular name preprocessor callable takes string input return another string removing html tags converting lower case instance tokenizer callable takes string input output sequence feature occurrences 
926: tokens 
927: analyzer callable wraps calls preprocessor tokenizer perform ltering grams extractions tokens 
928: make preprocessor tokenizer analyzers aware model parameters possible derive class override build_preprocessor build_tokenizer build_analyzer factory method instead customizing vectorizer useful handle asian languages use explicit word separator whitespace instance 
929: image feature extraction patch extraction extract_patches_2d function extracts patches image stored two dimensional array three dimensional color information along third axis rebuilding image patches use reconstruct_from_patches_2d example let use generate 4x4 pixel picture color channels rgb format import numpy sklearn feature_extraction import image one_image arange reshape one_image array channel fake rgb picture random_state patches image extract_patches_2d one_image max_patches patches shape patches array patches image extract_patches_2d one_image patches shape patches array let try reconstruct original image patches averaging overlapping areas reconstructed image reconstruct_from_patches_2d patches testing assert_array_equal one_image reconstructed dataset transformations scikit learn user guide release patchextractor class works way extract_patches_2d supports multiple images input implemented estimator used pipelines see five_images arange reshape patches image patchextractor transform five_images patches shape connectivity graph image several estimators scikit learn use connectivity information features samples instance ward clustering hierarchical clustering cluster together neighboring pixels image thus forming contiguous patches purpose estimators use connectivity matrix giving samples connected function img_to_graph returns matrix image similarly grid_to_graph build connectivity matrix images given shape image matrices used impose connectivity estimators use connectivity information ward clustering hierarchical clustering also build precomputed kernels similarity matrices 
930: note examples demo structured ward hierarchical clustering lena image spectral clustering image segmentation feature agglomeration univariate selection kernel approximation submodule contains functions approximate feature mappings correspond certain kernels used example support vector machines see support vector machines following feature functions perform non linear transformations input serve basis linear classication algorithms advantage using approximate explicit feature maps compared kernel trick makes use feature maps implicitly explicit mappings better suited online learning signicantly reduce cost learning large datasets standard kernelized svms scale well large datasets using approximate kernel map possible use much efcient linear svms particularly combination kernel map approximations sgdclassifier make nonlinear learning large datasets possible 
931: chapter user guide scikit learn user guide release since much empirical work using approximate embeddings advisable compare results exact kernel methods possible 
932: radial basis function kernel rbfsampler constructs approximate mapping radial basis function kernel mapping relies monte carlo approximation kernel values fit function performs monte carlo sampling whereas transform method performs mapping data inherent randomness process results may vary different calls fit function fit function takes two arguments n_components target dimensionality feature transform gamma parameter rbf kernel higher n_components result better approximation kernel yield results similar produced kernel svm note tting feature function actually depend data given fit function dimensionality data used details method found rr2007 
933: figure comparing exact rbf kernel left approximation right examples explicit feature map approximation rbf kernels additive chi squared kernel chi squared kernel kernel histograms often used computer vision chi squared kernel given cid 2xiyi since kernel additive possible treat components separately embedding makes possible sample fourier transform regular intervals instead approximating using monte carlo sampling class additivechi2sampler implements component wise deterministic sampling component sampled times yielding dimensions per input dimension multiple two stems real complex part fourier transform literature usually choosen transforming dataset size n_samples n_features case approximate feature map provided additivechi2sampler combined approximate feature map provided rbfsampler yield approximate feature map exponentiated chi squared kernel see vz2010 details vvz2010 combination rbfsampler 
934: dataset transformations scikit learn user guide release skewed chi squared kernel skewed chi squared kernel given cid properties similar exponentiated chi squared kernel often used computer vision allows simple monte carlo approximation feature map usage skewedchi2sampler usage described rbfsampler difference free parameter called motivation mapping mathematical details see ls2010 
935: mathematical details kernel methods like support vector machines kernelized pca rely property reproducing kernel hilbert spaces positive denite kernel function called mercer kernel guaranteed exists mapping hilbert space denotes inner product hilbert space algorithm linear support vector machine pca relies scalar product data points one may use value corresponds applying algorithm mapped data points advantage using mapping never calculated explicitly allowing arbitrary large features even innite one drawback kernel methods might necessary store many kernel values optimiza tion kernelized classier applied new data needs computed make predictions possibly many different training set classes submodule allow approximate embedding thereby working explicitly representa tions obviates need apply kernel store training examples 
936: references dataset loading utilities sklearn datasets package embeds small toy datasets introduced getting started section evaluate impact scale dataset n_samples n_features controlling statistical properties data typically correlation informativeness features also possible generate synthetic data package also features helpers fetch larger datasets commonly used machine learning community benchmark algorithm data comes real world 
937: general dataset api three distinct kinds dataset interfaces different types datasets simplest one interface sample images described sample images section 
938: chapter user guide scikit learn user guide release dataset generation functions svmlight loader share simplistic interface returning tuple con sisting n_samples n_features numpy array array length n_samples containing targets toy datasets well real world datasets datasets fetched mldata org sophisti cated structure functions return bunch dictionary accessible dict key syntax datasets least two keys data containg array shape n_samples n_features except 20newsgroups target numpy array length n_features containing targets datasets also contain description descr contain feature_names target_names see dataset descriptions details 
939: toy datasets scikit learn comes small standard datasets require download external website 
940: load return boston house prices dataset regression load_boston load return iris dataset classication load_iris load_diabetes load return diabetes dataset regression load_digits n_class load return digits dataset classication load_linnerud load return linnerud dataset multivariate regression 
941: datasets useful quickly illustrate behavior various algorithms implemented scikit however often small representative real world machine learning tasks 
942: sample images scikit also embed couple sample jpeg images published creative commons license authors image useful test algorithms pipeline data 
943: load_sample_images load_sample_image image_name load numpy array single sample image load sample images image manipulation 
944: warning default coding images based uint8 dtype spare memory often machine learning algorithms work best input converted oating point representation rst also plan use pylab imshow dont forget scale range done following example 
945: examples color quantization using means dataset loading utilities scikit learn user guide release sample generators addition scikit learn includes various random sample generators used build artical datasets controled size complexity 
946: generate random class classication problem 
947: make_classification n_samples n_features make_multilabel_classification n_samples generate random multilabel classication problem make_regression n_samples n_features make_blobs n_samples n_features centers make_friedman1 n_samples n_features make_friedman2 n_samples noise random_state make_friedman3 n_samples noise random_state make_hastie_10_2 n_samples random_state make_low_rank_matrix n_samples make_sparse_coded_signal n_samples make_sparse_uncorrelated n_samples make_spd_matrix n_dim random_state make_swiss_roll n_samples noise random_state make_s_curve n_samples noise random_state make_sparse_spd_matrix dim alpha generate random regression problem generate isotropic gaussian blobs clustering generate friedman regression problem generate friedman regression problem generate friedman regression problem generates data binary classication used generate mostly low rank matrix bell shaped singular values generate signal sparse combination dictionary elements generate random regression problem sparse uncorrelated design generate random symmetric positive denite matrix generate swiss roll dataset generate curve dataset generate sparse symetric denite positive matrix 
948: datasets svmlight libsvm format scikit learn includes utility functions loading datasets svmlight libsvm format format line takes form label feature feature value feature feature value format especially suitable sparse datasets module scipy sparse csr matrices used numpy arrays used may load dataset like follows sklearn datasets import load_svmlight_file x_train y_train load_svmlight_file path train_dataset txt 
949: may also load two datasets x_train y_train x_test y_test load_svmlight_files 
950: path train_dataset txt path test_dataset txt chapter user guide scikit learn user guide release case x_train x_test guaranteed number features another way achieve result number features x_test y_test load_svmlight_file 
951: path test_dataset txt n_features x_train shape related links public datasets svmlight libsvm format http www csie ntu edu cjlin libsvmtools datasets faster api compatible implementation https github com mblondel svmlight loader olivetti faces dataset dataset contains set face images taken april april laboratories cambridge website describing original dataset defunct archived copies accessed internet archives wayback machine described original website ten different images distinct subjects subjects images taken different times varying lighting facial expressions open closed eyes smiling smiling facial details glasses glasses images taken dark homogeneous background subjects upright frontal position tolerance side movement 
952: image quantized grey levels stored unsigned bit integers loader convert oating point values interval easier work many algorithms target database integer indicating identity person pictured however examples per class relatively small dataset interesting unsupervised semi supervised perspective original dataset consisted version available consists 64x64 images using images please give credit laboratories cambridge 
953: newsgroups text dataset newsgroups dataset comprises around newsgroups posts topics splitted two subsets one training development one testing performance evaluation split train test set based upon messages posted specic date rst one sklearn datasets fetch_20newsgroups module contains returns sklearn feature_extraction text vectorizer custom parameters extract feature vectors second one sklearn datasets fetch_20newsgroups_vectorized returns ready use features necessary use feature extractor 
954: two loaders raw text extractors fed list text feature les usage sklearn datasets fetch_20newsgroups function data fetching caching functions downloads data archive original newsgroups website extracts archive contents scikit_learn_data 20news_home folder calls sklearn datasets load_file either training testing set folder dataset loading utilities scikit learn user guide release sklearn datasets import fetch_20newsgroups newsgroups_train fetch_20newsgroups subset train pprint import pprint pprint list newsgroups_train target_names alt atheism comp graphics comp windows misc comp sys ibm hardware comp sys mac hardware comp windows misc forsale rec autos rec motorcycles rec sport baseball rec sport hockey sci crypt sci electronics sci med sci space soc religion christian talk politics guns talk politics mideast talk politics misc talk religion misc real data lies filenames target attributes target attribute integer index category newsgroups_train filenames shape newsgroups_train target shape newsgroups_train target array possible load sub selection categories passing list categories load fetch_20newsgroups function cats alt atheism sci space newsgroups_train fetch_20newsgroups subset train categories cats list newsgroups_train target_names alt atheism sci space newsgroups_train filenames shape newsgroups_train target shape newsgroups_train target array order feed predictive clustering models text data one rst need turn text vec tors numerical values suitable statistical analysis achieved utilities sklearn feature_extraction text demonstrated following example extract idf vectors unigram tokens sklearn feature_extraction text import vectorizer documents open read newsgroups_train filenames vectorizer vectorizer vectors vectorizer fit_transform documents chapter user guide scikit learn user guide release vectors shape extracted idf vectors sparse average non zero components sample dimensional space less non zero features vectors nnz vectors shape sklearn datasets fetch_20newsgroups_vectorized function returns ready use tdf features instead names 
955: examples sample pipeline text feature extraction evaluation classication text documents using sparse features downloading datasets mldata org repository mldata org public repository machine learning data supported pascal network sklearn datasets package able directly download data sets repository using function fetch_mldata dataname example download mnist digit recognition database sklearn datasets import fetch_mldata mnist fetch_mldata mnist original data_home custom_data_home mnist database contains total examples handwritten digits size 28x28 pixels labeled mnist data shape mnist target shape unique mnist target array rst download dataset cached locally path specied data_home keyword argument defaults scikit_learn_data listdir path join custom_data_home mldata mnist original mat data sets mldata org adhere strict naming formatting convention fetch_mldata able make sense common cases allows tailor defaults individual datasets data arrays mldata org often shaped n_features n_samples posite scikit learn convention fetch_mldata transposes matrix default transpose_data keyword controls behavior iris fetch_mldata iris data_home custom_data_home iris data shape iris fetch_mldata iris transpose_data false 
956: data_home custom_data_home dataset loading utilities scikit learn user guide release iris data shape datasets multiple columns fetch_mldata tries identify target data columns rename target data done looking arrays named label data dataset failing choosing rst array target second data behavior changed target_name data_name keywords setting specic name index number name order columns datasets found mldata org tab data iris2 fetch_mldata datasets uci iris target_name data_name iris3 fetch_mldata datasets uci iris target_name class 
957: data_name double0 data_home custom_data_home data_home custom_data_home labeled faces wild face recognition dataset dataset collection jpeg pictures famous people collected internet details available ofcial website http vis www umass edu lfw picture centered single face typical task called face verication given pair two pictures binary classier must predict whether two images person alternative task face recognition face identication given picture face unknown person identify name person referring gallery previously seen pictures identied persons face verication face recognition tasks typically performed output model trained perform face detection popular model face detection called viola jones implemented opencv library lfw faces extracted face detector various online websites 
958: usage scikit learn provides two loaders automatically download cache parse metadata les decode jpeg convert interesting slices memmaped numpy arrays dataset size rst load typically takes couple minutes fully decode relevant part jpeg les numpy arrays dataset loaded following times loading times less 200ms using memmaped version memoized disk scikit_learn_data lfw_home folder using joblib rst loader used face identication task multi class classication task hence supervised learning sklearn datasets import fetch_lfw_people lfw_people fetch_lfw_people min_faces_per_person resize print name name lfw_people target_names ariel sharon colin powell donald rumsfeld george bush gerhard schroeder hugo chavez tony blair default slice rectangular shape around face removing background chapter user guide scikit learn user guide release lfw_people data dtype dtype float32 lfw_people data shape lfw_people images shape faces assigned single person target array lfw_people target shape list lfw_people target second loader typically used face verication task sample pair two picture belonging person sklearn datasets import fetch_lfw_pairs lfw_pairs_train fetch_lfw_pairs subset train list lfw_pairs_train target_names different persons person lfw_pairs_train pairs shape lfw_pairs_train data shape lfw_pairs_train target shape fetch_lfw_people fetch_lfw_pairs function possible get additional dimension rgb color channels passing color true case shape fetch_lfw_pairs datasets subdived subsets development train set development test set evaluation 10_folds set meant compute performance metrics using folds cross validation scheme 
959: references labeled faces wild database studying face recognition unconstrained environments gary huang manu ramesh tamara berg erik learned miller university massachusetts amherst technical report october 
960: examples faces recognition example using eigenfaces svms dataset loading utilities scikit learn user guide release reference class function reference scikit learn please refer full user guide details class function raw specications may enough give full guidelines uses 
961: list modules sklearn cluster clustering classes functions sklearn covariance covariance estimators sklearn cross_validation cross validation sklearn datasets datasets sklearn decomposition matrix decomposition sklearn ensemble ensemble methods sklearn feature_extraction feature extraction loaders samples generator images text sklearn feature_selection feature selection sklearn gaussian_process gaussian processes sklearn grid_search grid search sklearn hmm hidden markov models sklearn kernel_approximation kernel approximation sklearn semi_supervised semi supervised learning sklearn lda linear discriminant analysis sklearn linear_model generalized linear models sklearn manifold manifold learning sklearn metrics metrics dense data sparse data classication metrics regression metrics clustering metrics pairwise metrics sklearn mixture gaussian mixture models sklearn multiclass multiclass multilabel classication multiclass multilabel classication strategies sklearn naive_bayes naive bayes sklearn neighbors nearest neighbors sklearn pls partial least squares sklearn pipeline pipeline sklearn preprocessing preprocessing normalization sklearn qda quadratic discriminant analysis sklearn svm support vector machines estimators low level methods sklearn tree decision trees sklearn utils utilities chapter user guide scikit learn user guide release sklearn cluster clustering sklearn cluster module gathers popular unsupervised clustering algorithms user guide see clustering section details 
962: classes cluster affinitypropagation damping cluster dbscan eps min_samples metric cluster kmeans init n_init max_iter cluster minibatchkmeans init max_iter mini batch means clustering cluster meanshift bandwidth seeds cluster spectralclustering mode cluster ward n_clusters memory meanshift clustering apply means projection normalized laplacian ward hierarchical clustering constructs tree cuts 
963: perform afnity propagation clustering data perform dbscan clustering vector array distance matrix means clustering sklearn cluster afnitypropagation class sklearn cluster affinitypropagation damping max_iter convit copy true perform afnity propagation clustering data parameters damping oat optional damping factor max_iter int optional maximum number iterations convit int optional number iterations change number estimated clusters stops convergence 
964: copy boolean optional make copy input data true default 
965: notes see examples plot_afnity_propagation example algorithmic complexity afnity propagation quadratic number points 
966: references brendan frey delbert dueck clustering passing messages data points science feb attributes cluster_centers_indices_ labels_ array n_clusters array n_samples indices cluster centers labels point reference scikit learn user guide release methods fit get_params deep set_params params compute afnity propagation clustering get parameters estimator set parameters estimator 
967: __init__ damping max_iter convit copy true fit none compute afnity propagation clustering 
968: parameters array n_points n_points matrix similarities points array n_points oat optional preferences point points larger values preferences likely chosen exemplars number exemplars clusters inuenced input preferences value preferences passed arguments set median input similarities 
969: damping oat optional damping factor copy boolean optional copy false afnity matrix modied inplace algorithm memory efciency get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn cluster dbscan class sklearn cluster dbscan eps min_samples metric euclidean random_state none perform dbscan clustering vector array distance matrix dbscan density based spatial clustering applications noise finds core samples high density expands clusters good data contains clusters similar density 
970: parameters eps oat optional maximum distance two samples considered neighborhood 
971: chapter user guide scikit learn user guide release min_samples int optional number samples neighborhood point considered core point 
972: metric string callable metric use calculating distance instances feature array metric string callable must one options allowed met rics pairwise calculate_distance metric parameter metric precomputed assumed distance matrix must square 
973: random_state numpy randomstate optional generator used initialize centers defaults numpy random 
974: notes see examples plot_dbscan example 
975: references ester kriegel sander density based algorithm discovering clusters large spatial databases noise proceedings 2nd international conference knowledge discovery data mining portland aaai press attributes array shape n_core_samples array shape n_core_samples n_features array shape n_samples indices core samples 
976: copy core sample found training 
977: cluster labels point dataset given noisy samples given label 
978: core_sample_indices_ components_ labels_ methods fit params get_params deep set_params params perform dbscan clustering vector array distance matrix get parameters estimator set parameters estimator 
979: __init__ eps min_samples metric euclidean random_state none fit params perform dbscan clustering vector array distance matrix 
980: parameters array n_samples n_samples n_samples n_features array distances samples feature array array treated feature array unless metric given precomputed 
981: params dict overwrite keywords __init__ 
982: reference scikit learn user guide release get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn cluster kmeans class sklearn cluster kmeans init means n_init max_iter verbose tol random_state none means clustering precompute_distances true copy_x true n_jobs parameters int optional default number clusters form well number centroids generate 
983: max_iter int maximum number iterations means algorithm single run 
984: n_init int optional default number time means algorithm run different centroid seeds nal results best output n_init consecutive runs terms inertia 
985: init means random ndarray method initialization defaults means means selects initial cluster centers mean clustering smart way speed convergence see section notes k_init details random choose observations rows random data initial centroids init array used seed centroids precompute_distances boolean precompute distances faster takes memory 
986: tol oat optional default relative tolerance inertia declare convergence n_jobs int number jobs use computation works breaking pairwise matrix n_jobs even slices computing parallel cpus used given parallel computing code used useful debuging n_jobs n_cpus n_jobs used thus n_jobs cpus one used 
987: chapter user guide scikit learn user guide release random_state integer numpy randomstate optional generator used initialize centers defaults global numpy random number generator 
988: integer given xes seed 
989: see also minibatchkmeansalternative online implementation incremental updates centers positions using mini batches large scale learning say n_samples 10k minibatchkmeans probably much faster default batch implementation 
990: notes means problem solved using lloyds algorithm average complexity given number samples number iteration worst case complexity given n_samples n_features arthur vassilvitskii slow means method socg2006 practice means algorithm fast one fastest clustering algorithms available falls local minima thats useful restart several times 
991: attributes cluster_centers_ array n_clusters n_features labels_ inertia_ oat methods coordinates cluster centers labels point value inertia criterion associated chosen partition 
992: fit fit_predict get_params deep predict score set_params params transform compute means compute cluster centers predict cluster index sample get parameters estimator predict closest cluster sample belongs opposite value means objective set parameters estimator transform data cluster distance space __init__ init means n_init pute_distances true verbose random_state none copy_x true n_jobs max_iter tol precom fit none compute means fit_predict compute cluster centers predict cluster index sample convenience method equivalent calling followed predict 
993: get_params deep true get parameters estimator reference scikit learn user guide release parameters deep boolean optional true return parameters estimator contained subobjects estimators 
994: predict predict closest cluster sample belongs vector quantization literature cluster_centers_ called code book value returned predict index closest code code book 
995: parameters array like sparse matrix shape n_samples n_features new data predict 
996: returns array shape n_samples index closest center sample belongs 
997: score opposite value means objective 
998: parameters array like sparse matrix shape n_samples n_features new data 
999: returns score oat opposite value means objective 
1000: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform none transform data cluster distance space new space dimension distance cluster centers note even sparse array returned transform typically dense 
1001: parameters array like sparse matrix shape n_samples n_features new data transform 
1002: returns x_new array shape n_samples transformed new space 
1003: sklearn cluster minibatchkmeans class sklearn cluster minibatchkmeans init means max_iter batch_size verbose compute_labels true random_state none tol max_no_improvement init_size none n_init chunk_size none mini batch means clustering parameters int optional default number clusters form well number centroids generate 
1004: chapter user guide scikit learn user guide release max_iter int optional maximum number iterations complete dataset stopping independently early stopping criterion heuristics 
1005: max_no_improvement int optional control early stopping based consecutive number mini batches yield improvement smoothed inertia disable convergence detection based inertia set max_no_improvement none 
1006: tol oat optional control early stopping based relative center changes measured smoothed variance normalized mean center squared position changes early stopping heuristics closer one used batch variant algorithms induces slight computational memory overhead inertia heuristic disable convergence detection based normalized center change set tol default 
1007: batch_size int optional default size mini batches 
1008: init_size int optional default batch_size number samples randomly sample speeding initialization sometimes expense accurracy algorithm initialized running batch kmeans random subset data needs larger 
1009: init means random ndarray method initialization defaults means means selects initial cluster centers mean clustering smart way speed convergence see section notes k_init details random choose observations rows random data initial centroids init array used seed centroids compute_labels boolean compute label assignements inertia complete dataset minibatch optimization converged 
1010: random_state integer numpy randomstate optional generator used initialize centers defaults global numpy random number generator 
1011: integer given xes seed 
1012: notes see http www eecs tufts edu dsculley papers fastkmeans pdf reference scikit learn user guide release attributes cluster_centers_ array n_clusters n_features labels_ inertia_ oat methods coordinates cluster centers labels point compute_labels set true value inertia criterion associated chosen partition compute_labels set true inertia dened sum square distances samples nearest neighbor 
1013: fit fit_predict get_params deep partial_fit predict score set_params params transform compute centroids chunking mini batches compute cluster centers predict cluster index sample get parameters estimator update means estimate single mini batch predict closest cluster sample belongs opposite value means objective set parameters estimator transform data cluster distance space __init__ init means max_iter batch_size verbose compute_labels true n_init tol max_no_improvement random_state none chunk_size none init_size none fit none compute centroids chunking mini batches 
1014: parameters array like shape n_samples n_features coordinates data points cluster fit_predict compute cluster centers predict cluster index sample convenience method equivalent calling followed predict 
1015: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1016: partial_fit none update means estimate single mini batch 
1017: parameters array like shape n_samples n_features coordinates data points cluster 
1018: predict predict closest cluster sample belongs vector quantization literature cluster_centers_ called code book value returned predict index closest code code book 
1019: chapter user guide scikit learn user guide release parameters array like sparse matrix shape n_samples n_features new data predict 
1020: returns array shape n_samples index closest center sample belongs 
1021: score opposite value means objective 
1022: parameters array like sparse matrix shape n_samples n_features new data 
1023: returns score oat opposite value means objective 
1024: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform none transform data cluster distance space new space dimension distance cluster centers note even sparse array returned transform typically dense 
1025: parameters array like sparse matrix shape n_samples n_features new data transform 
1026: returns x_new array shape n_samples transformed new space 
1027: sklearn cluster meanshift class sklearn cluster meanshift bandwidth none seeds none bin_seeding false clus meanshift clustering ter_all true parameters bandwidth oat optional bandwith used rbf kernel set bandwidth estimated see cluster ing estimate_bandwidth seeds array n_samples n_features optional set seeds used initialize kernels seeds calculated cluster ing get_bin_seeds bandwidth grid size default values parame ters 
1028: cluster_all boolean default true true points clustered even orphans within kernel orphans assigned nearest kernel false orphans given cluster label 
1029: reference scikit learn user guide release notes scalability implementation uses kernel ball tree look members kernel complexity log lower dimensions number samples number points higher dimensions complexity tend towards scalability boosted using fewer seeds examply using higher value min_bin_freq get_bin_seeds function note estimate_bandwidth function much less scalable mean shift algorithm bottleneck used 
1030: references dorin comaniciu peter meer mean shift robust approach toward feature space analysis ieee trans actions pattern analysis machine intelligence 
1031: attributes cluster_centers_ labels_ methods array n_clusters n_features coordinates cluster centers labels point fit get_params deep set_params params compute meanshift get parameters estimator set parameters estimator 
1032: __init__ bandwidth none seeds none bin_seeding false cluster_all true fit compute meanshift parameters array n_samples n_features input points get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self chapter user guide scikit learn user guide release sklearn cluster spectralclustering class sklearn cluster spectralclustering mode none random_state none n_init apply means projection normalized laplacian practice spectral clustering useful structure individual clusters highly non convex generally measure center spread cluster suitable description complete cluster instance clusters nested circles plan afnity adjacency matrix graph method used normalized graph cuts 
1033: parameters integer optional dimension projection subspace 
1034: mode none arpack amg eigenvalue decomposition strategy use amg requires pyamg installed faster large sparse problems may also lead instabilities random_state int seed randomstate instance none default pseudo random number generator used initialization lobpcg eigen vec tors decomposition mode amg means initialization 
1035: n_init int optional default number time means algorithm run different centroid seeds nal results best output n_init consecutive runs terms inertia 
1036: references cuts image normalized http citeseer ist psu edu viewdoc summary doi http citeseerx ist psu edu viewdoc summary doi segmentation clustering spectral tutorial jianbo shi jitendra malik ulrike von luxburg attributes labels_ labels point methods fit get_params deep set_params params compute spectral clustering afnity matrix get parameters estimator set parameters estimator 
1037: __init__ mode none random_state none n_init fit compute spectral clustering afnity matrix parameters array like sparse matrix shape n_samples n_samples afnity matrix describing pairwise similarity data also reference scikit learn user guide release jacency matrix graph embed must symmetric entries must positive zero zero means elements nothing common whereas high values mean elements strongly similar 
1038: notes afnity matrix distance matrix means identical elements high values means dissimilar elements transformed similarity matrix well suited algorithm applying gaussian heat kernel exp delta another alternative take symmetric version nearest neighbors connectivity matrix points pyamg package installed used greatly speeds computation 
1039: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn cluster ward class sklearn cluster ward n_clusters memory memory cachedir none connectivity none ward hierarchical clustering constructs tree cuts 
1040: copy true n_components none parameters n_clusters int ndarray number clusters 
1041: connectivity sparse matrix 
1042: connectivity matrix denes sample neigbhoring samples following given structure data default none hiearchical clustering algorithm unstructured 
1043: memory instance joblib memory string used cache output computation tree default caching done string given path caching directory 
1044: copy bool copy connectivity matrix work inplace 
1045: n_components int optional chapter user guide scikit learn user guide release number connected components graph dened connectivity matrix set estimated 
1046: attributes children_ labels_ n_leaves_ array like shape n_nodes array n_points int list children nodes leaves tree appear cluster labels point number leaves hiearchical tree 
1047: methods fit get_params deep set_params params fit hierarchical clustering data get parameters estimator set parameters estimator 
1048: __init__ n_clusters n_components none memory memory cachedir none connectivity none copy true fit fit hierarchical clustering data parameters array like shape n_samples n_features samples observations 
1049: returns self get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self functions cluster estimate_bandwidth quantile cluster k_means init cluster ward_tree connectivity cluster affinity_propagation convit cluster dbscan eps min_samples cluster mean_shift bandwidth seeds cluster spectral_clustering afnity estimate bandwith use meanshift algorithm means clustering algorithm ward clustering based feature matrix perform afnity propagation clustering data perform dbscan clustering vector array distance matrix perform meanshift clustering data using kernel apply means projection normalized laplacian reference scikit learn user guide release sklearn cluster estimate_bandwidth sklearn cluster estimate_bandwidth quantile n_samples none random_state estimate bandwith use meanshift algorithm parameters array n_samples n_features input points quantile oat default means median pairwise distances used n_samples int number samples use none samples used 
1050: random_state int randomstate pseudo number generator state used random sampling 
1051: returns bandwidth oat bandwidth parameter sklearn cluster k_means sklearn cluster k_means init means precompute_distances true tol n_init random_state none means clustering algorithm 
1052: max_iter copy_x true n_jobs verbose false parameters array like oats shape n_samples n_features observations cluster 
1053: int number clusters form well number centroids generate 
1054: max_iter int optional default maximum number iterations means algorithm run 
1055: n_init int optional default number time means algorithm run different centroid seeds nal results best output n_init consecutive runs terms inertia 
1056: init means random ndarray callable optional method initialization default means means selects initial cluster centers mean clustering smart way speed convergence see section notes k_init details random generate centroids gaussian mean variance estimated data ndarray passed shape gives initial centers callable passed take arguments random state return initialization tol oat optional chapter user guide scikit learn user guide release relative increment results declaring convergence 
1057: verbose boolean optional verbosity mode random_state integer numpy randomstate optional generator used initialize centers defaults global numpy random number generator 
1058: integer given xes seed 
1059: copy_x boolean optional pre computing distances numerically accurate center data rst copy_x true original data modied false original data modied put back function returns small numerical differences may introduced subtracting adding data mean 
1060: n_jobs int number jobs use computation works breaking pairwise matrix n_jobs even slices computing parallel cpus used given parallel computing code used useful debuging n_jobs n_cpus n_jobs used thus n_jobs cpus one used 
1061: returns centroid oat ndarray shape n_features centroids found last iteration means label integer ndarray shape n_samples label code index centroid ith observation closest 
1062: inertia oat nal value inertia criterion sum squared distances closest centroid observations training set 
1063: sklearn cluster ward_tree sklearn cluster ward_tree connectivity none n_components none copy true ward clustering based feature matrix inertia matrix uses heapq based representation structured version takes account topological structure samples 
1064: parameters array shape n_samples n_features feature matrix representing n_samples samples clustered connectivity sparse matrix 
1065: connectivity matrix denes sample neigbhoring samples following given structure data matrix assumed symmetric upper trian gular half used default none ward algorithm unstructured 
1066: n_components int optional number connected components none number connected components estimated connectivity matrix 
1067: copy bool optional reference scikit learn user guide release make copy connectivity work inplace connectivity lil type copy case 
1068: returns children list pairs lenght n_nodes list children nodes leaves tree empty list children 
1069: n_components sparse matrix 
1070: number connected components graph 
1071: n_leaves int number leaves tree sklearn cluster afnity_propagation sklearn cluster affinity_propagation none convit max_iter damping perform afnity propagation clustering data copy true verbose false parameters array n_points n_points matrix similarities points array n_points oat optional preferences point points larger values preferences likely chosen exemplars number exemplars clusters inuenced input preferences value preferences passed arguments set median input similarities resulting moderate number clusters smaller amount clusters set minimum value similarities 
1072: damping oat optional damping factor copy boolean optional copy false afnity matrix modied inplace algorithm memory efciency verbose boolean optional verbosity level returns cluster_centers_indices array n_clusters index clusters centers labels array n_points cluster labels point notes see examples plot_afnity_propagation example 
1073: references brendan frey delbert dueck clustering passing messages data points science feb chapter user guide scikit learn user guide release sklearn cluster dbscan sklearn cluster dbscan eps min_samples metric euclidean random_state none perform dbscan clustering vector array distance matrix 
1074: parameters array n_samples n_samples n_samples n_features array distances samples feature array array treated feature array unless metric given precomputed 
1075: eps oat optional maximum distance two samples considered neighborhood 
1076: min_samples int optional number samples neighborhood point considered core point 
1077: metric string callable metric use calculating distance instances feature array metric string callable must one options allowed met rics pairwise calculate_distance metric parameter metric precomputed assumed distance matrix must square 
1078: random_state numpy randomstate optional generator used initialize centers defaults numpy random 
1079: returns core_samples array n_core_samples indices core samples 
1080: labels array n_samples cluster labels point noisy samples given label 
1081: notes see examples plot_dbscan example 
1082: references ester kriegel sander density based algorithm discovering clusters large spatial databases noise proceedings 2nd international conference knowledge discovery data mining portland aaai press sklearn cluster mean_shift sklearn cluster mean_shift bandwidth none seeds none bin_seeding false clus perform meanshift clustering data using kernel seed using binning technique scalability 
1083: ter_all true max_iterations parameters array n_samples n_features input points reference scikit learn user guide release bandwidth oat optional kernel bandwidth bandwidth dened set using heuristic given median pairwise distances seeds array n_seeds n_features point used initial kernel locations bin_seeding boolean true initial kernel locations locations points rather location discretized version points points binned onto grid whose coarseness corresponds bandwidth setting option true speed algorithm fewer seeds initialized default value false ignored seeds argument none min_bin_freq int optional speed algorithm accept bins least min_bin_freq points seeds dened set 
1084: returns cluster_centers array n_clusters n_features coordinates cluster centers labels array n_samples cluster labels point notes see examples plot_meanshift example 
1085: sklearn cluster spectral_clustering sklearn cluster spectral_clustering afnity n_components none mode none ran dom_state none n_init apply means projection normalized laplacian practice spectral clustering useful structure individual clusters highly non convex generally measure center spread cluster suitable description complete cluster instance clusters nested circles plan afnity adjacency matrix graph method used normalized graph cuts 
1086: parameters afnity array like sparse matrix shape n_samples n_samples afnity matrix describing relationship samples embed must metric possible examples adjacency matrix graph heat kernel pairwise distance matrix samples symmetic nearest neighbours connectivity matrix samples 
1087: integer optional number clusters extract 
1088: chapter user guide scikit learn user guide release n_components integer optional default number eigen vectors use spectral embedding mode none arpack amg eigenvalue decomposition strategy use amg requires pyamg installed faster large sparse problems may also lead instabilities random_state int seed randomstate instance none default pseudo random number generator used initialization lobpcg eigen vec tors decomposition mode amg means initialization 
1089: n_init int optional default number time means algorithm run different centroid seeds nal results best output n_init consecutive runs terms inertia 
1090: returns labels array integers shape n_samples labels clusters 
1091: centers array integers shape indices cluster centers notes graph contain one connect component elsewhere results make little sense algorithm solves normalized cut normalized spectral clustering 
1092: references cuts image normalized http citeseer ist psu edu viewdoc summary doi http citeseerx ist psu edu viewdoc summary doi segmentation clustering spectral tutorial jianbo shi jitendra malik ulrike von luxburg sklearn covariance covariance estimators sklearn covariance module includes methods algorithms robustly estimate covariance fea tures given set points precision matrix dened inverse covariance also estimated covariance estimation closely related theory gaussian graphical models user guide see covariance estimation section details 
1093: covariance empiricalcovariance covariance ellipticenvelope covariance graphlasso alpha mode tol covariance graphlassocv alphas covariance ledoitwolf store_precision covariance mincovdet store_precision covariance oas store_precision covariance shrunkcovariance maximum likelihood covariance estimator object detecting outliers gaussian distributed dataset sparse inverse covariance estimation penalized estimator sparse inverse covariance cross validated choice penality ledoitwolf estimator minimum covariance determinant mcd robust estimator covariance oracle approximating shrinkage estimator covariance estimator shrinkage reference scikit learn user guide release sklearn covariance empiricalcovariance class sklearn covariance empiricalcovariance store_precision true sume_centered false maximum likelihood covariance estimator parameters store_precision bool species estimated precision stored attributes covari ance_ preci sion_ ndarray shape n_features n_features ndarray shape n_features n_features estimated covariance matrix estimated pseudo inverse matrix stored store_precision true methods error_norm comp_cov norm scaling squared computes mean squared error two covariance estimators fit get_params deep mahalanobis observations score x_test assume_centered set_params params fits maximum likelihood estimator covariance model get parameters estimator computes mahalanobis distances given observations computes log likelihood gaussian data set self covariance_ estimator covariance matrix set parameters estimator 
1094: __init__ store_precision true assume_centered false parameters store_precision bool specify estimated precision stored assume_centered boolean true data centered computation useful working data whose mean almost exactly zero false data centered computa tion 
1095: error_norm comp_cov norm frobenius scaling true squared true computes mean squared error two covariance estimators sense frobenius norm parameters comp_cov array like shape n_features n_features covariance compare 
1096: norm str type norm used compute error available error types frobenius fault sqrt spectral sqrt max eigenvalues error comp_cov self covariance_ 
1097: scaling bool true default squared error norm divided n_features false squared error norm rescaled 
1098: chapter user guide scikit learn user guide release squared bool whether compute squared error norm error norm true default squared error norm returned false error norm returned 
1099: returns mean squared error sense frobenius norm self comp_cov covariance estimators fit fits maximum likelihood estimator covariance model according given training data param eters 
1100: parameters array like shape n_samples n_features training data n_samples number samples n_features number features 
1101: returns self object returns self get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1102: mahalanobis observations computes mahalanobis distances given observations provided observations assumed centered one may want center using location estimate rst 
1103: parameters observations array like shape n_observations n_features observations mahalanobis distances compute 
1104: returns mahalanobis_distance array shape n_observations mahalanobis distances observations 
1105: score x_test assume_centered false computes log likelihood gaussian data set self covariance_ estimator covariance matrix 
1106: parameters x_test array like shape n_samples n_features test data compute likelihood n_samples number sam ples n_features number features 
1107: returns res oat likelihood data set self covariance_ estimator covariance matrix 
1108: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self reference scikit learn user guide release sklearn covariance ellipticenvelope class sklearn covariance ellipticenvelope store_precision true assume_centered false object detecting outliers gaussian distributed dataset 
1109: support_fraction none contamination parameters store_precision bool specify estimated precision stored assume_centered boolean true support robust location covariance estimates computed covariance estimate recomputed without centering data useful work data whose mean signicantly equal zero exactly zero false robust location covariance directly computed fastmcd algorithm without additional treatment 
1110: support_fraction oat support_fraction proportion points included support raw mcd estimate default none implies minimum value support_fraction used within algorithm n_sample n_features contamination oat contamination amount contamination data set proportion outliers data set 
1111: see also empiricalcovariance mincovdet notes outlier detection covariance estimation may break perform well high dimensional settings particular one always take care work n_samples n_features 
1112: references attributes contamination oat contamination location_ array like shape n_features covariance_ array like shape n_features n_features precision_ array like shape n_features n_features support_ array like shape n_samples amount contamination data set proportion outliers data set estimated robust location estimated robust covariance matrix estimated pseudo inverse matrix stored store_precision true mask observations used compute robust estimates location shape 
1113: methods chapter user guide scikit learn user guide release apply correction raw minimum covariance determinant estimates compute decision function given observations 
1114: correct_covariance data decision_function raw_mahalanobis error_norm comp_cov norm scaling squared computes mean squared error two covariance estimators fit get_params deep mahalanobis observations predict reweight_covariance data score set_params params get parameters estimator computes mahalanobis distances given observations outlyingness observations according tted model reweight raw minimum covariance determinant estimates returns mean accuracy given test data labels set parameters estimator 
1115: __init__ store_precision true tion correct_covariance data assume_centered false support_fraction none contamina apply correction raw minimum covariance determinant estimates correction using empirical correction factor suggested rousseeuw van driessen rouseeuw1984 
1116: parameters data array like shape n_samples n_features data matrix features samples data set must one used compute raw estimates 
1117: returns covariance_corrected array like shape n_features n_features corrected robust covariance estimate 
1118: decision_function raw_mahalanobis false compute decision function given observations 
1119: parameters array like shape n_samples n_features raw_mahalanobis bool whether consider raw mahalanobis distances decision function must false default compatibility others outlier detection tools 
1120: returns decision array like shape n_samples values decision function observations equal mahalanobis distances raw_mahalanobis true default raw_mahalanobis true equal cubic root shifted mahalanobis distances case threshold outlier ensures compatibility outlier detection tools one class svm 
1121: error_norm comp_cov norm frobenius scaling true squared true computes mean squared error two covariance estimators sense frobenius norm parameters comp_cov array like shape n_features n_features covariance compare 
1122: norm str type norm used compute error available error types frobenius fault sqrt spectral sqrt max eigenvalues error comp_cov self covariance_ 
1123: reference scikit learn user guide release scaling bool true default squared error norm divided n_features false squared error norm rescaled 
1124: squared bool whether compute squared error norm error norm true default squared error norm returned false error norm returned 
1125: returns mean squared error sense frobenius norm self comp_cov covariance estimators fit get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1126: mahalanobis observations computes mahalanobis distances given observations provided observations assumed centered one may want center using location estimate rst 
1127: parameters observations array like shape n_observations n_features observations mahalanobis distances compute 
1128: returns mahalanobis_distance array shape n_observations mahalanobis distances observations 
1129: predict outlyingness observations according tted model 
1130: parameters array like shape n_samples n_features returns is_outliers array shape n_samples dtype bool observations tells whether considered outlier accord ing tted model 
1131: threshold oat values less outlying points decision function 
1132: reweight_covariance data reweight raw minimum covariance determinant estimates reweight observations using rousseeuws method equivalent deleting outlying observations data set computing location covariance estimates rouseeuw1984 parameters data array like shape n_samples n_features data matrix features samples data set must one used compute raw estimates 
1133: returns location_reweighted array like shape n_features reweighted robust location estimate 
1134: covariance_reweighted array like shape n_features n_features chapter user guide scikit learn user guide release reweighted robust covariance estimate 
1135: support_reweighted array like type boolean shape n_samples mask observations used compute reweighted robust loca tion covariance estimates 
1136: score returns mean accuracy given test data labels 
1137: parameters array like shape n_samples n_features training set 
1138: array like shape n_samples labels 
1139: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn covariance graphlasso class sklearn covariance graphlasso alpha mode tol max_iter ver sparse inverse covariance estimation penalized estimator 
1140: bose false parameters alpha positive oat optional regularization parameter higher alpha regularization sparser inverse covariance cov_init array n_features n_features optional initial guess covariance mode lars lasso solver use coordinate descent lars use lars sparse derlying graphs elsewhere prefer numerically stable 
1141: tol positive oat optional tolerance declare convergence dual gap goes value iterations stopped max_iter integer optional maximum number iterations verbose boolean optional verbose true objective function dual gap plotted iteration see also graph_lasso graphlassocv reference scikit learn user guide release attributes covariance_ precision_ array like shape n_features n_features array like shape n_features n_features estimated covariance matrix estimated pseudo inverse matrix 
1142: methods error_norm comp_cov norm scaling squared computes mean squared error two covariance estimators fit get_params deep mahalanobis observations score x_test assume_centered set_params params get parameters estimator computes mahalanobis distances given observations computes log likelihood gaussian data set self covariance_ estimator covariance matrix set parameters estimator 
1143: __init__ alpha mode tol max_iter verbose false error_norm comp_cov norm frobenius scaling true squared true computes mean squared error two covariance estimators sense frobenius norm parameters comp_cov array like shape n_features n_features covariance compare 
1144: norm str type norm used compute error available error types frobenius fault sqrt spectral sqrt max eigenvalues error comp_cov self covariance_ 
1145: scaling bool true default squared error norm divided n_features false squared error norm rescaled 
1146: squared bool whether compute squared error norm error norm true default squared error norm returned false error norm returned 
1147: returns mean squared error sense frobenius norm self comp_cov covariance estimators get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1148: mahalanobis observations computes mahalanobis distances given observations provided observations assumed centered one may want center using location estimate rst 
1149: parameters observations array like shape n_observations n_features chapter user guide scikit learn user guide release observations mahalanobis distances compute 
1150: returns mahalanobis_distance array shape n_observations mahalanobis distances observations 
1151: score x_test assume_centered false computes log likelihood gaussian data set self covariance_ estimator covariance matrix 
1152: parameters x_test array like shape n_samples n_features test data compute likelihood n_samples number sam ples n_features number features 
1153: returns res oat likelihood data set self covariance_ estimator covariance matrix 
1154: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn covariance graphlassocv class sklearn covariance graphlassocv alphas n_renements sparse inverse covariance cross validated choice penality max_iter mode n_jobs verbose false none tol parameters alphas integer list positive oat optional integer given xes number points grids alpha used list given gives grid used see notes class docstring details 
1155: n_renements strictly positive integer number time grid rened used explicit values alphas passed 
1156: crossvalidation generator optional see sklearn cross_validation module none passed default fold strategy tol positive oat optional tolerance declare convergence dual gap goes value iterations stopped max_iter integer optional maximum number iterations mode lars lasso solver use coordinate descent lars use lars sparse derlying graphs elsewhere prefer numerically stable 
1157: n_jobs int optional reference scikit learn user guide release number jobs run parallel default verbose boolean optional verbose true objective function dual gap print iteration see also graph_lasso graphlasso notes search optimal alpha done iteratively rened grid rst cross validated scores grid computed new rened grid center around maximum one challenges face solvers fail converge well conditioned estimate corresponding values alpha come missing values optimum may close missing values 
1158: attributes covariance_ precision_ alpha_ oat cv_alphas_ list oat cv_scores array n_alphas n_folds methods array like shape n_features n_features array like shape n_features n_features estimated covariance matrix estimated precision matrix inverse covariance penalization parameter selected penalization parameters explored log likelihood score left data across folds 
1159: error_norm comp_cov norm scaling squared computes mean squared error two covariance estimators fit get_params deep mahalanobis observations score x_test assume_centered set_params params get parameters estimator computes mahalanobis distances given observations computes log likelihood gaussian data set self covariance_ estimator covariance matrix set parameters estimator 
1160: __init__ alphas n_renements none tol max_iter mode n_jobs verbose false error_norm comp_cov norm frobenius scaling true squared true computes mean squared error two covariance estimators sense frobenius norm parameters comp_cov array like shape n_features n_features covariance compare 
1161: norm str type norm used compute error available error types frobenius fault sqrt spectral sqrt max eigenvalues error comp_cov self covariance_ 
1162: chapter user guide scikit learn user guide release scaling bool true default squared error norm divided n_features false squared error norm rescaled 
1163: squared bool whether compute squared error norm error norm true default squared error norm returned false error norm returned 
1164: returns mean squared error sense frobenius norm self comp_cov covariance estimators get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1165: mahalanobis observations computes mahalanobis distances given observations provided observations assumed centered one may want center using location estimate rst 
1166: parameters observations array like shape n_observations n_features observations mahalanobis distances compute 
1167: returns mahalanobis_distance array shape n_observations mahalanobis distances observations 
1168: score x_test assume_centered false computes log likelihood gaussian data set self covariance_ estimator covariance matrix 
1169: parameters x_test array like shape n_samples n_features test data compute likelihood n_samples number sam ples n_features number features 
1170: returns res oat likelihood data set self covariance_ estimator covariance matrix 
1171: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn covariance ledoitwolf class sklearn covariance ledoitwolf store_precision true assume_centered false ledoitwolf estimator reference scikit learn user guide release ledoit wolf particular form shrinkage shrinkage coefcient computed using ledoit wolfs formula described well conditioned estimator large dimensional covariance matrices ledoit wolf journal multivariate analysis volume issue february pages 
1172: parameters store_precision bool specify estimated precision stored notes regularised covariance shrinkage cov shrinkage identity n_features trace cov n_features shinkage given ledoit wolf formula see references references well conditioned estimator large dimensional covariance matrices ledoit wolf journal mul tivariate analysis volume issue february pages 
1173: attributes covariance_ precision_ shrinkage_ oat shrinkage methods array like shape n_features n_features array like shape n_features n_features estimated covariance matrix estimated pseudo inverse matrix stored store_precision true coefcient convex combination used computation shrunk estimate 
1174: error_norm comp_cov norm scaling squared computes mean squared error two covariance estimators fit assume_centered get_params deep mahalanobis observations score x_test assume_centered set_params params fits ledoit wolf shrunk covariance model get parameters estimator computes mahalanobis distances given observations computes log likelihood gaussian data set self covariance_ estimator covariance matrix set parameters estimator 
1175: __init__ store_precision true assume_centered false parameters store_precision bool specify estimated precision stored assume_centered boolean true data centered computation useful working data whose mean almost exactly zero false data centered computa tion 
1176: error_norm comp_cov norm frobenius scaling true squared true chapter user guide scikit learn user guide release computes mean squared error two covariance estimators sense frobenius norm parameters comp_cov array like shape n_features n_features covariance compare 
1177: norm str type norm used compute error available error types frobenius fault sqrt spectral sqrt max eigenvalues error comp_cov self covariance_ 
1178: scaling bool true default squared error norm divided n_features false squared error norm rescaled 
1179: squared bool whether compute squared error norm error norm true default squared error norm returned false error norm returned 
1180: returns mean squared error sense frobenius norm self comp_cov covariance estimators fit assume_centered false fits ledoit wolf shrunk covariance model according given training data parameters 
1181: parameters array like shape n_samples n_features training data n_samples number samples n_features number features 
1182: assume_centered boolean true data centered computation usefull work data whose mean signicantly equal zero exactly zero false data centered computation 
1183: returns self object returns self get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1184: mahalanobis observations computes mahalanobis distances given observations provided observations assumed centered one may want center using location estimate rst 
1185: parameters observations array like shape n_observations n_features observations mahalanobis distances compute 
1186: returns mahalanobis_distance array shape n_observations mahalanobis distances observations 
1187: reference scikit learn user guide release score x_test assume_centered false computes log likelihood gaussian data set self covariance_ estimator covariance matrix 
1188: parameters x_test array like shape n_samples n_features test data compute likelihood n_samples number sam ples n_features number features 
1189: returns res oat likelihood data set self covariance_ estimator covariance matrix 
1190: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn covariance mincovdet class sklearn covariance mincovdet store_precision true assume_centered false sup minimum covariance determinant mcd robust estimator covariance port_fraction none random_state none parameters store_precision bool specify estimated precision stored assume_centered boolean true support robust location covariance estimates computed covariance estimate recomputed without centering data useful work data whose mean signicantly equal zero exactly zero false robust location covariance directly computed fastmcd algorithm without additional treatment 
1191: support_fraction oat support_fraction proportion points included support raw mcd estimate default none implies minimum value support_fraction used within algorithm n_sample n_features random_state integer numpy randomstate optional random generator used integer given xes seed defaults global numpy random number generator 
1192: references rouseeuw1984 rouseeuw1999 butler1993 chapter user guide scikit learn user guide release raw robust estimated location correction reweighting raw robust estimated covariance correction reweighting mask observations used compute raw robust estimates location shape correction reweighting estimated robust location estimated robust covariance matrix estimated pseudo inverse matrix stored store_precision true mask observations used compute robust estimates location shape 
1193: attributes raw_location_ array like shape n_features raw_covariance_ array like shape n_features n_features raw_support_ array like shape n_samples location_ array like shape n_features covariance_ array like shape n_features n_features precision_ array like shape n_features n_features support_ array like shape n_samples methods correct_covariance data apply correction raw minimum covariance determinant estimates error_norm comp_cov norm scaling squared computes mean squared error two covariance estimators fits minimum covariance determinant fastmcd algorithm fit get parameters estimator get_params deep mahalanobis observations computes mahalanobis distances given observations reweight raw minimum covariance determinant estimates reweight_covariance data computes log likelihood gaussian data set self covariance_ estimator covariance matrix score x_test assume_centered set_params params set parameters estimator 
1194: __init__ store_precision true dom_state none correct_covariance data assume_centered false support_fraction none ran apply correction raw minimum covariance determinant estimates correction using empirical correction factor suggested rousseeuw van driessen rouseeuw1984 
1195: parameters data array like shape n_samples n_features data matrix features samples data set must one used compute raw estimates 
1196: returns covariance_corrected array like shape n_features n_features corrected robust covariance estimate 
1197: error_norm comp_cov norm frobenius scaling true squared true computes mean squared error two covariance estimators sense frobenius norm parameters comp_cov array like shape n_features n_features covariance compare 
1198: norm str reference scikit learn user guide release type norm used compute error available error types frobenius fault sqrt spectral sqrt max eigenvalues error comp_cov self covariance_ 
1199: scaling bool true default squared error norm divided n_features false squared error norm rescaled 
1200: squared bool whether compute squared error norm error norm true default squared error norm returned false error norm returned 
1201: returns mean squared error sense frobenius norm self comp_cov covariance estimators fit fits minimum covariance determinant fastmcd algorithm 
1202: parameters array like shape n_samples n_features training data n_samples number samples n_features number features 
1203: returns self object returns self get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1204: mahalanobis observations computes mahalanobis distances given observations provided observations assumed centered one may want center using location estimate rst 
1205: parameters observations array like shape n_observations n_features observations mahalanobis distances compute 
1206: returns mahalanobis_distance array shape n_observations mahalanobis distances observations 
1207: reweight_covariance data reweight raw minimum covariance determinant estimates reweight observations using rousseeuws method equivalent deleting outlying observations data set computing location covariance estimates rouseeuw1984 parameters data array like shape n_samples n_features data matrix features samples data set must one used compute raw estimates 
1208: returns location_reweighted array like shape n_features reweighted robust location estimate 
1209: chapter user guide scikit learn user guide release covariance_reweighted array like shape n_features n_features reweighted robust covariance estimate 
1210: support_reweighted array like type boolean shape n_samples mask observations used compute reweighted robust loca tion covariance estimates 
1211: score x_test assume_centered false computes log likelihood gaussian data set self covariance_ estimator covariance matrix 
1212: parameters x_test array like shape n_samples n_features test data compute likelihood n_samples number sam ples n_features number features 
1213: returns res oat likelihood data set self covariance_ estimator covariance matrix 
1214: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn covariance oas class sklearn covariance oas store_precision true assume_centered false oracle approximating shrinkage estimator oas particular form shrinkage described shrinkage algorithms mmse covariance estimation chen ieee trans sign proc volume issue october formula used correspond one given article taken matlab programm available authors webpage https tbayes eecs umich edu yilun covestimation 
1215: parameters store_precision bool specify estimated precision stored notes regularised covariance shrinkage cov shrinkage identity n_features trace cov n_features shinkage given oas formula see references references shrinkage algorithms mmse covariance estimation chen ieee trans sign proc volume issue october 
1216: reference scikit learn user guide release attributes covariance_ precision_ shrinkage_ oat shrinkage methods array like shape n_features n_features array like shape n_features n_features estimated covariance matrix estimated pseudo inverse matrix stored store_precision true coefcient convex combination used computation shrunk estimate 
1217: error_norm comp_cov norm scaling squared computes mean squared error two covariance estimators fit assume_centered get_params deep mahalanobis observations score x_test assume_centered set_params params fits oracle approximating shrinkage covariance model get parameters estimator computes mahalanobis distances given observations computes log likelihood gaussian data set self covariance_ estimator covariance matrix set parameters estimator 
1218: __init__ store_precision true assume_centered false parameters store_precision bool specify estimated precision stored assume_centered boolean true data centered computation useful working data whose mean almost exactly zero false data centered computa tion 
1219: error_norm comp_cov norm frobenius scaling true squared true computes mean squared error two covariance estimators sense frobenius norm parameters comp_cov array like shape n_features n_features covariance compare 
1220: norm str type norm used compute error available error types frobenius fault sqrt spectral sqrt max eigenvalues error comp_cov self covariance_ 
1221: scaling bool true default squared error norm divided n_features false squared error norm rescaled 
1222: squared bool whether compute squared error norm error norm true default squared error norm returned false error norm returned 
1223: returns mean squared error sense frobenius norm self comp_cov covariance estimators chapter user guide scikit learn user guide release fit assume_centered false fits oracle approximating shrinkage covariance model according given training data rameters 
1224: parameters array like shape n_samples n_features training data n_samples number samples n_features number features 
1225: assume_centered boolean true data centered computation usefull work data whose mean signicantly equal zero exactly zero false data centered computation 
1226: returns self object returns self get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1227: mahalanobis observations computes mahalanobis distances given observations provided observations assumed centered one may want center using location estimate rst 
1228: parameters observations array like shape n_observations n_features observations mahalanobis distances compute 
1229: returns mahalanobis_distance array shape n_observations mahalanobis distances observations 
1230: score x_test assume_centered false computes log likelihood gaussian data set self covariance_ estimator covariance matrix 
1231: parameters x_test array like shape n_samples n_features test data compute likelihood n_samples number sam ples n_features number features 
1232: returns res oat likelihood data set self covariance_ estimator covariance matrix 
1233: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self reference scikit learn user guide release sklearn covariance shrunkcovariance class sklearn covariance shrunkcovariance store_precision true shrinkage covariance estimator shrinkage parameters store_precision bool specify estimated precision stored shrinkage oat shrinkage coefcient convex combination used computation shrunk estimate 
1234: notes regularized covariance given shrinkage cov shrinkage identity n_features trace cov n_features attributes covariance_ precision_ shrinkage oat shrinkage methods array like shape n_features n_features array like shape n_features n_features estimated covariance matrix estimated pseudo inverse matrix stored store_precision true coefcient convex combination used computation shrunk estimate 
1235: error_norm comp_cov norm scaling squared computes mean squared error two covariance estimators fit assume_centered get_params deep mahalanobis observations score x_test assume_centered set_params params fits shrunk covariance model according given training data parameters get parameters estimator computes mahalanobis distances given observations computes log likelihood gaussian data set self covariance_ estimator covariance matrix set parameters estimator 
1236: __init__ store_precision true shrinkage error_norm comp_cov norm frobenius scaling true squared true computes mean squared error two covariance estimators sense frobenius norm parameters comp_cov array like shape n_features n_features covariance compare 
1237: norm str type norm used compute error available error types frobenius fault sqrt spectral sqrt max eigenvalues error chapter user guide scikit learn user guide release comp_cov self covariance_ 
1238: scaling bool true default squared error norm divided n_features false squared error norm rescaled 
1239: squared bool whether compute squared error norm error norm true default squared error norm returned false error norm returned 
1240: returns mean squared error sense frobenius norm self comp_cov covariance estimators fit assume_centered false fits shrunk covariance model according given training data parameters 
1241: parameters array like shape n_samples n_features training data n_samples number samples n_features number features 
1242: assume_centered boolean true data centered computation usefull work data whose mean signicantly equal zero exactly zero false data centered computation 
1243: returns self object returns self get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1244: mahalanobis observations computes mahalanobis distances given observations provided observations assumed centered one may want center using location estimate rst 
1245: parameters observations array like shape n_observations n_features observations mahalanobis distances compute 
1246: returns mahalanobis_distance array shape n_observations mahalanobis distances observations 
1247: score x_test assume_centered false computes log likelihood gaussian data set self covariance_ estimator covariance matrix 
1248: parameters x_test array like shape n_samples n_features test data compute likelihood n_samples number sam ples n_features number features 
1249: returns res oat reference scikit learn user guide release likelihood data set self covariance_ estimator covariance matrix 
1250: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self computes maximum likelihood covariance estimator covariance empirical_covariance covariance ledoit_wolf assume_centered estimates shrunk ledoit wolf covariance matrix covariance shrunk_covariance emp_cov calculates covariance matrix shrunk diagonal covariance oas assume_centered covariance graph_lasso emp_cov alpha estimate covariance oracle approximating shrinkage algorithm penalized covariance estimator sklearn covariance empirical_covariance sklearn covariance empirical_covariance assume_centered false computes maximum likelihood covariance estimator parameters ndarray shape n_samples n_features data compute covariance estimate assume_centered boolean true data centered computation useful working data whose mean almost exactly zero false data centered computa tion 
1251: returns covariance ndarray shape n_features n_features empirical covariance maximum likelihood estimator sklearn covariance ledoit_wolf sklearn covariance ledoit_wolf assume_centered false estimates shrunk ledoit wolf covariance matrix 
1252: parameters array like shape n_samples n_features data compute covariance estimate assume_centered boolean true data centered computation usefull work data whose mean signicantly equal zero exactly zero false data centered computation 
1253: returns shrunk_cov array like shape n_features n_features shrunk covariance shrinkage oat coefcient convex combination used computation shrunk estimate 
1254: chapter user guide scikit learn user guide release notes regularised shrunk covariance shrinkage cov shrinkage identity n_features trace cov n_features sklearn covariance shrunk_covariance sklearn covariance shrunk_covariance emp_cov shrinkage calculates covariance matrix shrunk diagonal parameters emp_cov array like shape n_features n_features covariance matrix shrunk shrinkage oat shrinkage coefcient convex combination used computation shrunk estimate 
1255: returns shrunk_cov array like shrunk covariance notes regularized shrunk covariance given shrinkage cov shrinkage identity n_features trace cov n_features sklearn covariance oas sklearn covariance oas assume_centered false estimate covariance oracle approximating shrinkage algorithm 
1256: parameters array like shape n_samples n_features data compute covariance estimate assume_centered boolean true data centered computation usefull work data whose mean signicantly equal zero exactly zero false data centered computation 
1257: returns shrunk_cov array like shape n_features n_features shrunk covariance shrinkage oat coefcient convex combination used computation shrunk estimate 
1258: reference scikit learn user guide release notes regularised shrunk covariance shrinkage cov shrinkage identity n_features trace cov n_features sklearn covariance graph_lasso sklearn covariance graph_lasso emp_cov alpha max_iter eps 2204460492503131e cov_init none mode verbose false tol return_costs false penalized covariance estimator parameters emp_cov ndarray shape n_features n_features empirical covariance compute covariance estimate alpha positive oat regularization parameter higher alpha regularization sparser inverse covariance cov_init array n_features n_features optional initial guess covariance mode lars lasso solver use coordinate descent lars use lars sparse derlying graphs elsewhere prefer numerically stable 
1259: tol positive oat optional tolerance declare convergence dual gap goes value iterations stopped max_iter integer optional maximum number iterations verbose boolean optional verbose true objective function dual gap printed iteration return_costs boolean optional return_costs true objective function dual gap iteration returned eps oat optional machine precision regularization computation cholesky diagonal fac tors increase ill conditioned systems 
1260: returns covariance ndarray shape n_features n_features estimated covariance matrix precision ndarray shape n_features n_features estimated sparse precision matrix costs list objective dual_gap pairs chapter user guide scikit learn user guide release list values objective function dual gap iteration returned return_costs true see also graphlasso graphlassocv notes algorithm employed solve problem glasso algorithm friedman biostatistics paper algorithm glasso package one possible difference glasso package diagonal coefcients penalized 
1261: sklearn cross_validation cross validation sklearn cross_validation module includes utilities cross validation performance evaluation user guide see cross validation evaluating estimator performance section details 
1262: cross_validation bootstrap cross_validation kfold indices cross_validation leaveonelabelout labels cross_validation leaveoneout indices cross_validation leaveplabelout labels cross_validation leavepout indices cross_validation stratifiedkfold indices cross_validation shufflesplit cross_validation stratifiedshufflesplit random sampling replacement cross validation iterator folds cross validation iterator leave one label_out cross validation iterator leave one cross validation iterator leave label_out cross validation iterator leave cross validation iterator stratied folds cross validation iterator random permutation cross validation iterator stratied shufesplit cross validation iterator sklearn cross_validation bootstrap class sklearn cross_validation bootstrap n_bootstraps train_size test_size none n_train none n_test none random_state none random sampling replacement cross validation iterator provides train test indices split data train test sets resampling input n_bootstraps times time new random split data performed samples drawn replacement side split build training test sets note contrary cross validation strategies bootstrapping allow samples occur several times splits however sample occurs train split never occur test split vice versa want sample occur probably use shufesplit cross validation instead 
1263: parameters int total number elements dataset 
1264: n_bootstraps int default number bootstrapping iterations train_size int oat default int number samples include training split smaller total number samples passed dataset 
1265: reference scikit learn user guide release oat represent proportion dataset include train split 
1266: test_size int oat none default none int number samples include training set smaller total number samples passed dataset oat represent proportion dataset include test split none n_test set complement n_train 
1267: random_state int randomstate pseudo number generator state used random sampling 
1268: see also shufflesplitcross validation using random permutations 
1269: examples sklearn import cross_validation cross_validation bootstrap random_state len print bootstrap n_bootstraps train_size test_size random_state train_index test_index train test train test train test print train train_index test test_index __init__ n_bootstraps dom_state none train_size test_size none n_train none n_test none ran sklearn cross_validation kfold class sklearn cross_validation kfold indices true shufe false random_state none folds cross validation iterator provides train test indices split data train test sets split dataset consecutive folds without shufing fold used validation set remaining fold form training set 
1270: parameters int total number elements int number folds indices boolean optional default true return train test split arrays indices rather boolean mask array integer indices required dealing sparse matrices since cannot indexed boolean masks 
1271: chapter user guide scikit learn user guide release shufe boolean optional whether shufe data splitting batches random_state int randomstate pseudo number generator state used random sampling 
1272: see also stratifiedkfoldtake label information account avoid building folds classification notes folds size trunc n_samples n_folds last one complementary 
1273: examples sklearn import cross_validation array array cross_validation kfold len print sklearn cross_validation kfold train_index test_index train test train test print train train_index test test_index x_train x_test train_index test_index y_train y_test train_index test_index __init__ indices true shufe false random_state none sklearn cross_validation leaveonelabelout class sklearn cross_validation leaveonelabelout labels indices true leave one label_out cross validation iterator provides train test indices split data according third party provided label label information used encode arbitrary domain specic stratications samples integers instance labels could year collection samples thus allow cross validation time based splits 
1274: parameters labels array like int shape n_samples arbitrary domain specic stratication data used draw splits 
1275: indices boolean optional default true return train test split arrays indices rather boolean mask array integer indices required dealing sparse matrices since cannot indexed boolean masks 
1276: reference scikit learn user guide release examples sklearn import cross_validation array array labels array lol cross_validation leaveonelabelout labels len lol print lol sklearn cross_validation leaveonelabelout labels train_index test_index lol train test print train train_index test test_index x_train x_test train_index test_index y_train y_test train_index test_index print x_train x_test y_train y_test train test __init__ labels indices true sklearn cross_validation leaveoneout class sklearn cross_validation leaveoneout indices true leave one cross validation iterator provides train test indices split data train test sets sample used test set singleton remaining samples form training set due high number test sets number samples cross validation method costly large datasets one favor kfold stratiedkfold shufesplit 
1277: parameters int total number elements indices boolean optional default true return train test split arrays indices rather boolean mask array integer indices required dealing sparse matrices since cannot indexed boolean masks 
1278: see also leaveonelabelout domain specific examples sklearn import cross_validation array array loo cross_validation leaveoneout chapter user guide scikit learn user guide release len loo print loo sklearn cross_validation leaveoneout train_index test_index loo train test train test print train train_index test test_index x_train x_test train_index test_index y_train y_test train_index test_index print x_train x_test y_train y_test __init__ indices true sklearn cross_validation leaveplabelout class sklearn cross_validation leaveplabelout labels indices true leave label_out cross validation iterator provides train test indices split data according third party provided label label information used encode arbitrary domain specic stratications samples integers instance labels could year collection samples thus allow cross validation time based splits difference leaveplabelout leaveonelabelout former builds test sets samples assigned different values labels latter uses samples assigned labels 
1279: parameters labels array like int shape n_samples arbitrary domain specic stratication data used draw splits 
1280: int number samples leave test split 
1281: indices boolean optional default true return train test split arrays indices rather boolean mask array integer indices required dealing sparse matrices since cannot indexed boolean masks 
1282: examples sklearn import cross_validation array array labels array lpl cross_validation leaveplabelout labels len lpl print lpl sklearn cross_validation leaveplabelout labels train_index test_index lpl 
1283: print train train_index test test_index x_train x_test train_index test_index reference scikit learn user guide release y_train y_test train_index test_index print x_train x_test y_train y_test train test train test train test __init__ labels indices true sklearn cross_validation leavepout class sklearn cross_validation leavepout indices true leave cross validation iterator provides train test indices split data train test sets test set built using samples remaining samples form training set due high number iterations grows number samples cross validation method costly large datasets one favor kfold stratiedkfold shufesplit 
1284: parameters int total number elements int size test sets indices boolean optional default true return train test split arrays indices rather boolean mask array integer indices required dealing sparse matrices since cannot indexed boolean masks 
1285: examples sklearn import cross_validation array array lpo cross_validation leavepout len lpo print lpo sklearn cross_validation leavepout train_index test_index lpo train test train test train test train test print train train_index test test_index x_train x_test train_index test_index y_train y_test train_index test_index chapter user guide scikit learn user guide release train test train test __init__ indices true sklearn cross_validation stratiedkfold class sklearn cross_validation stratifiedkfold indices true stratied folds cross validation iterator provides train test indices split data train test sets cross validation object variation kfold returns stratied folds folds made preserving percentage samples class 
1286: parameters array n_samples samples split folds int number folds indices boolean optional default true return train test split arrays indices rather boolean mask array integer indices required dealing sparse matrices since cannot indexed boolean masks 
1287: notes folds size trunc n_samples n_folds last one complementary 
1288: examples sklearn import cross_validation array array skf cross_validation stratifiedkfold len skf print skf sklearn cross_validation stratifiedkfold labels train_index test_index skf train test train test print train train_index test test_index x_train x_test train_index test_index y_train y_test train_index test_index __init__ indices true reference scikit learn user guide release sklearn cross_validation shufesplit class sklearn cross_validation shufflesplit n_iterations train_size none dom_state none train_fraction none indices true test_size ran test_fraction none random permutation cross validation iterator yields indices split data training test sets note contrary cross validation strategies random splits guarantee folds different although still likely sizeable datasets 
1289: parameters int total number elements dataset 
1290: n_iterations int default number shufing splitting iterations 
1291: test_size oat default int oat represent proportion dataset include test split int represents absolute number test samples 
1292: train_size oat int none default none oat represent proportion dataset include train split int represents absolute number train samples none value automatically set complement test fraction 
1293: indices boolean optional default true return train test split arrays indices rather boolean mask array integer indices required dealing sparse matrices since cannot indexed boolean masks 
1294: random_state int randomstate pseudo random number generator state used random sampling 
1295: see also bootstrapcross validation using sampling replacement 
1296: examples test_size random_state sklearn import cross_validation cross_validation shufflesplit n_iterations len print shufflesplit n_iterations test_size indices true train_index test_index train test print train train_index test test_index chapter user guide scikit learn user guide release train test train test train_size test_size random_state cross_validation shufflesplit n_iterations train_index test_index train test train test train test print train train_index test test_index __init__ n_iterations test_size train_size none indices true random_state none test_fraction none train_fraction none sklearn cross_validation stratiedshufesplit class sklearn cross_validation stratifiedshufflesplit n_iterations test_size indices true train_size none random_state none stratied shufesplit cross validation iterator provides train test indices split data train test sets cross validation object merge stratiedkfold shufesplit returns stratied randomized folds folds made preserving percentage samples class note like shufesplit strategy stratied random splits guarantee folds different although still likely sizeable datasets 
1297: parameters array n_samples labels samples 
1298: n_iterations int default number shufing splitting iterations 
1299: test_size oat default int oat represent proportion dataset include test split int represents absolute number test samples 
1300: train_size oat int none default none oat represent proportion dataset include train split int represents absolute number train samples none value automatically set complement test fraction 
1301: indices boolean optional default true return train test split arrays indices rather boolean mask array integer indices required dealing sparse matrices since cannot indexed boolean masks 
1302: examples reference scikit learn user guide release sklearn cross_validation import stratifiedshufflesplit array array sss stratifiedshufflesplit test_size random_state len sss print sss stratifiedshufflesplit labels n_iterations train_index test_index sss train test train test train test print train train_index test test_index x_train x_test train_index test_index y_train y_test train_index test_index __init__ n_iterations test_size train_size none indices true random_state none cross_validation train_test_split arrays cross_validation cross_val_score estimator cross_validation permutation_test_score evaluate signicance cross validated score permutations cross_validation check_cv classier split arrays matrices random train test subsets evaluate score cross validation input checker utility building user friendly way 
1303: sklearn cross_validation train_test_split sklearn cross_validation train_test_split arrays options split arrays matrices random train test subsets quick utility wraps calls check_arrays iter shufflesplit n_samples next application input data single call splitting optionally subsampling data oneliner 
1304: parameters arrays sequence arrays scipy sparse matrices shape python lists tuples occurring arrays converted numpy arrays 
1305: test_size oat default int oat represent proportion dataset include test split int represents absolute number test samples 
1306: train_size oat int none default none oat represent proportion dataset include train split int represents absolute number train samples none value automatically set complement test fraction 
1307: random_state int randomstate pseudo random number generator state used random sampling 
1308: dtype numpy dtype instance none default enforce specic dtype 
1309: examples chapter user guide scikit learn user guide release import numpy sklearn cross_validation import train_test_split arange reshape range array test_size random_state a_train a_test b_train b_test train_test_split a_train array b_train array a_test array b_test array sklearn cross_validation cross_val_score sklearn cross_validation cross_val_score estimator score_func none none none n_jobs verbose evaluate score cross validation parameters estimator estimator object implementing object use data array like shape least data 
1310: array like optional target variable try predict case supervised learning 
1311: score_func callable optional callable priority score function estimator non supervised set ting none takes test data x_test argument supervised setting takes test target y_true test prediction y_pred arguments 
1312: cross validation generator optional cross validation generator none fold cross validation used fold strati cross validation supplied estimator classier 
1313: n_jobs integer optional number cpus use computation means cpus 
1314: verbose integer optional reference scikit learn user guide release verbosity level sklearn cross_validation permutation_test_score sklearn cross_validation permutation_test_score estimator score_func none n_jobs random_state ver n_permutations labels none bose evaluate signicance cross validated score permutations parameters estimator estimator object implementing object use data array like shape least data 
1315: array like target variable try predict case supervised learning 
1316: score_func callable callable taking arguments test targets y_test predicted targets y_pred returns oat score functions expected return bigger value better result otherwise returned value correspond value see returns details 
1317: integer crossvalidation generator optional integer passed number fold default specic crossvalidation jects passed see sklearn cross_validation module list possible objects n_jobs integer optional number cpus use computation means cpus 
1318: labels array like shape n_samples optional labels constrain permutation among groups samples label 
1319: random_state randomstate int seed default random number generator instance dene state random permutations generator 
1320: verbose integer optional verbosity level returns score oat true score without permuting targets 
1321: permutation_scores array shape n_permutations scores obtained permutations 
1322: pvalue oat returned value equals value score_func returns bigger numbers better scores zero_one score_func rather loss function lower better mean_squared_error actually complement value value 
1323: chapter user guide scikit learn user guide release notes function implements test ojala garriga permutation tests studying classier performance journal machine learning research vol sklearn cross_validation check_cv sklearn cross_validation check_cv none none classier false input checker utility building user friendly way 
1324: parameters integer generator instance none input specifying generator use integer case number folds kfold none case fold used another object used generator 
1325: ndarray data cross val object applied ndarray target variable supervised learning problem classier boolean optional whether task classication task case stratied kfold used 
1326: sklearn datasets datasets sklearn datasets module includes utilities load datasets including methods load fetch popular reference datasets also features articial data generators user guide see dataset loading utilities section details 
1327: loaders deprecated use fetch_20newsgroups instead download_if_missing false load lenames newsgroups dataset 
1328: datasets load_20newsgroups args kwargs datasets fetch_20newsgroups data_home datasets fetch_20newsgroups_vectorized load newsgroups dataset transform idf vectors datasets load_boston datasets load_diabetes datasets load_digits n_class datasets load_files container_path datasets load_iris datasets load_lfw_pairs download_if_missing datasets fetch_lfw_pairs subset datasets load_lfw_people download_if_missing datasets fetch_lfw_people data_home datasets load_linnerud datasets fetch_olivetti_faces data_home datasets load_sample_image image_name load return boston house prices dataset regression load return diabetes dataset regression load return digits dataset classication load text les categories subfolder names load return iris dataset classication alias fetch_lfw_pairs download_if_missing false loader labeled faces wild lfw pairs dataset alias fetch_lfw_people download_if_missing false loader labeled faces wild lfw people dataset load return linnerud dataset multivariate regression loader olivetti faces data set load numpy array single sample image reference continued next page scikit learn user guide release datasets load_sample_images datasets load_svmlight_file n_features load sample images image manipulation load datasets svmlight libsvm format sparse csr matrix table continued previous page sklearn datasets load_20newsgroups sklearn datasets load_20newsgroups args kwargs deprecated use fetch_20newsgroups instead download_if_missing false alias fetch_20newsgroups download_if_missing false 
1329: see fetch_20newsgroups __doc__ documentation parameter list 
1330: sklearn datasets fetch_20newsgroups sklearn datasets fetch_20newsgroups data_home none cate gories none shufe true random_state load_if_missing true subset train load lenames newsgroups dataset 
1331: parameters subset train test optional select dataset load train training set test test set shufed ordering 
1332: data_home optional default none specify download cache folder datasets none scikit learn data stored scikit_learn_data subfolders 
1333: categories none collection string unicode none default load categories none list category names load categories ignored 
1334: shufe bool optional whether shufe data might important models make sumption samples independent identically distributed stochastic gradient descent 
1335: random_state numpy random number generator seed integer used shufe dataset 
1336: download_if_missing optional true default false raise ioerror data locally available instead trying download data source site 
1337: sklearn datasets fetch_20newsgroups_vectorized sklearn datasets fetch_20newsgroups_vectorized subset train data_home none load newsgroups dataset transform idf vectors transformation done using default settings convenience function sklearn feature_extraction text vectorizer advanced usage stopword ltering gram extraction etc combine fetch_20newsgroups custom vectorizer countvectorizer 
1338: idf chapter user guide scikit learn user guide release parameters subset train test optional select dataset load train training set test test set shufed ordering 
1339: data_home optional default none specify download cache folder datasets none scikit learn data stored scikit_learn_data subfolders 
1340: returns bunch bunch object bunch data sparse matrix shape n_samples n_features bunch target array shape n_samples bunch target_names list length n_classes sklearn datasets load_boston sklearn datasets load_boston load return boston house prices dataset regression 
1341: samples total dimensionality features targets real positive real 
1342: returns data bunch dictionary like object interesting attributes data data learn target regression targets target_names meaning labels descr full description dataset 
1343: examples sklearn datasets import load_boston boston load_boston boston data shape sklearn datasets load_diabetes sklearn datasets load_diabetes load return diabetes dataset regression 
1344: samples total dimensionality features targets real integer returns data bunch dictionary like object interesting attributes data data learn target regression target sample 
1345: reference scikit learn user guide release sklearn datasets load_digits sklearn datasets load_digits n_class load return digits dataset classication datapoint 8x8 image digit 
1346: classes samples per class samples total dimensionality features integers parameters n_class integer optional default number classes return 
1347: returns data bunch dictionary like object interesting attributes data data learn images images corresponding sample target classication labels sample target_names meaning labels descr full description dataset 
1348: examples load data visualize images sklearn datasets import load_digits digits load_digits digits data shape import pylab gray matshow digits images show sklearn datasets load_les sklearn datasets load_files container_path load_content true charse_error strict random_state description none shufe true categories none charset none load text les categories subfolder names individual samples assumed les stored two levels folder structure following container_folder category_1_folder le_1 txt le_2 txt le_42 txt category_2_folder le_43 txt le_44 txt 
1349: folder names used supervised signal label names indivial names important function try extract features numpy array scipy sparse matrix load_content false try load les memory use utf text les scikit learn classication clustering algorithm rst need use sklearn features text module build feature extraction transformer suits problem 
1350: addition chapter user guide similar feature extractors build kind unstructured data input images audio video 
1351: scikit learn user guide release parameters container_path string unicode path main folder holding one subfolder per category description string unicode optional default none paragraph describing characteristic dataset source reference etc 
1352: categories collection strings none optional default none none default load categories none list category names load categories ignored 
1353: load_content boolean optional default true whether load content different les true data attribute con taining text information present data structure returned lenames attribute gives path les 
1354: charset string none default none none try decode content les images non text content none charset use decode text les load_content true 
1355: charset_error strict ignore replace instruction byte sequence given analyze contains characters given charset default strict meaning unicodedecodeerror raised values ignore replace 
1356: shufe bool optional default true whether shufe data might important models make sumption samples independent identically distributed stochastic gradient descent 
1357: random_state int randomstate instance none optional default int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used random 
1358: returns data bunch dictionary like object interesting attributes either data raw text data learn lenames les holding target classication labels integer index target_names meaning labels descr full description dataset 
1359: sklearn datasets load_iris sklearn datasets load_iris load return iris dataset classication iris dataset classic easy multi class classication dataset 
1360: reference scikit learn user guide release classes samples per class samples total dimensionality features real positive returns data bunch dictionary like object interesting attributes data data learn target classication labels target_names meaning labels feature_names meaning features descr full description dataset 
1361: examples lets say interested samples want know class name 
1362: sklearn datasets import load_iris data load_iris data target array list data target_names setosa versicolor virginica sklearn datasets load_lfw_pairs sklearn datasets load_lfw_pairs download_if_missing false kwargs alias fetch_lfw_pairs download_if_missing false check fetch_lfw_pairs __doc__ documentation parameter list 
1363: sklearn datasets fetch_lfw_pairs sklearn datasets fetch_lfw_pairs subset train funneled true size color false slice_ slice none slice none download_if_missing true data_home none loader labeled faces wild lfw pairs dataset dataset collection jpeg pictures famous people collected internet details available ofcial website http vis www umass edu lfw picture centered single face pixel channel color rgb encoded oat range task called face verication given pair two pictures binary classier must predict whether two images person ofcial readme txt task described restricted task sure implement unrestricted variant correctly left unsupported 
1364: parameters subset optional default train chapter user guide scikit learn user guide release select dataset load train development training set test develop ment test set 10_folds ofcial evaluation set meant used folds cross validation 
1365: data_home optional default none specify another download cache folder datasets default scikit learn data stored scikit_learn_data subfolders 
1366: funneled boolean optional default true download use funneled variant dataset 
1367: resize oat optional default ratio used resize face picture 
1368: color boolean optional default false keep rgb channels instead averaging single gray level channel color true shape data one dimension shape color false slice_ optional provide custom slice height width extract interesting part jpeg les avoid use statistical correlation background download_if_missing optional true default false raise ioerror data locally available instead trying download data source site 
1369: sklearn datasets load_lfw_people sklearn datasets load_lfw_people download_if_missing false kwargs alias fetch_lfw_people download_if_missing false check fetch_lfw_people __doc__ documentation parameter list 
1370: sklearn datasets fetch_lfw_people sklearn datasets fetch_lfw_people data_home none funneled true min_faces_per_person none slice_ slice none download_if_missing true resize color false slice none loader labeled faces wild lfw people dataset dataset collection jpeg pictures famous people collected internet details available ofcial website http vis www umass edu lfw picture centered single face pixel channel color rgb encoded oat range task called face recognition identication given picture face name person given training set gallery 
1371: parameters data_home optional default none reference scikit learn user guide release specify another download cache folder datasets default scikit learn data stored scikit_learn_data subfolders 
1372: funneled boolean optional default true download use funneled variant dataset 
1373: resize oat optional default ratio used resize face picture 
1374: min_faces_per_person int optional default none extracted dataset retain pictures people min_faces_per_person different pictures 
1375: least color boolean optional default false keep rgb channels instead averaging single gray level channel color true shape data one dimension shape color false slice_ optional provide custom slice height width extract interesting part jpeg les avoid use statistical correlation background download_if_missing optional true default false raise ioerror data locally available instead trying download data source site 
1376: sklearn datasets load_linnerud sklearn datasets load_linnerud load return linnerud dataset multivariate regression samples total dimensionality data targets features integer targets integer returns data bunch dictionary like object interesting attributes data targets two mul tivariate datasets data corresponding exercise targets corresponding physiological measurements well feature_names target_names 
1377: sklearn datasets fetch_olivetti_faces sklearn datasets fetch_olivetti_faces data_home none download_if_missing true shufe false random_state loader olivetti faces data set 
1378: parameters data_home optional default none specify another download cache folder datasets default scikit learn data stored scikit_learn_data subfolders 
1379: shufe boolean optional true order dataset shufed avoid images person grouped 
1380: download_if_missing optional true default chapter user guide scikit learn user guide release false raise ioerror data locally available instead trying download data source site 
1381: random_state optional integer randomstate object seed random number generator used shufe data 
1382: notes dataset consists pictures individuals original database available defunct http www research att com facedatabase html version retrieved comes matlab format personal web page sam roweis http www nyu edu roweis sklearn datasets load_sample_image sklearn datasets load_sample_image image_name load numpy array single sample image parameters image_name china jpg ower jpg name sample image loaded returns img array image numpy array height width color examples sklearn datasets import load_sample_image china load_sample_image china jpg china dtype dtype uint8 china shape flower load_sample_image flower jpg flower dtype dtype uint8 flower shape sklearn datasets load_sample_images sklearn datasets load_sample_images load sample images image manipulation loads china flower 
1383: returns data bunch dictionary like object following attributes images two sample images lenames names images descr full description dataset 
1384: reference scikit learn user guide release examples load data visualize images sklearn datasets import load_sample_images dataset load_sample_images len dataset images first_img_data dataset images first_img_data shape first_img_data dtype dtype uint8 sklearn datasets load_svmlight_le sklearn datasets load_svmlight_file n_features none dtype type numpy oat64 mul tilabel false zero_based auto load datasets svmlight libsvm format sparse csr matrix format text based format one sample per line store zero valued features hence suitable sparse dataset rst element line used store target variable predict format used default format svmlight libsvm command line programs parsing text based source expensive working repeatedly dataset recom mended wrap loader joblib memory cache store memmapped backup csr results rst call benet near instantaneous loading memmapped structures subsequent calls implementation naive allocate much memory slow since written python large datasets recommended use optimized loader https github com mblondel svmlight loader parameters str like open binary mode path load n_features int none number features use none inferred argument useful load several les subsets bigger sliced dataset subset might example every feature hence inferred shape might vary one slice another 
1385: multilabel boolean optional samples may several labels see http www csie ntu edu cjlin libsvmtools datasets multilabel html zero_based boolean auto optional whether column indices zero based true one based false set auto heuristic check applied determine contents kinds les occur wild unfortunately self identifying using auto true always safe 
1386: returns scipy sparse matrix shape n_samples n_features chapter user guide scikit learn user guide release ndarray shape n_samples multilabel case list tuples length n_samples 
1387: see also load_svmlight_filessimilar function loading multiple les format enforcing samples generator datasets make_blobs n_samples n_features datasets make_classification n_samples datasets make_circles n_samples shufe datasets make_friedman1 n_samples datasets make_friedman2 n_samples noise datasets make_friedman3 n_samples noise datasets make_hastie_10_2 n_samples datasets make_low_rank_matrix n_samples datasets make_moons n_samples shufe datasets make_multilabel_classification datasets make_regression n_samples datasets make_s_curve n_samples noise datasets make_sparse_coded_signal n_samples generate signal sparse combination dictionary elements datasets make_sparse_spd_matrix dim datasets make_sparse_uncorrelated datasets make_spd_matrix n_dim random_state datasets make_swiss_roll n_samples noise generate isotropic gaussian blobs clustering generate random class classication problem make large circle containing smaller circle 2di generate friedman regression problem generate friedman regression problem generate friedman regression problem generates data binary classication used generate mostly low rank matrix bell shaped singular values make two interleaving half circles generate random multilabel classication problem generate random regression problem generate curve dataset 
1388: generate sparse symetric denite positive matrix generate random regression problem sparse uncorrelated design generate random symmetric positive denite matrix generate swiss roll dataset 
1389: sklearn datasets make_blobs sklearn datasets make_blobs n_samples n_features generate isotropic gaussian blobs clustering 
1390: center_box shufe true random_state none centers cluster_std parameters n_samples int optional default total number points equally divided among clusters 
1391: n_features int optional default number features sample 
1392: centers int array shape n_centers n_features optional default number centers generate xed center locations 
1393: cluster_std oat sequence oats optional default standard deviation clusters 
1394: center_box pair oats min max optional default bounding box cluster center centers generated random 
1395: shufe boolean optional default true reference scikit learn user guide release shufe samples 
1396: random_state int randomstate instance none optional default none int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used random 
1397: returns array shape n_samples n_features generated samples 
1398: array shape n_samples integer labels cluster membership sample 
1399: examples sklearn datasets samples_generator import make_blobs make_blobs n_samples centers n_features shape array random_state sklearn datasets make_classication sklearn datasets make_classification n_samples n_informative n_redundant n_classes n_clusters_per_class weights none ip_y class_sep hypercube true shift scale shufe true random_state none n_features n_repeated generate random class classication problem 
1400: parameters n_samples int optional default number samples 
1401: n_features int optional default total number features comprise n_informative informative features n_redundant redundant features n_repeated dupplicated features n_features n_informative n_redundant n_repeated useless features drawn random 
1402: n_informative int optional default number informative features class composed number gaussian clusters located around vertices hypercube subspace dimension n_informative cluster informative features drawn independently randomly linearly combined order add covariance clusters placed vertices hypercube 
1403: n_redundant int optional default number redundant features features generated random linear com binations informative features 
1404: n_repeated int optional default chapter user guide scikit learn user guide release number dupplicated features drawn randomly informative dundant features 
1405: n_classes int optional default number classes labels classication problem 
1406: n_clusters_per_class int optional default number clusters per class 
1407: weights list oats none default none proportions samples assigned class none classes balanced note len weights n_classes last class weight automatically inferred 
1408: ip_y oat optional default fraction samples whose class randomly exchanged 
1409: class_sep oat optional default factor multiplying hypercube dimension 
1410: hypercube boolean optional default true true clusters put vertices hypercube false clusters put vertices random polytope 
1411: shift oat none optional default shift features specied value none features shifted random value drawn class_sep class_sep scale oat none optional default multiply features specied value random value drawn note scaling happens shifting 
1412: none features scaled shufe boolean optional default true shufe samples features 
1413: random_state int randomstate instance none optional default none int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used random 
1414: returns array shape n_samples n_features generated samples 
1415: array shape n_samples integer labels class membership sample 
1416: notes algorithm adapted guyon designed generate madelon dataset 
1417: reference scikit learn user guide release references r48 sklearn datasets make_circles sklearn datasets make_circles n_samples shufe true noise none random_state none make large circle containing smaller circle 2di simple toy dataset visualize clustering classication algorithms 
1418: factor parameters n_samples int optional default total number points generated shufe bool optional default true whether shufe samples 
1419: noise double none default none standard deviation gaussian noise added data 
1420: factor double default scale factor inner outer circle 
1421: sklearn datasets make_friedman1 sklearn datasets make_friedman1 n_samples n_features noise ran dom_state none generate friedman regression problem dataset described friedman breiman inputs independent features uniformly distributed interval output created according formula sin noise 
1422: n_features features actually used compute remaining features independent number features 
1423: parameters n_samples int optional default number samples 
1424: n_features int optional default number features least 
1425: noise oat optional default standard deviation gaussian noise applied output 
1426: random_state int randomstate instance none optional default none int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used random 
1427: chapter user guide scikit learn user guide release returns array shape n_samples n_features input samples 
1428: array shape n_samples output values 
1429: references r49 r50 sklearn datasets make_friedman2 sklearn datasets make_friedman2 n_samples noise random_state none generate friedman regression problem dataset described friedman breiman inputs independent features uniformly distributed intervals 
1430: output created according formula noise 
1431: parameters n_samples int optional default number samples 
1432: noise oat optional default standard deviation gaussian noise applied output 
1433: random_state int randomstate instance none optional default none int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used random 
1434: returns array shape n_samples input samples 
1435: array shape n_samples output values 
1436: references r51 r52 reference scikit learn user guide release sklearn datasets make_friedman3 sklearn datasets make_friedman3 n_samples noise random_state none generate friedman regression problem dataset described friedman breiman inputs independent features uniformly distributed intervals 
1437: output created according formula arctan noise 
1438: parameters n_samples int optional default number samples 
1439: noise oat optional default standard deviation gaussian noise applied output 
1440: random_state int randomstate instance none optional default none int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used random 
1441: returns array shape n_samples input samples 
1442: array shape n_samples output values 
1443: references r53 r54 sklearn datasets make_hastie_10_2 sklearn datasets make_hastie_10_2 n_samples random_state none generates data binary classication used hastie example ten features standard independent gaussian target dened sum else parameters n_samples int optional default number samples 
1444: random_state int randomstate instance none optional default none chapter user guide scikit learn user guide release int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used random 
1445: returns array shape n_samples input samples 
1446: array shape n_samples output values 
1447: references hastie tibshirani friedman elements statistical learning springer sklearn datasets make_low_rank_matrix sklearn datasets make_low_rank_matrix n_samples n_features effective_rank generate mostly low rank matrix bell shaped singular values variance explained bell shaped curve width effective_rank low rank part singular values prole tail_strength random_state none tail_strength exp effective_rank remaining singular values tail fat decreasing tail_strength exp effective_rank 
1448: low rank part prole considered structured signal part data tail considered noisy part data cannot summarized low number linear components singular vectors kind singular proles often seen practice instance gray level pictures faces idf vectors text documents crawled web parameters n_samples int optional default number samples 
1449: n_features int optional default number features 
1450: effective_rank int optional default approximate number singular vectors required explain data linear combinations 
1451: tail_strength oat optional default relative importance fat noisy tail singular values prole random_state int randomstate instance none optional default none reference scikit learn user guide release int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used random 
1452: returns array shape n_samples n_features matrix 
1453: sklearn datasets make_moons sklearn datasets make_moons n_samples shufe true noise none random_state none make two interleaving half circles simple toy dataset visualize clustering classication algorithms 
1454: parameters n_samples int optional default total number points generated 
1455: shufe bool optional default true whether shufe samples 
1456: noise double none default none standard deviation gaussian noise added data 
1457: sklearn datasets make_multilabel_classication sklearn datasets make_multilabel_classification n_samples n_classes n_labels allow_unlabeled true dom_state none n_features length ran generate random multilabel classication problem sample generative process pick number labels poisson n_labels times choose class multinomial theta pick document length poisson length times choose word multinomial theta_c process rejection sampling used make sure never zero n_classes document length never zero likewise reject classes already chosen 
1458: parameters n_samples int optional default number samples 
1459: n_features int optional default total number features 
1460: n_classes int optional default number classes classication problem 
1461: n_labels int optional default average number labels per instance number labels follows poisson distri bution never takes value 
1462: chapter user guide scikit learn user guide release length int optional default sum features number words documents 
1463: allow_unlabeled bool optional default true true instances might belong class 
1464: random_state int randomstate instance none optional default none int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used random 
1465: returns array shape n_samples n_features generated samples 
1466: list tuples label sets 
1467: sklearn datasets make_regression sklearn datasets make_regression n_samples n_informative bias effective_rank none tail_strength noise shufe true coef false random_state none n_features generate random regression problem input set either well conditioned default low rank fat tail singular prole see make_low_rank_matrix details output generated applying potentially biased random linear regression model n_informative nonzero regressors previously generated input gaussian centered noise adjustable scale 
1468: parameters n_samples int optional default number samples 
1469: n_features int optional default number features 
1470: n_informative int optional default number informative features number features used build linear model used generate output 
1471: bias oat optional default bias term underlying linear model 
1472: effective_rank int none optional default none none approximate number singular vectors required explain input data linear combinations using kind singular spectrum input allows generator reproduce correlations often observed practice none input set well conditioned centered gaussian unit variance 
1473: tail_strength oat optional default relative importance fat noisy tail singular values prole effec tive_rank none 
1474: reference scikit learn user guide release noise oat optional default standard deviation gaussian noise applied output 
1475: shufe boolean optional default true shufe samples features coef boolean optional default false true coefcients underlying linear model returned 
1476: random_state int randomstate instance none optional default none int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used random 
1477: returns array shape n_samples n_features input samples 
1478: array shape n_samples output values 
1479: coef array shape n_features optional coefcient underlying linear model returned coef true 
1480: sklearn datasets make_s_curve sklearn datasets make_s_curve n_samples noise random_state none generate curve dataset 
1481: parameters n_samples int optional default number sample points curve 
1482: noise oat optional default standard deviation gaussian noise 
1483: random_state int randomstate instance none optional default none int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used random 
1484: returns array shape n_samples points 
1485: array shape n_samples univariate position sample according main dimension points manifold 
1486: sklearn datasets make_sparse_coded_signal sklearn datasets make_sparse_coded_signal n_samples generate signal sparse combination dictionary elements 
1487: n_components n_features n_nonzero_coefs random_state none chapter user guide returns matrix n_features n_components n_components n_samples column exactly n_nonzero_coefs non zero elements 
1488: scikit learn user guide release parameters n_samples int number samples generate n_components int number components dictionary n_features int number features dataset generate n_nonzero_coefs int number active non zero coefcients sample random_state int randomstate instance optional default none seed used pseudo random number generator returns data array shape n_features n_samples encoded signal 
1489: dictionary array shape n_features n_components dictionary normalized components code array shape n_components n_samples sparse code column matrix exactly n_nonzero_coefs non zero items 
1490: sklearn datasets make_sparse_spd_matrix sklearn datasets make_sparse_spd_matrix dim alpha smallest_coef dom_state none norm_diag false ran largest_coef generate sparse symetric denite positive matrix 
1491: parameters dim integer optional default size random matrix generate 
1492: alpha oat optional default probability coefcient non zero see notes 
1493: random_state int randomstate instance none optional default none int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used random 
1494: returns prec array shape dim dim notes sparsity actually imposed cholesky factor matrix thus alpha translate directly lling fraction matrix 
1495: reference scikit learn user guide release sklearn datasets make_sparse_uncorrelated sklearn datasets make_sparse_uncorrelated n_samples n_features ran generate random regression problem sparse uncorrelated design dataset described celeux dom_state none rst features informative remaining features useless 
1496: parameters n_samples int optional default number samples 
1497: n_features int optional default number features 
1498: random_state int randomstate instance none optional default none int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used random 
1499: returns array shape n_samples n_features input samples 
1500: array shape n_samples output values 
1501: references r55 sklearn datasets make_spd_matrix sklearn datasets make_spd_matrix n_dim random_state none generate random symmetric positive denite matrix 
1502: parameters n_dim int matrix dimension 
1503: random_state int randomstate instance none optional default none int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used random 
1504: returns array shape n_dim n_dim random symmetric positive denite matrix 
1505: chapter user guide scikit learn user guide release sklearn datasets make_swiss_roll sklearn datasets make_swiss_roll n_samples noise random_state none generate swiss roll dataset 
1506: parameters n_samples int optional default number sample points curve 
1507: noise oat optional default standard deviation gaussian noise 
1508: random_state int randomstate instance none optional default none int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used random 
1509: returns array shape n_samples points 
1510: array shape n_samples univariate position sample according main dimension points manifold 
1511: notes algorithm marsland 
1512: references r56 sklearn decomposition matrix decomposition sklearn decomposition module includes matrix decomposition algorithms including among others pca nmf ica algorithms module regarded dimensionality reduction techniques user guide see decomposing signals components matrix factorization problems section details 
1513: decomposition pca n_components copy whiten decomposition probabilisticpca decomposition projectedgradientnmf decomposition randomizedpca n_components decomposition kernelpca n_components decomposition fastica n_components decomposition nmf n_components init decomposition sparsepca n_components decomposition minibatchsparsepca n_components decomposition sparsecoder dictionary decomposition dictionarylearning n_atoms decomposition minibatchdictionarylearning n_atoms mini batch dictionary learning principal component analysis pca additional layer top pca adds probabilistic evaluationprincipal component analysis pca non negative matrix factorization projected gradient nmf principal component analysis pca using randomized svd kernel principal component analysis kpca fastica fast algorithm independent component analysis non negative matrix factorization projected gradient nmf sparse principal components analysis sparsepca mini batch sparse principal components analysis sparse coding dictionary learning reference scikit learn user guide release sklearn decomposition pca class sklearn decomposition pca n_components none copy true whiten false principal component analysis pca linear dimensionality reduction using singular value decomposition data keeping signicant singular vectors project data lower dimensional space implementation uses scipy linalg implementation singular value decomposition works dense arrays scalable large dimensional data time complexity implementation assuming n_samples n_features 
1514: parameters n_components int none string number components keep n_components set components kept n_components min n_samples n_features n_components mle minkas mle used guess dimension n_components select number components amount vari ance needs explained greater percentage specied n_components copy bool false data passed overwritten whiten bool optional true false default components_ vectors divided n_samples times singular values ensure uncorrelated outputs unit component wise variances whitening remove information transformed signal relative vari ance scales components sometime improve predictive accuracy downstream estimators making data respect hard wired assumptions 
1515: see also probabilisticpca randomizedpca kernelpca sparsepca notes n_components mle class uses method thomas minka automatic choice dimensionality pca nips due implementation subtleties singular value decomposition svd used imple mentation running twice matrix lead principal components signs ipped change direction reason important always use estimator object transform data consistent fashion 
1516: examples import numpy sklearn decomposition import pca array pca pca n_components pca fit pca copy true n_components whiten false chapter user guide scikit learn user guide release print pca explained_variance_ratio_ 
1517: attributes compo nents_ array n_components n_features array n_components plained_variance_ratio_ components maximum variance 
1518: percentage variance explained selected components set components stored sum explained variances equal methods fit fit_transform get_params deep inverse_transform transform data back original space set_params params transform set parameters estimator apply dimensionality reduction 
1519: fit model fit model apply dimensionality reduction get parameters estimator __init__ n_components none copy true whiten false fit none params fit model 
1520: parameters array like shape n_samples n_features training data n_samples number samples n_features number features 
1521: returns self object returns instance fit_transform none params fit model apply dimensionality reduction 
1522: parameters array like shape n_samples n_features training data n_samples number samples n_features number features 
1523: returns x_new array like shape n_samples n_components get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1524: inverse_transform transform data back original space return input x_original whose transform would parameters array like shape n_samples n_components reference scikit learn user guide release new data n_samples number samples n_components number components 
1525: returns x_original array like shape n_samples n_features notes whitening enabled inverse_transform compute exact inverse operation transform 
1526: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform apply dimensionality reduction 
1527: parameters array like shape n_samples n_features new data n_samples number samples n_features number features 
1528: returns x_new array like shape n_samples n_components sklearn decomposition probabilisticpca class sklearn decomposition probabilisticpca n_components none copy true whiten false additional layer top pca adds probabilistic evaluationprincipal component analysis pca linear dimensionality reduction using singular value decomposition data keeping signicant singular vectors project data lower dimensional space implementation uses scipy linalg implementation singular value decomposition works dense arrays scalable large dimensional data time complexity implementation assuming n_samples n_features 
1529: parameters n_components int none string number components keep n_components set components kept n_components min n_samples n_features n_components mle minkas mle used guess dimension n_components select number components amount vari ance needs explained greater percentage specied n_components copy bool false data passed overwritten whiten bool optional true false default components_ vectors divided n_samples times singular values ensure uncorrelated outputs unit component wise variances 
1530: chapter user guide scikit learn user guide release whitening remove information transformed signal relative vari ance scales components sometime improve predictive accuracy downstream estimators making data respect hard wired assumptions 
1531: see also probabilisticpca randomizedpca kernelpca sparsepca notes n_components mle class uses method thomas minka automatic choice dimensionality pca nips due implementation subtleties singular value decomposition svd used imple mentation running twice matrix lead principal components signs ipped change direction reason important always use estimator object transform data consistent fashion 
1532: examples import numpy sklearn decomposition import pca array pca pca n_components pca fit pca copy true n_components whiten false print pca explained_variance_ratio_ 
1533: attributes compo nents_ array n_components n_features array n_components plained_variance_ratio_ components maximum variance 
1534: percentage variance explained selected components set components stored sum explained variances equal methods additionally pca learns covariance model fit model apply dimensionality reduction get parameters estimator fit homoscedastic fit_transform get_params deep inverse_transform transform data back original space score set_params params transform return score associated new data set parameters estimator apply dimensionality reduction 
1535: __init__ n_components none copy true whiten false fit none homoscedastic true reference scikit learn user guide release additionally pca learns covariance model parameters array shape n_samples n_dim data homoscedastic bool optional true average variance across remaining dimensions fit_transform none params fit model apply dimensionality reduction 
1536: parameters array like shape n_samples n_features training data n_samples number samples n_features number features 
1537: returns x_new array like shape n_samples n_components get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1538: inverse_transform transform data back original space return input x_original whose transform would parameters array like shape n_samples n_components new data n_samples number samples n_components number components 
1539: returns x_original array like shape n_samples n_features notes whitening enabled inverse_transform compute exact inverse operation transform 
1540: score none return score associated new data parameters array shape n_samples n_dim data test returns array shape n_samples log likelihood row current model set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform apply dimensionality reduction 
1541: chapter user guide scikit learn user guide release parameters array like shape n_samples n_features new data n_samples number samples n_features number features 
1542: returns x_new array like shape n_samples n_components sklearn decomposition projectedgradientnmf class sklearn decomposition projectedgradientnmf n_components none sparseness none tol nls_max_iter beta init nndsvdar eta max_iter non negative matrix factorization projected gradient nmf parameters array like sparse matrix shape n_samples n_features data model 
1543: n_components int none number components n_components set components kept init nndsvd nndsvda nndsvdar int randomstate method used initialize procedure default nndsvdar valid options nndsvd nonnegative double singular value decomposition nndsvd initialization better sparseness nndsvda nndsvd zeros filled average better sparsity desired nndsvdar nndsvd zeros filled small random values generally faster less accurate alternative nndsvda sparsity desired int seed randomstate non negative random matrices sparseness data components none default none enforce sparsity model 
1544: beta double default degree sparseness sparseness none larger values mean sparseness 
1545: eta double default degree correctness mantain sparsity none smaller values mean larger error 
1546: tol double default tolerance value used stopping conditions 
1547: max_iter int default number iterations compute 
1548: nls_max_iter int default number iterations nls subproblem 
1549: reference scikit learn user guide release notes implements lin projected gradient methods non negative matrix factorization neural computation http www csie ntu edu cjlin nmf hoyer non negative matrix factorization sparseness constraints journal machine learning search nndsvd introduced boutsidis gallopoulos svd based initialization head start nonnegative matrix factorization pattern recognition http www rpi edu boutsc les nndsvd pdf examples import numpy array sklearn decomposition import projectedgradientnmf model projectedgradientnmf n_components init model fit projectedgradientnmf beta eta init max_iter n_components nls_max_iter sparseness none tol model components_ array model reconstruction_err_ model projectedgradientnmf n_components init model fit projectedgradientnmf beta eta init max_iter n_components sparseness components nls_max_iter sparseness components tol model components_ array 
1550: model reconstruction_err_ 
1551: array n_components n_features number attributes compo nents_ recon struc tion_err_ methods non negative components data frobenius norm matrix difference training data reconstructed data produced model computed sparse input matrices expensive terms memory 
1552: fit learn nmf model data 
1553: continued next page chapter user guide scikit learn user guide release table continued previous page fit_transform learn nmf model data returns transformed data get_params deep set_params params transform get parameters estimator set parameters estimator transform data according tted nmf model __init__ n_components none init nndsvdar sparseness none beta eta tol max_iter nls_max_iter fit none params learn nmf model data 
1554: parameters array like sparse matrix shape n_samples n_features data matrix decomposed returns self fit_transform none learn nmf model data returns transformed data efcient calling followed transform 
1555: parameters array like sparse matrix shape n_samples n_features data matrix decomposed returns data array n_samples n_components transformed data get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform transform data according tted nmf model parameters array like sparse matrix shape n_samples n_features data matrix transformed model returns data array n_samples n_components transformed data sklearn decomposition randomizedpca class sklearn decomposition randomizedpca n_components copy true iterated_power principal component analysis pca using randomized svd whiten false random_state none reference scikit learn user guide release linear dimensionality reduction using approximated singular value decomposition data keeping signicant singular vectors project data lower dimensional space implementation uses randomized svd implementation handle scipy sparse numpy dense arrays input 
1556: parameters n_components int maximum number components keep default 
1557: copy bool false data passed overwritten iterated_power int optional number iteration power method default 
1558: whiten bool optional true false default components_ vectors divided singular values ensure uncorrelated outputs unit component wise variances whitening remove information transformed signal relative vari ance scales components sometime improve predictive accuracy downstream estimators making data respect hard wired assumptions 
1559: random_state int randomstate instance none default pseudo random number generator seed control none use numpy random sin gleton 
1560: see also pca probabilisticpca references halko2009 mrt examples import numpy sklearn decomposition import randomizedpca array pca randomizedpca n_components pca fit randomizedpca copy true iterated_power n_components random_state mtrand randomstate object whiten false print pca explained_variance_ratio_ 
1561: chapter user guide attributes compo nents_ array n_components n_features array n_components plained_variance_ratio_ scikit learn user guide release components maximum variance 
1562: percentage variance explained selected components set components stored sum explained variances equal methods fit fit_transform get_params deep inverse_transform transform data back original space set_params params transform fit model data fit data transform get parameters estimator set parameters estimator apply dimensionality reduction 
1563: __init__ n_components copy true iterated_power whiten false random_state none fit none fit model data 
1564: parameters array like scipy sparse matrix shape n_samples n_features training vector n_samples number samples n_features num ber features returns self object returns instance 
1565: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
1566: parameters numpy array shape n_samples n_features training set 
1567: numpy array shape n_samples target values 
1568: returns x_new numpy array shape n_samples n_features_new transformed array 
1569: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
1570: get_params deep true get parameters estimator reference scikit learn user guide release parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1571: inverse_transform transform data back original space return input x_original whose transform would parameters array like scipy sparse matrix shape n_samples n_components new data n_samples number samples n_components number components 
1572: returns x_original array like shape n_samples n_features notes whitening enabled inverse_transform compute exact inverse operation transform 
1573: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform apply dimensionality reduction 
1574: parameters array like scipy sparse matrix shape n_samples n_features new data n_samples number samples n_features number features 
1575: returns x_new array like shape n_samples n_components sklearn decomposition kernelpca class sklearn decomposition kernelpca n_components none degree gamma t_inverse_transform false tol max_iter none coef0 kernel linear alpha eigen_solver auto kernel principal component analysis kpca non linear dimensionality reduction use kernels 
1576: parameters n_components int none number components none non zero components kept 
1577: kernel linear poly rbf sigmoid precomputed kernel default linear degree int optional degree poly rbf sigmoid kernels default 
1578: gamma oat optional kernel coefcient rbf poly kernels default n_features 
1579: chapter user guide scikit learn user guide release coef0 oat optional independent term poly sigmoid kernels 
1580: alpha int hyperparameter ridge regression t_inverse_transform true default learns inverse transform t_inverse_transform bool learn inverse transform non precomputed kernels image point default false learn pre eigen_solver string auto dense arpack select eigensolver use n_components much less number training samples arpack may efcient dense eigensolver 
1581: tol oat convergence tolerance arpack default optimal value chosen arpack max_iter int maximum number iterations arpack default none optimal value chosen arpack references kernel pca intoduced bernhard schoelkopf alexander smola klaus robert mueller kernel principal component analysis advances kernel methods mit press cambridge usa 
1582: attributes lambdas_ alphas_ dual_coef_ x_transformed_t_ eigenvalues eigenvectors centered kernel matrix inverse transform matrix projection tted data kernel principal components methods fit fit_transform get_params deep inverse_transform transform back original space set parameters estimator set_params params transform transform 
1583: fit model data fit model data transform get parameters estimator __init__ n_components none kernel linear gamma t_inverse_transform false eigen_solver auto tol max_iter none degree coef0 alpha fit none fit model data 
1584: parameters array like shape n_samples n_features reference scikit learn user guide release training vector n_samples number samples n_features num ber features returns self object returns instance fit_transform none params fit model data transform 
1585: parameters array like shape n_samples n_features training vector n_samples number samples n_features num ber features 
1586: returns x_new array like shape n_samples n_components get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1587: inverse_transform transform back original space 
1588: parameters array like shape n_samples n_components returns x_new array like shape n_samples n_features references learning find pre images bakir 
1589: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform transform 
1590: parameters array like shape n_samples n_features returns x_new array like shape n_samples n_components sklearn decomposition fastica class sklearn decomposition fastica n_components none algorithm parallel whiten true fun_args none ran fun logcosh max_iter dom_state none fastica fast algorithm independent component analysis fun_prime tol w_init none parameters n_components int optional number components use none passed used 
1591: chapter user guide scikit learn user guide release algorithm parallel deation apply parallel deational algorithm fastica whiten boolean optional whiten false data already considered whitened whitening performed 
1592: fun logcosh exp cube callable non linear function used fastica loop approximate negentropy func tion passed derivative passed fun_prime argument 
1593: fun_prime none callable derivative non linearity used 
1594: max_iter int optional maximum number iterations tol oat optional tolerance update iteration w_init none n_components n_components ndarray mixing matrix used initialize algorithm 
1595: random_state int randomstate pseudo number generator state used random sampling 
1596: notes implementation based hyvarinen oja independent component analysis algorithms appli cations neural networks attributes unmixing_matrix_ array n_components n_samples unmixing matrix methods fit get_mixing_matrix compute mixing matrix get_params deep set_params params transform get parameters estimator set parameters estimator apply mixing matrix recover sources __init__ n_components none algorithm parallel whiten true fun_args none max_iter tol w_init none random_state none fun logcosh fun_prime get_mixing_matrix compute mixing matrix get_params deep true reference scikit learn user guide release get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform apply mixing matrix recover sources sklearn decomposition nmf class sklearn decomposition nmf n_components none init nndsvdar sparseness none beta non negative matrix factorization projected gradient nmf eta tol max_iter nls_max_iter parameters array like sparse matrix shape n_samples n_features data model 
1597: n_components int none number components n_components set components kept init nndsvd nndsvda nndsvdar int randomstate method used initialize procedure default nndsvdar valid options nndsvd nonnegative double singular value decomposition nndsvd initialization better sparseness nndsvda nndsvd zeros filled average better sparsity desired nndsvdar nndsvd zeros filled small random values generally faster less accurate alternative nndsvda sparsity desired int seed randomstate non negative random matrices sparseness data components none default none enforce sparsity model 
1598: beta double default degree sparseness sparseness none larger values mean sparseness 
1599: eta double default degree correctness mantain sparsity none smaller values mean larger error 
1600: tol double default tolerance value used stopping conditions 
1601: chapter user guide scikit learn user guide release max_iter int default number iterations compute 
1602: nls_max_iter int default number iterations nls subproblem 
1603: notes implements lin projected gradient methods non negative matrix factorization neural computation http www csie ntu edu cjlin nmf hoyer non negative matrix factorization sparseness constraints journal machine learning search nndsvd introduced boutsidis gallopoulos svd based initialization head start nonnegative matrix factorization pattern recognition http www rpi edu boutsc les nndsvd pdf examples import numpy array sklearn decomposition import projectedgradientnmf model projectedgradientnmf n_components init model fit projectedgradientnmf beta eta init max_iter n_components nls_max_iter sparseness none tol model components_ array model reconstruction_err_ model projectedgradientnmf n_components init model fit projectedgradientnmf beta eta init max_iter n_components sparseness components nls_max_iter sparseness components tol model components_ array 
1604: model reconstruction_err_ 
1605: reference scikit learn user guide release array n_components n_features number attributes compo nents_ recon struc tion_err_ methods non negative components data frobenius norm matrix difference training data reconstructed data produced model computed sparse input matrices expensive terms memory 
1606: learn nmf model data 
1607: fit fit_transform learn nmf model data returns transformed data get_params deep set_params params transform get parameters estimator set parameters estimator transform data according tted nmf model __init__ n_components none max_iter nls_max_iter init nndsvdar sparseness none beta eta tol fit none params learn nmf model data 
1608: parameters array like sparse matrix shape n_samples n_features data matrix decomposed returns self fit_transform none learn nmf model data returns transformed data efcient calling followed transform 
1609: parameters array like sparse matrix shape n_samples n_features data matrix decomposed returns data array n_samples n_components transformed data get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self chapter user guide scikit learn user guide release transform transform data according tted nmf model parameters array like sparse matrix shape n_samples n_features data matrix transformed model returns data array n_samples n_components transformed data sklearn decomposition sparsepca class sklearn decomposition sparsepca n_components max_iter u_init none dom_state none alpha ridge_alpha tol method lars n_jobs ran v_init none verbose false sparse principal components analysis sparsepca finds set sparse components optimally reconstruct data amount sparseness control lable coefcient penalty given parameter alpha 
1610: parameters n_components int number sparse atoms extract 
1611: alpha oat sparsity controlling parameter higher values lead sparser components 
1612: ridge_alpha oat amount ridge shrinkage apply order improve conditioning calling transform method 
1613: max_iter int maximum number iterations perform 
1614: tol oat tolerance stopping condition 
1615: method lars lars uses least angle regression method solve lasso problem lin ear_model lars_path uses coordinate descent method compute lasso lution linear_model lasso lars faster estimated components sparse 
1616: n_jobs int number parallel jobs run 
1617: u_init array shape n_samples n_atoms initial values loadings warm restart scenarios 
1618: v_init array shape n_atoms n_features initial values components warm restart scenarios 
1619: verbose degree verbosity printed output 
1620: random_state int randomstate reference scikit learn user guide release pseudo number generator state used random sampling 
1621: see also pca minibatchsparsepca dictionarylearning attributes components_ error_ array n_components n_features array sparse components extracted data vector errors iteration 
1622: methods fit fit_transform get_params deep set_params params transform ridge_alpha least squares projection data onto sparse components 
1623: fit model data fit data transform get parameters estimator set parameters estimator 
1624: __init__ n_components alpha ridge_alpha max_iter tol method lars n_jobs u_init none v_init none verbose false random_state none fit none fit model data 
1625: parameters array like shape n_samples n_features training vector n_samples number samples n_features num ber features returns self object returns instance 
1626: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
1627: parameters numpy array shape n_samples n_features training set 
1628: numpy array shape n_samples target values 
1629: returns x_new numpy array shape n_samples n_features_new transformed array 
1630: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
1631: get_params deep true get parameters estimator chapter user guide scikit learn user guide release parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform ridge_alpha none least squares projection data onto sparse components avoid instability issues case system determined regularization applied ridge regression via ridge_alpha parameter note sparse pca components orthogonality enforced pca hence one cannot use simple linear projection 
1632: parameters array shape n_samples n_features test data transformed must number features data used train model 
1633: ridge_alpha oat default amount ridge shrinkage apply order improve conditioning 
1634: returns x_new array shape n_samples n_components transformed data 
1635: sklearn decomposition minibatchsparsepca class sklearn decomposition minibatchsparsepca n_components alpha call ridge_alpha back none chunk_size verbose false shufe true n_jobs method lars random_state none n_iter mini batch sparse principal components analysis finds set sparse components optimally reconstruct data amount sparseness control lable coefcient penalty given parameter alpha 
1636: parameters n_components int number sparse atoms extract alpha int sparsity controlling parameter higher values lead sparser components 
1637: ridge_alpha oat amount ridge shrinkage apply order improve conditioning calling transform method 
1638: n_iter int number iterations perform mini batch reference scikit learn user guide release callback callable callable gets invoked every iterations chunk_size int number features take mini batch verbose degree output procedure print shufe boolean whether shufe data splitting batches n_jobs int number parallel jobs run autodetect 
1639: method lars uses least angle regression method solve lasso problem lin lars ear_model lars_path uses coordinate descent method compute lasso lution linear_model lasso lars faster estimated components sparse 
1640: random_state int randomstate pseudo number generator state used random sampling 
1641: see also pca sparsepca dictionarylearning attributes components_ error_ array n_components n_features array sparse components extracted data vector errors iteration 
1642: methods fit fit_transform get_params deep set_params params transform ridge_alpha least squares projection data onto sparse components 
1643: fit model data fit data transform get parameters estimator set parameters estimator 
1644: __init__ n_components alpha ridge_alpha n_iter callback none chunk_size verbose false shufe true n_jobs method lars random_state none fit none fit model data 
1645: parameters array like shape n_samples n_features training vector n_samples number samples n_features num ber features returns self object returns instance 
1646: chapter user guide scikit learn user guide release fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
1647: parameters numpy array shape n_samples n_features training set 
1648: numpy array shape n_samples target values 
1649: returns x_new numpy array shape n_samples n_features_new transformed array 
1650: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
1651: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform ridge_alpha none least squares projection data onto sparse components avoid instability issues case system determined regularization applied ridge regression via ridge_alpha parameter note sparse pca components orthogonality enforced pca hence one cannot use simple linear projection 
1652: parameters array shape n_samples n_features test data transformed must number features data used train model 
1653: ridge_alpha oat default amount ridge shrinkage apply order improve conditioning 
1654: returns x_new array shape n_samples n_components transformed data 
1655: reference scikit learn user guide release sklearn decomposition sparsecoder class sklearn decomposition sparsecoder dictionary transform_n_nonzero_coefs none form_alpha none split_sign false n_jobs transform_algorithm omp trans sparse coding finds sparse representation data xed precomputed dictionary row result solution sparse coding problem goal sparse array code code dictionary parameters dictionary array n_atoms n_features dictionary atoms used sparse coding lines assumed normalized unit norm 
1656: transform_algorithm lasso_lars lasso_cd lars omp threshold algorithm used transform data lars uses least angle regression method lin ear_model lars_path lasso_lars uses lars compute lasso solution lasso_cd uses coordinate descent method compute lasso solution linear_model lasso lasso_lars faster estimated components sparse omp uses orthogonal matching pursuit estimate sparse solution threshold squashes zero coef cients less alpha projection dictionary transform_n_nonzero_coefs int n_features default number nonzero coefcients target column solution used algorithm lars algorithm omp overridden alpha omp case 
1657: transform_alpha oat default algorithm lasso_lars algorithm lasso_cd alpha penalty applied norm algorithm threshold alpha absolute value threshold coefcients squashed zero algorithm omp alpha toler ance parameter value reconstruction error targeted case overrides n_nonzero_coefs 
1658: split_sign bool false default whether split sparse feature vector concatenation negative part positive part improve performance downstream classiers 
1659: n_jobs int number parallel jobs run see also dictionarylearning minibatchsparsepca sparse_encode minibatchdictionarylearning sparsepca attributes components_ array n_atoms n_features unchanged dictionary atoms chapter user guide scikit learn user guide release methods fit fit_transform get_params deep set_params params transform nothing return estimator unchanged fit data transform get parameters estimator set parameters estimator encode data sparse combination dictionary atoms 
1660: __init__ dictionary form_alpha none split_sign false n_jobs transform_algorithm omp transform_n_nonzero_coefs none trans fit none nothing return estimator unchanged method implement usual api hence work pipelines 
1661: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
1662: parameters numpy array shape n_samples n_features training set 
1663: numpy array shape n_samples target values 
1664: returns x_new numpy array shape n_samples n_features_new transformed array 
1665: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
1666: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform none encode data sparse combination dictionary atoms coding method determined object parameter transform_algorithm 
1667: parameters array shape n_samples n_features reference scikit learn user guide release test data transformed must number features data used train model 
1668: returns x_new array shape n_samples n_components transformed data sklearn decomposition dictionarylearning class sklearn decomposition dictionarylearning n_atoms tol transform_algorithm omp form_n_nonzero_coefs none form_alpha none code_init none bose false dom_state none max_iter t_algorithm lars trans trans n_jobs ver ran alpha dict_init none split_sign false dictionary learning finds dictionary set atoms best used represent data using sparse code solves optimization problem argmin alpha v_k n_atoms parameters n_atoms int number dictionary elements extract alpha int sparsity controlling parameter max_iter int maximum number iterations perform tol oat tolerance numerical error t_algorithm lars lars uses least angle regression method solve lasso problem lin ear_model lars_path uses coordinate descent method compute lasso lution linear_model lasso lars faster estimated components sparse 
1669: transform_algorithm lasso_lars lasso_cd lars omp threshold algorithm used transform data lars uses least angle regression method lin ear_model lars_path lasso_lars uses lars compute lasso solution lasso_cd uses coordinate descent method compute lasso solution linear_model lasso lasso_lars faster estimated components sparse omp uses orthogonal matching pursuit estimate sparse solution threshold squashes zero coef cients less alpha projection dictionary transform_n_nonzero_coefs int n_features default chapter user guide scikit learn user guide release number nonzero coefcients target column solution used algorithm lars algorithm omp overridden alpha omp case 
1670: transform_alpha oat default algorithm lasso_lars algorithm lasso_cd alpha penalty applied norm algorithm threshold alpha absolute value threshold coefcients squashed zero algorithm omp alpha toler ance parameter value reconstruction error targeted case overrides n_nonzero_coefs 
1671: split_sign bool false default whether split sparse feature vector concatenation negative part positive part improve performance downstream classiers 
1672: n_jobs int number parallel jobs run code_init array shape n_samples n_atoms initial value code warm restart dict_init array shape n_atoms n_features initial values dictionary warm restart verbose degree verbosity printed output random_state int randomstate pseudo number generator state used random sampling 
1673: see also sparsecoder minibatchdictionarylearning sparsepca minibatchsparsepca notes references mairal bach http www ens sierra pdfs icml09 pdf ponce sapiro online dictionary learning sparse coding attributes components_ error_ array n_atoms n_features array dictionary atoms extracted data vector errors iteration methods fit fit_transform fit model data fit data transform continued next page reference scikit learn user guide release table continued previous page get_params deep set_params params transform get parameters estimator set parameters estimator encode data sparse combination dictionary atoms 
1674: __init__ n_atoms alpha form_algorithm omp n_jobs dom_state none code_init none dict_init none verbose false max_iter tol transform_n_nonzero_coefs none t_algorithm lars trans transform_alpha none split_sign false ran fit none fit model data 
1675: parameters array like shape n_samples n_features training vector n_samples number samples n_features num ber features returns self object returns object fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
1676: parameters numpy array shape n_samples n_features training set 
1677: numpy array shape n_samples target values 
1678: returns x_new numpy array shape n_samples n_features_new transformed array 
1679: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
1680: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self chapter user guide scikit learn user guide release transform none encode data sparse combination dictionary atoms coding method determined object parameter transform_algorithm 
1681: parameters array shape n_samples n_features test data transformed must number features data used train model 
1682: returns x_new array shape n_samples n_components transformed data sklearn decomposition minibatchdictionarylearning class sklearn decomposition minibatchdictionarylearning n_atoms n_iter chunk_size pha t_algorithm lars n_jobs shufe true dict_init none trans form_algorithm omp trans form_n_nonzero_coefs none transform_alpha none ver bose false split_sign false random_state none mini batch dictionary learning finds dictionary set atoms best used represent data using sparse code solves optimization problem argmin alpha v_k n_atoms parameters n_atoms int number dictionary elements extract alpha int sparsity controlling parameter n_iter int total number iterations perform t_algorithm lars lars uses least angle regression method solve lasso problem lin ear_model lars_path uses coordinate descent method compute lasso lution linear_model lasso lars faster estimated components sparse 
1683: transform_algorithm lasso_lars lasso_cd lars omp threshold algorithm used transform data lars uses least angle regression method lin ear_model lars_path lasso_lars uses lars compute lasso solution lasso_cd uses coordinate descent method compute lasso solution linear_model lasso lasso_lars faster estimated components sparse omp uses orthogonal reference scikit learn user guide release matching pursuit estimate sparse solution threshold squashes zero coef cients less alpha projection dictionary transform_n_nonzero_coefs int n_features default number nonzero coefcients target column solution used algorithm lars algorithm omp overridden alpha omp case 
1684: transform_alpha oat default algorithm lasso_lars algorithm lasso_cd alpha penalty applied norm algorithm threshold alpha absolute value threshold coefcients squashed zero algorithm omp alpha toler ance parameter value reconstruction error targeted case overrides n_nonzero_coefs 
1685: split_sign bool false default whether split sparse feature vector concatenation negative part positive part improve performance downstream classiers 
1686: n_jobs int number parallel jobs run dict_init array shape n_atoms n_features initial value dictionary warm restart scenarios verbose degree verbosity printed output chunk_size int number samples mini batch shufe bool whether shufe samples forming batches random_state int randomstate pseudo number generator state used random sampling 
1687: see also sparsecoder dictionarylearning sparsepca minibatchsparsepca notes references mairal bach http www ens sierra pdfs icml09 pdf ponce sapiro online dictionary learning sparse coding attributes components_ array n_atoms n_features components extracted data chapter user guide methods scikit learn user guide release fit fit_transform get_params deep partial_fit iter_offset updates model using data mini batch set_params params transform fit model data fit data transform get parameters estimator set parameters estimator encode data sparse combination dictionary atoms 
1688: __init__ n_atoms alpha n_iter t_algorithm lars n_jobs chunk_size shuf true dict_init none transform_algorithm omp transform_n_nonzero_coefs none transform_alpha none verbose false split_sign false random_state none fit none fit model data 
1689: parameters array like shape n_samples n_features training vector n_samples number samples n_features num ber features returns self object returns instance 
1690: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
1691: parameters numpy array shape n_samples n_features training set 
1692: numpy array shape n_samples target values 
1693: returns x_new numpy array shape n_samples n_features_new transformed array 
1694: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
1695: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1696: partial_fit none iter_offset updates model using data mini batch 
1697: parameters array like shape n_samples n_features reference scikit learn user guide release training vector n_samples number samples n_features num ber features returns self object returns instance 
1698: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform none encode data sparse combination dictionary atoms coding method determined object parameter transform_algorithm 
1699: parameters array shape n_samples n_features test data transformed must number features data used train model 
1700: returns x_new array shape n_samples n_components transformed data decomposition fastica n_components decomposition dict_learning n_atoms alpha decomposition dict_learning_online decomposition sparse_encode dictionary perform fast independent component analysis solves dictionary learning matrix factorization problem solves dictionary learning matrix factorization problem online sparse coding sklearn decomposition fastica sklearn decomposition fastica n_components none algorithm parallel whiten true fun_args max_iter fun logcosh tol w_init none random_state none fun_prime perform fast independent component analysis 
1701: parameters array like shape n_samples n_features training vector n_samples number samples n_features number features 
1702: n_components int optional number components extract none dimension reduction performed 
1703: algorithm parallel deation optional apply parallel deational fastica algorithm 
1704: whiten boolean optional true perform initial whitening data false data assumed already preprocessed centered normed white otherwise get incorrect results case parameter n_components ignored 
1705: fun string function optional chapter user guide scikit learn user guide release functional form function used approximation neg entropy could either logcosh exp cube also provide function case derivative provided via argument fun_prime fun_prime empty string function optional see fun 
1706: fun_args dictionary optional empty fun logcosh fun_args take value alpha max_iter int optional maximum number iterations perform tol oat optional positive scalar giving tolerance mixing matrix considered converged w_init n_components n_components array optional initial mixing array dimension comp comp none default array normal used source_only boolean optional true sources matrix returned random_state int randomstate pseudo number generator state used random sampling 
1707: returns n_components array none whiten true pre whitening matrix projects data onto rst comp principal components whiten false none 
1708: n_components n_components array estimated mixing matrix mixing matrix obtained dot n_components array estimated source matrix notes data matrix considered linear combination non gaussian independent components columns contain independent components linear mixing matrix short ica attempts mix data estimating mixing matrix implementation originally made data shape n_features n_samples input transposed algorithm applied makes slightly faster fortran ordered input implemented using fastica hyvarinen oja independent component analysis algorithms applications neural networks reference scikit learn user guide release sklearn decomposition dict_learning sklearn decomposition dict_learning code_init none random_state none n_atoms method lars n_jobs callback none solves dictionary learning matrix factorization problem finds best dictionary corresponding sparse code approximating data matrix solving alpha max_iter tol dict_init none verbose false argmin alpha v_k n_atoms dictionary sparse code 
1709: parameters array shape n_samples n_features data matrix n_atoms int number dictionary atoms extract 
1710: alpha int sparsity controlling parameter 
1711: max_iter int maximum number iterations perform 
1712: tol oat tolerance stopping condition 
1713: method lars lars uses least angle regression method solve lasso problem lin ear_model lars_path uses coordinate descent method compute lasso lution linear_model lasso lars faster estimated components sparse 
1714: n_jobs int number parallel jobs run autodetect 
1715: dict_init array shape n_atoms n_features initial value dictionary warm restart scenarios 
1716: code_init array shape n_samples n_atoms initial value sparse code warm restart scenarios 
1717: callback callable gets invoked every iterations 
1718: verbose degree output procedure print 
1719: random_state int randomstate pseudo number generator state used random sampling 
1720: returns code array shape n_samples n_atoms chapter user guide sparse code factor matrix factorization dictionary array shape n_atoms n_features dictionary factor matrix factorization 
1721: errors array vector errors iteration 
1722: see also dict_learning_online sparsepca minibatchsparsepca dictionarylearning sklearn decomposition dict_learning_online scikit learn user guide release minibatchdictionarylearning sklearn decomposition dict_learning_online n_atoms alpha dict_init none turn_code true back none shufe true iter_offset random_state none chunk_size n_jobs n_iter call verbose false method lars solves dictionary learning matrix factorization problem online finds best dictionary corresponding sparse code approximating data matrix solving argmin alpha v_k n_atoms dictionary sparse code accomplished repeatedly iterating mini batches slicing input data 
1723: parameters array shape n_samples n_features data matrix n_atoms int number dictionary atoms extract alpha int sparsity controlling parameter n_iter int number iterations perform return_code boolean whether also return code dictionary dict_init array shape n_atoms n_features initial value dictionary warm restart scenarios callback callable gets invoked every iterations chunk_size int number samples take batch verbose reference scikit learn user guide release degree output procedure print shufe boolean whether shufe data splitting batches n_jobs int number parallel jobs run autodetect 
1724: method lars lars uses least angle regression method solve lasso problem lin ear_model lars_path uses coordinate descent method compute lasso lution linear_model lasso lars faster estimated components sparse 
1725: iter_offset int default number previous iterations completed dictionary used initialization random_state int randomstate pseudo number generator state used random sampling 
1726: returns code array shape n_samples n_atoms sparse code returned return_code true dictionary array shape n_atoms n_features solutions dictionary learning problem see also dict_learning dictionarylearning minibatchdictionarylearning sparsepca minibatchsparsepca sklearn decomposition sparse_encode sklearn decomposition sparse_encode dictionary rithm lasso_lars pha none init none max_iter n_jobs copy_gram none gram none cov none n_nonzero_coefs none algo copy_cov true sparse coding row result solution sparse coding problem goal sparse array code code dictionary parameters array shape n_samples n_features data matrix dictionary array shape n_atoms n_features dictionary matrix solve sparse coding data algorithms assume normalized rows meaningful output 
1727: gram array shape n_atoms n_atoms precomputed gram matrix dictionary dictionary cov array shape n_atoms n_samples chapter user guide scikit learn user guide release precomputed covariance dictionary algorithm lasso_lars lasso_cd lars omp threshold lars uses least angle regression method linear_model lars_path lasso_lars uses lars compute lasso solution lasso_cd uses coordinate descent method compute lasso solution linear_model lasso lasso_lars faster timated components sparse omp uses orthogonal matching pursuit estimate sparse solution threshold squashes zero coefcients less alpha projection dictionary n_nonzero_coefs int n_features default number nonzero coefcients target column solution used algorithm lars algorithm omp overridden alpha omp case 
1728: alpha oat default algorithm lasso_lars algorithm lasso_cd alpha penalty applied norm algorithm threhold alpha absolute value threshold coefcients squashed zero algorithm omp alpha toler ance parameter value reconstruction error targeted case overrides n_nonzero_coefs 
1729: init array shape n_samples n_atoms initialization value sparse codes used algorithm lasso_cd 
1730: max_iter int default maximum number iterations perform algorithm lasso_cd 
1731: copy_cov boolean optional whether copy precomputed covariance matrix false may overwritten 
1732: n_jobs int optional number parallel jobs run 
1733: returns code array shape n_samples n_atoms sparse codes see also sklearn linear_model lars_path sklearn linear_model lasso sparsecoder sklearn linear_model orthogonal_mp sklearn ensemble ensemble methods sklearn ensemble module includes ensemble based methods classication regression user guide see ensemble methods section details 
1734: ensemble randomforestclassifier ensemble randomforestregressor ensemble extratreesclassifier ensemble extratreesregressor n_estimators ensemble gradientboostingclassifier loss gradient boosting classication continued next page random forest classier random forest regressor extra trees classier extra trees regressor 
1735: reference scikit learn user guide release ensemble gradientboostingregressor loss gradient boosting regression 
1736: table continued previous page sklearn ensemble randomforestclassier class sklearn ensemble randomforestclassifier n_estimators criterion gini min_samples_split max_depth none min_samples_leaf min_density max_features auto bootstrap true com pute_importances false oob_score false n_jobs random_state none verbose random forest classier random forest meta estimator number classical decision trees various sub samples dataset use averaging improve predictive accuracy control tting 
1737: parameters n_estimators integer optional default number trees forest 
1738: criterion string optional default gini function measure quality split supported criteria gini gini impurity entropy information gain note parameter tree specic 
1739: max_depth integer none optional default none maximum depth tree none nodes expanded leaves pure leaves contain less min_samples_split samples note parameter tree specic 
1740: min_samples_split integer optional default minimum number samples required split internal node note parame ter tree specic 
1741: min_samples_leaf integer optional default minimum number samples newly created leaves split discarded split one leaves would contain less min_samples_leaf samples note parameter tree specic 
1742: min_density oat optional default parameter controls trade optimization heuristic controls minimum density sample_mask fraction samples mask density falls threshold mask recomputed input data packed results min_density equals one partitions always represented data copying copies original data otherwise partitions represented bit masks aka sample masks note parameter tree specic 
1743: max_features int string none optional default auto number features consider looking best split max_features sqrt n_features classication tasks auto max_features n_features regression problems sqrt max_features sqrt n_features log2 max_features log2 n_features none max_features n_features 
1744: chapter user guide scikit learn user guide release note parameter tree specic 
1745: bootstrap boolean optional default true whether bootstrap samples used building trees 
1746: compute_importances boolean optional default true whether computed feature_importances_ attribute calling 
1747: importances feature stored oob_score bool whether use bag samples estimate generalization error 
1748: n_jobs integer optional default number jobs run parallel number jobs set number cores 
1749: random_state int randomstate instance none optional default none int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used random 
1750: verbose int optional default controlls verbosity tree building process 
1751: see also decisiontreeclassifier extratreesclassifier references r59 attributes fea ture_importances_ oob_score_ array shape n_features oat oob_decision_function_array shape n_samples n_classes feature importances higher important feature score training dataset obtained using bag estimate decision function computed bag estimate training set 
1752: methods fit fit_transform get_params deep predict predict_log_proba predict_proba score build forest trees training set fit data transform get parameters estimator predict class predict class log probabilities predict class probabilities returns mean accuracy given test data labels continued next page reference scikit learn user guide release set_params params transform threshold reduce important features 
1753: table continued previous page set parameters estimator 
1754: __init__ n_estimators criterion gini max_depth none min_samples_leaf min_density max_features auto bootstrap true pute_importances false oob_score false n_jobs random_state none verbose min_samples_split com fit build forest trees training set 
1755: parameters array like shape n_samples n_features training input samples 
1756: array like shape n_samples target values integers correspond classes classication real numbers regression 
1757: returns self object returns self 
1758: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
1759: parameters numpy array shape n_samples n_features training set 
1760: numpy array shape n_samples target values 
1761: returns x_new numpy array shape n_samples n_features_new transformed array 
1762: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
1763: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1764: predict predict class predicted class input sample computed majority prediction trees forest 
1765: parameters array like shape n_samples n_features input samples 
1766: returns array shape n_samples chapter user guide scikit learn user guide release predicted classes 
1767: predict_log_proba predict class log probabilities predicted class log probabilities input sample computed mean predicted class log probabilities trees forest 
1768: parameters array like shape n_samples n_features input samples 
1769: returns array shape n_samples class log probabilities input samples classes ordered arithmetical order predict_proba predict class probabilities predicted class probabilities input sample computed mean predicted class probabilities trees forest 
1770: parameters array like shape n_samples n_features input samples 
1771: returns array shape n_samples class probabilities input samples classes ordered arithmetical order 
1772: score returns mean accuracy given test data labels 
1773: parameters array like shape n_samples n_features training set 
1774: array like shape n_samples labels 
1775: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform threshold none reduce important features 
1776: parameters array scipy sparse matrix shape n_samples n_features input samples 
1777: threshold string oat none optional default none threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor mean may also used none available object attribute threshold used otherwise mean used default 
1778: reference scikit learn user guide release returns x_r array shape n_samples n_selected_features input samples selected features 
1779: sklearn ensemble randomforestregressor class sklearn ensemble randomforestregressor n_estimators criterion mse min_samples_split max_depth none min_density min_samples_leaf max_features auto bootstrap true com pute_importances false oob_score false n_jobs random_state none verbose random forest regressor random forest meta estimator number classical decision trees various sub samples dataset use averaging improve predictive accuracy control tting 
1780: parameters n_estimators integer optional default number trees forest 
1781: criterion string optional default mse function measure quality split supported criterion mse mean squared error note parameter tree specic 
1782: max_depth integer none optional default none maximum depth tree none nodes expanded leaves pure leaves contain less min_samples_split samples note parameter tree specic 
1783: min_samples_split integer optional default minimum number samples required split internal node note parame ter tree specic 
1784: min_samples_leaf integer optional default minimum number samples newly created leaves split discarded split one leaves would contain less min_samples_leaf samples note parameter tree specic 
1785: min_density oat optional default parameter controls trade optimization heuristic controls minimum density sample_mask fraction samples mask density falls threshold mask recomputed input data packed results min_density equals one partitions always represented data copying copies original data otherwise partitions represented bit masks aka sample masks note parameter tree specic 
1786: max_features int string none optional default auto number features consider looking best split max_features sqrt n_features classication tasks auto max_features n_features regression problems sqrt max_features sqrt n_features log2 max_features log2 n_features 
1787: chapter user guide scikit learn user guide release none max_features n_features 
1788: note parameter tree specic 
1789: bootstrap boolean optional default true whether bootstrap samples used building trees 
1790: compute_importances boolean optional default true whether computed feature_importances_ attribute calling 
1791: importances feature stored oob_score bool whether use bag samples estimate generalization error 
1792: n_jobs integer optional default number jobs run parallel number jobs set number cores 
1793: random_state int randomstate instance none optional default none int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used random 
1794: verbose int optional default controlls verbosity tree building process 
1795: see also decisiontreeregressor extratreesregressor references r60 attributes fea ture_importances_ oob_score_ array shape n_features oat oob_prediction_ array shape n_samples feature mportances higher important feature score training dataset obtained using bag estimate prediction computed bag estimate training set 
1796: methods fit fit_transform get_params deep predict score build forest trees training set fit data transform get parameters estimator predict regression target returns coefcient determination prediction continued next page reference scikit learn user guide release table continued previous page set_params params transform threshold reduce important features 
1797: set parameters estimator 
1798: __init__ n_estimators criterion mse max_depth none min_samples_leaf min_density max_features auto bootstrap true pute_importances false oob_score false n_jobs random_state none verbose min_samples_split com fit build forest trees training set 
1799: parameters array like shape n_samples n_features training input samples 
1800: array like shape n_samples target values integers correspond classes classication real numbers regression 
1801: returns self object returns self 
1802: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
1803: parameters numpy array shape n_samples n_features training set 
1804: numpy array shape n_samples target values 
1805: returns x_new numpy array shape n_samples n_features_new transformed array 
1806: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
1807: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1808: predict predict regression target predicted regression target input sample computed mean predicted regression targets trees forest 
1809: parameters array like shape n_samples n_features input samples 
1810: chapter user guide scikit learn user guide release returns array shape n_samples predicted values 
1811: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
1812: parameters array like shape n_samples n_features training set 
1813: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform threshold none reduce important features 
1814: parameters array scipy sparse matrix shape n_samples n_features input samples 
1815: threshold string oat none optional default none threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor mean may also used none available object attribute threshold used otherwise mean used default 
1816: returns x_r array shape n_samples n_selected_features input samples selected features 
1817: sklearn ensemble extratreesclassier class sklearn ensemble extratreesclassifier n_estimators max_depth none min_samples_leaf max_features auto bootstrap false pute_importances false n_jobs random_state none verbose criterion gini min_samples_split min_density com oob_score false extra trees classier class implements meta estimator number randomized decision trees extra trees various sub samples dataset use averaging improve predictive accuracy control tting 
1818: parameters n_estimators integer optional default number trees forest 
1819: reference scikit learn user guide release criterion string optional default gini function measure quality split supported criteria gini gini impurity entropy information gain note parameter tree specic 
1820: max_depth integer none optional default none maximum depth tree none nodes expanded leaves pure leaves contain less min_samples_split samples note parameter tree specic 
1821: min_samples_split integer optional default minimum number samples required split internal node note parame ter tree specic 
1822: min_samples_leaf integer optional default minimum number samples newly created leaves split discarded split one leaves would contain less min_samples_leaf samples note parameter tree specic 
1823: min_density oat optional default parameter controls trade optimization heuristic controls minimum density sample_mask fraction samples mask density falls threshold mask recomputed input data packed results min_density equals one partitions always represented data copying copies original data otherwise partitions represented bit masks aka sample masks note parameter tree specic 
1824: max_features int string none optional default auto number features consider looking best split 
1825: max_features sqrt n_features classication tasks auto max_features n_features regression problems sqrt max_features sqrt n_features log2 max_features log2 n_features none max_features n_features 
1826: note parameter tree specic 
1827: bootstrap boolean optional default false whether bootstrap samples used building trees 
1828: compute_importances boolean optional default true whether computed feature_importances_ attribute calling 
1829: importances feature stored oob_score bool whether use bag samples estimate generalization error 
1830: n_jobs integer optional default number jobs run parallel number jobs set number cores 
1831: random_state int randomstate instance none optional default none chapter user guide scikit learn user guide release int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used random 
1832: verbose int optional default controlls verbosity tree building process 
1833: see also sklearn tree extratreeclassifierbase classier ensemble randomforestclassifierensemble classier based trees optimal splits 
1834: references r57 attributes fea ture_importances_ oob_score_ array shape n_features oat oob_decision_function_array shape n_samples n_classes feature mportances higher important feature score training dataset obtained using bag estimate decision function computed bag estimate training set 
1835: methods fit fit_transform get_params deep predict predict_log_proba predict_proba score set_params params transform threshold reduce important features 
1836: build forest trees training set fit data transform get parameters estimator predict class predict class log probabilities predict class probabilities returns mean accuracy given test data labels set parameters estimator 
1837: __init__ n_estimators criterion gini max_depth none min_samples_leaf min_density max_features auto bootstrap false pute_importances false oob_score false n_jobs random_state none verbose min_samples_split com fit build forest trees training set 
1838: parameters array like shape n_samples n_features training input samples 
1839: array like shape n_samples target values integers correspond classes classication real numbers regression 
1840: reference scikit learn user guide release returns self object returns self 
1841: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
1842: parameters numpy array shape n_samples n_features training set 
1843: numpy array shape n_samples target values 
1844: returns x_new numpy array shape n_samples n_features_new transformed array 
1845: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
1846: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1847: predict predict class predicted class input sample computed majority prediction trees forest 
1848: parameters array like shape n_samples n_features input samples 
1849: returns array shape n_samples predicted classes 
1850: predict_log_proba predict class log probabilities predicted class log probabilities input sample computed mean predicted class log probabilities trees forest 
1851: parameters array like shape n_samples n_features input samples 
1852: returns array shape n_samples class log probabilities input samples classes ordered arithmetical order predict_proba predict class probabilities 
1853: chapter user guide scikit learn user guide release predicted class probabilities input sample computed mean predicted class probabilities trees forest 
1854: parameters array like shape n_samples n_features input samples 
1855: returns array shape n_samples class probabilities input samples classes ordered arithmetical order 
1856: score returns mean accuracy given test data labels 
1857: parameters array like shape n_samples n_features training set 
1858: array like shape n_samples labels 
1859: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform threshold none reduce important features 
1860: parameters array scipy sparse matrix shape n_samples n_features input samples 
1861: threshold string oat none optional default none threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor mean may also used none available object attribute threshold used otherwise mean used default 
1862: returns x_r array shape n_samples n_selected_features input samples selected features 
1863: sklearn ensemble extratreesregressor class sklearn ensemble extratreesregressor n_estimators max_depth none min_samples_leaf max_features auto pute_importances false n_jobs random_state none verbose criterion mse min_samples_split min_density com oob_score false bootstrap false extra trees regressor class implements meta estimator number randomized decision trees extra trees various sub samples dataset use averaging improve predictive accuracy control tting 
1864: reference scikit learn user guide release parameters n_estimators integer optional default number trees forest 
1865: criterion string optional default mse function measure quality split supported criterion mse mean squared error note parameter tree specic 
1866: max_depth integer none optional default none none nodes expanded leaves maximum depth tree pure leaves contain less min_samples_split samples note parameter tree specic 
1867: min_samples_split integer optional default minimum number samples required split internal node note parame ter tree specic 
1868: min_samples_leaf integer optional default minimum number samples newly created leaves split discarded split one leaves would contain less min_samples_leaf samples note parameter tree specic 
1869: min_density oat optional default parameter controls trade optimization heuristic controls minimum density sample_mask fraction samples mask density falls threshold mask recomputed input data packed results min_density equals one partitions always represented data copying copies original data otherwise partitions represented bit masks aka sample masks note parameter tree specic 
1870: max_features int string none optional default auto number features consider looking best split max_features sqrt n_features classication tasks auto max_features n_features regression problems sqrt max_features sqrt n_features log2 max_features log2 n_features none max_features n_features 
1871: note parameter tree specic 
1872: bootstrap boolean optional default false whether bootstrap samples used building trees note parameter tree specic 
1873: compute_importances boolean optional default true computed whether feature_importances_ attribute calling 
1874: importances feature stored oob_score bool whether use bag samples estimate generalization error 
1875: n_jobs integer optional default chapter user guide scikit learn user guide release number jobs run parallel number jobs set number cores 
1876: random_state int randomstate instance none optional default none int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used random 
1877: verbose int optional default controlls verbosity tree building process 
1878: see also sklearn tree extratreeregressorbase estimator ensemble randomforestregressorensemble regressor using trees optimal splits 
1879: references r58 attributes fea ture_importances_ oob_score_ array shape n_features oat oob_prediction_ array shape n_samples feature mportances higher important feature score training dataset obtained using bag estimate prediction computed bag estimate training set 
1880: methods fit fit_transform get_params deep predict score set_params params transform threshold reduce important features 
1881: build forest trees training set fit data transform get parameters estimator predict regression target returns coefcient determination prediction set parameters estimator 
1882: __init__ n_estimators criterion mse max_depth none min_samples_leaf min_density max_features auto bootstrap false pute_importances false oob_score false n_jobs random_state none verbose min_samples_split com fit build forest trees training set 
1883: parameters array like shape n_samples n_features training input samples 
1884: array like shape n_samples reference scikit learn user guide release target values integers correspond classes classication real numbers regression 
1885: returns self object returns self 
1886: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
1887: parameters numpy array shape n_samples n_features training set 
1888: numpy array shape n_samples target values 
1889: returns x_new numpy array shape n_samples n_features_new transformed array 
1890: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
1891: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1892: predict predict regression target predicted regression target input sample computed mean predicted regression targets trees forest 
1893: parameters array like shape n_samples n_features input samples 
1894: returns array shape n_samples predicted values 
1895: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
1896: parameters array like shape n_samples n_features training set 
1897: array like shape n_samples returns oat chapter user guide scikit learn user guide release set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform threshold none reduce important features 
1898: parameters array scipy sparse matrix shape n_samples n_features input samples 
1899: threshold string oat none optional default none threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor mean may also used none available object attribute threshold used otherwise mean used default 
1900: returns x_r array shape n_samples n_selected_features input samples selected features 
1901: sklearn ensemble gradientboostingclassier class sklearn ensemble gradientboostingclassifier loss deviance learn_rate subsam n_estimators ple min_samples_split min_samples_leaf max_depth init none random_state none gradient boosting classication builds additive model forward stage wise fashion allows optimization arbitrary differen tiable loss functions stage n_classes_ regression trees negative gradient binomial multinomial deviance loss function binary classication special case single regression tree induced 
1902: parameters loss deviance optional default deviance loss function optimized deviance refers deviance logistic regression classication probabilistic outputs refers least squares regression 
1903: learn_rate oat optional default learning rate shrinks contribution tree learn_rate trade learn_rate n_estimators 
1904: n_estimators int default number boosting stages perform gradient boosting fairly robust tting large number usually results better performance 
1905: max_depth integer optional default maximum depth individual regression estimators maximum depth limits number nodes tree tune parameter best performance best value depends interaction input variables 
1906: reference scikit learn user guide release min_samples_split integer optional default minimum number samples required split internal node 
1907: min_samples_leaf integer optional default minimum number samples required leaf node 
1908: subsample oat optional default fraction samples used tting individual base learners smaller results stochastic gradient boosting subsample interacts parameter n_estimators 
1909: see also sklearn tree decisiontreeclassifier randomforestclassifier references friedman greedy function approximation gradient boosting machine annals statistics vol 
1910: friedman stochastic gradient boosting hastie tibshirani friedman elements statistical learning springer 
1911: examples samples labels sklearn ensemble import gradientboostingclassifier gradientboostingclassifier fit samples labels print predict methods fit fit_stage x_argsorted y_pred get_params deep predict predict_proba score set_params params staged_decision_function fit gradient boosting model fit another stage n_classes_ trees boosting model get parameters estimator predict class predict class probabilities returns mean accuracy given test data labels set parameters estimator compute decision function 
1912: __init__ loss deviance learn_rate n_estimators subsample min_samples_split min_samples_leaf max_depth init none random_state none fit fit gradient boosting model 
1913: parameters array like shape n_samples n_features training vectors n_samples number samples n_features num chapter user guide scikit learn user guide release ber features use fortran style avoid memory copies 
1914: array like shape n_samples target values integers classication real numbers regression classication labels must correspond classes n_classes_ returns self object returns self 
1915: fit_stage x_argsorted y_pred sample_mask fit another stage n_classes_ trees boosting model 
1916: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1917: predict predict class 
1918: parameters array like shape n_samples n_features input samples 
1919: returns array shape n_samples predicted classes 
1920: predict_proba predict class probabilities 
1921: parameters array like shape n_samples n_features input samples 
1922: returns array shape n_samples class probabilities input samples classes ordered arithmetical order 
1923: score returns mean accuracy given test data labels 
1924: parameters array like shape n_samples n_features training set 
1925: array like shape n_samples labels 
1926: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self reference scikit learn user guide release staged_decision_function compute decision function method allows monitoring determine error testing set stage 
1927: parameters array like shape n_samples n_features input samples 
1928: returns array shape n_samples n_classes decision function input samples classes ordered arithmetical order regression binary classication special cases n_classes 
1929: sklearn ensemble gradientboostingregressor class sklearn ensemble gradientboostingregressor loss n_estimators ple min_samples_leaf init none random_state none learn_rate subsam min_samples_split max_depth gradient boosting regression builds additive model forward stage wise fashion allows optimization arbitrary differ entiable loss functions stage regression tree negative gradient given loss function 
1930: parameters loss lad optional default loss function optimized refers least squares regression lad least absolute deviation highly robust loss function soley based order information input variables 
1931: learn_rate oat optional default learning rate shrinks contribution tree learn_rate trade learn_rate n_estimators 
1932: n_estimators int default number boosting stages perform gradient boosting fairly robust tting large number usually results better performance 
1933: max_depth integer optional default maximum depth individual regression estimators maximum depth limits number nodes tree tune parameter best performance best value depends interaction input variables 
1934: min_samples_split integer optional default minimum number samples required split internal node 
1935: min_samples_leaf integer optional default minimum number samples required leaf node 
1936: subsample oat optional default fraction samples used tting individual base learners smaller results stochastic gradient boosting subsample interacts parameter n_estimators 
1937: chapter user guide scikit learn user guide release see also sklearn tree decisiontreeregressor randomforestregressor references friedman greedy function approximation gradient boosting machine annals statistics vol 
1938: friedman stochastic gradient boosting hastie tibshirani friedman elements statistical learning springer 
1939: examples samples labels sklearn ensemble import gradientboostingregressor gradientboostingregressor fit samples labels print predict 32806997e attributes fea ture_importances_ array shape n_features oob_score_ array shape n_estimators train_score_ array shape n_estimators methods feature importances higher important feature 
1940: score training dataset obtained using bag estimate score oob_score_ deviance loss model iteration bag sample score train_score_ deviance loss model iteration bag sample subsample deviance training data 
1941: fit fit_stage x_argsorted y_pred get_params deep predict score set_params params staged_decision_function staged_predict fit gradient boosting model fit another stage n_classes_ trees boosting model get parameters estimator predict regression target returns coefcient determination prediction set parameters estimator compute decision function predict regression target stage 
1942: __init__ loss min_samples_leaf max_depth init none random_state none learn_rate n_estimators subsample min_samples_split fit fit gradient boosting model 
1943: reference scikit learn user guide release parameters array like shape n_samples n_features training vectors n_samples number samples n_features num ber features use fortran style avoid memory copies 
1944: array like shape n_samples target values integers classication real numbers regression classication labels must correspond classes n_classes_ returns self object returns self 
1945: fit_stage x_argsorted y_pred sample_mask fit another stage n_classes_ trees boosting model 
1946: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1947: predict predict regression target 
1948: parameters array like shape n_samples n_features input samples 
1949: returns array shape n_samples predicted values 
1950: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
1951: parameters array like shape n_samples n_features training set 
1952: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self staged_decision_function compute decision function method allows monitoring determine error testing set stage 
1953: parameters array like shape n_samples n_features chapter user guide scikit learn user guide release input samples 
1954: returns array shape n_samples n_classes decision function input samples classes ordered arithmetical order regression binary classication special cases n_classes 
1955: staged_predict predict regression target stage method allows monitoring determine error testing set stage 
1956: parameters array like shape n_samples n_features input samples 
1957: returns array shape n_samples predicted value input samples 
1958: sklearn feature_extraction feature extraction sklearn feature_extraction module deals feature extraction raw data currently includes methods extract features text images user guide see feature extraction section details 
1959: feature_extraction dictvectorizer dtype transforms lists feature value mappings vectors 
1960: sklearn feature_extraction dictvectorizer class sklearn feature_extraction dictvectorizer dtype type numpy oat64 separa tor sparse true transforms lists feature value mappings vectors transformer turns lists mappings dict like objects feature names feature values numpy arrays scipy sparse matrices use scikit learn estimators feature values strings transformer binary one hot aka one coding one boolean valued feature constructed possible string values feature take instance feature take values ham spam become two features output one signifying ham spam features occur sample mapping zero value resulting array matrix 
1961: parameters dtype callable optional type feature values passed numpy array scipy sparse matrix constructors dtype argument 
1962: separator string optional separator string used constructing new features one hot coding 
1963: sparse boolean optional whether transform produce scipy sparse matrices true default 
1964: reference scikit learn user guide release examples sklearn feature_extraction import dictvectorizer dictvectorizer sparse false foo bar foo baz fit_transform array inverse_transform true transform foo unseen_feature array bar foo baz foo methods fit fit_transform get_feature_names get_params deep inverse_transform dict_type transform array sparse matrix back feature mappings restrict support indices set_params params transform learn list feature name indices mappings learn list feature name indices mappings transform returns list feature names ordered indices get parameters estimator restrict features support set parameters estimator transform feature value dicts array sparse matrix 
1965: __init__ dtype type numpy oat64 separator sparse true fit none learn list feature name indices mappings 
1966: parameters mapping iterable mappings dict mapping feature names arbitrary python objects feature values strings convertible dtype 
1967: ignored returns self fit_transform none learn list feature name indices mappings transform like followed transform 
1968: parameters mapping iterable mappings dict mapping feature names arbitrary python objects feature values strings convertible dtype 
1969: ignored returns array sparse matrix feature vectors always 
1970: get_feature_names returns list feature names ordered indices 
1971: chapter user guide scikit learn user guide release one coding applied categorical features include constructed feature names original ones 
1972: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
1973: inverse_transform dict_type type dict transform array sparse matrix back feature mappings must produced dictvectorizers transform t_transform method may passed transformers preserve number features order case one hot one coding constructed feature names values returned rather original ones 
1974: parameters array like sparse matrix shape n_samples n_features sample matrix 
1975: dict_type callable optional constructor feature mappings must conform collections mapping api 
1976: returns list dict_type objects length n_samples feature mappings samples 
1977: restrict support indices false restrict features support 
1978: parameters support array like boolean mask list indices returned get_support member feature lectors 
1979: indices boolean optional whether support list indices 
1980: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform none transform feature value dicts array sparse matrix named features encountered t_transform silently ignored 
1981: parameters mapping iterable mappings length n_samples dict mapping feature names arbitrary python objects feature values strings convertible dtype 
1982: ignored returns array sparse matrix reference scikit learn user guide release feature vectors always 
1983: images sklearn feature_extraction image submodule gathers utilities extract features images 
1984: feature_extraction image img_to_graph img feature_extraction image grid_to_graph n_x n_y feature_extraction image extract_patches_2d feature_extraction image reconstruct_from_patches_2d reconstruct image patches feature_extraction image patchextractor extracts patches collection images graph pixel pixel gradient connections graph pixel pixel connections reshape image collection patches sklearn feature_extraction image img_to_graph sklearn feature_extraction image img_to_graph img mask none return_as class scipy sparse coo coo_matrix dtype none graph pixel pixel gradient connections edges weighted gradient values parameters img ndarray image mask ndarray booleans optional optional mask image consider part pixels 
1985: return_as ndarray sparse matrix class optional class use build returned adjacency matrix 
1986: dtype none dtype optional data returned sparse matrix default dtype img sklearn feature_extraction image grid_to_graph sklearn feature_extraction image grid_to_graph n_x n_y n_z return_as class mask none scipy sparse coo coo_matrix dtype type int graph pixel pixel connections edges exist voxels connected 
1987: parameters n_x int dimension axis n_y int dimension axis n_z int optional default dimension axis mask ndarray booleans optional chapter user guide scikit learn user guide release optional mask image consider part pixels 
1988: return_as ndarray sparse matrix class optional class use build returned adjacency matrix 
1989: dtype dtype optional default int data returned sparse matrix default int sklearn feature_extraction image extract_patches_2d sklearn feature_extraction image extract_patches_2d image max_patches none dom_state none patch_size ran reshape image collection patches resulting patches allocated dedicated array 
1990: parameters image array shape image_height image_width image_height image_width n_channels original image data color images last dimension species channel rgb image would n_channels 
1991: patch_size tuple ints patch_height patch_width dimensions one patch max_patches integer oat optional default none maximum number patches extract max_patches oat taken proportion total number patches 
1992: random_state int randomstate pseudo number generator state used random sampling use max_patches none 
1993: returns patches array shape n_patches patch_height patch_width n_patches patch_height patch_width n_channels collection patches extracted image n_patches either max_patches total number patches extracted 
1994: examples sklearn feature_extraction import image one_image arange reshape one_image array patches image extract_patches_2d one_image patches shape patches array patches array reference scikit learn user guide release patches array sklearn feature_extraction image reconstruct_from_patches_2d sklearn feature_extraction image reconstruct_from_patches_2d patches reconstruct image patches patches assumed overlap image constructed lling patches left right top bottom averaging overlapping regions 
1995: age_size parameters patches array shape n_patches patch_height patch_width n_patches patch_height patch_width n_channels complete set patches patches contain colour information channels indexed along last dimension rgb patches would n_channels 
1996: image_size tuple ints image_height image_width image_height image_width n_channels size image recon structed returns image array shape image_size reconstructed image sklearn feature_extraction image patchextractor class sklearn feature_extraction image patchextractor patch_size max_patches none random_state none extracts patches collection images parameters patch_size tuple ints patch_height patch_width dimensions one patch max_patches integer oat optional default none maximum number patches per image extract max_patches oat taken mean proportion total number patches 
1997: random_state int randomstate pseudo number generator state used random sampling 
1998: methods fit get_params deep set_params params transform nothing return estimator unchanged get parameters estimator set parameters estimator transforms image samples matrix patch data 
1999: __init__ patch_size max_patches none random_state none chapter user guide scikit learn user guide release fit none nothing return estimator unchanged method implement usual api hence work pipelines 
2000: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform transforms image samples matrix patch data 
2001: parameters array shape n_samples image_height image_width n_samples image_height image_width n_channels array images extract patches color images last dimension species channel rgb image would n_channels 
2002: returns patches array shape n_patches patch_height patch_width n_patches patch_height patch_width n_channels collection patches extracted images n_patches either n_samples max_patches total num ber patches extracted 
2003: text sklearn feature_extraction text submodule gathers utilities build feature vectors text doc uments 
2004: feature_extraction text countvectorizer convert collection raw documents matrix token counts feature_extraction text tfidftransformer transform count matrix normalized tfidf representation feature_extraction text tfidfvectorizer convert collection raw documents matrix idf features 
2005: reference scikit learn user guide release sklearn feature_extraction text countvectorizer class sklearn feature_extraction text countvectorizer input content charset utf charset_error strict low preproces tokenizer none strip_accents none ercase true sor none stop_words none ken_pattern min_n lyzer word max_features none ulary none dtype type long max_n ana max_df vocab binary false convert collection raw documents matrix token counts implementation produces sparse representation counts using scipy sparse coo_matrix provide priori dictionary use analyzer kind feature selection number features equal vocabulary size found analysing data default analyzer simple stop word ltering english 
2006: parameters input string lename content lename sequence passed argument expected list lenames need reading fetch raw content analyze sequence items must read method like object called fetch bytes memory otherwise input expected sequence strings bytes items expected analyzed directly 
2007: charset string utf default bytes les given analyze charset used decode 
2008: charset_error strict ignore replace instruction byte sequence given analyze contains characters given charset default strict meaning unicodedecodeerror raised values ignore replace 
2009: strip_accents ascii unicode none remove accents preprocessing step ascii fast method works characters direct ascii mapping unicode slightly slower method works characters none default nothing 
2010: analyzer string word char callable whether feature made word character grams callable passed used extract sequence features raw unpro cessed input 
2011: preprocessor callable none default override preprocessing string transformation stage preserving tokenizing grams generation steps 
2012: tokenizer callable none default chapter user guide scikit learn user guide release override string tokenization step preserving preprocessing grams generation steps 
2013: min_n integer lower boundary range values different grams extracted 
2014: max_n integer upper boundary range values different grams extracted values min_n max_n used 
2015: stop_words string english list none default string passed _check_stop_list appropriate stop list returned currently supported string value list list assumed contain stop words removed resulting tokens none stop words used max_df set value range automatically detect lter stop words based intra corpus document frequency terms 
2016: token_pattern string regular expression denoting constitutes token used tokenize word default regexp select tokens letters characters punctuation completely ignored always treated token separator 
2017: max_df oat range optional default building vocabulary ignore terms term frequency strictly higher given threshold corpus specic stop words parameter ignored vocabulary none 
2018: max_features optional none default none build vocabulary consider top max_features ordered term frequency across corpus parameter ignored vocabulary none 
2019: binary boolean false default true non zero counts set useful discrete probabilistic models model binary events rather integer counts 
2020: dtype type optional type matrix returned t_transform transform 
2021: methods build_analyzer build_preprocessor build_tokenizer decode doc fit raw_documents fit_transform raw_documents learn vocabulary dictionary return count vectors return callable handles preprocessing tokenization return function preprocess text tokenization return function split string sequence tokens decode input string unicode symbols learn vocabulary dictionary tokens raw documents reference scikit learn user guide release get_feature_names get_params deep get_stop_words inverse_transform set_params params transform raw_documents table continued previous page array mapping feature integer indicex feature name get parameters estimator build fetch effective stop words list return terms per document nonzero entries set parameters estimator extract token counts raw text documents using vocabulary tted one provided constructor 
2022: __init__ input content lowercase true ken_pattern min_n max_n max_features none vocabulary none binary false dtype type long strip_accents none analyzer word max_df charset utf preprocessor none charset_error strict tokenizer none stop_words none build_analyzer return callable handles preprocessing tokenization build_preprocessor return function preprocess text tokenization build_tokenizer return function split string sequence tokens decode doc decode input string unicode symbols decoding strategy depends vectorizer parameters 
2023: fit raw_documents none learn vocabulary dictionary tokens raw documents parameters raw_documents iterable iterable yields either str unicode objects returns self fit_transform raw_documents none learn vocabulary dictionary return count vectors efcient calling followed transform 
2024: parameters raw_documents iterable iterable yields either str unicode objects returns vectors array n_samples n_features get_feature_names array mapping feature integer indicex feature name get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2025: get_stop_words build fetch effective stop words list inverse_transform return terms per document nonzero entries 
2026: chapter user guide scikit learn user guide release parameters array sparse matrix shape n_samples n_features returns x_inv list arrays len n_samples list arrays terms 
2027: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform raw_documents extract token counts raw text documents using vocabulary tted one provided constructor 
2028: parameters raw_documents iterable iterable yields either str unicode objects returns vectors sparse matrix n_samples n_features sklearn feature_extraction text tdftransformer class sklearn feature_extraction text tfidftransformer norm use_idf true sublin smooth_idf true ear_tf false transform count matrix normalized tfidf representation means term frequency tfidf means term frequency times inverse document frequency com mon term weighting scheme information retrieval also found good use document classication goal using tfidf instead raw frequencies occurrence token given document scale impact tokens occur frequently given corpus hence empirically less informative features occur small fraction training corpus smart notation used class implements several tfidf variants always natural idf iff use_idf given otherwise normalization iff norm iff norm none 
2029: parameters norm none optional norm used normalize term vectors none normalization 
2030: use_idf boolean optional enable inverse document frequency reweighting 
2031: smooth_idf boolean optional smooth idf weights adding one document frequencies extra document seen containing every term collection exactly prevents zero divisions 
2032: sublinear_tf boolean optional apply sublinear scaling replace log 
2033: references yates2011 msr2008 reference scikit learn user guide release methods fit fit_transform get_params deep set_params params transform copy learn idf vector global term weights fit data transform get parameters estimator set parameters estimator transform count matrix tfidf representation __init__ norm use_idf true smooth_idf true sublinear_tf false fit none learn idf vector global term weights parameters sparse matrix n_samples n_features matrix term token counts fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2034: parameters numpy array shape n_samples n_features training set 
2035: numpy array shape n_samples target values 
2036: returns x_new numpy array shape n_samples n_features_new transformed array 
2037: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
2038: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform copy true transform count matrix tfidf representation parameters sparse matrix n_samples n_features chapter user guide scikit learn user guide release matrix term token counts returns vectors sparse matrix n_samples n_features sklearn feature_extraction text tdfvectorizer class sklearn feature_extraction text tfidfvectorizer input content charset utf charset_error strict low strip_accents none ercase true preproces sor none tokenizer none ana lyzer word stop_words none token_pattern min_n max_df vocab max_features none binary false ulary none dtype type long norm use_idf true smooth_idf true sublinear_tf false max_n convert collection raw documents matrix idf features equivalent countvectorizer followed tdftransformer see also countvectorizertokenize documents count occurrences token return sparse matrix tfidftransformerapply term frequency inverse document frequency normalization sparse matrix occurrence counts 
2039: methods return callable handles preprocessing tokenization return function preprocess text tokenization return function split string sequence tokens decode input string unicode symbols learn conversion law documents array data build_analyzer build_preprocessor build_tokenizer decode doc fit raw_documents fit_transform raw_documents learn representation return vectors get_feature_names get_params deep get_stop_words inverse_transform set_params params transform raw_documents copy array mapping feature integer indicex feature name get parameters estimator build fetch effective stop words list return terms per document nonzero entries set parameters estimator transform raw text documents tfidf vectors __init__ input content lower charset utf case true preprocessor none tokenizer none analyzer word stop_words none token_pattern min_n max_n max_df max_features none vocabulary none use_idf true smooth_idf true sublinear_tf false dtype type long charset_error strict strip_accents none binary false norm reference scikit learn user guide release build_analyzer return callable handles preprocessing tokenization build_preprocessor return function preprocess text tokenization build_tokenizer return function split string sequence tokens decode doc decode input string unicode symbols decoding strategy depends vectorizer parameters 
2040: fit raw_documents learn conversion law documents array data fit_transform raw_documents none learn representation return vectors parameters raw_documents iterable iterable yields either str unicode objects returns vectors array n_samples n_features get_feature_names array mapping feature integer indicex feature name get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2041: get_stop_words build fetch effective stop words list inverse_transform return terms per document nonzero entries 
2042: parameters array sparse matrix shape n_samples n_features returns x_inv list arrays len n_samples list arrays terms 
2043: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform raw_documents copy true transform raw text documents tfidf vectors parameters raw_documents iterable iterable yields either str unicode objects returns vectors sparse matrix n_samples n_features chapter user guide scikit learn user guide release sklearn feature_selection feature selection sklearn feature_selection module implements feature selection algorithms currently includes uni variate lter selection methods recursive feature elimination algorithm user guide see feature selection section details 
2044: feature_selection selectpercentile score_func feature_selection selectkbest score_func feature_selection selectfpr score_func alpha feature_selection selectfdr score_func alpha feature_selection selectfwe score_func alpha feature_selection rfe estimator step feature_selection rfecv estimator step filter select best percentile p_values filter select lowest values filter select pvalues alpha based fpr test filter select values estimated false discovery rate filter select values corresponding family wise error rate feature ranking recursive feature elimination feature ranking recursive feature elimination cross validated selection best number features 
2045: sklearn feature_selection selectpercentile class sklearn feature_selection selectpercentile score_func percentile filter select best percentile p_values parameters score_func callable function taking two arrays returning arrays scores pvalues percentile int optional percent features keep methods fit fit_transform get_params deep get_support indices inverse_transform transform new matrix using selected features set_params params transform evaluate function fit data transform get parameters estimator return mask list features indices selected 
2046: set parameters estimator transform new matrix using selected features __init__ score_func percentile fit evaluate function fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2047: parameters numpy array shape n_samples n_features training set 
2048: numpy array shape n_samples target values 
2049: returns x_new numpy array shape n_samples n_features_new reference scikit learn user guide release transformed array 
2050: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
2051: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2052: get_support indices false return mask list features indices selected 
2053: inverse_transform transform new matrix using selected features set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform transform new matrix using selected features sklearn feature_selection selectkbest class sklearn feature_selection selectkbest score_func filter select lowest values 
2054: parameters score_func callable function taking two arrays returning pair arrays scores pvalues 
2055: int optional number top features select 
2056: notes ties features equal values broken unspecied way 
2057: methods fit fit_transform get_params deep evaluate function fit data transform get parameters estimator continued next page chapter user guide scikit learn user guide release table continued previous page get_support indices inverse_transform transform new matrix using selected features set_params params transform set parameters estimator transform new matrix using selected features return mask list features indices selected 
2058: __init__ score_func fit evaluate function fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2059: parameters numpy array shape n_samples n_features training set 
2060: numpy array shape n_samples target values 
2061: returns x_new numpy array shape n_samples n_features_new transformed array 
2062: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
2063: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2064: get_support indices false return mask list features indices selected 
2065: inverse_transform transform new matrix using selected features set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform transform new matrix using selected features reference scikit learn user guide release sklearn feature_selection selectfpr class sklearn feature_selection selectfpr score_func alpha filter select pvalues alpha based fpr test fpr test stands false positive rate test controls total amount false detections 
2066: parameters score_func callable function taking two arrays returning arrays scores pvalues alpha oat optional highest value features kept methods fit fit_transform get_params deep get_support indices inverse_transform transform new matrix using selected features set_params params transform evaluate function fit data transform get parameters estimator return mask list features indices selected 
2067: set parameters estimator transform new matrix using selected features __init__ score_func alpha fit evaluate function fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2068: parameters numpy array shape n_samples n_features training set 
2069: numpy array shape n_samples target values 
2070: returns x_new numpy array shape n_samples n_features_new transformed array 
2071: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
2072: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2073: chapter user guide scikit learn user guide release get_support indices false return mask list features indices selected 
2074: inverse_transform transform new matrix using selected features set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform transform new matrix using selected features sklearn feature_selection selectfdr class sklearn feature_selection selectfdr score_func alpha filter select values estimated false discovery rate uses benjamini hochberg procedure alpha target false discovery rate 
2075: parameters score_func callable function taking two arrays returning arrays scores pvalues alpha oat optional highest uncorrected value features keep methods fit fit_transform get_params deep get_support indices inverse_transform transform new matrix using selected features set_params params transform evaluate function fit data transform get parameters estimator return mask list features indices selected 
2076: set parameters estimator transform new matrix using selected features __init__ score_func alpha fit evaluate function fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2077: parameters numpy array shape n_samples n_features training set 
2078: numpy array shape n_samples reference scikit learn user guide release target values 
2079: returns x_new numpy array shape n_samples n_features_new transformed array 
2080: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
2081: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2082: get_support indices false return mask list features indices selected 
2083: inverse_transform transform new matrix using selected features set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform transform new matrix using selected features sklearn feature_selection selectfwe class sklearn feature_selection selectfwe score_func alpha filter select values corresponding family wise error rate parameters score_func callable function taking two arrays returning arrays scores pvalues alpha oat optional highest uncorrected value features keep methods fit fit_transform get_params deep get_support indices inverse_transform transform new matrix using selected features evaluate function fit data transform get parameters estimator return mask list features indices selected 
2084: continued next page chapter user guide scikit learn user guide release table continued previous page set_params params transform set parameters estimator transform new matrix using selected features __init__ score_func alpha fit evaluate function fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2085: parameters numpy array shape n_samples n_features training set 
2086: numpy array shape n_samples target values 
2087: returns x_new numpy array shape n_samples n_features_new transformed array 
2088: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
2089: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2090: get_support indices false return mask list features indices selected 
2091: inverse_transform transform new matrix using selected features set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform transform new matrix using selected features reference scikit learn user guide release sklearn feature_selection rfe class sklearn feature_selection rfe estimator n_features_to_select step feature ranking recursive feature elimination given external estimator assigns weights features coefcients linear model goal recursive feature elimination rfe select features recursively considering smaller smaller sets features first estimator trained initial set features weights assigned one features whose absolute weights smallest pruned current set features procedure recursively repeated pruned set desired number features select eventually reached 
2092: parameters estimator object supervised learning estimator method updates coef_ attribute holds tted parameters important features must correspond high absolute values coef_ array instance case supervised learning algorithms support vector classiers generalized linear models svm linear_model mod ules 
2093: n_features_to_select int number features select step int oat optional default greater equal step corresponds integer number features remove iteration within step corresponds percentage rounded features remove iteration 
2094: references r61 examples following example shows retrieve right informative features friedman dataset 
2095: sklearn datasets import make_friedman1 sklearn feature_selection import rfe sklearn svm import svr make_friedman1 n_samples n_features random_state estimator svr kernel linear selector rfe estimator step selector selector fit selector support_ array true true true true true false false false false false dtype bool selector ranking_ array chapter user guide scikit learn user guide release number selected features mask selected features 
2096: feature ranking ranking_ corresponds ranking position feature selected estimated best features assigned rank 
2097: attributes n_features_int sup port_ rank ing_ array shape n_features array shape n_features methods fit get_params deep predict score set_params params transform fit rfe model underlying estimator selected get parameters estimator reduce selected features predict using reduce selected features return score set parameters estimator reduce selected features elimination 
2098: __init__ estimator n_features_to_select step fit fit rfe model underlying estimator selected features 
2099: parameters array shape n_samples n_features training input samples array shape n_samples target values 
2100: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2101: predict reduce selected features predict using underlying estimator 
2102: parameters array shape n_samples n_features input samples 
2103: returns array shape n_samples predicted target values 
2104: score reduce selected features return score underlying estimator 
2105: parameters array shape n_samples n_features input samples 
2106: array shape n_samples target values 
2107: reference scikit learn user guide release set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform reduce selected features elimination 
2108: parameters array shape n_samples n_features input samples 
2109: returns x_r array shape n_samples n_selected_features input samples features selected elimination 
2110: sklearn feature_selection rfecv class sklearn feature_selection rfecv estimator step none loss_func none feature ranking recursive feature elimination cross validatedselection best number fea tures 
2111: parameters estimator object supervised learning estimator method updates coef_ attribute holds tted parameters important features must correspond high absolute values coef_ array instance case supervised learning algorithms support vector classiers generalized linear models svm linear_model mod ules 
2112: step int oat optional default greater equal step corresponds integer number features remove iteration within step corresponds percentage rounded features remove iteration 
2113: int cross validation generator optional default none int number folds none fold cross validation performed fault specic cross validation objects also passed see sklearn cross_validation module details 
2114: loss_function function optional default none loss function minimize cross validation none score function estimator maximized 
2115: references r62 chapter user guide scikit learn user guide release examples following example shows retrieve priori known informative features friedman dataset 
2116: sklearn datasets import make_friedman1 sklearn feature_selection import rfecv sklearn svm import svr make_friedman1 n_samples n_features random_state estimator svr kernel linear selector rfecv estimator step selector selector fit selector support_ array true true true true true false false false false false dtype bool selector ranking_ array attributes n_features_int sup port_ rank ing_ array shape n_features array shape n_features cv_scores_array shape n_subsets_of_features number selected features cross validation mask selected features 
2117: feature ranking ranking_ corresponds ranking position feature selected estimated best features assigned rank cross validation scores cv_scores_ corresponds score subset features 
2118: methods fit get_params deep predict score set_params params transform fit rfe model automatically tune number selected get parameters estimator reduce selected features predict using reduce selected features return score set parameters estimator reduce selected features elimination 
2119: __init__ estimator step none loss_func none fit fit rfe model automatically tune number selected features 
2120: parameters array shape n_samples n_features training vector n_samples number samples n_features total number features 
2121: array shape n_samples target values integers classication real numbers regression 
2122: get_params deep true get parameters estimator reference scikit learn user guide release parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2123: predict reduce selected features predict using underlying estimator 
2124: parameters array shape n_samples n_features input samples 
2125: returns array shape n_samples predicted target values 
2126: score reduce selected features return score underlying estimator 
2127: parameters array shape n_samples n_features input samples 
2128: array shape n_samples target values 
2129: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform reduce selected features elimination 
2130: parameters array shape n_samples n_features input samples 
2131: returns x_r array shape n_samples n_selected_features input samples features selected elimination 
2132: feature_selection chi2 feature_selection f_classif feature_selection f_regression center univariate linear regression tests compute chi squared statistic class feature combination compute anova value provided sample sklearn feature_selection chi2 sklearn feature_selection chi2 compute chi squared statistic class feature combination transformer used select n_features features highest values chi square statistic either boolean multinomially distributed data term counts document classication relative classes recall statistic measures dependence stochastic variables transformer based function weeds features likely independent class therefore irrelevant classication 
2133: chapter user guide parameters array like sparse matrix shape n_samples n_features_in scikit learn user guide release sample vectors 
2134: array like shape n_samples target vector class labels 
2135: notes complexity algorithm n_classes n_features 
2136: sklearn feature_selection f_classif sklearn feature_selection f_classif compute anova value provided sample parameters array shape n_samples n_features set regressors sthat tested sequentially array shape n_samples data matrix returns array shape set values pval array shape set values sklearn feature_selection f_regression sklearn feature_selection f_regression center true univariate linear regression tests quick linear model testing effect single regressor sequentially many regressors done steps regressor interest data orthogonalized wrt constant regressors cross correlation data regressors computed converted score value parameters array shape n_samples n_features set regressors sthat tested sequentially array shape n_samples data matrix center true bool true centered returns array shape set values pval array shape set values reference scikit learn user guide release sklearn gaussian_process gaussian processes sklearn gaussian_process module implements scalar gaussian process based predictions user guide see gaussian processes section details 
2137: gaussian_process gaussianprocess regr gaussian process model class 
2138: sklearn gaussian_process gaussianprocess class sklearn gaussian_process gaussianprocess regr constant thetal none corr squared_exponential beta0 none storage_mode full verbose false thetau none theta0 optimizer fmin_cobyla ran normalize true dom_start nugget 2204460492503131e ran dom_state none gaussian process model class 
2139: parameters regr string callable optional regression function returning array outputs linear regression functional basis number observations n_samples greater size basis default assumes simple constant regression trend available built regression models constant linear quadratic corr string callable optional stationary autocorrelation function returning autocorrelation two points default assumes squared exponential autocorrelation model built corre lation models absolute_exponential squared_exponential generalized_exponential cubic linear beta0 double array_like optional regression weight vector perform ordinary kriging default assumes uni versal kriging vector beta regression weights estimated using maximum likelihood principle 
2140: storage_mode string optional string specifying whether cholesky decomposition correlation matrix stored class storage_mode full storage_mode light default assumes storage_mode full cholesky decomposition cor relation matrix stored might useful parameter one interested mse plan estimate blup correlation matrix required 
2141: verbose boolean optional boolean specifying verbose level default verbose false 
2142: theta0 double array_like optional chapter user guide scikit learn user guide release array shape n_features parameters autocorrelation model thetal thetau also specied theta0 considered starting point maximum likelihood rstimation best set parameters default assumes isotropic autocorrelation model theta0 
2143: thetal double array_like optional array shape matching theta0s lower bound autocorrelation parame ters maximum likelihood estimation default none skips maximum likelihood estimation uses theta0 
2144: thetau double array_like optional array shape matching theta0s upper bound autocorrelation parame ters maximum likelihood estimation default none skips maximum likelihood estimation uses theta0 
2145: normalize boolean optional input observations centered reduced wrt means standard deviations estimated n_samples observations provided default normalize true data normalized ease maximum likelihood estimation 
2146: nugget double ndarray optional nugget introduce nugget effect allow smooth predictions noisy data ndarray must length number data points used nugget added diagonal assumed training covariance way acts tikhonov regularization problem special case squared exponential correlation function nugget mathematically represents variance input values default assumes nugget close machine precision sake robustness nugget machine_epsilon 
2147: optimizer string optional string specifying optimization algorithm used default uses fmin_cobyla algorithm scipy optimize available optimizers fmin_cobyla welch welch optimizer dued welch see reference wbswm1992 consists iterating several one dimensional optimizations instead running one single multi dimensional optimization 
2148: random_start int optional number times maximum likelihood estimation performed random starting point rst mle always uses specied starting point theta0 next starting points picked random according exponential distribution log uniform thetal thetau default use random starting point ran dom_start 
2149: random_state integer numpy randomstate optional generator used shufe sequence coordinates theta welch opti mizer integer given xes seed defaults global numpy random number generator 
2150: reference scikit learn user guide release notes presentation implementation based translation dace matlab toolbox see reference nlns2002 
2151: references nlns2002 wbswm1992 examples import numpy sklearn gaussian_process import gaussianprocess array sin ravel gaussianprocess theta0 thetal thetau fit gaussianprocess beta0 none 
2152:  
2153: methods arg_max_reduced_likelihood_function function estimates autocorrelation parameters theta maximizer reduced likelihood function fit get_params deep predict eval_mse batch_size reduced_likelihood_function theta score set_params params gaussian process model tting method get parameters estimator function evaluates gaussian process model function determines blup parameters evaluates reduced returns coefcient determination prediction set parameters estimator 
2154: __init__ regr constant corr squared_exponential beta0 none theta0 thetal none bose false dom_start normalize true nugget 2204460492503131e random_state none thetau none optimizer fmin_cobyla storage_mode full ver ran arg_max_reduced_likelihood_function function estimates autocorrelation parameters theta maximizer reduced likelihood function minimization opposite reduced likelihood function used convenience parameters self parameters stored gaussian process model object returns optimal_theta array_like best set autocorrelation parameters sought maximizer reduced likeli hood function 
2155: optimal_reduced_likelihood_function_value double optimal reduced likelihood function value 
2156: optimal_par dict blup parameters associated thetaopt 
2157: chapter user guide scikit learn user guide release fit gaussian process model tting method 
2158: parameters double array_like array shape n_samples n_features input observations made 
2159: double array_like array shape n_features observations scalar output pre dicted 
2160: returns self tted gaussian process model object awaiting data perform predictions 
2161: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2162: predict eval_mse false batch_size none function evaluates gaussian process model 
2163: parameters array_like array shape n_eval n_features giving point prediction made 
2164: eval_mse boolean optional boolean specifying whether mean squared error evaluated fault assumes evalmse false evaluates blup mean prediction 
2165: batch_size integer optional integer giving maximum number points evaluated simulatneously depending available memory default none given points eval uated time 
2166: returns array_like array shape n_eval best linear unbiased prediction 
2167: mse array_like optional eval_mse true array shape n_eval mean squared error 
2168: reduced_likelihood_function theta none function determines blup parameters evaluates reduced likelihood function given autocorrelation parameters theta maximizing function wrt autocorrelation parameters theta equivalent maximizing likeli hood assumed joint gaussian distribution observations evaluated onto design experi ments 
2169: parameters theta array_like optional array containing autocorrelation parameters gaussian process model parameters determined default uses built autocorrelation rameters theta self theta 
2170: reference scikit learn user guide release returns reduced_likelihood_function_value double value reduced likelihood function associated given autocorrelation parameters theta 
2171: par dict dictionary containing requested gaussian process model parameters sigma2gaussian process variance betageneralized least squares regression weights universal kriging given beta0 ordinary kriging 
2172: gammagaussian process weights ccholesky decomposition correlation matrix ftsolution linear equation system gqr decomposition matrix 
2173: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
2174: parameters array like shape n_samples n_features training set 
2175: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self gaussian_process correlation_models absolute_exponential gaussian_process correlation_models squared_exponential gaussian_process correlation_models generalized_exponential generalized exponential correlation model gaussian_process correlation_models pure_nugget gaussian_process correlation_models cubic gaussian_process correlation_models linear gaussian_process regression_models constant gaussian_process regression_models linear gaussian_process regression_models quadratic spatial independence correlation model pure nugget cubic correlation model linear correlation model zero order polynomial constant regression model first order polynomial linear regression model second order polynomial quadratic regression model 
2176: absolute exponential autocorrelation model squared exponential correlation model radial basis function 
2177: sklearn gaussian_process correlation_models absolute_exponential sklearn gaussian_process correlation_models absolute_exponential theta absolute exponential autocorrelation model ornstein uhlenbeck stochastic process chapter user guide scikit learn user guide release theta theta exp sum theta_i dx_i parameters theta array_like array shape isotropic anisotropic giving autocorrelation parame ter 
2178: array_like array shape n_eval n_features giving componentwise distances locations correlation model evaluated 
2179: returns array_like array shape n_eval containing values autocorrelation model 
2180: sklearn gaussian_process correlation_models squared_exponential sklearn gaussian_process correlation_models squared_exponential theta squared exponential correlation model radial basis function innitely differentiable stochastic process smooth theta theta exp sum theta_i dx_i parameters theta array_like array shape isotropic anisotropic giving autocorrelation parame ter 
2181: array_like array shape n_eval n_features giving componentwise distances locations correlation model evaluated 
2182: returns array_like array shape n_eval containing values autocorrelation model 
2183: sklearn gaussian_process correlation_models generalized_exponential sklearn gaussian_process correlation_models generalized_exponential theta generalized exponential correlation model useful one know smoothness function predicted theta theta exp sum theta_i dx_i parameters theta array_like array shape isotropic anisotropic giving autocorrelation rameter theta 
2184: array_like reference scikit learn user guide release array shape n_eval n_features giving componentwise distances locations correlation model evaluated 
2185: returns array_like array shape n_eval values autocorrelation model 
2186: sklearn gaussian_process correlation_models pure_nugget sklearn gaussian_process correlation_models pure_nugget theta spatial independence correlation model pure nugget useful one wants solve ordinary least squares problem theta theta sum dx_i otherwise parameters theta array_like none 
2187: array_like array shape n_eval n_features giving componentwise distances locations correlation model evaluated 
2188: returns array_like array shape n_eval values autocorrelation model 
2189: sklearn gaussian_process correlation_models cubic sklearn gaussian_process correlation_models cubic theta cubic correlation model theta theta prod max theta_j d_ij theta_j d_ij parameters theta array_like array shape isotropic anisotropic giving autocorrelation parame ter 
2190: array_like array shape n_eval n_features giving componentwise distances locations correlation model evaluated 
2191: returns array_like array shape n_eval values autocorrelation model 
2192: chapter user guide scikit learn user guide release sklearn gaussian_process correlation_models linear sklearn gaussian_process correlation_models linear theta linear correlation model theta theta prod max theta_j d_ij parameters theta array_like array shape isotropic anisotropic giving autocorrelation parame ter 
2193: array_like array shape n_eval n_features giving componentwise distances locations correlation model evaluated 
2194: returns array_like array shape n_eval values autocorrelation model 
2195: sklearn gaussian_process regression_models constant sklearn gaussian_process regression_models constant zero order polynomial constant regression model parameters array_like array shape n_eval n_features giving locations regression model evaluated 
2196: returns array_like array shape n_eval values regression model 
2197: sklearn gaussian_process regression_models linear sklearn gaussian_process regression_models linear first order polynomial linear regression model x_1 x_n parameters array_like array shape n_eval n_features giving locations regression model evaluated 
2198: returns array_like array shape n_eval values regression model 
2199: reference scikit learn user guide release sklearn gaussian_process regression_models quadratic sklearn gaussian_process regression_models quadratic second order polynomial quadratic regression model x_i x_i x_j parameters array_like array shape n_eval n_features giving locations regression model evaluated 
2200: returns array_like array shape n_eval values regression model 
2201: sklearn grid_search grid search sklearn grid_search includes utilities tune parameters estimator user guide see grid search setting estimator parameters section details 
2202: grid_search gridsearchcv estimator param_grid grid search parameters classier grid_search itergrid param_grid generators combination various parameter lists given sklearn grid_search gridsearchcv class sklearn grid_search gridsearchcv estimator param_grid score_func none iid true pre_dispatch n_jobs ret true t_params none none loss_func none n_jobs verbose grid search parameters classier important members predict gridsearchcv implements method predict method like classier except parameters classier used predict optimized cross validation 
2203: parameters estimator object type implements predict methods object type instantiated grid point 
2204: param_grid dict dictionary parameters names string keys lists parameter settings try values 
2205: loss_func callable optional function takes arguments compares order evaluate performance prediciton small good none passed score estimator maximized score_func callable optional function takes arguments compares order evaluate perfor mance prediction high good none passed score estimator maximized 
2206: t_params dict optional chapter user guide scikit learn user guide release parameters pass method n_jobs int optional number jobs run parallel default pre_dispatch int string optional controls number jobs get dispatched parallel execution reducing number useful avoid explosion memory consumption jobs get dispatched cpus process parameter none case jobs immediatly created spawned use lightweight fast running jobs avoid delays due demand spawning jobs int giving exact number total jobs spawned string giving expression function n_jobs n_jobs iid boolean optional true data assumed identically distributed across folds loss minimized total loss per sample mean loss across folds 
2207: integer crossvalidation generator optional integer passed number fold default specic crossvalidation jects passed see sklearn cross_validation module list possible objects ret boolean ret best estimator entire dataset predictions using gridsearch instance tting 
2208: false impossible make verbose integer controls verbosity higher messages 
2209: see also itergridgenerates combinations hyperparameter grid sklearn cross_validation train_test_splitutility function split data development set usable tting gridsearchcv instance evaluation set nal evaluation 
2210: notes parameters selected maximize score left data unless explicit score_func passed case used instead loss function loss_func passed overrides score functions minimized n_jobs set value higher one data copied point grid n_jobs times done efciency reasons individual jobs take little time may raise errors dataset large enough memory available workaround case set pre_dispatch memory copied pre_dispatch many times reasonable value pre_dispatch n_jobs 
2211: examples reference scikit learn user guide release sklearn import svm grid_search datasets iris datasets load_iris parameters kernel linear rbf svr svm svc clf grid_search gridsearchcv svr parameters clf fit iris data iris target gridsearchcv none estimator svc cache_size coef0 degree gamma kernel rbf probability false shrinking true tol fit_params iid true loss_func none n_jobs param_grid attributes grid_scores_dict oat best_estimator_estimator best_score_ oat best_params_dict methods contains scores parameter combinations param_grid 
2212: estimator choosen grid search estimator gave highest score smallest loss specied left data score best_estimator left data parameter setting gave best results hold data 
2213: fit get_params deep score set_params params run sets parameters get parameters estimator set parameters estimator 
2214: __init__ estimator param_grid loss_func none score_func none t_params none n_jobs iid true ret true none verbose pre_dispatch n_jobs best_estimator deprecated gridsearchcv best_estimator deprecated removed version please use gridsearchcv best_estimator_ instead 
2215: best_score deprecated gridsearchcv best_score deprecated removed version please use gridsearchcv best_score_ instead 
2216: fit none params run sets parameters returns best classier parameters array n_samples n_features training vector n_samples number samples n_features num ber features 
2217: array like shape n_samples optional target vector relative classication none unsupervised learning 
2218: chapter user guide scikit learn user guide release get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn grid_search itergrid class sklearn grid_search itergrid param_grid generators combination various parameter lists given parameters param_grid dict string sequence parameter grid explore dictionary mapping estimator parameters quences allowed values 
2219: returns params dict string yields dictionaries mapping estimator parameter one allowed values 
2220: see also gridsearchcvuses itergrid perform full parallelized grid search 
2221: examples sklearn grid_search import itergrid param_grid true false list itergrid param_grid true false true false __init__ param_grid sklearn hmm hidden markov models sklearn hmm module implements hidden markov models warning sklearn hmm orphaned undocumented known numerical stability issues nobody volun teers write documentation make stable module removed version user guide see hidden markov models section details 
2222: hmm gaussianhmm n_components hmm multinomialhmm n_components hmm gmmhmm n_components n_mix startprob hidden markov model gaussin mixture emissions hidden markov model gaussian emissions hidden markov model multinomial discrete emissions reference scikit learn user guide release sklearn hmm gaussianhmm class sklearn hmm gaussianhmm n_components transmat none algorithm viterbi means_prior none means_weight vars_prior covars_weight random_state none startprob none transmat_prior none covariance_type diag startprob_prior none hidden markov model gaussian emissions representation hidden markov model probability distribution class allows easy evaluation sampling maximum likelihood estimation parameters hmm 
2223: parameters n_components int number states 
2224: _covariance_type string string describing type covariance parameters use must one spherical tied diag full defaults diag 
2225: see also gmmgaussian mixture model examples sklearn hmm import gaussianhmm gaussianhmm n_components gaussianhmm algorithm viterbi covariance_type diag covars_prior covars_weight means_prior none means_weight n_components random_state none startprob none startprob_prior transmat none transmat_prior attributes sklearn hmm multinomialhmm class sklearn hmm multinomialhmm n_components prob_prior none random_state none hidden markov model multinomial discrete emissions see also startprob none start transmat_prior none algorithm viterbi transmat none gaussianhmmhmm gaussian emissions examples sklearn hmm import multinomialhmm multinomialhmm n_components multinomialhmm algorithm viterbi n_components random_state none chapter user guide scikit learn user guide release startprob none startprob_prior transmat none transmat_prior attributes n_components n_symbols transmat startprob emissionprob random_state randomstate int seed default methods int int array shape n_components n_components array shape n_components array shape n_components n_symbols number states model number possible symbols emitted model observations matrix transition probabilities states 
2226: initial state occupation distribution 
2227: probability emitting given symbol state 
2228: random number generator instance find likely state sequence corresponding obs compute log probability model compute posteriors decode obs algorithm eval obs fit obs n_iter thresh params init_params estimate model parameters get_params deep predict obs algorithm predict_proba obs rvs args kwargs sample random_state score obs set_params params get parameters estimator find likely state sequence corresponding obs compute posterior probability state model deprecated rvs deprecated removed use sample instead generate random samples model compute log probability model set parameters estimator 
2229: __init__ n_components startprob none transmat none startprob_prior none trans mat_prior none algorithm viterbi random_state none create hidden markov model multinomial emissions 
2230: parameters n_components int number states 
2231: algorithm decoder algorithm decode obs algorithm viterbi find likely state sequence corresponding obs uses selected algorithm decoding 
2232: parameters obs array_like shape n_features list n_features dimensional data points row corresponds single data point 
2233: algorithm string one decoder_algorithms decoder algorithm used reference scikit learn user guide release returns logprob oat log probability maximum likelihood path hmm state_sequence array_like shape index likely states observation see also evalcompute log probability model posteriors scorecompute log probability model emissionprob_ emission probability distribution state 
2234: eval obs compute log probability model compute posteriors implements rank beam pruning forward backward algorithm speed inference large models 
2235: parameters obs array_like shape n_features sequence n_features dimensional data points row corresponds single point sequence returns logprob oat log likelihood sequence obs posteriors array_like shape n_components posterior probabilities state observation see also scorecompute log probability model decodefind likely state sequence corresponding obs fit obs n_iter thresh params abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz init_params abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz kwargs estimate model parameters initialization step performed entering algorithm want avoid step set keyword argument init_params empty string likewise would like initialization call method n_iter 
2236: parameters obs list list array like observation sequences shape n_i n_features 
2237: n_iter int optional number iterations perform 
2238: thresh oat optional convergence threshold params string optional chapter user guide scikit learn user guide release controls parameters updated training process contain com bination startprob transmat means covars etc defaults parameters init_params string optional controls parameters initialized prior training contain combination startprob transmat means covars etc defaults parameters 
2239: notes general logprob non decreasing unless aggressive pruning used decreasing logprob generally sign overtting covariance parameter getting small getting training data decreasing covars_prior 
2240: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2241: predict obs algorithm viterbi find likely state sequence corresponding obs 
2242: parameters obs array_like shape n_features list n_features dimensional data points row corresponds single data point 
2243: returns state_sequence array_like shape index likely states observation predict_proba obs compute posterior probability state model parameters obs array_like shape n_features list n_features dimensional data points row corresponds single data point 
2244: returns array like shape n_components returns probability sample state model 
2245: rvs args kwargs deprecated rvs deprecated removed use sample instead sample random_state none generate random samples model 
2246: parameters int number samples generate 
2247: random_state randomstate int seed default random number generator instance none given objects random_state used returns obs hidden_states obs array_like length list samples reference scikit learn user guide release hidden_states array_like length list hidden states score obs compute log probability model 
2248: parameters obs array_like shape n_features sequence n_features dimensional data points row corresponds single data point 
2249: returns logprob oat log likelihood obs see also evalcompute log probability model posteriors decodefind likely state sequence corresponding obs set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self startprob_ mixing startprob state 
2250: transmat_ matrix transition probabilities 
2251: sklearn hmm gmmhmm class sklearn hmm gmmhmm n_components n_mix hidden markov model gaussin mixture emissions see also start prob_prior none transmat_prior none algorithm viterbi gmms none covariance_type diag covars_prior random_state none startprob none transmat none gaussianhmmhmm gaussian emissions examples sklearn hmm import gmmhmm gmmhmm n_components n_mix covariance_type diag gmmhmm algorithm viterbi covariance_type diag covars_prior gmms gmm covariance_type none init_params wmc min_covar n_components n_init n_iter params wmc random_state none thresh gmm covariance_type none init_params wmc min_covar n_components n_init n_iter params wmc random_state none thresh n_components n_mix random_state none startprob none startprob_prior transmat none transmat_prior chapter user guide scikit learn user guide release int array shape n_components n_components array shape n_components array gmm objects length n_components number states model matrix transition probabilities states initial state occupation distribution gmm emission distributions state random number generator instance attributes n_components transmat startprob gmms random_state randomstate int seed default methods find likely state sequence corresponding obs compute log probability model compute posteriors decode obs algorithm eval obs fit obs n_iter thresh params init_params estimate model parameters get_params deep predict obs algorithm predict_proba obs rvs args kwargs sample random_state score obs set_params params get parameters estimator find likely state sequence corresponding obs compute posterior probability state model deprecated rvs deprecated removed use sample instead generate random samples model compute log probability model set parameters estimator 
2252: __init__ n_components n_mix transmat none startprob_prior none transmat_prior none algorithm viterbi gmms none covariance_type diag vars_prior random_state none startprob none create hidden markov model gmm emissions 
2253: parameters n_components int number states 
2254: algorithm decoder algorithm covariance_type covariance type model must one spherical tied diag full 
2255: decode obs algorithm viterbi find likely state sequence corresponding obs uses selected algorithm decoding 
2256: parameters obs array_like shape n_features list n_features dimensional data points row corresponds single data point 
2257: algorithm string one decoder_algorithms decoder algorithm used returns logprob oat log probability maximum likelihood path hmm reference scikit learn user guide release state_sequence array_like shape index likely states observation see also evalcompute log probability model posteriors scorecompute log probability model eval obs compute log probability model compute posteriors implements rank beam pruning forward backward algorithm speed inference large models 
2258: parameters obs array_like shape n_features sequence n_features dimensional data points row corresponds single point sequence returns logprob oat log likelihood sequence obs posteriors array_like shape n_components posterior probabilities state observation see also scorecompute log probability model decodefind likely state sequence corresponding obs fit obs n_iter thresh params abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz init_params abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz kwargs estimate model parameters initialization step performed entering algorithm want avoid step set keyword argument init_params empty string likewise would like initialization call method n_iter 
2259: parameters obs list list array like observation sequences shape n_i n_features 
2260: n_iter int optional number iterations perform 
2261: thresh oat optional convergence threshold params string optional controls parameters updated training process contain com bination startprob transmat means covars etc defaults parameters init_params string optional controls parameters initialized prior training contain combination startprob transmat means covars etc defaults parameters 
2262: chapter user guide scikit learn user guide release notes general logprob non decreasing unless aggressive pruning used decreasing logprob generally sign overtting covariance parameter getting small getting training data decreasing covars_prior 
2263: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2264: predict obs algorithm viterbi find likely state sequence corresponding obs 
2265: parameters obs array_like shape n_features list n_features dimensional data points row corresponds single data point 
2266: returns state_sequence array_like shape index likely states observation predict_proba obs compute posterior probability state model parameters obs array_like shape n_features list n_features dimensional data points row corresponds single data point 
2267: returns array like shape n_components returns probability sample state model 
2268: rvs args kwargs deprecated rvs deprecated removed use sample instead sample random_state none generate random samples model 
2269: parameters int number samples generate 
2270: random_state randomstate int seed default random number generator instance none given objects random_state used returns obs hidden_states obs array_like length list samples hidden_states array_like length list hidden states score obs compute log probability model 
2271: parameters obs array_like shape n_features sequence n_features dimensional data points row corresponds single data point 
2272: returns logprob oat reference scikit learn user guide release log likelihood obs see also evalcompute log probability model posteriors decodefind likely state sequence corresponding obs set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self startprob_ mixing startprob state 
2273: transmat_ matrix transition probabilities 
2274: sklearn kernel_approximation kernel approximation sklearn kernel_approximation module implements several approximate kernel feature maps base fourier transforms user guide see kernel approximation section details 
2275: kernel_approximation rbfsampler gamma kernel_approximation additivechi2sampler approximate feature map additive chi kernel kernel_approximation skewedchi2sampler approximates feature map skewed chi squared kernel monte approximates feature map rbf kernel monte carlo approximation sklearn kernel_approximation rbfsampler class sklearn kernel_approximation rbfsampler gamma n_components ran approximates feature map rbf kernel monte carlo approximation fourier transform 
2276: dom_state none parameters gamma oat parameter rbf kernel exp gamma n_components int number monte carlo samples per original feature equals dimensionality computed feature space 
2277: random_state int randomstate optional int random_state seed used random number generator randomstate instance random_state random number generator 
2278: notes see random features large scale kernel machines rahimi benjamin recht 
2279: chapter user guide scikit learn user guide release methods fit fit_transform get_params deep set_params params transform fit model fit data transform get parameters estimator set parameters estimator apply approximate feature map 
2280: __init__ gamma n_components random_state none fit none fit model samples random projection according n_features 
2281: parameters array like sparse matrix shape n_samples n_features training data n_samples number samples n_features number features 
2282: returns self object returns transformer 
2283: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2284: parameters numpy array shape n_samples n_features training set 
2285: numpy array shape n_samples target values 
2286: returns x_new numpy array shape n_samples n_features_new transformed array 
2287: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
2288: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object 
2289: reference scikit learn user guide release returns self transform none apply approximate feature map 
2290: parameters array like sparse matrix shape n_samples n_features new data n_samples number samples n_features number features 
2291: returns x_new array like shape n_samples n_components sklearn kernel_approximation additivechi2sampler class sklearn kernel_approximation additivechi2sampler sample_steps sam ple_interval none approximate feature map additive chi kernel uses sampling fourier transform kernel characteristic regular intervals since kernel approximated additive components input vectors treated separately entry original space transformed 2sample_steps features sample_steps parameter method typical values include optimal choices sampling interval certain data ranges computed see reference default values reasonable 
2292: parameters sample_steps int optional gives number complex sampling points 
2293: sample_interval oat optional sampling interval must specied sample_steps 
2294: notes see efcient additive kernels via explicit feature maps vedaldi zisserman computer vision pattern recognition methods fit fit_transform get_params deep set_params params transform set parameters fit data transform get parameters estimator set parameters estimator apply approximate feature map 
2295: __init__ sample_steps sample_interval none fit none set parameters 
2296: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2297: chapter user guide scikit learn user guide release parameters numpy array shape n_samples n_features training set 
2298: numpy array shape n_samples target values 
2299: returns x_new numpy array shape n_samples n_features_new transformed array 
2300: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
2301: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform none apply approximate feature map 
2302: parameters array like shape n_samples n_features returns x_new array like shape n_samples n_features sklearn kernel_approximation skewedchi2sampler class sklearn kernel_approximation skewedchi2sampler skewedness n_components dom_state none ran approximates feature map skewed chi squared kernel monte carlo approximation fourier transform 
2303: parameters skewedness oat skewedness parameter kernel needs cross validated 
2304: n_components int number monte carlo samples per original feature equals dimensionality computed feature space 
2305: random_state int randomstate optional int random_state seed used random number generator randomstate instance random_state random number generator 
2306: reference scikit learn user guide release notes see random fourier approximations skewed multiplicative histogram kernels fuxin catalin ionescu cristian sminchisescu 
2307: methods fit fit_transform get_params deep set_params params transform fit model fit data transform get parameters estimator set parameters estimator apply approximate feature map 
2308: __init__ skewedness n_components random_state none fit none fit model samples random projection according n_features 
2309: parameters array like shape n_samples n_features training data n_samples number samples n_features number features 
2310: returns self object returns transformer 
2311: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2312: parameters numpy array shape n_samples n_features training set 
2313: numpy array shape n_samples target values 
2314: returns x_new numpy array shape n_samples n_features_new transformed array 
2315: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
2316: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2317: chapter user guide scikit learn user guide release set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform none apply approximate feature map 
2318: parameters array like shape n_samples n_features new data n_samples number samples n_features number features 
2319: returns x_new array like shape n_samples n_components sklearn semi_supervised semi supervised learning sklearn semi_supervised module implements semi supervised learning algorithms algorithms utilized small amounts labeled data large amounts unlabeled data classication tasks module includes label propagation user guide see semi supervised section details 
2320: semi_supervised labelpropagation kernel label propagation classier semi_supervised labelspreading kernel labelspreading model semi supervised learning sklearn semi_supervised labelpropagation class sklearn semi_supervised labelpropagation kernel rbf gamma n_neighbors alpha max_iters tol label propagation classier parameters kernel knn rbf string identier kernel function use rbf knn kernels currently supported gamma oat parameter rbf kernel n_neighbors integer parameter knn kernel alpha oat clamping factor max_iters oat change maximum number iterations allowed tol oat convergence tolerance threshold consider system steady state see also reference scikit learn user guide release labelspreadingalternate label proagation strategy robust noise references xiaojin zhu zoubin ghahramani bel http pages wisc edu jerryzhu pub cmu cald pdf propagation 
2321: technical report cmu cald carnegie mellon university learning labeled unlabeled data examples sklearn import datasets sklearn semi_supervised import labelpropagation label_prop_model labelpropagation iris datasets load_iris random_unlabeled_points random random_integers labels copy iris target labels random_unlabeled_points label_prop_model fit iris data labels labelpropagation size len iris target methods fit get_params deep predict predict_proba score set_params params fit semi supervised label propagation model based get parameters estimator performs inductive inference across model predict probability possible outcome returns mean accuracy given test data labels set parameters estimator 
2322: __init__ kernel rbf gamma n_neighbors alpha max_iters tol fit fit semi supervised label propagation model based input data provided matrix labeled unlabeled corresponding label matrix dedicated marker value unlabeled samples 
2323: parameters array like shape n_samples n_features n_samples n_samples size matrix created array_like shape n_samples n_labeled_samples unlabeled points marked unlabeled samples transductively assigned labels returns self returns instance self 
2324: get_params deep true get parameters estimator parameters deep boolean optional chapter user guide scikit learn user guide release true return parameters estimator contained subobjects estimators 
2325: predict performs inductive inference across model 
2326: parameters array_like shape n_samples n_features returns array_like shape n_samples predictions input data predict_proba predict probability possible outcome compute probability estimates single sample possible outcome seen training categorical distribution 
2327: parameters array_like shape n_samples n_features returns probabilities array shape n_samples n_classes normalized probability distributions across class labels score returns mean accuracy given test data labels 
2328: parameters array like shape n_samples n_features training set 
2329: array like shape n_samples labels 
2330: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn semi_supervised labelspreading class sklearn semi_supervised labelspreading kernel rbf gamma n_neighbors labelspreading model semi supervised learning model similar basic label propgation algorithm uses afnity matrix based normalized graph laplacian soft clamping across labels 
2331: pha max_iters tol parameters kernel knn rbf string identier kernel function use rbf knn kernels currently supported gamma oat parameter rbf kernel n_neighbors integer reference scikit learn user guide release parameter knn kernel alpha oat clamping factor max_iters oat maximum number iterations allowed tol oat convergence tolerance threshold consider system steady state see also labelpropagationunregularized graph based semi supervised learning references dengyong zhou olivier bousquet thomas navin lal jason weston bernhard schlkopf learning local global consistency http citeseer ist psu edu viewdoc summary doi examples sklearn import datasets sklearn semi_supervised import labelspreading label_prop_model labelspreading iris datasets load_iris random_unlabeled_points random random_integers labels copy iris target labels random_unlabeled_points label_prop_model fit iris data labels labelspreading size len iris target methods fit get_params deep predict predict_proba score set_params params fit semi supervised label propagation model based get parameters estimator performs inductive inference across model predict probability possible outcome returns mean accuracy given test data labels set parameters estimator 
2332: __init__ kernel rbf gamma n_neighbors alpha max_iters tol fit fit semi supervised label propagation model based input data provided matrix labeled unlabeled corresponding label matrix dedicated marker value unlabeled samples 
2333: parameters array like shape n_samples n_features chapter user guide scikit learn user guide release n_samples n_samples size matrix created array_like shape n_samples n_labeled_samples unlabeled points marked unlabeled samples transductively assigned labels returns self returns instance self 
2334: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2335: predict performs inductive inference across model 
2336: parameters array_like shape n_samples n_features returns array_like shape n_samples predictions input data predict_proba predict probability possible outcome compute probability estimates single sample possible outcome seen training categorical distribution 
2337: parameters array_like shape n_samples n_features returns probabilities array shape n_samples n_classes normalized probability distributions across class labels score returns mean accuracy given test data labels 
2338: parameters array like shape n_samples n_features training set 
2339: array like shape n_samples labels 
2340: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn lda linear discriminant analysis sklearn lda module implements linear discriminant analysis lda user guide see linear quadratic discriminant analysis section details 
2341: reference scikit learn user guide release lda lda n_components priors linear discriminant analysis lda sklearn lda lda class sklearn lda lda n_components none priors none linear discriminant analysis lda classier linear decision boundary generated tting class conditional densities data using bayes rule model gaussian density class assuming classes share covariance matrix tted model also used reduce dimensionality input projecting discrim inative directions 
2342: parameters n_components int number components n_classes dimensionality reduction priors array optional shape n_classes priors classes see also sklearn qda qdaquadratic discriminant analysis examples import numpy sklearn lda import lda array array clf lda clf fit lda n_components none priors none print clf predict attributes means_ xbar_ priors_ covariance_ array like shape n_classes n_features oat shape n_features array like shape n_classes array like shape n_features n_features covariance matrix shared classes class means mean class priors sum methods decision_function fit store_covariance tol fit_transform function return decision function values related fit lda model according given training data parameters fit data transform chapter user guide scikit learn user guide release table continued previous page get_params deep predict predict_log_proba predict_proba score set_params params transform get parameters estimator function classication array test vectors function return posterior log probabilities classication function return posterior probabilities classication returns mean accuracy given test data labels set parameters estimator project data maximize class separation large separation projected class means small variance within class 
2343: __init__ n_components none priors none decision_function function return decision function values related class array test vectors 
2344: parameters array like shape n_samples n_features returns array shape n_samples n_classes fit store_covariance false tol fit lda model according given training data parameters 
2345: parameters array like shape n_samples n_features training vector n_samples number samples n_features num ber features 
2346: array shape n_samples target values integers store_covariance boolean true covariance matrix shared classes computed stored self covariance_ attribute 
2347: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2348: parameters numpy array shape n_samples n_features training set 
2349: numpy array shape n_samples target values 
2350: returns x_new numpy array shape n_samples n_features_new transformed array 
2351: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
2352: get_params deep true get parameters estimator parameters deep boolean optional reference scikit learn user guide release true return parameters estimator contained subobjects estimators 
2353: predict function classication array test vectors predicted class sample returned 
2354: parameters array like shape n_samples n_features returns array shape n_samples predict_log_proba function return posterior log probabilities classication according class array test vectors 
2355: parameters array like shape n_samples n_features returns array shape n_samples n_classes predict_proba function return posterior probabilities classication according class array test vectors 
2356: parameters array like shape n_samples n_features returns array shape n_samples n_classes score returns mean accuracy given test data labels 
2357: parameters array like shape n_samples n_features training set 
2358: array like shape n_samples labels 
2359: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform project data maximize class separation large separation projected class means small variance within class 
2360: parameters array like shape n_samples n_features returns x_new array shape n_samples n_components sklearn linear_model generalized linear models includes ridge regression sklearn linear_model module implements genelarized linear models bayesian regression lasso elastic net estimators computed least angle regression coordinate scent also implements stochastic gradient descent related algorithms user guide see generalized linear models section details 
2361: chapter user guide scikit learn user guide release dense data linear_model linearregression linear_model ridge alpha t_intercept linear_model ridgeclassifier alpha linear_model ridgeclassifiercv alphas linear_model ridgecv alphas linear_model lasso alpha t_intercept linear_model lassocv eps n_alphas linear_model elasticnet alpha rho linear_model elasticnetcv rho eps linear_model lars t_intercept verbose linear_model lassolars alpha linear_model larscv t_intercept linear_model lassolarscv t_intercept linear_model lassolarsic criterion linear_model logisticregression penalty linear_model orthogonalmatchingpursuit linear_model perceptron penalty alpha linear_model sgdclassifier loss penalty linear_model sgdregressor loss penalty linear_model bayesianridge n_iter tol linear_model ardregression n_iter tol linear_model randomizedlasso alpha linear_model randomizedlogisticregression randomized logistic regression ordinary least squares linear regression linear least squares regularization classier using ridge regression ridge classier built cross validation ridge regression built cross validation linear model trained prior regularizer aka lasso lasso linear model iterative tting along regularization path linear model trained prior regularizer elastic net model iterative tting along regularization path least angle regression model lar lasso model least angle regression lars cross validated least angle regression model cross validated lasso using lars algorithm lasso model lars using bic aic model selection logistic regression aka logit maxent classier orthogonal mathching pursuit model omp perceptron linear model tted minimizing regularized empirical loss sgd linear model tted minimizing regularized empirical loss sgd bayesian ridge regression bayesian ard regression randomized lasso sklearn linear_model linearregression class sklearn linear_model linearregression t_intercept true normalize false ordinary least squares linear regression 
2362: parameters t_intercept boolean optional copy_x true wether calculate intercept model set false intercept used calculations data expected already centered 
2363: normalize boolean optional true regressors normalized notes implementation point view plain ordinary least squares numpy linalg lstsq wrapped predictor object 
2364: attributes coef_ intercept_ array array estimated coefcients linear regression problem independent term linear model 
2365: reference scikit learn user guide release methods decision_function decision function linear model fit n_jobs get_params deep predict score set_params params fit linear model get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
2366: __init__ t_intercept true normalize false copy_x true decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2367: fit n_jobs fit linear model 
2368: parameters numpy array sparse matrix shape n_samples n_features training data numpy array shape n_samples n_responses target values n_jobs number jobs use computation 
2369: cpus used provide speedup n_response sufcient large problems returns self returns instance self 
2370: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2371: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2372: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
2373: parameters array like shape n_samples n_features chapter user guide scikit learn user guide release training set 
2374: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn linear_model ridge class sklearn linear_model ridge alpha t_intercept true normalize false copy_x true tol linear least squares regularization model solves regression model loss function linear least squares function regulariza tion given norm also known ridge regression tikhonov regularization estimator built support multi variate regression array shape n_samples n_responses 
2375: parameters alpha oat small positive values alpha improve conditioning problem reduce variance estimates alpha corresponds linear models logisticregression linearsvc 
2376: t_intercept boolean whether calculate intercept model set false intercept used calculations data expected already centered 
2377: normalize boolean optional true regressors normalized copy_x boolean optional default true true copied else may overwritten 
2378: tol oat precision solution 
2379: see also ridgeclassifier ridgecv examples sklearn linear_model import ridge import numpy n_samples n_features random seed random randn n_samples random randn n_samples n_features clf ridge alpha clf fit reference scikit learn user guide release ridge alpha copy_x true fit_intercept true normalize false tol attributes coef_ array shape n_features n_responses n_features weight vector 
2380: methods decision_function fit sample_weight solver get_params deep predict score set_params params decision function linear model fit ridge regression model get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
2381: __init__ alpha t_intercept true normalize false copy_x true tol decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2382: fit sample_weight solver auto fit ridge regression model parameters array like sparse matrix shape n_samples n_features training data array like shape n_samples n_samples n_responses target values sample_weight oat numpy array shape n_samples individual weights sample solver auto dense_cholesky sparse_cg delse_cholesky use standard solver use computational routines scipy linalg solve function sparse_cg use conjugate gradient solver found scipy sparse linalg auto chose appropriate depending matrix 
2383: returns self returns instance self 
2384: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2385: chapter user guide scikit learn user guide release predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2386: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
2387: parameters array like shape n_samples n_features training set 
2388: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn linear_model ridgeclassier class sklearn linear_model ridgeclassifier alpha t_intercept true normalize false copy_x true tol class_weight none classier using ridge regression 
2389: parameters alpha oat small positive values alpha improve conditioning problem reduce variance estimates alpha corresponds linear models logisticregression linearsvc 
2390: t_intercept boolean whether calculate intercept model set false intercept used calculations data expected already centered 
2391: normalize boolean optional true regressors normalized copy_x boolean optional default true true copied else may overwritten 
2392: tol oat precision solution class_weight dict optional reference scikit learn user guide release weights associated classes form class_label weight given classes supposed weight one 
2393: see also ridge ridgeclassifiercv notes multi class classication n_class classiers trained one versus approach concretely implemented taking advantage multi variate response support ridge 
2394: attributes coef_ array shape n_features n_classes n_features weight vector 
2395: methods decision_function fit solver get_params deep predict score set_params params fit ridge regression model get parameters estimator predict target values according tted model returns coefcient determination prediction set parameters estimator 
2396: __init__ alpha class_weight none t_intercept true normalize false copy_x true tol fit solver auto fit ridge regression model 
2397: parameters array like sparse matrix shape n_samples n_features training data array like shape n_samples target values solver auto dense_cholesky sparse_cg solver use computational routines delse_cholesky use standard scipy linalg solve function sparse_cg use conjugate gradient solver found scipy sparse linalg auto chose appropriate depending matrix 
2398: returns self returns instance self 
2399: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2400: chapter user guide scikit learn user guide release predict predict target values according tted model 
2401: parameters array like shape n_samples n_features returns array shape n_samples score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
2402: parameters array like shape n_samples n_features training set 
2403: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn linear_model ridgeclassiercv class sklearn linear_model ridgeclassifiercv alphas array 
2404: normalize false t_intercept true score_func none loss_func none none class_weight none ridge classier built cross validation default performs generalized cross validation form efcient leave one cross validation currently n_features n_samples case handled efciently 
2405: parameters alphas numpy array shape n_alpha array alpha values try small positive values alpha improve conditioning problem reduce variance estimates alpha corresponds linear models logisticregression linearsvc 
2406: t_intercept boolean whether calculate intercept model set false intercept used calculations data expected already centered 
2407: normalize boolean optional true regressors normalized score_func callable optional function takes arguments compares order evaluate performance prediction big good none passed score estimator maximized loss_func callable optional reference scikit learn user guide release function takes arguments compares order evaluate performance prediction small good none passed score estimator maximized cross validation generator optional none generalized cross validation efcient leave one used 
2408: class_weight dict optional weights associated classes form class_label weight given classes supposed weight one 
2409: see also ridgeridge regression ridgeclassifierridge classier ridgecvridge regression built cross validation notes multi class classication n_class classiers trained one versus approach concretely implemented taking advantage multi variate response support ridge 
2410: methods decision_function fit sample_weight class_weight get_params deep predict score set_params params fit ridge classier get parameters estimator predict target values according tted model returns coefcient determination prediction set parameters estimator 
2411: __init__ alphas array 
2412: t_intercept true normalize false score_func none loss_func none none class_weight none fit sample_weight class_weight none fit ridge classier 
2413: parameters array like shape n_samples n_features training vectors n_samples number samples n_features num ber features 
2414: array like shape n_samples target values 
2415: sample_weight oat numpy array shape n_samples sample weight class_weight dict optional weights associated classes form class_label weight given classes supposed weight one 
2416: returns self object chapter user guide scikit learn user guide release returns self get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2417: predict predict target values according tted model 
2418: parameters array like shape n_samples n_features returns array shape n_samples score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
2419: parameters array like shape n_samples n_features training set 
2420: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn linear_model ridgecv class sklearn linear_model ridgecv alphas array 
2421: t_intercept true malize false score_func none loss_func none none gcv_mode none ridge regression built cross validation default performs generalized cross validation form efcient leave one cross validation 
2422: parameters alphas numpy array shape n_alpha array alpha values try small positive values alpha improve conditioning problem reduce variance estimates alpha corresponds linear models logisticregression linearsvc 
2423: t_intercept boolean whether calculate intercept model set false intercept used calculations data expected already centered 
2424: normalize boolean optional reference scikit learn user guide release true regressors normalized score_func callable optional function takes arguments compares order evaluate performance prediction big good none passed score estimator maximized loss_func callable optional function takes arguments compares order evaluate performance prediction small good none passed score estimator maximized cross validation generator optional none generalized cross validation efcient leave one used 
2425: see also ridgeridge regression ridgeclassifierridge classier ridgecvridge regression built cross validation attributes coef_ gcv_mode methods shape n_features array n_classes n_features none auto svd eigen tional weight vector 
2426: flag indicating strategy use performing generalized cross validation options auto use svd n_samples n_features otherwise use eigen svd force computation via singular value decomposition eigen force computation via eigendecomposition auto mode default intended pick cheaper tion two depending upon shape training data 
2427: decision_function decision function linear model fit sample_weight get_params deep predict score set_params params fit ridge regression model get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
2428: __init__ alphas array 
2429: t_intercept true normalize false score_func none loss_func none none gcv_mode none decision_function decision function linear model parameters numpy array shape n_samples n_features chapter user guide scikit learn user guide release returns array shape n_samples returns predicted values 
2430: fit sample_weight fit ridge regression model parameters array like shape n_samples n_features training data array like shape n_samples n_samples n_responses target values sample_weight oat array like shape n_samples sample weight returns self returns self 
2431: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2432: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2433: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
2434: parameters array like shape n_samples n_features training set 
2435: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self reference scikit learn user guide release sklearn linear_model lasso class sklearn linear_model lasso alpha pute auto warm_start false positive false t_intercept true copy_x true max_iter linear model trained prior regularizer aka lasso optimization objective lasso normalize false precom tol n_samples 2_2 alpha technically lasso model optimizing objective function elastic net rho penalty 
2436: parameters alpha oat optional constant multiplies term defaults t_intercept boolean whether calculate intercept model set false intercept used calculations data expected already centered 
2437: normalize boolean optional true regressors normalized copy_x boolean optional default true true copied else may overwritten 
2438: precompute true false auto array like whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
2439: max_iter int optional maximum number iterations tol oat optional tolerance optimization updates smaller tol optimization code checks dual gap optimality continues smaller tol 
2440: warm_start bool optional set true reuse solution previous call initialization otherwise erase previous solution 
2441: positive bool optional set true forces coefcients positive 
2442: see also lars_path sklearn decomposition sparse_encode lasso_path lassolars lassocv lassolarscv notes algorithm used model coordinate descent 
2443: chapter user guide avoid unnecessary memory duplication argument method directly passed fortran contiguous numpy array 
2444: scikit learn user guide release examples sklearn import linear_model clf linear_model lasso alpha clf fit lasso alpha copy_x true fit_intercept true max_iter normalize false positive false precompute auto tol warm_start false print clf coef_ print clf intercept_ 
2445: attributes coef_ intercept_ array shape n_features oat parameter vector fomulation formula independent term decision function 
2446: methods decision_function decision function linear model fit coef_init get_params deep predict score set_params params fit elastic net model coordinate descent get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
2447: __init__ alpha t_intercept true normalize false precompute auto max_iter tol warm_start false positive false copy_x true decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2448: fit none coef_init none fit elastic net model coordinate descent parameters ndarray n_samples n_features data ndarray n_samples target array like optional reference scikit learn user guide release dot precomputed useful gram matrix precomputed 
2449: coef_init ndarray shape n_features initial coefents warm start optimization notes coordinate descent algorithm considers column data time hence automatically convert input fortran contiguous numpy array necessary avoid memory allocation advised allocate initial data memory directly using format 
2450: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2451: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2452: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
2453: parameters array like shape n_samples n_features training set 
2454: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn linear_model lassocv class sklearn linear_model lassocv eps n_alphas alphas none t_intercept true max_iter precompute auto lasso linear model iterative tting along regularization path normalize false tol copy_x true none verbose false chapter user guide scikit learn user guide release best model selected cross validation optimization objective lasso n_samples 2_2 alpha parameters eps oat optional length path eps means alpha_min alpha_max 
2455: n_alphas int optional number alphas along regularization path alphas numpy array optional list alphas compute models none alphas set automatically precompute true false auto array like whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
2456: max_iter int optional maximum number iterations tol oat optional tolerance optimization updates smaller tol optimization code checks dual gap optimality continues smaller tol 
2457: integer crossvalidation generator optional integer passed number fold default specic crossvalidation jects passed see sklearn cross_validation module list possible objects verbose bool integer amount verbosity see also lars_path lasso_path lassolars lasso lassolarscv notes see examples linear_model lasso_path_with_crossvalidation example avoid unnecessary memory duplication argument method directly passed fortran contiguous numpy array 
2458: attributes alpha_ oat coef_ intercept_ mse_path_ array shape n_alphas n_folds array shape n_features oat amount penalization choosen cross validation parameter vector fomulation formula independent term decision function mean square error test set fold varying alpha reference scikit learn user guide release methods decision_function fit get_params deep path eps n_alphas alphas compute lasso path coordinate descent predict score set_params params predict using linear model returns coefcient determination prediction set parameters estimator 
2459: decision function linear model fit linear model coordinate descent along decreasing alphas get parameters estimator __init__ eps n_alphas alphas none t_intercept true normalize false precom pute auto max_iter tol copy_x true none verbose false decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2460: fit fit linear model coordinate descent along decreasing alphas using cross validation parameters numpy array shape n_samples n_features training data pass directly fortran contiguous data avoid unnecessary memory duplication numpy array shape n_samples target values get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators precompute auto none eps t_intercept true normalize false copy_x true verbose false params static path n_alphas alphas none compute lasso path coordinate descent optimization objective lasso n_samples 2_2 alpha parameters numpy array shape n_samples n_features training data pass directly fortran contiguous data avoid unnecessary memory duplication numpy array shape n_samples target values eps oat optional length path eps means alpha_min alpha_max chapter user guide scikit learn user guide release n_alphas int optional number alphas along regularization path alphas numpy array optional list alphas compute models none alphas set automatically precompute true false auto array like whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
2461: array like optional dot precomputed useful gram matrix precomputed 
2462: t_intercept bool fit intercept normalize boolean optional true regressors normalized copy_x boolean optional default true true copied else may overwritten 
2463: verbose bool integer amount verbosity params kwargs keyword arguments passed lasso objects returns models list models along regularization path see also lars_path sklearn decomposition sparse_encode lasso lassolars lassocv lassolarscv notes see examples linear_model plot_lasso_coordinate_descent_path example avoid unnecessary memory duplication argument method directly passed fortran contiguous numpy array 
2464: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2465: score returns coefcient determination prediction 
2466: reference scikit learn user guide release coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
2467: parameters array like shape n_samples n_features training set 
2468: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn linear_model elasticnet class sklearn linear_model elasticnet alpha ize false copy_x true tive false linear model trained prior regularizer minimizes objective function t_intercept true rho normal precompute auto max_iter tol warm_start false posi n_samples 2_2 alpha rho alpha rho 2_2 interested controlling penalty separately keep mind equivalent alpha rho parameter rho corresponds alpha glmnet package alpha corresponds lambda param eter glmnet specically rho lasso penalty currently rho reliable unless supply sequence alpha 
2469: parameters alpha oat constant multiplies penalty terms defaults see notes exact mathematical meaning parameter rho oat elasticnet mixing parameter rho rho penalty penalty rho penalty rho penalty combination t_intercept bool whether intercept estimated false data assumed already centered 
2470: normalize boolean optional chapter user guide scikit learn user guide release true regressors normalized precompute true false auto array like whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
2471: max_iter int optional maximum number iterations copy_x boolean optional default false true copied else may overwritten 
2472: tol oat optional tolerance optimization updates smaller tol optimization code checks dual gap optimality continues smaller tol 
2473: warm_start bool optional set true reuse solution previous call initialization otherwise erase previous solution 
2474: positive bool optional set true forces coefcients positive 
2475: notes avoid unnecessary memory duplication argument method directly passed fortran contiguous numpy array 
2476: methods decision_function decision function linear model fit coef_init get_params deep predict score set_params params fit elastic net model coordinate descent get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
2477: __init__ alpha rho t_intercept true normalize false precompute auto max_iter copy_x true tol warm_start false positive false decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2478: fit none coef_init none fit elastic net model coordinate descent parameters ndarray n_samples n_features reference scikit learn user guide release data ndarray n_samples target array like optional dot precomputed useful gram matrix precomputed 
2479: coef_init ndarray shape n_features initial coefents warm start optimization notes coordinate descent algorithm considers column data time hence automatically convert input fortran contiguous numpy array necessary avoid memory allocation advised allocate initial data memory directly using format 
2480: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2481: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2482: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
2483: parameters array like shape n_samples n_features training set 
2484: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self chapter user guide sklearn linear_model elasticnetcv scikit learn user guide release class sklearn linear_model elasticnetcv rho eps n_alphas alphas none t_intercept true precom pute auto max_iter tol none copy_x true verbose n_jobs normalize false elastic net model iterative tting along regularization path best model selected cross validation 
2485: parameters rho oat optional oat passed elasticnet scaling penalties rho penalty penalty rho penalty rho penalty combination parameter list case different values tested cross validation one giving best prediction score used note good choice list values rho often put values close lasso less close ridge eps oat optional length path eps means alpha_min alpha_max 
2486: n_alphas int optional number alphas along regularization path alphas numpy array optional list alphas compute models none alphas set automatically precompute true false auto array like whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
2487: max_iter int optional maximum number iterations tol oat optional tolerance optimization updates smaller tol optimization code checks dual gap optimality continues smaller tol 
2488: integer crossvalidation generator optional integer passed number fold default specic crossvalidation jects passed see sklearn cross_validation module list possible objects verbose bool integer amount verbosity n_jobs integer optional number cpus use cross validation use cpus note used multiple values rho given 
2489: see also enet_path elasticnet reference scikit learn user guide release notes see examples linear_model lasso_path_with_crossvalidation example avoid unnecessary memory duplication argument method directly passed fortran contiguous numpy array parameter rho corresponds alpha glmnet package alpha corresponds lambda param eter glmnet specically optimization objective n_samples 2_2 alpha rho alpha rho 2_2 interested controlling penalty separately keep mind equivalent alpha rho attributes alpha_ oat rho_ oat coef_ intercept_ mse_path_ array shape n_rho n_alpha n_folds methods array shape n_features oat amount penalization choosen cross validation compromise penalization choosen cross validation parameter vector fomulation formula independent term decision function mean square error test set fold varying rho alpha decision_function fit get_params deep path rho eps n_alphas alphas compute elastic net path coordinate descent predict score set_params params predict using linear model returns coefcient determination prediction set parameters estimator 
2490: decision function linear model fit linear model coordinate descent along decreasing alphas get parameters estimator __init__ rho eps n_alphas alphas none t_intercept true normalize false tol none copy_x true verbose precompute auto max_iter n_jobs decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples chapter user guide scikit learn user guide release returns predicted values 
2491: fit fit linear model coordinate descent along decreasing alphas using cross validation parameters numpy array shape n_samples n_features training data pass directly fortran contiguous data avoid unnecessary memory duplication numpy array shape n_samples target values get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2492: static path rho eps n_alphas alphas none precompute auto none t_intercept true normalize false copy_x true verbose false params compute elastic net path coordinate descent elastic net optimization function n_samples 2_2 alpha rho alpha rho 2_2 parameters numpy array shape n_samples n_features training data pass directly fortran contiguous data avoid unnecessary memory duplication numpy array shape n_samples target values rho oat optional oat passed elasticnet scaling penalties rho corresponds lasso eps oat length path eps means alpha_min alpha_max n_alphas int optional number alphas along regularization path alphas numpy array optional list alphas compute models none alphas set automatically precompute true false auto array like whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
2493: array like optional dot precomputed useful gram matrix precomputed 
2494: reference scikit learn user guide release t_intercept bool fit intercept normalize boolean optional true regressors normalized copy_x boolean optional default true true copied else may overwritten 
2495: verbose bool integer amount verbosity params kwargs keyword arguments passed lasso objects returns models list models along regularization path see also elasticnet elasticnetcv notes see examples plot_lasso_coordinate_descent_path example 
2496: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2497: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
2498: parameters array like shape n_samples n_features training set 
2499: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self chapter user guide sklearn linear_model lars scikit learn user guide release class sklearn linear_model lars t_intercept true verbose false normalize true precom pute auto n_nonzero_coefs eps 2204460492503131e copy_x true least angle regression model lar parameters n_nonzero_coefs int optional target number non zero coefcients use inf limit 
2500: t_intercept boolean whether calculate intercept model set false intercept used calculations data expected already centered 
2501: verbose boolean integer optional sets verbosity amount normalize boolean optional true regressors normalized precompute true false auto array like whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
2502: copy_x boolean optional default true true copied else may overwritten 
2503: eps oat optional machine precision regularization computation cholesky diagonal fac tors increase ill conditioned systems unlike tol parameter iterative optimization based algorithms parameter control tolerance optimization 
2504: see also lars_path larscv sklearn decomposition sparse_encode http wikipedia org wiki least_angle_regression examples sklearn import linear_model clf linear_model lars n_nonzero_coefs clf fit lars copy_x true eps fit_intercept true n_nonzero_coefs normalize true precompute auto verbose false print clf coef_ attributes coef_ intercept_ array shape n_features oat parameter vector fomulation formula independent term decision function 
2505: reference scikit learn user guide release methods decision_function decision function linear model fit get_params deep predict score set_params params fit model using training data get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
2506: __init__ t_intercept true precompute auto n_nonzero_coefs eps 2204460492503131e copy_x true verbose false normalize true decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2507: fit fit model using training data 
2508: parameters array like shape n_samples n_features training data 
2509: array like shape n_samples target values returns self object returns instance self 
2510: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2511: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2512: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
2513: parameters array like shape n_samples n_features training set 
2514: chapter user guide scikit learn user guide release array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn linear_model lassolars class sklearn linear_model lassolars alpha t_intercept true malize true eps 2204460492503131e copy_x true precompute auto verbose false max_iter lasso model least angle regression lars linear model trained prior regularizer optimization objective lasso n_samples 2_2 alpha parameters t_intercept boolean whether calculate intercept model set false intercept used calculations data expected already centered 
2515: verbose boolean integer optional sets verbosity amount normalize boolean optional true regressors normalized copy_x boolean optional default true true copied else may overwritten 
2516: precompute true false auto array like whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
2517: max_iter integer optional maximum number iterations perform 
2518: eps oat optional machine precision regularization computation cholesky diagonal fac tors increase ill conditioned systems unlike tol parameter iterative optimization based algorithms parameter control tolerance optimization 
2519: see also lars_path lasso_path lasso lassocv lassolarscv sklearn decomposition sparse_encode http wikipedia org wiki least_angle_regression reference scikit learn user guide release examples sklearn import linear_model clf linear_model lassolars alpha clf fit lassolars alpha copy_x true eps fit_intercept true max_iter normalize true precompute auto verbose false print clf coef_ 
2520: attributes coef_ intercept_ array shape n_features oat parameter vector fomulation formula independent term decision function 
2521: methods decision_function decision function linear model fit get_params deep predict score set_params params fit model using training data get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
2522: __init__ alpha t_intercept true verbose false normalize true precompute auto max_iter eps 2204460492503131e copy_x true decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2523: fit fit model using training data 
2524: parameters array like shape n_samples n_features training data 
2525: array like shape n_samples target values returns self object returns instance self 
2526: get_params deep true get parameters estimator parameters deep boolean optional chapter user guide scikit learn user guide release true return parameters estimator contained subobjects estimators 
2527: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2528: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
2529: parameters array like shape n_samples n_features training set 
2530: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn linear_model larscv class sklearn linear_model larscv t_intercept true verbose false max_iter normal ize true precompute auto none max_n_alphas n_jobs eps 2204460492503131e copy_x true cross validated least angle regression model parameters t_intercept boolean whether calculate intercept model set false intercept used calculations data expected already centered 
2531: verbose boolean integer optional sets verbosity amount normalize boolean optional true regressors normalized copy_x boolean optional default true true copied else may overwritten 
2532: precompute true false auto array like whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
2533: reference scikit learn user guide release max_iter integer optional maximum number iterations perform 
2534: crossvalidation generator optional see sklearn cross_validation module none passed default fold strategy max_n_alphas integer optional maximum number points path used compute residuals cross validation n_jobs integer optional number cpus use cross validation use cpus eps oat optional machine precision regularization computation cholesky diagonal fac tors increase ill conditioned systems 
2535: see also lars_path lassolars lassolarscv attributes coef_ intercept_ coef_path array shape n_features n_alpha methods array shape n_features oat parameter vector fomulation formula independent term decision function varying values coefcients along path decision_function decision function linear model fit get_params deep predict score set_params params fit model using training data get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
2536: __init__ t_intercept true verbose false max_iter normalize true precompute auto none max_n_alphas n_jobs eps 2204460492503131e copy_x true decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2537: fit fit model using training data 
2538: parameters array like shape n_samples n_features chapter user guide scikit learn user guide release training data 
2539: array like shape n_samples target values returns self object returns instance self 
2540: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2541: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2542: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
2543: parameters array like shape n_samples n_features training set 
2544: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn linear_model lassolarscv class sklearn linear_model lassolarscv t_intercept true verbose false max_iter precompute auto n_jobs cross validated lasso using lars algorithm optimization objective lasso normalize true none eps 2204460492503131e copy_x true max_n_alphas n_samples 2_2 alpha parameters t_intercept boolean reference scikit learn user guide release whether calculate intercept model set false intercept used calculations data expected already centered 
2545: verbose boolean integer optional sets verbosity amount normalize boolean optional true regressors normalized precompute true false auto array like whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
2546: max_iter integer optional maximum number iterations perform 
2547: crossvalidation generator optional see sklearn cross_validation module none passed default fold strategy max_n_alphas integer optional maximum number points path used compute residuals cross validation n_jobs integer optional number cpus use cross validation use cpus eps oat optional machine precision regularization computation cholesky diagonal fac tors increase ill conditioned systems 
2548: copy_x boolean optional default true true copied else may overwritten 
2549: see also lars_path lassolars larscv lassocv notes object solves problem lassocv object however unlike lassocv relevent alphas values general property stable however fragile heavily multicollinear datasets efcient lassocv small number features selected compared total number instance samples compared number features 
2550: chapter user guide scikit learn user guide release array shape n_features oat parameter vector fomulation formula independent term decision function varying values coefcients along path different values alpha along path values alpha along path different folds mean square error left fold along path alpha values given cv_alphas attributes coef_ intercept_ coef_path array shape n_features n_alpha alphas_ array shape n_alpha cv_alphas array shape n_cv_alphas cv_mse_path_ array shape n_folds n_cv_alphas methods decision_function decision function linear model fit get_params deep predict score set_params params fit model using training data get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
2551: __init__ t_intercept true verbose false max_iter normalize true precompute auto none max_n_alphas n_jobs eps 2204460492503131e copy_x true decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2552: fit fit model using training data 
2553: parameters array like shape n_samples n_features training data 
2554: array like shape n_samples target values returns self object returns instance self 
2555: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2556: reference scikit learn user guide release predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2557: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
2558: parameters array like shape n_samples n_features training set 
2559: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn linear_model lassolarsic class sklearn linear_model lassolarsic criterion aic lasso model lars using bic aic model selection optimization objective lasso verbose false normalize true precompute auto max_iter eps 2204460492503131e copy_x true t_intercept true n_samples 2_2 alpha aic akaike information criterion bic bayes information criterion criteria useful select value regularization parameter making trade goodness complexity model good model explain well data simple 
2560: parameters criterion bic aic type criterion use 
2561: t_intercept boolean whether calculate intercept model set false intercept used calculations data expected already centered 
2562: verbose boolean integer optional sets verbosity amount normalize boolean optional true regressors normalized chapter user guide scikit learn user guide release copy_x boolean optional default true true copied else may overwritten 
2563: precompute true false auto array like whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
2564: max_iter integer optional maximum number iterations perform used early stopping 
2565: eps oat optional machine precision regularization computation cholesky diagonal fac tors increase ill conditioned systems unlike tol parameter iterative optimization based algorithms parameter control tolerance optimization 
2566: see also lars_path lassolars lassolarscv notes estimation number degrees freedom given degrees freedom lasso hui zou trevor hastie robert tibshirani ann statist volume number http wikipedia org wiki akaike_information_criterion http wikipedia org wiki bayesian_information_criterion examples sklearn import linear_model clf linear_model lassolarsic criterion bic clf fit lassolarsic copy_x true criterion bic eps fit_intercept true max_iter normalize true precompute auto verbose false print clf coef_ 
2567: attributes coef_ intercept_ alpha_ array shape n_features oat oat parameter vector fomulation formula independent term decision function alpha parameter chosen information criterion methods decision_function decision function linear model continued next page reference scikit learn user guide release table continued previous page fit copy_x get_params deep predict score set_params params fit model using training data get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
2568: __init__ criterion aic t_intercept true verbose false normalize true precompute auto max_iter eps 2204460492503131e copy_x true decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2569: fit copy_x true fit model using training data 
2570: parameters array like shape n_samples n_features training data 
2571: array like shape n_samples target values returns self object returns instance self 
2572: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2573: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2574: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
2575: parameters array like shape n_samples n_features training set 
2576: array like shape n_samples returns oat chapter user guide scikit learn user guide release set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn linear_model logisticregression class sklearn linear_model logisticregression penalty dual false cept_scaling class_weight none t_intercept true tol inter logistic regression aka logit maxent classier multiclass case training algorithm uses one ova scheme rather true multinomial class implements regularized logistic regression using liblinear library handle dense sparse input use ordered arrays csr matrices containing bit oats optimal performance input format converted copied 
2577: parameters penalty string used specify norm used penalization dual boolean dual primal formulation dual formulation implemented penalty prefer dual false n_samples n_features 
2578: oat none optional default none species strength regularization smaller bigger regular ization none set n_samples 
2579: t_intercept bool default true species constant bias intercept added decision function intercept_scaling oat default self t_intercept true instance vector becomes self intercept_scaling synthetic feature constant value equals intercept_scaling appended instance vector intercept becomes intercept_scaling synthetic feature weight note synthetic feature weight subject regularization features lessen effect regularization synthetic feature weight therefore intercept intercept_scaling increased tol oat optional tolerance stopping criteria see also linearsvc reference scikit learn user guide release notes underlying implementation uses random number generator select features tting model thus uncommon slightly different results input data happens try smaller tol parameter references liblinear library large linear classicationhttp www csie ntu edu cjlin liblinear hsiang fang lan huang chih jen lin dual coordinate descentmethods machine learning 
2580: regression gistic maximum entropy models http www csie ntu edu cjlin papers maxent_dual pdf attributes array shape n_classes n_features array shape n_classes coef_ ter cept_ methods coefcient features decision function coef_ readonly property derived raw_coef_ follows internal memory layout liblinear intercept bias added decision function available parameter intercept set true decision_function decision function value according trained model fit class_weight fit_transform get_params deep predict predict_log_proba predict_proba score set_params params transform threshold reduce important features 
2581: fit model according given training data fit data transform get parameters estimator predict target values according tted model log probability estimates probability estimates returns mean accuracy given test data labels set parameters estimator 
2582: __init__ penalty dual false class_weight none decision_function tol t_intercept true intercept_scaling decision function value according trained model 
2583: parameters array like shape n_samples n_features returns array like shape n_samples n_class returns decision function sample class model 
2584: fit class_weight none fit model according given training data 
2585: parameters array like sparse matrix shape n_samples n_features training vector n_samples number samples n_features num ber features 
2586: chapter user guide scikit learn user guide release array like shape n_samples target vector relative class_weight dict auto optional weights associated classes given classes supposed weight one 
2587: returns self object returns self 
2588: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2589: parameters numpy array shape n_samples n_features training set 
2590: numpy array shape n_samples target values 
2591: returns x_new numpy array shape n_samples n_features_new transformed array 
2592: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
2593: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2594: predict predict target values according tted model 
2595: parameters array like sparse matrix shape n_samples n_features returns array shape n_samples predict_log_proba log probability estimates returned estimates classes ordered label classes 
2596: parameters array like shape n_samples n_features returns array like shape n_samples n_classes returns log probabilities sample class model classes ordered arithmetical order 
2597: predict_proba probability estimates returned estimates classes ordered label classes 
2598: reference scikit learn user guide release parameters array like shape n_samples n_features returns array like shape n_samples n_classes returns probability sample class model classes ordered arithmetical order 
2599: score returns mean accuracy given test data labels 
2600: parameters array like shape n_samples n_features training set 
2601: array like shape n_samples labels 
2602: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform threshold none reduce important features 
2603: parameters array scipy sparse matrix shape n_samples n_features input samples 
2604: threshold string oat none optional default none threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor mean may also used none available object attribute threshold used otherwise mean used default 
2605: returns x_r array shape n_samples n_selected_features input samples selected features 
2606: sklearn linear_model orthogonalmatchingpursuit class sklearn linear_model orthogonalmatchingpursuit copy_x true copy_gram true copy_xy true n_nonzero_coefs none tol none normalize true pute_gram false t_intercept true precom orthogonal mathching pursuit model omp parameters n_nonzero_coefs int optional desired number non zero entries solution none default value set n_features 
2607: tol oat optional chapter user guide scikit learn user guide release maximum norm residual none overrides n_nonzero_coefs 
2608: t_intercept boolean optional whether calculate intercept model set false intercept used calculations data expected already centered 
2609: normalize boolean optional false regressors assumed already normalized 
2610: precompute_gram true false auto whether use precomputed gram matrix speed calculations improves performance n_targets n_samples large note already matrices pass directly method 
2611: copy_x bool optional whether design matrix must copied algorithm false value helpful already fortran ordered otherwise copy made anyway 
2612: copy_gram bool optional whether gram matrix must copied algorithm false value helpful already fortran ordered otherwise copy made anyway 
2613: copy_xy bool optional whether covariance vector must copied algorithm false may overwritten 
2614: see also orthogonal_mp decomposition sparse_encode decomposition sparse_encode_parallel orthogonal_mp_gram lars_path lars lassolars notes orthogonal matching pursuit introduced mallat zhang matching pursuits time frequency dictionaries ieee transactions signal processing vol december http blanche polytechnique mallat papiers mallatpursuit93 pdf implementation based rubinstein zibulevsky elad efcient implementation svd algorithm using batch orthogonal matching pursuit technical report technion april http www technion ronrubin publications ksvd omp pdf attributes coef_ intercept_ array shape n_features n_features n_targets oat array shape n_targets parameter vector fomulation formula independent term decision function 
2615: methods decision_function decision function linear model fit gram get_params deep fit model using training data get parameters estimator continued next page reference scikit learn user guide release table continued previous page predict score set_params params predict using linear model returns coefcient determination prediction set parameters estimator 
2616: __init__ copy_x true copy_gram true t_intercept true normalize true precompute_gram false copy_xy true n_nonzero_coefs none tol none decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2617: fit gram none none fit model using training data 
2618: parameters array like shape n_samples n_features training data 
2619: array like shape n_samples n_samples n_targets target values 
2620: gram array like shape n_features n_features optional gram matrix input data array like shape n_features n_features n_targets optional input targets multiplied returns self object returns instance self 
2621: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2622: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2623: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
2624: parameters array like shape n_samples n_features chapter user guide scikit learn user guide release training set 
2625: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn linear_model perceptron class sklearn linear_model perceptron penalty none t_intercept true n_iter shufe false verbose eta0 n_jobs seed class_weight none warm_start false alpha perceptron parameters penalty none elasticnet penalty aka regularization term used defaults none 
2626: alpha oat constant multiplies regularization term regularization used defaults t_intercept bool whether intercept estimated false data assumed already centered defaults true 
2627: n_iter int optional number passes training data aka epochs defaults 
2628: shufe bool optional whether training data shufed epoch defaults false 
2629: seed int optional seed pseudo random number generator use shufing data 
2630: verbose integer optional verbosity level n_jobs integer optional number cpus use ova one versus multi class problems computation means cpus defaults 
2631: eta0 double constant updates multiplied defaults 
2632: class_weight dict class_label reference scikit learn user guide release preset class_weight parameter weights associated classes given classes supposed weight one auto mode uses values automatically adjust weights inversely propor tional class frequencies 
2633: warm_start bool optional set true reuse solution previous call initialization otherwise erase previous solution 
2634: see also sgdclassifier notes perceptron sgdclassier share underlying implementation fact perceptron equivalent sgdclassier loss perceptron eta0 learning_rate constant penalty none 
2635: references http wikipedia org wiki perceptron references therein 
2636: attributes coef_ n_features intercept_ methods array shape n_features n_classes else n_classes array shape n_classes else n_classes weights assigned features constants decision function 
2637: decision_function fit coef_init intercept_init fit_transform get_params deep partial_fit classes class_weight predict predict_proba score set_params params transform threshold predict signed distance hyperplane aka condence score fit linear model stochastic gradient descent fit data transform get parameters estimator fit linear model stochastic gradient descent predict using linear model predict class membership probability returns mean accuracy given test data labels set parameters estimator reduce important features 
2638: __init__ penalty none alpha t_intercept true n_iter shufe false verbose eta0 n_jobs seed class_weight none warm_start false classes deprecated removed use classes_ instead 
2639: decision_function predict signed distance hyperplane aka condence score chapter user guide scikit learn user guide release parameters array like sparse matrix shape n_samples n_features returns array shape n_samples n_classes else n_samples n_classes signed distances hyperplane 
2640: fit coef_init none intercept_init none class_weight none sample_weight none fit linear model stochastic gradient descent 
2641: parameters array like sparse matrix shape n_samples n_features training data numpy array shape n_samples target values coef_init array shape n_classes n_features initial coefents warm start optimization 
2642: intercept_init array shape n_classes initial intercept warm start optimization 
2643: sample_weight array like shape n_samples optional weights applied individual samples provided uniform weights assumed 
2644: returns self returns instance self 
2645: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2646: parameters numpy array shape n_samples n_features training set 
2647: numpy array shape n_samples target values 
2648: returns x_new numpy array shape n_samples n_features_new transformed array 
2649: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
2650: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2651: partial_fit classes none class_weight none sample_weight none fit linear model stochastic gradient descent 
2652: parameters array like sparse matrix shape n_samples n_features subset training data reference scikit learn user guide release numpy array shape n_samples subset target values classes array shape n_classes classes across calls partial_t obtained via unique y_all y_all target vector entire dataset argument required rst call partial_t omitted subsequent calls note doesnt need contain labels classes 
2653: sample_weight array like shape n_samples optional weights applied individual samples provided uniform weights assumed 
2654: returns self returns instance self 
2655: predict predict using linear model parameters array like sparse matrix shape n_samples n_features returns array shape n_samples array containing predicted class labels 
2656: predict_proba predict class membership probability parameters array like sparse matrix shape n_samples n_features returns array shape n_samples n_classes else n_samples n_classes contains membership probabilities positive class 
2657: score returns mean accuracy given test data labels 
2658: parameters array like shape n_samples n_features training set 
2659: array like shape n_samples labels 
2660: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform threshold none reduce important features 
2661: parameters array scipy sparse matrix shape n_samples n_features input samples 
2662: threshold string oat none optional default none chapter user guide scikit learn user guide release threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor mean may also used none available object attribute threshold used otherwise mean used default 
2663: returns x_r array shape n_samples n_selected_features input samples selected features 
2664: sklearn linear_model sgdclassier class sklearn linear_model sgdclassifier loss hinge penalty t_intercept true alpha shuf learn power_t rho n_iter false verbose n_jobs seed ing_rate optimal class_weight none warm_start false eta0 linear model tted minimizing regularized empirical loss sgd sgd stands stochastic gradient descent gradient loss estimated sample time model updated along way decreasing strength schedule aka learning rate regularizer penalty added loss function shrinks model parameters towards zero vector using either squared euclidean norm absolute norm combination elastic net parameter update crosses value regularizer update truncated allow learning sparse models achieve online feature selection implementation works data represented dense numpy arrays oating point values features 
2665: parameters loss str hinge log modied_huber loss function used defaults hinge hinge loss margin loss used standard linear svm models log loss loss logistic regression models used probability estimation binary classiers modied_huber another smooth loss brings tolerance outliers 
2666: penalty str elasticnet penalty aka regularization term used defaults standard regularizer linear svm models elasticnet migh bring sparsity model feature selection achievable 
2667: alpha oat constant multiplies regularization term defaults rho oat elastic net mixing parameter rho defaults 
2668: t_intercept bool whether intercept estimated false data assumed already centered defaults true 
2669: n_iter int optional number passes training data aka epochs defaults 
2670: shufe bool optional whether training data shufed epoch defaults false 
2671: reference scikit learn user guide release seed int optional seed pseudo random number generator use shufing data 
2672: verbose integer optional verbosity level n_jobs integer optional number cpus use ova one versus multi class problems computation means cpus defaults 
2673: learning_rate string optional learning rate constant eta eta0 optimal eta default invscaling eta eta0 pow power_t eta0 double initial learning rate default 
2674: power_t double exponent inverse scaling learning rate default 
2675: class_weight dict class_label preset class_weight parameter weights associated classes given classes supposed weight one auto mode uses values automatically adjust weights inversely propor tional class frequencies 
2676: warm_start bool optional set true reuse solution previous call initialization otherwise erase previous solution 
2677: see also linearsvc logisticregression perceptron examples import numpy sklearn import linear_model array array clf linear_model sgdclassifier clf fit sgdclassifier alpha class_weight none eta0 fit_intercept true learning_rate optimal loss hinge n_iter n_jobs penalty power_t rho seed shuffle false verbose warm_start false print clf predict chapter user guide scikit learn user guide release array shape n_features n_classes else n_classes array shape n_classes else n_classes weights assigned features constants decision function 
2678: attributes coef_ n_features intercept_ methods decision_function fit coef_init intercept_init fit_transform get_params deep partial_fit classes class_weight predict predict_proba score set_params params transform threshold predict signed distance hyperplane aka condence score fit linear model stochastic gradient descent fit data transform get parameters estimator fit linear model stochastic gradient descent predict using linear model predict class membership probability returns mean accuracy given test data labels set parameters estimator reduce important features 
2679: __init__ loss hinge penalty alpha rho t_intercept true n_iter shuf false verbose n_jobs seed learning_rate optimal eta0 power_t class_weight none warm_start false classes deprecated removed use classes_ instead 
2680: decision_function predict signed distance hyperplane aka condence score parameters array like sparse matrix shape n_samples n_features returns array shape n_samples n_classes else n_samples n_classes signed distances hyperplane 
2681: fit coef_init none intercept_init none class_weight none sample_weight none fit linear model stochastic gradient descent 
2682: parameters array like sparse matrix shape n_samples n_features training data numpy array shape n_samples target values coef_init array shape n_classes n_features initial coefents warm start optimization 
2683: intercept_init array shape n_classes initial intercept warm start optimization 
2684: sample_weight array like shape n_samples optional weights applied individual samples provided uniform weights assumed 
2685: returns self returns instance self 
2686: reference scikit learn user guide release fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2687: parameters numpy array shape n_samples n_features training set 
2688: numpy array shape n_samples target values 
2689: returns x_new numpy array shape n_samples n_features_new transformed array 
2690: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
2691: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2692: partial_fit classes none class_weight none sample_weight none fit linear model stochastic gradient descent 
2693: parameters array like sparse matrix shape n_samples n_features subset training data numpy array shape n_samples subset target values classes array shape n_classes classes across calls partial_t obtained via unique y_all y_all target vector entire dataset argument required rst call partial_t omitted subsequent calls note doesnt need contain labels classes 
2694: sample_weight array like shape n_samples optional weights applied individual samples provided uniform weights assumed 
2695: returns self returns instance self 
2696: predict predict using linear model parameters array like sparse matrix shape n_samples n_features returns array shape n_samples array containing predicted class labels 
2697: predict_proba predict class membership probability chapter user guide scikit learn user guide release parameters array like sparse matrix shape n_samples n_features returns array shape n_samples n_classes else n_samples n_classes contains membership probabilities positive class 
2698: score returns mean accuracy given test data labels 
2699: parameters array like shape n_samples n_features training set 
2700: array like shape n_samples labels 
2701: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform threshold none reduce important features 
2702: parameters array scipy sparse matrix shape n_samples n_features input samples 
2703: threshold string oat none optional default none threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor mean may also used none available object attribute threshold used otherwise mean used default 
2704: returns x_r array shape n_samples n_selected_features input samples selected features 
2705: sklearn linear_model sgdregressor class sklearn linear_model sgdregressor loss squared_loss penalty t_intercept true verbose rho false ing_rate invscaling warm_start false n_iter seed alpha shuf learn power_t eta0 linear model tted minimizing regularized empirical loss sgd sgd stands stochastic gradient descent gradient loss estimated sample time model updated along way decreasing strength schedule aka learning rate regularizer penalty added loss function shrinks model parameters towards zero vector using either squared euclidean norm absolute norm combination elastic net reference scikit learn user guide release parameter update crosses value regularizer update truncated allow learning sparse models achieve online feature selection implementation works data represented dense numpy arrays oating point values features 
2706: parameters loss str squared_loss huber loss function used defaults squared_loss refers ordinary least squares huber epsilon insensitive loss function robust regression 
2707: penalty str elasticnet penalty aka regularization term used defaults standard regularizer linear svm models elasticnet migh bring sparsity model feature selection achievable 
2708: alpha oat constant multiplies regularization term defaults rho oat elastic net mixing parameter rho defaults 
2709: t_intercept bool whether intercept estimated false data assumed already centered defaults true 
2710: n_iter int optional number passes training data aka epochs defaults 
2711: shufe bool optional whether training data shufed epoch defaults false 
2712: seed int optional seed pseudo random number generator use shufing data 
2713: verbose integer optional verbosity level 
2714: oat epsilon epsilon insensitive huber loss function loss huber 
2715: learning_rate string optional learning rate constant eta eta0 optimal eta invscaling eta eta0 pow power_t default eta0 double optional initial learning rate default 
2716: power_t double optional exponent inverse scaling learning rate default 
2717: warm_start bool optional set true reuse solution previous call initialization otherwise erase previous solution 
2718: chapter user guide scikit learn user guide release see also ridge elasticnet lasso svr examples import numpy sklearn import linear_model n_samples n_features random seed random randn n_samples random randn n_samples n_features clf linear_model sgdregressor clf fit sgdregressor alpha eta0 fit_intercept true learning_rate invscaling loss squared_loss n_iter penalty power_t rho seed shuffle false verbose warm_start false attributes coef_ intercept_ array shape n_features weights asigned features array shape intercept term 
2719: methods decision_function fit coef_init intercept_init fit_transform get_params deep partial_fit sample_weight predict score set_params params transform threshold predict using linear model fit linear model stochastic gradient descent fit data transform get parameters estimator fit linear model stochastic gradient descent predict using linear model returns coefcient determination prediction set parameters estimator reduce important features 
2720: __init__ loss squared_loss penalty alpha rho t_intercept true n_iter eta0 learning_rate invscaling seed shufe false power_t warm_start false verbose decision_function predict using linear model parameters array like sparse matrix shape n_samples n_features returns array shape n_samples predicted target values per element 
2721: fit coef_init none intercept_init none sample_weight none fit linear model stochastic gradient descent 
2722: parameters array like sparse matrix shape n_samples n_features reference scikit learn user guide release training data numpy array shape n_samples target values coef_init array shape n_features initial coefents warm start optimization 
2723: intercept_init array shape initial intercept warm start optimization 
2724: sample_weight array like shape n_samples optional weights applied individual samples unweighted 
2725: returns self returns instance self 
2726: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2727: parameters numpy array shape n_samples n_features training set 
2728: numpy array shape n_samples target values 
2729: returns x_new numpy array shape n_samples n_features_new transformed array 
2730: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
2731: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2732: partial_fit sample_weight none fit linear model stochastic gradient descent 
2733: parameters array like sparse matrix shape n_samples n_features subset training data numpy array shape n_samples subset target values sample_weight array like shape n_samples optional weights applied individual samples provided uniform weights assumed 
2734: returns self returns instance self 
2735: chapter user guide scikit learn user guide release predict predict using linear model parameters array like sparse matrix shape n_samples n_features returns array shape n_samples predicted target values per element 
2736: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
2737: parameters array like shape n_samples n_features training set 
2738: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform threshold none reduce important features 
2739: parameters array scipy sparse matrix shape n_samples n_features input samples 
2740: threshold string oat none optional default none threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor mean may also used none available object attribute threshold used otherwise mean used default 
2741: returns x_r array shape n_samples n_selected_features input samples selected features 
2742: sklearn linear_model bayesianridge class sklearn linear_model bayesianridge n_iter alpha_2 normalize false copy_x true verbose false compute_score false tol lambda_1 alpha_1 lambda_2 t_intercept true bayesian ridge regression fit bayesian ridge model optimize regularization parameters lambda precision weights alpha precision noise 
2743: parameters array shape n_samples n_features reference scikit learn user guide release training vectors 
2744: array shape length target values training vectors n_iter int optional maximum number iterations default 
2745: tol oat optional stop algorithm converged default 
2746: alpha_1 oat optional hyper parameter shape parameter gamma distribution prior alpha parameter default alpha_2 oat optional hyper parameter inverse scale parameter rate parameter gamma distribution prior alpha parameter default 
2747: lambda_1 oat optional hyper parameter shape parameter gamma distribution prior lambda parameter default 
2748: lambda_2 oat optional hyper parameter inverse scale parameter rate parameter gamma distribution prior lambda parameter default compute_score boolean optional true compute objective function step model default false t_intercept boolean optional wether calculate intercept model set false intercept used calculations data expected already centered default true 
2749: normalize boolean optional default false true regressors normalized copy_x boolean optional default true true copied else may overwritten 
2750: verbose boolean optional default false verbose mode tting model 
2751: notes see examples linear_model plot_bayesian_ridge example 
2752: examples chapter user guide scikit learn user guide release sklearn import linear_model clf linear_model bayesianridge clf fit bayesianridge alpha_1 alpha_2 compute_score false copy_x true fit_intercept true lambda_1 lambda_2 n_iter normalize false tol verbose false clf predict array attributes coef_ alpha_ lambda_ scores_ array shape n_features coefcients regression model mean distribution oat array shape n_features oat estimated precision noise estimated precisions weights computed value objective function maximized methods decision_function decision function linear model fit get_params deep predict score set_params params fit model get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
2753: __init__ n_iter tol alpha_1 alpha_2 lambda_1 lambda_2 ver compute_score false t_intercept true normalize false bose false copy_x true decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2754: fit fit model parameters numpy array shape n_samples n_features training data numpy array shape n_samples target values returns self returns instance self 
2755: get_params deep true get parameters estimator parameters deep boolean optional reference scikit learn user guide release true return parameters estimator contained subobjects estimators 
2756: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2757: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
2758: parameters array like shape n_samples n_features training set 
2759: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn linear_model ardregression class sklearn linear_model ardregression n_iter tol alpha_2 alpha_1 lambda_1 lambda_2 thresh old_lambda t_intercept true normal ize false copy_x true verbose false compute_score false bayesian ard regression fit weights regression model using ard prior weights regression model assumed gaussian distributions also estimate parameters lambda precisions distributions weights alpha precision distribution noise estimation done iterative procedures evidence maximization parameters array shape n_samples n_features training vectors 
2760: array shape n_samples target values training vectors n_iter int optional maximum number iterations default tol oat optional chapter user guide scikit learn user guide release stop algorithm converged default 
2761: alpha_1 oat optional hyper parameter shape parameter gamma distribution prior alpha parameter default 
2762: alpha_2 oat optional hyper parameter inverse scale parameter rate parameter gamma distribution prior alpha parameter default 
2763: lambda_1 oat optional hyper parameter shape parameter gamma distribution prior lambda parameter default 
2764: lambda_2 oat optional hyper parameter inverse scale parameter rate parameter gamma distribution prior lambda parameter default 
2765: compute_score boolean optional true compute objective function step model default false 
2766: threshold_lambda oat optional threshold removing pruning weights high precision computation default 
2767: t_intercept boolean optional wether calculate intercept model set false intercept used calculations data expected already centered default true 
2768: normalize boolean optional true regressors normalized copy_x boolean optional default true 
2769: true copied else may overwritten 
2770: verbose boolean optional default false verbose mode tting model 
2771: notes see examples linear_model plot_ard example 
2772: examples sklearn import linear_model clf linear_model ardregression clf fit ardregression alpha_1 alpha_2 compute_score false copy_x true fit_intercept true lambda_1 lambda_2 n_iter normalize false threshold_lambda tol verbose false reference scikit learn user guide release clf predict array attributes coef_ alpha_ lambda_ sigma_ scores_ array shape n_features oat array shape n_features array shape n_features n_features oat coefcients regression model mean distribution estimated precision noise estimated precisions weights estimated variance covariance matrix weights computed value objective function maximized methods decision_function decision function linear model fit get_params deep predict score set_params params fit ardregression model according given training data get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
2773: __init__ n_iter tol alpha_1 alpha_2 lambda_1 lambda_2 normal threshold_lambda t_intercept true compute_score false ize false copy_x true verbose false decision_function decision function linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2774: fit fit ardregression model according given training data parameters iterative procedure maximize evidence parameters array like shape n_samples n_features training vector n_samples number samples n_features num ber features 
2775: array shape n_samples target values integers returns self returns instance self 
2776: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2777: chapter user guide scikit learn user guide release predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2778: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
2779: parameters array like shape n_samples n_features training set 
2780: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn linear_model randomizedlasso class sklearn linear_model randomizedlasso alpha aic scaling sample_fraction selection_threshold verbose false precompute auto eps 2204460492503131e n_jobs mem n_resampling t_intercept true normalize true max_iter pre_dispatch n_jobs ory memory cachedir none random_state none randomized lasso randomized lasso works resampling train data computing lasso resampling short features selected often good features also known stability selection 
2781: parameters alpha oat aic bic regularization parameter alpha parameter lasso warning alpha parameter stability selection article scaling 
2782: scaling oat alpha parameter stability selection article used randomly scale features 
2783: sample_fraction oat fraction samples used randomized design samples used 
2784: reference scikit learn user guide release t_intercept boolean whether calculate intercept model set false intercept used calculations data expected already centered 
2785: verbose boolean integer optional sets verbosity amount normalize boolean optional true regressors normalized precompute true false auto whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
2786: max_iter integer optional maximum number iterations perform lars algorithm 
2787: eps oat optional machine precision regularization computation cholesky diagonal fac tors increase ill conditioned systems unlike tol parameter iterative optimization based algorithms parameter control tolerance optimization 
2788: n_jobs integer optional number cpus use resampling use cpus random_state int randomstate instance none optional default none int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used random 
2789: pre_dispatch int string optional controls number jobs get dispatched parallel execution reducing number useful avoid explosion memory consumption jobs get dispatched cpus process parameter none case jobs immediatly created spawned use lightweight fast running jobs avoid delays due demand spawning jobs int giving exact number total jobs spawned string giving expression function n_jobs n_jobs memory instance joblib memory string used internal caching default caching done string given thepath caching directory 
2790: see also randomizedlogisticregression logisticregression notes see examples linear_model plot_sparse_recovery example 
2791: chapter user guide scikit learn user guide release references stability selection nicolai meinshausen peter buhlmann journal royal statistical society series volume issue pages september doi examples sklearn linear_model import randomizedlasso randomized_lasso randomizedlasso attributes scores_ array shape n_features all_scores_array shape n_features n_reg_parameter methods feature scores 
2792: feature scores values regularization parameter reference article suggests scores_ max all_scores_ 
2793: fit fit_transform get_params deep get_support indices inverse_transform transform new matrix using selected features set_params params transform fit model using training data fit data transform get parameters estimator return mask list features indices selected 
2794: set parameters estimator transform new matrix using selected features __init__ alpha aic scaling sample_fraction n_resampling normalize true selec precom random_state none tion_threshold pute auto n_jobs pre_dispatch n_jobs memory memory cachedir none eps 2204460492503131e t_intercept true verbose false max_iter fit fit model using training data 
2795: parameters array like shape n_samples n_features training data 
2796: array like shape n_samples target values returns self object returns instance self 
2797: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2798: parameters numpy array shape n_samples n_features reference scikit learn user guide release training set 
2799: numpy array shape n_samples target values 
2800: returns x_new numpy array shape n_samples n_features_new transformed array 
2801: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
2802: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2803: get_support indices false return mask list features indices selected 
2804: inverse_transform transform new matrix using selected features set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform transform new matrix using selected features sklearn linear_model randomizedlogisticregression class sklearn linear_model randomizedlogisticregression scaling sample_fraction n_resampling lection_threshold tol t_intercept true verbose false malize true ran dom_state none n_jobs pre_dispatch n_jobs mem ory memory cachedir none randomized logistic regression randomized regression works resampling train data computing logisticregression sampling short features selected often good features also known stability selection 
2805: parameters oat chapter user guide scikit learn user guide release regularization parameter logisticregression 
2806: scaling oat alpha parameter stability selection article used randomly scale features 
2807: sample_fraction oat fraction samples used randomized design samples used 
2808: t_intercept boolean whether calculate intercept model set false intercept used calculations data expected already centered 
2809: verbose boolean integer optional sets verbosity amount normalize boolean optional true regressors normalized tol oat optional tolerance stopping criteria logisticregression n_jobs integer optional number cpus use resampling use cpus random_state int randomstate instance none optional default none int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used random 
2810: pre_dispatch int string optional controls number jobs get dispatched parallel execution reducing number useful avoid explosion memory consumption jobs get dispatched cpus process parameter none case jobs immediatly created spawned use lightweight fast running jobs avoid delays due demand spawning jobs int giving exact number total jobs spawned string giving expression function n_jobs n_jobs memory instance joblib memory string used internal caching default caching done string given thepath caching directory 
2811: see also randomizedlasso lasso elasticnet notes see examples linear_model plot_randomized_lasso example 
2812: reference scikit learn user guide release references stability selection nicolai meinshausen peter buhlmann journal royal statistical society series volume issue pages september doi examples sklearn linear_model import randomizedlogisticregression randomized_logistic randomizedlogisticregression attributes scores_ array shape n_features all_scores_array shape n_features n_reg_parameter methods feature scores 
2813: feature scores values regularization parameter reference article suggests scores_ max all_scores_ 
2814: fit fit_transform get_params deep get_support indices inverse_transform transform new matrix using selected features set_params params transform fit model using training data fit data transform get parameters estimator return mask list features indices selected 
2815: set parameters estimator transform new matrix using selected features __init__ scaling sample_fraction n_resampling selection_threshold random_state none tol t_intercept true n_jobs pre_dispatch n_jobs memory memory cachedir none verbose false normalize true fit fit model using training data 
2816: parameters array like shape n_samples n_features training data 
2817: array like shape n_samples target values returns self object returns instance self 
2818: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2819: parameters numpy array shape n_samples n_features chapter user guide scikit learn user guide release training set 
2820: numpy array shape n_samples target values 
2821: returns x_new numpy array shape n_samples n_features_new transformed array 
2822: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
2823: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2824: get_support indices false return mask list features indices selected 
2825: inverse_transform transform new matrix using selected features set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform transform new matrix using selected features linear_model lasso_path eps linear_model lars_path gram linear_model orthogonal_mp linear_model orthogonal_mp_gram gram gram orthogonal matching pursuit omp linear_model lasso_stability_path compute lasso path coordinate descent compute least angle regression lasso path orthogonal matching pursuit omp stabiliy path based randomized lasso estimates sklearn linear_model lasso_path sklearn linear_model lasso_path eps n_alphas alphas none precom pute auto none t_intercept true normalize false copy_x true verbose false params compute lasso path coordinate descent optimization objective lasso n_samples 2_2 alpha parameters numpy array shape n_samples n_features reference scikit learn user guide release training data pass directly fortran contiguous data avoid unnecessary memory duplication numpy array shape n_samples target values eps oat optional length path eps means alpha_min alpha_max n_alphas int optional number alphas along regularization path alphas numpy array optional list alphas compute models none alphas set automatically precompute true false auto array like whether use precomputed gram matrix speed calculations set auto let decide gram matrix also passed argument 
2826: array like optional dot precomputed useful gram matrix precomputed t_intercept bool fit intercept normalize boolean optional true regressors normalized copy_x boolean optional default true true copied else may overwritten 
2827: verbose bool integer amount verbosity params kwargs keyword arguments passed lasso objects returns models list models along regularization path see also lars_path lasso lassolars lassocv lassolarscv sklearn decomposition sparse_encode notes see examples linear_model plot_lasso_coordinate_descent_path example avoid unnecessary memory duplication argument method directly passed fortran contiguous numpy array 
2828: chapter user guide scikit learn user guide release sklearn linear_model lars_path sklearn linear_model lars_path none gram none max_iter alpha_min method lar copy_x true eps 2204460492503131e copy_gram true verbose false compute least angle regression lasso path optimization objective lasso n_samples 2_2 alpha parameters array shape n_samples n_features input data array shape n_samples input targets max_iter integer optional maximum number iterations perform set innity limit gram none auto array shape n_features n_features optional precomputed gram matrix auto gram matrix precomputed given samples features alpha_min oat optional minimum correlation along path alpha parameter lasso 
2829: method lar lasso corresponds regularization parameter species returned model select lar least angle regression lasso lasso 
2830: eps oat optional machine precision regularization computation cholesky diagonal fac tors increase ill conditioned systems 
2831: copy_x bool false overwritten 
2832: copy_gram bool false gram overwritten 
2833: returns alphas array shape max_features maximum covariances absolute value iteration 
2834: active array shape max_features indices active variables end path 
2835: coefs array shape n_features max_features coefcients along path see also lasso_path lassolars lars lassolarscv larscv sklearn decomposition sparse_encode reference scikit learn user guide release notes http wikipedia org wiki least angle_regression http wikipedia org wiki lasso_ statistics lasso_method sklearn linear_model orthogonal_mp sklearn linear_model orthogonal_mp tol none precom n_nonzero_coefs none pute_gram false copy_x true orthogonal matching pursuit omp solves n_targets orthogonal matching pursuit problems instance problem form parametrized number non zero coefcients using n_nonzero_coefs argmin xgamma subject gamma nonzero coefs parametrized error using parameter tol argmin gamma subject xgamma tol parameters array shape n_samples n_features input data columns assumed unit norm 
2836: array shape n_samples n_samples n_targets input targets n_nonzero_coefs int desired number non zero entries solution none default value set n_features 
2837: tol oat maximum norm residual none overrides n_nonzero_coefs 
2838: precompute_gram true false auto whether perform precomputations n_samples large 
2839: copy_x bool optional improves performance n_targets whether design matrix must copied algorithm false value helpful already fortran ordered otherwise copy made anyway 
2840: returns coef array shape n_features n_features n_targets coefcients omp solution see also orthogonalmatchingpursuit decomposition sparse_encode decomposition sparse_encode_parallel orthogonal_mp_gram lars_path notes orthogonal matching pursuit introduced mallat zhang matching pursuits time frequency dictionaries ieee transactions signal processing vol december http blanche polytechnique mallat papiers mallatpursuit93 pdf chapter user guide scikit learn user guide release implementation based rubinstein zibulevsky elad efcient implementation svd algorithm using batch orthogonal matching pursuit technical report technion april http www technion ronrubin publications ksvd omp pdf sklearn linear_model orthogonal_mp_gram sklearn linear_model orthogonal_mp_gram gram n_nonzero_coefs none norms_squared none copy_xy true tol none copy_gram true gram orthogonal matching pursuit omp solves n_targets orthogonal matching pursuit problems using gram matrix product 
2841: parameters gram array shape n_features n_features gram matrix input data array shape n_features n_features n_targets input targets multiplied n_nonzero_coefs int desired number non zero entries solution none default value set n_features 
2842: tol oat maximum norm residual none overrides n_nonzero_coefs 
2843: norms_squared array like shape n_targets squared norms lines required tol none 
2844: copy_gram bool optional whether gram matrix must copied algorithm false value helpful already fortran ordered otherwise copy made anyway 
2845: copy_xy bool optional whether covariance vector must copied algorithm false may overwritten 
2846: returns coef array shape n_features n_features n_targets coefcients omp solution see also orthogonalmatchingpursuit orthogonal_mp lars_path decomposition sparse_encode decomposition sparse_encode_parallel notes orthogonal matching pursuit introduced mallat zhang matching pursuits time frequency dictionaries ieee transactions signal processing vol december http blanche polytechnique mallat papiers mallatpursuit93 pdf reference scikit learn user guide release implementation based rubinstein zibulevsky elad efcient implementation svd algorithm using batch orthogonal matching pursuit technical report technion april http www technion ronrubin publications ksvd omp pdf sklearn linear_model lasso_stability_path sklearn linear_model lasso_stability_path dom_state none n_grid eps 8817841970012523e verbose false scaling ran n_resampling sample_fraction n_jobs stabiliy path based randomized lasso estimates parameters array like shape n_samples n_features training data 
2847: array like shape n_samples target values 
2848: scaling oat alpha parameter stability selection article used randomly scale features 
2849: random_state integer numpy randomstate optional generator used randomize design 
2850: n_resampling int number randomized models 
2851: n_grid int number grid points path linearly reinterpolated grid computing scores 
2852: sample_fraction oat fraction samples used randomized design samples used 
2853: eps oat smallest value alpha alpha_max considered n_jobs integer optional number cpus use resampling use cpus verbose boolean integer optional sets verbosity amount returns alphas_grid array shape n_grid grid points alpha alpha_max scores_path array shape n_features n_grid scores feature along path 
2854: chapter user guide scikit learn user guide release notes see examples linear_model plot_randomized_lasso example 
2855: sparse data sklearn linear_model sparse submodule sparse counterpart sklearn linear_model module user guide see generalized linear models section details 
2856: linear_model sparse lasso alpha linear_model sparse elasticnet alpha rho linear_model sparse sgdclassifier args linear_model sparse sgdregressor args kwargs linear_model logisticregression penalty linear model trained prior regularizer linear model trained prior regularizer logistic regression aka logit maxent classier 
2857: sklearn linear_model sparse lasso class sklearn linear_model sparse lasso alpha t_intercept false normalize false linear model trained prior regularizer implementation works scipy sparse dense coef_ technically elastic net penalty set zero 
2858: max_iter tol parameters alpha oat constant multiplies term defaults coef_ ndarray shape n_features initial coefents warm start optimization t_intercept bool whether intercept estimated false data assumed already centered 
2859: methods decision_function decision function linear model fit get_params deep predict score set_params params fit current model coordinate descent get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
2860: __init__ alpha t_intercept false normalize false max_iter tol decision_function decision function linear model parameters scipy sparse matrix shape n_samples n_features reference scikit learn user guide release returns array shape n_samples predicted real values fit fit current model coordinate descent expected sparse matrix maximum efciency use sparse matrix csc format scipy sparse csc_matrix get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2861: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2862: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
2863: parameters array like shape n_samples n_features training set 
2864: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn linear_model sparse elasticnet class sklearn linear_model sparse elasticnet alpha rho t_intercept false linear model trained prior regularizer implementation works scipy sparse dense coef_ rho lasso penalty currently rho reliable unless supply sequence alpha 
2865: malize false max_iter tol parameters alpha oat constant multiplies term defaults rho oat chapter user guide scikit learn user guide release elasticnet mixing parameter rho 
2866: t_intercept bool whether intercept estimated false data assumed already centered todo t_intercept true yet implemented notes parameter rho corresponds alpha glmnet package alpha corresponds lambda param eter glmnet 
2867: methods decision_function decision function linear model fit get_params deep predict score set_params params fit current model coordinate descent get parameters estimator predict using linear model returns coefcient determination prediction set parameters estimator 
2868: __init__ alpha rho t_intercept false normalize false max_iter tol decision_function decision function linear model parameters scipy sparse matrix shape n_samples n_features returns array shape n_samples predicted real values fit fit current model coordinate descent expected sparse matrix maximum efciency use sparse matrix csc format scipy sparse csc_matrix get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2869: predict predict using linear model parameters numpy array shape n_samples n_features returns array shape n_samples returns predicted values 
2870: score returns coefcient determination prediction 
2871: reference scikit learn user guide release coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
2872: parameters array like shape n_samples n_features training set 
2873: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn linear_model sparse sgdclassier class sklearn linear_model sparse sgdclassifier args kwargs methods decision_function fit coef_init intercept_init fit_transform get_params deep partial_fit classes class_weight predict predict_proba score set_params params transform threshold predict signed distance hyperplane aka condence score fit linear model stochastic gradient descent fit data transform get parameters estimator fit linear model stochastic gradient descent predict using linear model predict class membership probability returns mean accuracy given test data labels set parameters estimator reduce important features 
2874: __init__ args kwargs deprecated removed use sklearn linear_model sgdclassier directly classes deprecated removed use classes_ instead 
2875: decision_function predict signed distance hyperplane aka condence score parameters array like sparse matrix shape n_samples n_features returns array shape n_samples n_classes else n_samples n_classes signed distances hyperplane 
2876: fit coef_init none intercept_init none class_weight none sample_weight none fit linear model stochastic gradient descent 
2877: parameters array like sparse matrix shape n_samples n_features chapter user guide scikit learn user guide release training data numpy array shape n_samples target values coef_init array shape n_classes n_features initial coefents warm start optimization 
2878: intercept_init array shape n_classes initial intercept warm start optimization 
2879: sample_weight array like shape n_samples optional weights applied individual samples provided uniform weights assumed 
2880: returns self returns instance self 
2881: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2882: parameters numpy array shape n_samples n_features training set 
2883: numpy array shape n_samples target values 
2884: returns x_new numpy array shape n_samples n_features_new transformed array 
2885: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
2886: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2887: partial_fit classes none class_weight none sample_weight none fit linear model stochastic gradient descent 
2888: parameters array like sparse matrix shape n_samples n_features subset training data numpy array shape n_samples subset target values classes array shape n_classes classes across calls partial_t obtained via unique y_all y_all target vector entire dataset argument required rst call partial_t omitted subsequent calls note doesnt need contain labels classes 
2889: reference scikit learn user guide release sample_weight array like shape n_samples optional weights applied individual samples provided uniform weights assumed 
2890: returns self returns instance self 
2891: predict predict using linear model parameters array like sparse matrix shape n_samples n_features returns array shape n_samples array containing predicted class labels 
2892: predict_proba predict class membership probability parameters array like sparse matrix shape n_samples n_features returns array shape n_samples n_classes else n_samples n_classes contains membership probabilities positive class 
2893: score returns mean accuracy given test data labels 
2894: parameters array like shape n_samples n_features training set 
2895: array like shape n_samples labels 
2896: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform threshold none reduce important features 
2897: parameters array scipy sparse matrix shape n_samples n_features input samples 
2898: threshold string oat none optional default none threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor mean may also used none available object attribute threshold used otherwise mean used default 
2899: returns x_r array shape n_samples n_selected_features input samples selected features 
2900: chapter user guide scikit learn user guide release sklearn linear_model sparse sgdregressor class sklearn linear_model sparse sgdregressor args kwargs methods decision_function fit coef_init intercept_init fit_transform get_params deep partial_fit sample_weight predict score set_params params transform threshold predict using linear model fit linear model stochastic gradient descent fit data transform get parameters estimator fit linear model stochastic gradient descent predict using linear model returns coefcient determination prediction set parameters estimator reduce important features 
2901: __init__ args kwargs deprecated removed use sklearn linear_model sgdregressor directly decision_function predict using linear model parameters array like sparse matrix shape n_samples n_features returns array shape n_samples predicted target values per element 
2902: fit coef_init none intercept_init none sample_weight none fit linear model stochastic gradient descent 
2903: parameters array like sparse matrix shape n_samples n_features training data numpy array shape n_samples target values coef_init array shape n_features initial coefents warm start optimization 
2904: intercept_init array shape initial intercept warm start optimization 
2905: sample_weight array like shape n_samples optional weights applied individual samples unweighted 
2906: returns self returns instance self 
2907: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2908: parameters numpy array shape n_samples n_features training set 
2909: reference scikit learn user guide release numpy array shape n_samples target values 
2910: returns x_new numpy array shape n_samples n_features_new transformed array 
2911: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
2912: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2913: partial_fit sample_weight none fit linear model stochastic gradient descent 
2914: parameters array like sparse matrix shape n_samples n_features subset training data numpy array shape n_samples subset target values sample_weight array like shape n_samples optional weights applied individual samples provided uniform weights assumed 
2915: returns self returns instance self 
2916: predict predict using linear model parameters array like sparse matrix shape n_samples n_features returns array shape n_samples predicted target values per element 
2917: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
2918: parameters array like shape n_samples n_features training set 
2919: array like shape n_samples returns oat set_params params set parameters estimator 
2920: chapter user guide scikit learn user guide release method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform threshold none reduce important features 
2921: parameters array scipy sparse matrix shape n_samples n_features input samples 
2922: threshold string oat none optional default none threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor mean may also used none available object attribute threshold used otherwise mean used default 
2923: returns x_r array shape n_samples n_selected_features input samples selected features 
2924: sklearn linear_model logisticregression class sklearn linear_model logisticregression penalty dual false cept_scaling class_weight none t_intercept true tol inter logistic regression aka logit maxent classier multiclass case training algorithm uses one ova scheme rather true multinomial class implements regularized logistic regression using liblinear library handle dense sparse input use ordered arrays csr matrices containing bit oats optimal performance input format converted copied 
2925: parameters penalty string used specify norm used penalization dual boolean dual primal formulation dual formulation implemented penalty prefer dual false n_samples n_features 
2926: oat none optional default none species strength regularization smaller bigger regular ization none set n_samples 
2927: t_intercept bool default true species constant bias intercept added decision function intercept_scaling oat default self t_intercept true instance vector becomes self intercept_scaling synthetic feature constant value equals intercept_scaling appended instance vector intercept becomes intercept_scaling synthetic feature weight note synthetic feature weight subject regularization features 
2928: reference scikit learn user guide release lessen effect regularization synthetic feature weight therefore intercept intercept_scaling increased tol oat optional tolerance stopping criteria see also linearsvc notes underlying implementation uses random number generator select features tting model thus uncommon slightly different results input data happens try smaller tol parameter references liblinear library large linear classicationhttp www csie ntu edu cjlin liblinear hsiang fang lan huang chih jen lin dual coordinate descentmethods machine learning 
2929: regression gistic maximum entropy models http www csie ntu edu cjlin papers maxent_dual pdf attributes array shape n_classes n_features array shape n_classes coef_ ter cept_ methods coefcient features decision function coef_ readonly property derived raw_coef_ follows internal memory layout liblinear intercept bias added decision function available parameter intercept set true decision_function decision function value according trained model fit class_weight fit_transform get_params deep predict predict_log_proba predict_proba score set_params params transform threshold reduce important features 
2930: fit model according given training data fit data transform get parameters estimator predict target values according tted model log probability estimates probability estimates returns mean accuracy given test data labels set parameters estimator 
2931: __init__ penalty dual false class_weight none decision_function tol t_intercept true intercept_scaling decision function value according trained model 
2932: parameters array like shape n_samples n_features chapter user guide scikit learn user guide release returns array like shape n_samples n_class returns decision function sample class model 
2933: fit class_weight none fit model according given training data 
2934: parameters array like sparse matrix shape n_samples n_features training vector n_samples number samples n_features num ber features 
2935: array like shape n_samples target vector relative class_weight dict auto optional weights associated classes given classes supposed weight one 
2936: returns self object returns self 
2937: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
2938: parameters numpy array shape n_samples n_features training set 
2939: numpy array shape n_samples target values 
2940: returns x_new numpy array shape n_samples n_features_new transformed array 
2941: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
2942: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2943: predict predict target values according tted model 
2944: parameters array like sparse matrix shape n_samples n_features returns array shape n_samples predict_log_proba log probability estimates returned estimates classes ordered label classes 
2945: reference scikit learn user guide release parameters array like shape n_samples n_features returns array like shape n_samples n_classes returns log probabilities sample class model classes ordered arithmetical order 
2946: predict_proba probability estimates returned estimates classes ordered label classes 
2947: parameters array like shape n_samples n_features returns array like shape n_samples n_classes returns probability sample class model classes ordered arithmetical order 
2948: score returns mean accuracy given test data labels 
2949: parameters array like shape n_samples n_features training set 
2950: array like shape n_samples labels 
2951: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform threshold none reduce important features 
2952: parameters array scipy sparse matrix shape n_samples n_features input samples 
2953: threshold string oat none optional default none threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor mean may also used none available object attribute threshold used otherwise mean used default 
2954: returns x_r array shape n_samples n_selected_features input samples selected features 
2955: sklearn manifold manifold learning sklearn manifold module implements data embedding techniques user guide see manifold learning section details 
2956: chapter user guide scikit learn user guide release manifold locallylinearembedding manifold isomap n_neighbors n_components locally linear embedding isomap embedding sklearn manifold locallylinearembedding class sklearn manifold locallylinearembedding n_neighbors n_components tol method standard modied_tol ran neighbors_algorithm auto eigen_solver auto reg max_iter hessian_tol dom_state none out_dim none locally linear embedding parameters n_neighbors integer number neighbors consider point 
2957: n_components integer number coordinates manifold reg oat regularization constant multiplies trace local covariance matrix dis tances 
2958: eigen_solver string auto arpack dense auto algorithm attempt choose best method input data arpack use arnoldi iteration shift invert mode method may dense matrix sparse matrix general linear operator 
2959: dense use standard dense matrix operations eigenvalue decomposition method must array matrix type method avoided large problems 
2960: tol oat optional tolerance arpack method used eigen_solver dense 
2961: max_iter integer maximum number eigen_solver dense 
2962: iterations arpack solver 
2963: used method string standard hessian modied standard use standard locally linear embedding algorithm see reference hessian use hessian eigenmap method method requires n_neighbors n_components n_components see reference modied use modied locally linear embedding algorithm see reference ltsa use local tangent space alignment algorithm see reference hessian_tol oat optional tolerance hessian eigenmapping method used method hessian modied_tol oat optional tolerance modied lle method used method modied reference scikit learn user guide release neighbors_algorithm string auto brute kd_tree ball_tree algorithm use nearest neighbors search passed neighbors nearestneighbors instance random_state numpy randomstate optional generator used initialize centers defaults numpy random used deter mine starting vector arpack iterations references r63 r64 r65 r66 attributes embed ding_vectors_ reconstruc tion_error_ nbrs_ array like shape n_components n_samples oat nearestneighbors object stores embedding vectors reconstruction error associated embedding_vectors_ stores nearest neighbors instance including balltree kdtree applicable 
2964: methods compute embedding vectors data fit fit_transform compute embedding vectors data transform get_params deep set_params params transform get parameters estimator set parameters estimator transform new points embedding space 
2965: __init__ n_neighbors tol max_iter method standard hessian_tol modied_tol neigh bors_algorithm auto random_state none out_dim none eigen_solver auto n_components reg fit none compute embedding vectors data parameters array like shape n_samples n_features training set 
2966: returns self returns instance self 
2967: fit_transform none compute embedding vectors data transform 
2968: parameters array like shape n_samples n_features training set 
2969: returns x_new array like shape n_samples n_components get_params deep true get parameters estimator chapter user guide scikit learn user guide release parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform transform new points embedding space 
2970: parameters array like shape n_samples n_features returns x_new array shape n_samples n_components notes scaling performed method discouraged use together methods scale invariant like svms sklearn manifold isomap class sklearn manifold isomap n_neighbors n_components tol max_iter none path_method auto neighbors_algorithm auto out_dim none eigen_solver auto isomap embedding non linear dimensionality reduction isometric mapping parameters n_neighbors integer number neighbors consider point 
2971: n_components integer number coordinates manifold eigen_solver auto arpack dense auto attempt choose efcient solver given problem arpack use arnoldi decomposition eigenvalues eigenvectors note arpack handle dense sparse data efciently dense use direct solver lapack eigenvalue decomposition 
2972: tol oat convergence tolerance passed arpack lobpcg used eigen_solver dense max_iter integer maximum number iterations arpack solver used eigen_solver dense path_method string auto reference scikit learn user guide release method use nding shortest path auto attempt choose best algorithm automatically floyd warshall algorithm dijkstra algorithm fibonacci heaps neighbors_algorithm string auto brute kd_tree ball_tree algorithm use nearest neighbors search passed neighbors nearestneighbors instance references tenenbaum silva langford global geometricframework nonlinear dimen sionality reduction science attributes embed ding_ ker nel_pca_ train ing_data_ nbrs_ array like shape n_samples n_components kernelpca object used implement embedding array like shape n_samples n_features sklearn neighbors nearestneighbors instance dist_matrix_ array like shape n_samples n_samples methods stores embedding vectors stores training data stores nearest neighbors instance including balltree kdtree applicable stores geodesic distance matrix training data fit fit_transform get_params deep reconstruction_error compute reconstruction error embedding set_params params transform compute embedding vectors data fit model data transform get parameters estimator set parameters estimator transform 
2973: __init__ n_neighbors tol max_iter none path_method auto neighbors_algorithm auto out_dim none n_components eigen_solver auto fit none compute embedding vectors data parameters array like sparse matrix balltree ckdtree nearestneighbors sample data shape n_samples n_features form numpy array sparse array precomputed tree nearestneighbors object 
2974: returns self returns instance self 
2975: fit_transform none fit model data transform 
2976: parameters array like sparse matrix balltree ckdtree chapter user guide scikit learn user guide release training vector n_samples number samples n_features num ber features 
2977: returns x_new array like shape n_samples n_components get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
2978: reconstruction_error compute reconstruction error embedding 
2979: returns reconstruction_error oat notes cost function isomap embedding frobenius_norm d_fit n_samples matrix distances input data d_t matrix distances output embedding x_t isomap kernel n_samples n_samples set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform transform implemented linking points graph geodesic distances training data first n_neighbors nearest neighbors found training data shortest geodesic distances point point training data computed order construct kernel embedding projection kernel onto embedding vectors training set 
2980: parameters array like shape n_samples n_features returns x_new array like shape n_samples n_components manifold locally_linear_embedding perform locally linear embedding analysis data 
2981: sklearn manifold locally_linear_embedding sklearn manifold locally_linear_embedding n_neighbors n_components reg eigen_solver auto tol max_iter method standard hessian_tol modied_tol random_state none out_dim none perform locally linear embedding analysis data 
2982: reference scikit learn user guide release parameters array like sparse matrix balltree ckdtree nearestneighbors sample data shape n_samples n_features form numpy array sparse array precomputed tree nearestneighbors object 
2983: n_neighbors integer number neighbors consider point 
2984: n_components integer number coordinates manifold 
2985: reg oat regularization constant multiplies trace local covariance matrix dis tances 
2986: eigen_solver string auto arpack dense auto algorithm attempt choose best method input data arpack use arnoldi iteration shift invert mode method may dense matrix sparse matrix general linear operator 
2987: dense use standard dense matrix operations eigenvalue decomposition method must array matrix type method avoided large problems 
2988: tol oat optional tolerance arpack method used eigen_solver dense 
2989: max_iter integer maximum number iterations arpack solver 
2990: method standard hessian modied ltsa standard use standard locally linear embedding algorithm see reference r67 hessian use hessian eigenmap method method requires n_neighbors n_components n_components see reference r68 modied use modied locally linear embedding algorithm see reference r69 ltsa use local tangent space alignment algorithm see reference r70 hessian_tol oat optional tolerance hessian eigenmapping method used method hessian modied_tol oat optional tolerance modied lle method used method modied random_state numpy randomstate optional generator used initialize centers defaults numpy random 
2991: returns array like shape n_samples n_components embedding vectors 
2992: squared_error oat reconstruction error embedding vectors equivalent norm fro reconstruction weights 
2993: chapter user guide scikit learn user guide release references r67 r68 r69 r70 sklearn metrics metrics sklearn metrics module includes score functions performance metrics pairwise metrics distance computations 
2994: classication metrics metrics confusion_matrix y_true y_pred metrics roc_curve y_true y_score metrics auc metrics precision_score y_true y_pred metrics recall_score y_true y_pred metrics fbeta_score y_true y_pred beta metrics f1_score y_true y_pred labels metrics precision_recall_fscore_support compute precisions recalls measures support class metrics classification_report y_true y_pred metrics precision_recall_curve y_true metrics zero_one_score y_true y_pred metrics zero_one y_true y_pred metrics hinge_loss y_true pred_decision compute confusion matrix evaluate accuracy classication compute receiver operating characteristic roc compute area curve auc using trapezoidal rule compute precision compute recall compute fbeta score compute score build text report showing main classication metrics compute precision recall pairs different probability thresholds zero one classication score accuracy zero one classication loss cumulated hinge loss non regularized 
2995: sklearn metrics confusion_matrix sklearn metrics confusion_matrix y_true y_pred labels none compute confusion matrix evaluate accuracy classication denition confusion matrix equal number observations known group predicted group parameters y_true array shape n_samples true targets y_pred array shape n_samples estimated targets returns array shape n_classes n_classes confusion matrix references http wikipedia org wiki confusion_matrix reference scikit learn user guide release sklearn metrics roc_curve sklearn metrics roc_curve y_true y_score compute receiver operating characteristic roc note implementation restricted binary classication task 
2996: parameters y_true array shape n_samples true binary labels y_score array shape n_samples target scores either probability estimates positive class condence values binary decisions 
2997: returns fpr array shape false positive rates tpr array shape true positive rates thresholds array shape thresholds y_score used compute fpr tpr references http wikipedia org wiki receiver_operating_characteristic examples import numpy sklearn import metrics array scores array fpr tpr thresholds metrics roc_curve scores fpr array sklearn metrics auc sklearn metrics auc compute area curve auc using trapezoidal rule parameters array shape coordinates array shape coordinates returns auc oat chapter user guide scikit learn user guide release examples import numpy sklearn import metrics array pred array fpr tpr thresholds metrics roc_curve pred metrics auc fpr tpr sklearn metrics precision_score sklearn metrics precision_score y_true labels none pos_label aver y_pred age weighted compute precision precision ratio number true positives number false positives precision intuitively ability classier label positive sample negative best value worst value 
2998: parameters y_true array shape n_samples true targets y_pred array shape n_samples predicted targets labels array integer array labels pos_label int binary classication case give label positive class default erything else pos_label considered belong negative class set none case multiclass classication 
2999: average string none micro macro weighted default multiclass classication case determines type averaging performed data macro average classes take imbalance account micro average instances takes imbalance account implies precision recall weighted average weighted support takes imbalance account result score precision recall 
3000: returns precision oat precision positive class binary classication weighted average preci sion class multiclass task sklearn metrics recall_score sklearn metrics recall_score y_true y_pred labels none pos_label average weighted compute recall reference scikit learn user guide release recall ratio number true positives number false negatives recall intuitively ability classier positive samples best value worst value 
3001: parameters y_true array shape n_samples true targets y_pred array shape n_samples predicted targets labels array integer array labels pos_label int binary classication case give label positive class default erything else pos_label considered belong negative class set none case multiclass classication 
3002: average string none micro macro weighted default multiclass classication case determines type averaging performed data macro average classes take imbalance account micro average instances takes imbalance account implies precision recall weighted average weighted support takes imbalance account result score precision recall 
3003: returns recall oat recall positive class binary classication weighted average recall class multiclass task 
3004: sklearn metrics fbeta_score sklearn metrics fbeta_score y_true beta labels none pos_label aver y_pred age weighted compute fbeta score f_beta score weighted harmonic mean precision recall reaching optimal value worst value beta parameter determines weight precision combined score beta lends weight precision beta favors precision beta considers precision beta inf recall 
3005: parameters y_true array shape n_samples true targets y_pred array shape n_samples predicted targets beta oat weight precision harmonic mean 
3006: chapter user guide scikit learn user guide release labels array integer array labels pos_label int binary classication case give label positive class default erything else pos_label considered belong negative class set none case multiclass classication 
3007: average string none micro macro weighted default multiclass classication case determines type averaging performed data macro average classes take imbalance account micro average instances takes imbalance account implies precision recall weighted average weighted support takes imbalance account result score precision recall 
3008: returns fbeta_score oat fbeta_score positive class binary classication weighted average fbeta_score class multiclass task 
3009: references baeza yates ribeiro neto modern information retrieval addison wesley http wikipedia org wiki f1_score sklearn metrics f1_score sklearn metrics f1_score y_true y_pred labels none pos_label average weighted compute score score interpreted weighted average precision recall score reaches best value worst score relative contribution precision recall score equal formular f_1 score f_1 precision recall precision recall see http wikipedia org wiki f1_score multi class case weighted average score class 
3010: parameters y_true array shape n_samples true targets y_pred array shape n_samples predicted targets labels array integer array labels pos_label int reference scikit learn user guide release binary classication case give label positive class default erything else pos_label considered belong negative class set none case multiclass classication 
3011: average string none micro macro weighted default multiclass classication case determines type averaging performed data macro average classes take imbalance account micro average instances takes imbalance account implies precision recall weighted average weighted support takes imbalance account result score precision recall 
3012: returns f1_score oat f1_score positive class binary classication weighted average f1_scores class multiclass task references http wikipedia org wiki f1_score sklearn metrics precision_recall_fscore_support sklearn metrics precision_recall_fscore_support y_true y_pred beta bels none age none pos_label aver compute precisions recalls measures support class precision ratio number true positives number false positives precision intuitively ability classier label positive sample negative recall ratio number true positives number false negatives recall intuitively ability classier positive samples f_beta score interpreted weighted harmonic mean precision recall f_beta score reaches best value worst score f_beta score weights recall beta much precision beta means recall precsion equally important support number occurrences class y_true pos_label none function returns average precision recall measure average one micro macro weighted 
3013: parameters y_true array shape n_samples true targets y_pred array shape n_samples predicted targets beta oat default strength recall versus precision score 
3014: chapter user guide scikit learn user guide release labels array integer array labels pos_label int binary classication case give label positive class default erything else pos_label considered belong negative class set none case multiclass classication 
3015: average string none micro macro weighted default multiclass classication case determines type averaging performed data macro average classes take imbalance account micro average instances takes imbalance account implies precision recall weighted average weighted support takes imbalance account result score precision recall 
3016: returns precision array shape n_unique_labels dtype double recall array shape n_unique_labels dtype double f1_score array shape n_unique_labels dtype double support array shape n_unique_labels dtype long references http wikipedia org wiki precision_and_recall sklearn metrics classication_report sklearn metrics classification_report y_true y_pred labels none target_names none build text report showing main classication metrics parameters y_true array shape n_samples true targets y_pred array shape n_samples estimated targets labels array shape n_labels optional list label indices include report target_names list strings optional display names matching labels order returns report string text summary precision recall score class reference scikit learn user guide release sklearn metrics precision_recall_curve sklearn metrics precision_recall_curve y_true probas_pred compute precision recall pairs different probability thresholds note implementation restricted binary classication task precision ratio number true positives number false positives precision intuitively ability classier label positive sample negative recall ratio number true positives number false negatives recall intuitively ability classier positive samples last precision recall values respectively corresponding threshold ensures graph starts axis 
3017: parameters y_true array shape n_samples true targets binary classication range probas_pred array shape n_samples estimated probabilities returns precision array shape precision values recall array shape recall values thresholds array shape thresholds y_score used compute precision recall sklearn metrics zero_one_score sklearn metrics zero_one_score y_true y_pred zero one classication score accuracy positive integer number good classications best performance return fraction correct predictions y_pred 
3018: parameters y_true array like shape n_samples gold standard labels 
3019: y_pred array like shape n_samples predicted labels returned classier 
3020: returns score oat sklearn metrics zero_one sklearn metrics zero_one y_true y_pred zero one classication loss positive integer number misclassications best performance return number errors chapter user guide scikit learn user guide release parameters y_true array like y_pred array like returns loss oat sklearn metrics hinge_loss sklearn metrics hinge_loss y_true pred_decision pos_label neg_label cumulated hinge loss non regularized assuming labels y_true encoded prediction mistake made margin y_true pred_decision always negative since signs disagree therefore margin always greater cumulated hinge loss therefore upperbounds number mistakes made classier 
3021: parameters y_true array shape n_samples true target integers pred_decision array shape n_samples n_samples n_classes predicted decisions output decision_function oats regression metrics metrics r2_score y_true y_pred metrics mean_squared_error y_true y_pred mean squared error regression loss coefcient determination regression score function sklearn metrics r2_score sklearn metrics r2_score y_true y_pred coefcient determination regression score function best possible score lower values worse 
3022: parameters y_true array like y_pred array like returns oat score notes symmetric function 
3023: references http wikipedia org wiki coefcient_of_determination reference scikit learn user guide release sklearn metrics mean_squared_error sklearn metrics mean_squared_error y_true y_pred mean squared error regression loss return positive oating point value best value 
3024: parameters y_true array like y_pred array like returns loss oat clustering metrics see clustering section user guide details sklearn metrics cluster submodule contains evaluation metrics cluster analysis results two forms evaluation supervised uses ground truth class values sample unsupervised measures quality model 
3025: metrics adjusted_rand_score labels_true metrics adjusted_mutual_info_score metrics homogeneity_completeness_v_measure compute homogeneity completeness measure scores metrics homogeneity_score labels_true metrics completeness_score labels_true metrics v_measure_score labels_true labels_pred metrics silhouette_score labels homogeneity metric cluster labeling given ground truth completeness metric cluster labeling given ground truth measure cluster labeling given ground truth compute mean silhouette coefcient samples 
3026: rand index adjusted chance adjusted mutual information two clusterings sklearn metrics adjusted_rand_score sklearn metrics adjusted_rand_score labels_true labels_pred rand index adjusted chance rand index computes similarity measure two clusterings considering pairs samples counting pairs assigned different clusters predicted true clusterings raw score adjusted chance ari score using following scheme ari expected_ri max expected_ri adjusted rand index thus ensured value close random labeling independently number clusters samples exactly clusterings identical permutation ari symmetric measure adjusted_rand_score adjusted_rand_score parameters labels_true int array shape n_samples ground truth class labels used reference labels_pred array shape n_samples cluster labels evaluate returns ari oat chapter user guide scikit learn user guide release similarity score random labelings ari close stands perfect match 
3027: see also adjusted_mutual_info_scoreadjusted mutual information references hubert1985 examples perfectly maching labelings score even sklearn metrics cluster import adjusted_rand_score adjusted_rand_score adjusted_rand_score labelings assign classes members clusters complete always pure hence penalized adjusted_rand_score 
3028: ari symmetric labelings pure clusters members coming classes unnec essary splits penalized adjusted_rand_score 
3029: classes members completely split across different clusters assignment totally incomplete hence ari low adjusted_rand_score sklearn metrics adjusted_mutual_info_score sklearn metrics adjusted_mutual_info_score labels_true labels_pred adjusted mutual information two clusterings adjusted mutual information ami adjustement mutual information score account chance accounts fact generally higher two clusterings larger number clusters regardless whether actually information shared two clusterings ami given ami max metric independent absolute values labels permutation class cluster label values wont change score value way metric furthermore symmetric switching label_true label_pred return score value useful measure agreement two independent label assignments strategies dataset real ground truth known 
3030: reference scikit learn user guide release mindful function order magnitude slower metrics adjusted rand index 
3031: parameters labels_true int array shape n_samples clustering data disjoint subsets 
3032: labels_pred array shape n_samples clustering data disjoint subsets 
3033: returns ami oat score stands perfectly complete labeling see also adjusted_rand_scoreadjusted rand index mutual_information_scoremutual information adjusted chance examples perfect labelings homogeneous complete hence score sklearn metrics cluster import adjusted_mutual_info_score adjusted_mutual_info_score adjusted_mutual_info_score classes members completly splitted across different clusters assignment totally complete hence ami null adjusted_mutual_info_score sklearn metrics homogeneity_completeness_v_measure sklearn metrics homogeneity_completeness_v_measure labels_true labels_pred compute homogeneity completeness measure scores metrics based normalized conditional entropy measures clustering labeling evaluate given knowledge ground truth class labels samples clustering result satises homogeneity clusters contain data points members single class clustering result satises completeness data points members given class elements cluster scores positive values larger values desirable metrics independent absolute values labels permutation class cluster label values wont change score values way measure furthermore symmetric swapping labels_true label_pred give score hold homogeneity completeness 
3034: parameters labels_true int array shape n_samples chapter user guide scikit learn user guide release ground truth class labels used reference labels_pred array shape n_samples cluster labels evaluate returns homogeneity oat score stands perfectly homogeneous labeling completeness oat score stands perfectly complete labeling v_measure oat harmonic mean rst two see also homogeneity_score completeness_score v_measure_score sklearn metrics homogeneity_score sklearn metrics homogeneity_score labels_true labels_pred homogeneity metric cluster labeling given ground truth clustering result satises homogeneity clusters contain data points members single class metric independent absolute values labels permutation class cluster label values wont change score value way metric symmetric switching label_true label_pred return completeness_score different general 
3035: parameters labels_true int array shape n_samples ground truth class labels used reference labels_pred array shape n_samples cluster labels evaluate returns homogeneity oat score stands perfectly homogeneous labeling see also completeness_score v_measure_score references r72 examples perfect labelings homegenous reference scikit learn user guide release sklearn metrics cluster import homogeneity_score homogeneity_score non pefect labelings futher split classes clusters perfectly homogeneous homogeneity_score homogeneity_score clusters include samples different classes make homogeneous labeling homogeneity_score homogeneity_score sklearn metrics completeness_score sklearn metrics completeness_score labels_true labels_pred completeness metric cluster labeling given ground truth clustering result satises completeness data points members given class elements cluster metric independent absolute values labels permutation class cluster label values wont change score value way metric symmetric switching label_true label_pred return homogeneity_score different general 
3036: parameters labels_true int array shape n_samples ground truth class labels used reference labels_pred array shape n_samples cluster labels evaluate returns completeness oat score stands perfectly complete labeling see also homogeneity_score v_measure_score references r71 examples perfect labelings complete sklearn metrics cluster import completeness_score completeness_score chapter user guide scikit learn user guide release non pefect labelings assign classes members clusters still complete completeness_score completeness_score classes members splitted across different clusters assignment cannot complete completeness_score completeness_score sklearn metrics v_measure_score sklearn metrics v_measure_score labels_true labels_pred measure cluster labeling given ground truth measure hormonic mean homogeneity completeness homogeneity completeness homogeneity completeness metric independent absolute values labels permutation class cluster label values wont change score value way metric furthermore symmetric switching label_true label_pred return score value useful measure agreement two independent label assignments strategies dataset real ground truth known 
3037: parameters labels_true int array shape n_samples ground truth class labels used reference labels_pred array shape n_samples cluster labels evaluate returns completeness oat score stands perfectly complete labeling see also homogeneity_score completeness_score references rosenberg2007 examples perfect labelings homogeneous complete hence score sklearn metrics cluster import v_measure_score v_measure_score v_measure_score reference scikit learn user guide release labelings assign classes members clusters complete homogeneous hence penal ized v_measure_score v_measure_score 
3038: labelings pure clusters members coming classes homogeneous necessary splits harms completeness thus penalize measure well v_measure_score v_measure_score 
3039: classes members completly splitted across different clusters assignment totally complete hence measure null v_measure_score clusters include samples totally different classes totally destroy homogeneity labeling hence v_measure_score sklearn metrics silhouette_score sklearn metrics silhouette_score labels metric euclidean sample_size none ran dom_state none kwds compute mean silhouette coefcient samples silhouette coefcient calculated using mean intra cluster distance mean nearest cluster distance sample silhouette coefcient sample max clarrify distance sample nearest cluster part function returns mean silhoeutte coefcient samples obtain values sample use silhouette_samples best value worst value values near indicate overlapping clusters negative values generally indicate sample assigned wrong cluster different cluster similar 
3040: parameters array n_samples_a n_samples_a metric precomputed n_samples_a n_features otherwise array pairwise distances samples feature array 
3041: labels array shape n_samples label values sample metric string callable metric string metric use calculating distance instances feature ray must one options allowed met rics pairwise pairwise_distances distance array use precomputed metric 
3042: sample_size int none chapter user guide scikit learn user guide release size sample use computing silhouette coefcient sample_size none sampling used 
3043: random_state integer numpy randomstate optional generator used initialize centers defaults global numpy random number generator 
3044: integer given xes seed 
3045: kwds optional keyword parameters parameters passed directly distance function using scipy spatial distance metric parameters still metric dependent see scipy docs usage examples 
3046: returns silhouette oat mean silhouette coefcient samples 
3047: references peter rousseeuw silhouettes graphical aid theinterpretation validation cluster analysis computational applied mathematics doi 
3048: http wikipedia org wiki silhouette_ clustering pairwise metrics sklearn metrics pairwise submodule implements utilities evaluate pairwise distances afnity sets samples module contains distance metrics kernels brief summary given two distance metrics function objects considered similar objects two objects exactly alike would distance zero one popular examples euclidean distance true metric must obey following four conditions positive definiteness symmetry triangle inequality kernels measures similarity objects considered similar objects kernel must also positive semi denite number ways convert distance metric similarity measure kernel let distance kernel exp gamma one heuristic choosing gamma num_features max metrics pairwise euclidean_distances considering rows vectors compute metrics pairwise manhattan_distances compute distances vectors metrics pairwise linear_kernel metrics pairwise polynomial_kernel metrics pairwise rbf_kernel gamma metrics pairwise distance_metrics compute linear kernel compute polynomial kernel compute rbf gaussian kernel valid metrics pairwise_distances reference continued next page scikit learn user guide release metrics pairwise pairwise_distances metrics pairwise kernel_metrics metrics pairwise pairwise_kernels compute distance matrix vector array optional valid metrics pairwise_kernels compute kernel arrays optional array 
3049: table continued previous page sklearn metrics pairwise euclidean_distances sklearn metrics pairwise euclidean_distances none squared false y_norm_squared none considering rows vectors compute distance matrix pair vectors efciency reasons euclidean distance pair row vector computed dist sqrt dot dot dot formulation two main advantages first computationally efcient dealing sparse data second varies remains unchanged right dot product dot pre computed 
3050: parameters array like sparse matrix shape n_samples_1 n_features array like sparse matrix shape n_samples_2 n_features y_norm_squared array like shape n_samples_2 optional pre computed dot products vectors sum axis squared boolean optional return squared euclidean distances 
3051: returns distances array sparse matrix shape n_samples_1 n_samples_2 examples sklearn metrics pairwise import euclidean_distances distance rows euclidean_distances array get distance origin euclidean_distances array 
3052: sklearn metrics pairwise manhattan_distances sklearn metrics pairwise manhattan_distances none sum_over_features true compute distances vectors sum_over_features equal false returns componentwise distances 
3053: parameters array_like array shape n_samples_x n_features 
3054: array_like optional array shape n_samples_y n_features 
3055: chapter user guide scikit learn user guide release sum_over_features bool default true true function returns pairwise distance matrix else returns component wise pairwise distances 
3056: returns array sum_over_features false shape n_samples_x n_samples_y n_features contains componentwise pairwise distances absolute difference else shape n_samples_x n_samples_y contains pairwise distances 
3057: examples sklearn metrics pairwise import manhattan_distances manhattan_distances array manhattan_distances array manhattan_distances array manhattan_distances array import numpy ones ones manhattan_distances sum_over_features false array sklearn metrics pairwise linear_kernel sklearn metrics pairwise linear_kernel none compute linear kernel 
3058: parameters array shape n_samples_1 n_features array shape n_samples_2 n_features returns gram matrix array shape n_samples_1 n_samples_2 sklearn metrics pairwise polynomial_kernel sklearn metrics pairwise polynomial_kernel none degree gamma coef0 compute polynomial kernel gamma coef0 degree parameters array shape n_samples_1 n_features array shape n_samples_2 n_features degree int returns gram matrix array shape n_samples_1 n_samples_2 reference scikit learn user guide release sklearn metrics pairwise rbf_kernel sklearn metrics pairwise rbf_kernel none gamma compute rbf gaussian kernel exp gamma parameters array shape n_samples_1 n_features array shape n_samples_2 n_features gamma oat returns gram matrix array shape n_samples_1 n_samples_2 sklearn metrics pairwise distance_metrics sklearn metrics pairwise distance_metrics valid metrics pairwise_distances function simply returns valid pairwise distance metrics description mapping valid strings valid distance metrics function map metric cityblock euclidean manhattan function sklearn pairwise manhattan_distances sklearn pairwise euclidean_distances sklearn pairwise manhattan_distances sklearn pairwise euclidean_distances sklearn pairwise manhattan_distances sklearn metrics pairwise pairwise_distances exists however allow verbose sklearn metrics pairwise pairwise_distances none metric euclidean n_jobs kwds compute distance matrix vector array optional method takes either vector array distance matrix returns distance matrix input vector array distances computed input distances matrix returned instead method provides safe way take distance matrix input preserving compatability many algorithms take vector array given default none returned matrix pairwise distance arrays please note support wise pairwise_distance_functions valid values metric sparse matrices currently limited metrics listed pair scikit learn euclidean manhattan cityblock scipy spatial distance braycurtis canberra chebyshev correlation cosine dice ham ming russell rao seuclidean sokalmichener sokalsneath sqeucludean yule see documentation scipy spatial distance details metrics 
3059: rogerstanimoto mahalanobis matching minkowski jaccard kulsinski chapter user guide scikit learn user guide release note case euclidean cityblock valid scipy spatial distance metrics values use scikit learn implementation faster support sparse matrices verbose description metrics scikit learn see __doc__ sklearn pairwise distance_metrics function 
3060: parameters array n_samples_a n_samples_a metric precomputed n_samples_a n_features otherwise array pairwise distances samples feature array 
3061: array n_samples_b n_features second feature array shape n_samples_a n_features 
3062: metric string callable metric use calculating distance instances feature array metric string must one options allowed scipy spatial distance pdist metric parameter metric listed pairwise pairwise_distance_functions metric precomputed assumed distance matrix alternatively metric callable function called pair instances rows resulting value recorded callable take two arrays input return value indicating distance 
3063: n_jobs int number jobs use computation works breaking pairwise matrix n_jobs even slices computing parallel cpus used given parallel computing code used useful debuging n_jobs n_cpus n_jobs used thus n_jobs cpus one used 
3064: kwds optional keyword parameters parameters passed directly distance function using scipy spatial distance metric parameters still metric dependent see scipy docs usage examples 
3065: returns array n_samples_a n_samples_a n_samples_a n_samples_b distance matrix distance ith jth vectors given matrix none none distance ith array jth array 
3066: sklearn metrics pairwise kernel_metrics sklearn metrics pairwise kernel_metrics valid metrics pairwise_kernels function simply returns valid pairwise distance metrics description mapping valid strings 
3067: valid distance metrics function map exists however allow verbose metric linear poly polynomial rbf sigmoid function sklearn pairwise linear_kernel sklearn pairwise polynomial_kernel sklearn pairwise polynomial_kernel sklearn pairwise rbf_kernel sklearn pairwise sigmoid_kernel reference scikit learn user guide release sklearn metrics pairwise pairwise_kernels sklearn metrics pairwise pairwise_kernels none metric linear ter_params false n_jobs kwds compute kernel arrays optional array method takes either vector array kernel matrix returns kernel matrix input vector array kernels computed input kernel matrix returned instead method provides safe way take kernel matrix input preserving compatability many algorithms take vector array given default none returned matrix pairwise kernel arrays valid values metric rbf sigmoid polynomial poly linear parameters array n_samples_a n_samples_a metric precomputed n_samples_a n_features otherwise array pairwise kernels samples feature array 
3068: array n_samples_b n_features second feature array shape n_samples_a n_features 
3069: metric string callable metric use calculating kernel instances feature array met ric string must one metrics pairwise pairwise_kernel_functions metric precomputed assumed kernel matrix alternatively met ric callable function called pair instances rows resulting value recorded callable take two arrays input return value indicating distance 
3070: n_jobs int number jobs use computation works breaking pairwise matrix n_jobs even slices computing parallel cpus used given parallel computing code used useful debuging n_jobs n_cpus n_jobs used thus n_jobs cpus one used 
3071: lter_params boolean whether lter invalid parameters 
3072: kwds optional keyword parameters parameters passed directly kernel function 
3073: returns array n_samples_a n_samples_a n_samples_a n_samples_b kernel matrix kernel ith jth vectors given matrix none none kernel ith array jth array 
3074: sklearn mixture gaussian mixture models sklearn mixture module implements mixture modeling algorithms 
3075: chapter user guide scikit learn user guide release user guide see gaussian mixture models section details 
3076: mixture gmm n_components covariance_type gaussian mixture model mixture dpgmm n_components mixture vbgmm n_components variational inference innite gaussian mixture model variational inference gaussian mixture model sklearn mixture gmm class sklearn mixture gmm n_components random_state none thresh min_covar n_iter n_init params wmc init_params wmc covariance_type diag gaussian mixture model representation gaussian mixture model probability distribution class allows easy evaluation sampling maximum likelihood estimation parameters gmm distribution initializes parameters every mixture component zero mean identity covariance 
3077: parameters n_components int optional number mixture components defaults 
3078: covariance_type string optional string describing type covariance parameters use must one spherical tied diag full defaults diag 
3079: random_state randomstate int seed default random number generator instance min_covar oat optional floor diagonal covariance matrix prevent overtting defaults 
3080: thresh oat optional convergence threshold 
3081: n_iter int optional number iterations perform 
3082: n_init int optional number initializations perform best results kept params string optional controls parameters updated training process contain combi nation weights means covars defaults wmc 
3083: init_params string optional controls parameters updated initialization process contain combination weights means covars defaults wmc 
3084: see also dpgmmininite gaussian mixture model using dirichlet process variational algorithm vbgmmfinite gaussian mixture model variational algorithm better situations might little data get good estimate covariance matrix 
3085: reference scikit learn user guide release examples import numpy sklearn import mixture random seed mixture gmm n_components generate random observations two modes centered use training obs concatenate random randn fit obs gmm covariance_type none init_params wmc min_covar random randn n_components n_init n_iter params wmc random_state none thresh round weights_ array round means_ array round covars_ array predict array round score array refit model new data initial parameters remain time even split two modes fit gmm covariance_type none init_params wmc min_covar n_components n_init n_iter params wmc random_state none thresh round weights_ array attributes weights_ means_ covars_ array shape n_components shape array n_features array n_components attribute stores mixing weights mixture compo nent mean parameters mixture component covariance parameters mixture component shape pends covariance_type n_components n_features n_features n_components n_features n_components n_features n_features full converged_ bool true reached false otherwise 
3086: convergence chapter user guide scikit learn user guide release methods akaike information criterion current model aic bayesian information criterion current model bic deprecated removed decode args kwargs evaluate model data eval estimate model parameters expectation maximization algorithm fit kwargs get parameters estimator get_params deep predict label data predict predict posterior probability data gaussian predict_proba rvs args kwargs deprecated removed sample n_samples random_state generate random samples model score set_params params compute log probability model set parameters estimator 
3087: __init__ n_components thresh min_covar n_iter n_init params wmc init_params wmc covariance_type diag random_state none aic akaike information criterion current model proposed data parameters array shape n_samples n_dimensions returns aic oat lower better bic bayesian information criterion current model proposed data parameters array shape n_samples n_dimensions returns bic oat lower better decode args kwargs deprecated removed use score predict method instead depending question find likely mixture components point 
3088: deprecated version removed version use score predict method instead depending question 
3089: parameters array_like shape n_features list n_features dimensional data points row corresponds single data point 
3090: returns logprobs array_like shape n_samples log probability point obs model 
3091: components array_like shape n_samples index likelihod mixture com ponents observation eval evaluate model data compute log probability model return posterior distribution responsibilities mixture component element 
3092: parameters array_like shape n_samples n_features reference scikit learn user guide release list n_features dimensional data points row corresponds single data point 
3093: returns logprob array_like shape n_samples log probabilities data point responsibilities array_like shape n_samples n_components posterior probabilities mixture component observation fit kwargs estimate model parameters expectation maximization algorithm initialization step performed entering algorithm want avoid step set keyword argument init_params empty string creating gmm object likewise would like initialization set n_iter 
3094: parameters array_like shape n_features list n_features dimensional data points row corresponds single data point 
3095: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3096: predict predict label data 
3097: parameters array like shape n_samples n_features returns array shape n_samples predict_proba predict posterior probability data gaussian model 
3098: parameters array like shape n_samples n_features returns responsibilities array like shape n_samples n_components returns probability sample gaussian state model 
3099: rvs args kwargs deprecated removed use score predict method instead depending question generate random samples model 
3100: deprecated version removed version use sample stead sample n_samples random_state none generate random samples model parameters n_samples int optional number samples generate defaults returns array_like shape n_samples n_features list samples score compute log probability model 
3101: chapter user guide scikit learn user guide release parameters array_like shape n_samples n_features list n_features dimensional data points row corresponds single data point 
3102: returns logprob array_like shape n_samples log probabilities data point set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn mixture dpgmm class sklearn mixture dpgmm n_components dom_state none n_iter params wmc init_params wmc covariance_type diag thresh ran verbose false min_covar none alpha variational inference innite gaussian mixture model dpgmm stands dirichlet process gaussian mixture model innite mixture model dirichlet process prior distribution number clusters practice approximate inference algo rithm uses truncated distribution xed maximum number components almost always number components actually used depends data stick breaking representation gaussian mixture model probability distribution class allows easy efcient inference approximate posterior distribution parameters gaussian mixture model variable number components smaller truncation parameter n_components initialization normally distributed means identity covariance proper convergence 
3103: parameters n_components int optional number mixture components defaults 
3104: covariance_type string optional string describing type covariance parameters use must one spherical tied diag full defaults diag 
3105: alpha oat optional real number representing concentration parameter dirichlet process intu itively dirichlet process likely start new cluster point add point cluster alpha elements higher alpha means clusters expected number clusters alpha log defaults 
3106: thresh oat optional convergence threshold 
3107: n_iter int optional maximum number iterations perform convergence 
3108: params string optional controls parameters updated training process contain combi nation weights means covars defaults wmc 
3109: reference scikit learn user guide release init_params string optional controls parameters updated initialization process contain combination weights means covars defaults wmc 
3110: see also gmmfinite gaussian mixture model vbgmmfinite gaussian mixture model variational algorithm better data attributes covariance_type string n_components weights_ means_ precisions_ int array shape n_components shape array n_features array n_components string describing type variance parameters used gmm must one spher ical tied diag full number mixture components mixing weights mixture component mean parameters mixture component precision inverse covariance rameters mixture compo nent shape depends covari ance_type n_components n_features n_features n_features n_components n_features n_components n_features n_features converged_ bool true reached false otherwise 
3111: convergence methods akaike information criterion current model aic bayesian information criterion current model bic deprecated removed decode args kwargs evaluate model data eval estimate model parameters variational algorithm fit kwargs get parameters estimator get_params deep returns lower bound model evidence based membership lower_bound predict label data predict predict posterior probability data gaussian predict_proba rvs args kwargs deprecated removed sample n_samples random_state generate random samples model score set_params params compute log probability model set parameters estimator 
3112: chapter user guide scikit learn user guide release __init__ n_components covariance_type diag alpha random_state none thresh verbose false min_covar none n_iter params wmc init_params wmc aic akaike information criterion current model proposed data parameters array shape n_samples n_dimensions returns aic oat lower better bic bayesian information criterion current model proposed data parameters array shape n_samples n_dimensions returns bic oat lower better decode args kwargs deprecated removed use score predict method instead depending question find likely mixture components point 
3113: deprecated version removed version use score predict method instead depending question 
3114: parameters array_like shape n_features list n_features dimensional data points row corresponds single data point 
3115: returns logprobs array_like shape n_samples log probability point obs model 
3116: components array_like shape n_samples index likelihod mixture com ponents observation eval evaluate model data compute bound log probability model return posterior distribution respon sibilities mixture component element done computing parameters mean eld observation 
3117: parameters array_like shape n_samples n_features list n_features dimensional data points row corresponds single data point 
3118: returns logprob array_like shape n_samples log probabilities data point responsibilities array_like shape n_samples n_components posterior probabilities mixture component observation fit kwargs estimate model parameters variational algorithm full derivation description algorithm see doc derivation derivation tex initialization step performed entering algorithm want avoid step set keyword argument init_params empty string creating object likewise would like initialization set n_iter 
3119: reference scikit learn user guide release parameters array_like shape n_features list n_features dimensional data points row corresponds single data point 
3120: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3121: lower_bound returns lower bound model evidence based membership predict predict label data 
3122: parameters array like shape n_samples n_features returns array shape n_samples predict_proba predict posterior probability data gaussian model 
3123: parameters array like shape n_samples n_features returns responsibilities array like shape n_samples n_components returns probability sample gaussian state model 
3124: rvs args kwargs deprecated removed use score predict method instead depending question generate random samples model 
3125: deprecated version removed version use sample stead sample n_samples random_state none generate random samples model parameters n_samples int optional number samples generate defaults returns array_like shape n_samples n_features list samples score compute log probability model 
3126: parameters array_like shape n_samples n_features list n_features dimensional data points row corresponds single data point 
3127: returns logprob array_like shape n_samples log probabilities data point set_params params set parameters estimator 
3128: chapter user guide scikit learn user guide release method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn mixture vbgmm class sklearn mixture vbgmm n_components dom_state none n_iter params wmc init_params wmc covariance_type diag thresh ran verbose false min_covar none alpha variational inference gaussian mixture model variational inference gaussian mixture model probability distribution class allows easy efcient inference approximate posterior distribution parameters gaussian mixture model xed number components initialization normally distributed means identity covariance proper convergence 
3129: parameters n_components int optional number mixture components defaults 
3130: covariance_type string optional string describing type covariance parameters use must one spherical tied diag full defaults diag 
3131: alpha oat optional real number representing concentration parameter dirichlet distribution intu itively higher value alpha likely variational mixture gaussians model use components defaults 
3132: see also gmmfinite gaussian mixture model dpgmmininite gaussian mixture model using dirichlet process fit reference scikit learn user guide release attributes covariance_type string n_features n_components weights_ means_ precisions_ int int read array shape n_components shape array n_features array n_components string describing type variance parameters used gmm must one spher ical tied diag full dimensionality gaussians number mixture components mixing weights mixture component mean parameters mixture component precision inverse covariance rameters mixture compo nent shape depends covari ance_type n_components n_features n_features n_features n_components n_features n_components n_features n_features converged_ bool true reached false otherwise 
3133: convergence methods aic akaike information criterion current model bic bayesian information criterion current model decode args kwargs deprecated removed eval evaluate model data fit kwargs estimate model parameters variational algorithm get_params deep get parameters estimator lower_bound returns lower bound model evidence based membership predict predict label data predict_proba predict posterior probability data gaussian rvs args kwargs deprecated removed sample n_samples random_state generate random samples model score set_params params compute log probability model set parameters estimator 
3134: __init__ n_components covariance_type diag alpha random_state none thresh verbose false min_covar none n_iter params wmc init_params wmc aic akaike information criterion current model proposed data parameters array shape n_samples n_dimensions returns aic oat lower better bic bayesian information criterion current model proposed data chapter user guide scikit learn user guide release parameters array shape n_samples n_dimensions returns bic oat lower better decode args kwargs deprecated removed use score predict method instead depending question find likely mixture components point 
3135: deprecated version removed version use score predict method instead depending question 
3136: parameters array_like shape n_features list n_features dimensional data points row corresponds single data point 
3137: returns logprobs array_like shape n_samples log probability point obs model 
3138: components array_like shape n_samples index likelihod mixture com ponents observation eval evaluate model data compute bound log probability model return posterior distribution respon sibilities mixture component element done computing parameters mean eld observation 
3139: parameters array_like shape n_samples n_features list n_features dimensional data points row corresponds single data point 
3140: returns logprob array_like shape n_samples log probabilities data point responsibilities array_like shape n_samples n_components posterior probabilities mixture component observation fit kwargs estimate model parameters variational algorithm full derivation description algorithm see doc derivation derivation tex initialization step performed entering algorithm want avoid step set keyword argument init_params empty string creating object likewise would like initialization set n_iter 
3141: parameters array_like shape n_features list n_features dimensional data points row corresponds single data point 
3142: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3143: reference scikit learn user guide release lower_bound returns lower bound model evidence based membership predict predict label data 
3144: parameters array like shape n_samples n_features returns array shape n_samples predict_proba predict posterior probability data gaussian model 
3145: parameters array like shape n_samples n_features returns responsibilities array like shape n_samples n_components returns probability sample gaussian state model 
3146: rvs args kwargs deprecated removed use score predict method instead depending question generate random samples model 
3147: deprecated version removed version use sample stead sample n_samples random_state none generate random samples model parameters n_samples int optional number samples generate defaults returns array_like shape n_samples n_features list samples score compute log probability model 
3148: parameters array_like shape n_samples n_features list n_features dimensional data points row corresponds single data point 
3149: returns logprob array_like shape n_samples log probabilities data point set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn multiclass multiclass multilabel classication multiclass multilabel classication strategies module implements multiclass learning algorithms chapter user guide scikit learn user guide release one rest one one one error correcting output codes estimators provided module meta estimators require base estimator provided constructor example possible use estimators turn binary classier regressor multiclass classier also possible use estimators multiclass estimators hope accuracy runtime performance improves user guide see multiclass multilabel algorithms section details 
3150: multiclass onevsrestclassifier estimator multiclass onevsoneclassifier estimator multiclass outputcodeclassifier estimator one rest ovr multiclass multilabel strategy one one multiclass strategy error correcting output code multiclass strategy sklearn multiclass onevsrestclassier class sklearn multiclass onevsrestclassifier estimator one rest ovr multiclass multilabel strategy also known one strategy consists tting one classier per class classier class tted classes addition computational efciency n_classes classiers needed one advantage approach interpretability since class represented one one classier possible gain knowledge class inspecting corresponding classier commonly used strategy multiclass classication fair default choice strategy also used multilabel learning classier used predict multiple labels instance tting sequence sequences labels list tuples rather single target vector multilabel learning number classes must least three since otherwise ovr reduces binary classication 
3151: parameters estimator estimator object estimator object implementing one decision_function predict_proba 
3152: attributes estimators_ bel_binarizer_ multilabel_ list n_classes estimators labelbinarizer object boolean estimators used predictions 
3153: object used transform multiclass labels binary labels vice versa whether onevsrestclassier multilabel classier 
3154: methods fit get_params deep predict score set_params params fit underlying estimators get parameters estimator predict multi class targets using underlying estimators 
3155: set parameters estimator 
3156: reference scikit learn user guide release __init__ estimator fit fit underlying estimators 
3157: parameters array like sparse matrix shape n_samples n_features data 
3158: array like shape n_samples sequence sequences len n_samplesmulti class targets sequence quences turns multilabel classication 
3159: returns self get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3160: multilabel_ whether multilabel classier predict predict multi class targets using underlying estimators 
3161: parameters array like sparse matrix shape n_samples n_features data 
3162: returns array like shape n_samples predicted multi class targets 
3163: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn multiclass onevsoneclassier class sklearn multiclass onevsoneclassifier estimator one one multiclass strategy strategy consists tting one classier per class pair prediction time class received votes selected since requires n_classes n_classes classiers method usually slower one rest due n_classes complexity however method may advantageous algorithms kernel algorithms dont scale well n_samples individual learning problem involves small subset data whereas one rest complete dataset used n_classes times 
3164: parameters estimator estimator object estimator object implementing predict 
3165: chapter user guide scikit learn user guide release attributes estimators_ classes_ list n_classes n_classes estimators numpy array shape n_classes estimators used predictions array containing labels 
3166: methods fit get_params deep predict score set_params params fit underlying estimators get parameters estimator predict multi class targets using underlying estimators returns mean accuracy given test data labels set parameters estimator 
3167: __init__ estimator fit fit underlying estimators 
3168: parameters array like sparse matrix shape n_samples n_features data 
3169: numpy array shape n_samples multi class targets 
3170: returns self get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3171: predict predict multi class targets using underlying estimators 
3172: parameters array like sparse matrix shape n_samples n_features data 
3173: returns numpy array shape n_samples predicted multi class targets 
3174: score returns mean accuracy given test data labels 
3175: parameters array like shape n_samples n_features training set 
3176: array like shape n_samples labels 
3177: returns oat reference scikit learn user guide release set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn multiclass outputcodeclassier class sklearn multiclass outputcodeclassifier estimator code_size ran dom_state none error correcting output code multiclass strategy output code based strategies consist representing class binary code array tting time one binary classier per bit code book tted prediction time classiers used project new points class space class closest points chosen main advantage strategies number classiers used controlled user either compressing model code_size making model robust errors code_size see documentation details 
3178: parameters estimator estimator object estimator object implementing one decision_function predict_proba 
3179: code_size oat percentage number classes used create code book number require fewer classiers one rest number greater require classiers one rest 
3180: random_state numpy randomstate optional generator used initialize codebook defaults numpy random 
3181: references r73 r74 r75 attributes estimators_ classes_ code_book_ list int n_classes code_size estimators numpy array shape n_classes numpy array shape n_classes code_size binary array containing code class 
3182: estimators used predictions array containing labels 
3183: methods fit get_params deep predict score set_params params fit underlying estimators get parameters estimator predict multi class targets using underlying estimators returns mean accuracy given test data labels set parameters estimator 
3184: chapter user guide scikit learn user guide release __init__ estimator code_size random_state none fit fit underlying estimators 
3185: parameters array like sparse matrix shape n_samples n_features data 
3186: numpy array shape n_samples multi class targets 
3187: returns self get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3188: predict predict multi class targets using underlying estimators 
3189: parameters array like sparse matrix shape n_samples n_features data 
3190: returns numpy array shape n_samples predicted multi class targets 
3191: score returns mean accuracy given test data labels 
3192: parameters array like shape n_samples n_features training set 
3193: array like shape n_samples labels 
3194: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self multiclass fit_ovr estimator multiclass predict_ovr estimators multiclass fit_ovo estimator multiclass predict_ovo estimators classes multiclass fit_ecoc estimator multiclass predict_ecoc estimators classes make predictions using error correcting output code strategy 
3195: fit one rest strategy make predictions using one rest strategy fit one one strategy make predictions using one one strategy fit error correcting output code strategy 
3196: reference scikit learn user guide release sklearn multiclass t_ovr sklearn multiclass fit_ovr estimator fit one rest strategy 
3197: sklearn multiclass predict_ovr sklearn multiclass predict_ovr estimators label_binarizer make predictions using one rest strategy 
3198: sklearn multiclass t_ovo sklearn multiclass fit_ovo estimator fit one one strategy 
3199: sklearn multiclass predict_ovo sklearn multiclass predict_ovo estimators classes make predictions using one one strategy 
3200: sklearn multiclass t_ecoc sklearn multiclass fit_ecoc estimator code_size random_state none fit error correcting output code strategy 
3201: parameters estimator estimator object estimator object implementing one decision_function predict_proba 
3202: code_size oat optional percentage number classes used create code book 
3203: random_state numpy randomstate optional generator used initialize codebook defaults numpy random 
3204: returns estimators list int n_classes code_size estimators estimators used predictions 
3205: classes numpy array shape n_classes array containing labels 
3206: code_book_ numpy array shape n_classes code_size binary array containing code class 
3207: sklearn multiclass predict_ecoc sklearn multiclass predict_ecoc estimators classes code_book make predictions using error correcting output code strategy 
3208: chapter user guide scikit learn user guide release sklearn naive_bayes naive bayes sklearn naive_bayes module implements naive bayes algorithms supervised learning methods based applying bayes theorem strong naive feature independence assumptions user guide see naive bayes section details 
3209: naive_bayes gaussiannb naive_bayes multinomialnb alpha t_prior naive bayes classier multinomial models naive_bayes bernoullinb alpha binarize naive bayes classier multivariate bernoulli models 
3210: gaussian naive bayes gaussiannb sklearn naive_bayes gaussiannb class sklearn naive_bayes gaussiannb gaussian naive bayes gaussiannb parameters array like shape n_samples n_features training vector n_samples number samples n_features num ber features 
3211: array shape n_samples target vector relative examples import numpy array array sklearn naive_bayes import gaussiannb clf gaussiannb clf fit gaussiannb print clf predict attributes class_prior_ theta_ sigma_ methods array shape n_classes array shape n_classes n_features mean feature per class array shape n_classes n_features probability class 
3212: variance feature per class fit get_params deep predict predict_log_proba return log probability estimates test vector predict_proba score fit gaussian naive bayes according get parameters estimator perform classication array test vectors 
3213: return probability estimates test vector returns mean accuracy given test data labels continued next page reference scikit learn user guide release set_params params set parameters estimator 
3214: table continued previous page __init__ __init__ initializes see help type signature class_prior deprecated gaussiannb class_prior deprecated removed version please use gaussiannb class_prior_ instead 
3215: fit fit gaussian naive bayes according parameters array like shape n_samples n_features training vectors n_samples number samples n_features num ber features 
3216: array like shape n_samples target values returns self object returns self get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3217: predict perform classication array test vectors 
3218: parameters array like shape n_samples n_features returns array shape n_samples predicted target values predict_log_proba return log probability estimates test vector 
3219: parameters array like shape n_samples n_features returns array like shape n_samples n_classes returns log probability sample class model classes ordered arithmetically 
3220: predict_proba return probability estimates test vector 
3221: parameters array like shape n_samples n_features returns array like shape n_samples n_classes returns probability sample class model classes ordered arithmetically 
3222: score returns mean accuracy given test data labels 
3223: chapter user guide scikit learn user guide release parameters array like shape n_samples n_features training set 
3224: array like shape n_samples labels 
3225: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sigma deprecated gaussiannb sigma deprecated removed version please use gaussiannb sigma_ instead 
3226: theta deprecated gaussiannb theta deprecated removed version please use gaussiannb theta_ instead 
3227: sklearn naive_bayes multinomialnb class sklearn naive_bayes multinomialnb alpha t_prior true naive bayes classier multinomial models multinomial naive bayes classier suitable classication discrete features word counts text classication multinomial distribution normally requires integer feature counts however practice fractional counts idf may also work 
3228: parameters alpha oat optional default additive laplace lidstone smoothing parameter smoothing 
3229: t_prior boolean whether learn class prior probabilities false uniform prior used 
3230: notes rationale behind names coef_ intercept_ naive bayes linear classier see rennie tackling poor assumptions naive bayes text classiers icml 
3231: examples import numpy random randint size array sklearn naive_bayes import multinomialnb clf multinomialnb clf fit multinomialnb alpha fit_prior true reference scikit learn user guide release print clf predict attributes intercept_ class_log_prior_ fea ture_log_prob_ coef_ array shape n_classes array shape n_classes n_features methods smoothed empirical log probability class 
3232: empirical log probability features given class x_i intercept_ coef_ properties referring class_log_prior_ feature_log_prob_ respectively fit sample_weight class_prior get_params deep predict predict_log_proba predict_proba score set_params params fit naive bayes classier according get parameters estimator perform classication array test vectors return log probability estimates test vector return probability estimates test vector returns mean accuracy given test data labels set parameters estimator 
3233: __init__ alpha t_prior true fit sample_weight none class_prior none fit naive bayes classier according parameters array like sparse matrix shape n_samples n_features training vectors n_samples number samples n_features num ber features 
3234: array like shape n_samples target values 
3235: sample_weight array like shape n_samples optional weights applied individual samples unweighted 
3236: class_prior array shape n_classes custom prior probability per class overrides t_prior parameter 
3237: returns self object returns self get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3238: predict perform classication array test vectors 
3239: chapter user guide scikit learn user guide release parameters array like shape n_samples n_features returns array shape n_samples predicted target values predict_log_proba return log probability estimates test vector 
3240: parameters array like shape n_samples n_features returns array like shape n_samples n_classes returns log probability sample class model classes ordered arithmetically 
3241: predict_proba return probability estimates test vector 
3242: parameters array like shape n_samples n_features returns array like shape n_samples n_classes returns probability sample class model classes ordered arithmetically 
3243: score returns mean accuracy given test data labels 
3244: parameters array like shape n_samples n_features training set 
3245: array like shape n_samples labels 
3246: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn naive_bayes bernoullinb class sklearn naive_bayes bernoullinb alpha binarize t_prior true naive bayes classier multivariate bernoulli models like multinomialnb classier suitable discrete data difference multinomialnb works occurrence counts bernoullinb designed binary boolean features 
3247: parameters alpha oat optional default additive laplace lidstone smoothing parameter smoothing 
3248: binarize oat none optional threshold binarizing mapping booleans sample features none input presumed already consist binary vectors 
3249: t_prior boolean reference scikit learn user guide release whether learn class prior probabilities false uniform prior used 
3250: references manning raghavan schtze introduction information retrieval cambridge univer sity press mccallum nigam comparison event models naive bayes text classication proc aaai icml workshop learning text categorization metsis androutsopoulos paliouras spam ltering naive bayes naive bayes 3rd conf email anti spam ceas 
3251: examples import numpy random randint size array sklearn naive_bayes import bernoullinb clf bernoullinb clf fit bernoullinb alpha binarize fit_prior true print clf predict attributes class_log_prior_ array shape n_classes fea array shape n_classes ture_log_prob_ n_features log probability class smoothed empirical log probability features given class x_i 
3252: methods fit sample_weight class_prior get_params deep predict predict_log_proba predict_proba score set_params params fit naive bayes classier according get parameters estimator perform classication array test vectors return log probability estimates test vector return probability estimates test vector returns mean accuracy given test data labels set parameters estimator 
3253: __init__ alpha binarize t_prior true fit sample_weight none class_prior none fit naive bayes classier according parameters array like sparse matrix shape n_samples n_features training vectors n_samples number samples n_features num ber features 
3254: array like shape n_samples chapter user guide scikit learn user guide release target values 
3255: sample_weight array like shape n_samples optional weights applied individual samples unweighted 
3256: class_prior array shape n_classes custom prior probability per class overrides t_prior parameter 
3257: returns self object returns self get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3258: predict perform classication array test vectors 
3259: parameters array like shape n_samples n_features returns array shape n_samples predicted target values predict_log_proba return log probability estimates test vector 
3260: parameters array like shape n_samples n_features returns array like shape n_samples n_classes returns log probability sample class model classes ordered arithmetically 
3261: predict_proba return probability estimates test vector 
3262: parameters array like shape n_samples n_features returns array like shape n_samples n_classes returns probability sample class model classes ordered arithmetically 
3263: score returns mean accuracy given test data labels 
3264: parameters array like shape n_samples n_features training set 
3265: array like shape n_samples labels 
3266: returns oat set_params params set parameters estimator 
3267: reference scikit learn user guide release method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn neighbors nearest neighbors sklearn neighbors module implements nearest neighbors algorithm user guide see nearest neighbors section details 
3268: neighbors nearestneighbors n_neighbors neighbors kneighborsclassifier neighbors radiusneighborsclassifier neighbors kneighborsregressor n_neighbors neighbors radiusneighborsregressor radius regression based neighbors within xed radius neighbors balltree neighbors nearestcentroid metric unsupervised learner implementing neighbor searches classier implementing nearest neighbors vote classier implementing vote among neighbors within given radius regression based nearest neighbors 
3269: ball tree fast nearest neighbor searches nearest centroid classier 
3270: sklearn neighbors nearestneighbors class sklearn neighbors nearestneighbors n_neighbors leaf_size warn_on_equidistant true radius algorithm auto unsupervised learner implementing neighbor searches parameters n_neighbors int optional default number neighbors use default k_neighbors queries 
3271: radius oat optional default range parameter space use default methradius_neighbors queries 
3272: algorithm auto ball_tree kd_tree brute optional algorithm used compute nearest neighbors ball_tree use balltree kd_tree use scipy spatial ckdtree brute use brute force search auto attempt decide appropriate algorithm based values passed fit method 
3273: note tting sparse input override setting parameter using brute force 
3274: leaf_size int optional default leaf size passed balltree ckdtree affect speed construction query well memory required store tree optimal value depends nature problem 
3275: warn_on_equidistant boolean optional defaults true 
3276: generate warning equidistant neighbors discarded classication regres sion based neighbors neighbor neighbor identical distances chapter user guide scikit learn user guide release different labels result dependent ordering training data method kd_tree warnings generated 
3277: integer optional default parameter minkowski metric sklearn metrics pairwise pairwise_distances equivalent using manhattan_distance euclidean_distance arbitrary minkowski_distance l_p used 
3278: see also kneighborsclassifier radiusneighborsregressor balltree radiusneighborsclassifier kneighborsregressor notes see nearest neighbors online documentation discussion choice algorithm leaf_size http wikipedia org wiki nearest_neighbor_algorithm examples sklearn neighbors import nearestneighbors samples neigh nearestneighbors neigh fit samples nearestneighbors neigh kneighbors return_distance false array neigh radius_neighbors return_distance false array methods fit get_params deep kneighbors n_neighbors return_distance kneighbors_graph n_neighbors mode radius_neighbors radius return_distance radius_neighbors_graph radius mode set_params params fit model using training data get parameters estimator finds neighbors point computes weighted graph neighbors points finds neighbors point within given radius computes weighted graph neighbors points set parameters estimator 
3279: __init__ n_neighbors radius algorithm auto leaf_size warn_on_equidistant true fit none fit model using training data parameters array like sparse matrix balltree ckdtree reference scikit learn user guide release training data array matrix shape n_samples n_features get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3280: kneighbors n_neighbors none return_distance true finds neighbors point returns distance parameters array like last dimension data new point n_neighbors int number neighbors get default value passed constructor 
3281: return_distance boolean optional defaults true 
3282: false distances returned returns dist array array representing lengths point present return_distance true ind array indices nearest points population matrix 
3283: examples following example construct neighborsclassier class array representing data set ask whos closest point samples sklearn neighbors import nearestneighbors neigh nearestneighbors n_neighbors neigh fit samples nearestneighbors algorithm auto leaf_size print neigh kneighbors array array see returns means element distance third element samples indexes start also query multiple points neigh kneighbors return_distance false array kneighbors_graph n_neighbors none mode connectivity computes weighted graph neighbors points parameters array like shape n_samples n_features sample data n_neighbors int chapter user guide scikit learn user guide release number neighbors sample default value passed constructor 
3284: mode connectivity distance optional type returned matrix connectivity return connectivity matrix ones zeros distance edges euclidean distance points returns sparse matrix csr format shape n_samples n_samples_t n_samples_t number samples tted data assigned weight edge connects 
3285: see also nearestneighbors radius_neighbors_graph examples sklearn neighbors import nearestneighbors neigh nearestneighbors n_neighbors neigh fit nearestneighbors algorithm auto leaf_size neigh kneighbors_graph todense matrix radius_neighbors radius none return_distance true finds neighbors point within given radius returns distance parameters array like last dimension data new point 
3286: radius oat limiting distance neighbors return default value passed constructor 
3287: return_distance boolean optional defaults true 
3288: false distances returned returns dist array array representing lengths point present return_distance true ind array indices nearest points population matrix 
3289: examples following example construnct neighborsclassier class array representing data set ask whos closest point reference scikit learn user guide release samples sklearn neighbors import nearestneighbors neigh nearestneighbors radius neigh fit samples nearestneighbors algorithm auto leaf_size print neigh radius_neighbors array array rst array returned contains distances points closer second array returned contains indices general multiple points queried time number neighbors point necessarily equal radius_neighbors returns array objects object array indices 
3290: radius_neighbors_graph radius none mode connectivity computes weighted graph neighbors points neighborhoods restricted points distance lower radius 
3291: parameters array like shape n_samples n_features sample data radius oat radius neighborhoods default value passed constructor 
3292: mode connectivity distance optional type returned matrix connectivity return connectivity matrix ones zeros distance edges euclidean distance points 
3293: returns sparse matrix csr format shape n_samples n_samples assigned weight edge connects 
3294: see also kneighbors_graph examples sklearn neighbors import nearestneighbors neigh nearestneighbors radius neigh fit nearestneighbors algorithm auto leaf_size neigh radius_neighbors_graph todense matrix set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self chapter user guide sklearn neighbors kneighborsclassier scikit learn user guide release class sklearn neighbors kneighborsclassifier n_neighbors algorithm auto warn_on_equidistant true weights uniform leaf_size classier implementing nearest neighbors vote 
3295: parameters n_neighbors int optional default number neighbors use default k_neighbors queries 
3296: weights str callable weight function used prediction possible values uniform uniform weights points neighborhood weighted equally distance weight points inverse distance case closer neigh bors query point greater inuence neighbors away callable user dened function accepts array distances returns array shape containing weights 
3297: uniform weights used default 
3298: algorithm auto ball_tree kd_tree brute optional algorithm used compute nearest neighbors ball_tree use balltree kd_tree use scipy spatial ckdtree brute use brute force search auto attempt decide appropriate algorithm based values passed fit method 
3299: note tting sparse input override setting parameter using brute force 
3300: leaf_size int optional default leaf size passed balltree ckdtree affect speed construction query well memory required store tree optimal value depends nature problem 
3301: warn_on_equidistant boolean optional defaults true 
3302: generate warning equidistant neighbors discarded classication regres sion based neighbors neighbor neighbor identical distances different labels result dependent ordering training data method kd_tree warnings generated 
3303: integer optional default parameter minkowski metric sklearn metrics pairwise pairwise_distances equivalent using manhattan_distance euclidean_distance arbitrary minkowski_distance l_p used 
3304: see also radiusneighborsclassifier nearestneighbors kneighborsregressor radiusneighborsregressor reference scikit learn user guide release notes see nearest neighbors online documentation discussion choice algorithm leaf_size http wikipedia org wiki nearest_neighbor_algorithm examples sklearn neighbors import kneighborsclassifier neigh kneighborsclassifier n_neighbors neigh fit kneighborsclassifier print neigh predict methods fit get_params deep kneighbors n_neighbors return_distance kneighbors_graph n_neighbors mode predict score set_params params fit model using training data target values get parameters estimator finds neighbors point computes weighted graph neighbors points predict class labels provided data returns mean accuracy given test data labels set parameters estimator 
3305: __init__ n_neighbors warn_on_equidistant true weights uniform algorithm auto leaf_size fit fit model using training data target values parameters array like sparse matrix balltree ckdtree training data array matrix shape n_samples n_features array like sparse matrix shape n_samples target values array integer values 
3306: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3307: kneighbors n_neighbors none return_distance true finds neighbors point returns distance parameters array like last dimension data chapter user guide scikit learn user guide release new point n_neighbors int number neighbors get default value passed constructor 
3308: return_distance boolean optional defaults true 
3309: false distances returned returns dist array array representing lengths point present return_distance true ind array indices nearest points population matrix 
3310: examples following example construct neighborsclassier class array representing data set ask whos closest point samples sklearn neighbors import nearestneighbors neigh nearestneighbors n_neighbors neigh fit samples nearestneighbors algorithm auto leaf_size print neigh kneighbors array array see returns means element distance third element samples indexes start also query multiple points neigh kneighbors return_distance false array kneighbors_graph n_neighbors none mode connectivity computes weighted graph neighbors points parameters array like shape n_samples n_features sample data n_neighbors int number neighbors sample default value passed constructor 
3311: mode connectivity distance optional type returned matrix connectivity return connectivity matrix ones zeros distance edges euclidean distance points returns sparse matrix csr format shape n_samples n_samples_t n_samples_t number samples tted data assigned weight edge connects 
3312: see also nearestneighbors radius_neighbors_graph reference scikit learn user guide release examples sklearn neighbors import nearestneighbors neigh nearestneighbors n_neighbors neigh fit nearestneighbors algorithm auto leaf_size neigh kneighbors_graph todense matrix predict predict class labels provided data parameters array array representing test points 
3313: returns labels array list class labels one data sample 
3314: score returns mean accuracy given test data labels 
3315: parameters array like shape n_samples n_features training set 
3316: array like shape n_samples labels 
3317: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn neighbors radiusneighborsclassier class sklearn neighbors radiusneighborsclassifier radius weights uniform algo rithm auto leaf_size lier_label none classier implementing vote among neighbors within given radius parameters radius oat optional default range parameter space use default methradius_neighbors queries 
3318: weights str callable weight function used prediction possible values uniform uniform weights points neighborhood weighted equally 
3319: chapter user guide scikit learn user guide release distance weight points inverse distance case closer neigh bors query point greater inuence neighbors away callable user dened function accepts array distances returns array shape containing weights 
3320: uniform weights used default 
3321: algorithm auto ball_tree kd_tree brute optional algorithm used compute nearest neighbors ball_tree use balltree kd_tree use scipy spatial ckdtree brute use brute force search auto attempt decide appropriate algorithm based values passed fit method 
3322: note tting sparse input override setting parameter using brute force 
3323: leaf_size int optional default leaf size passed balltree ckdtree affect speed construction query well memory required store tree optimal value depends nature problem 
3324: integer optional default parameter minkowski metric sklearn metrics pairwise pairwise_distances equivalent using manhattan_distance euclidean_distance arbitrary minkowski_distance l_p used 
3325: outlier_label int optional default none label given outlier samples samples neighbors given radius set none valueerror raised outlier detected 
3326: see also kneighborsclassifier nearestneighbors notes radiusneighborsregressor kneighborsregressor see nearest neighbors online documentation discussion choice algorithm leaf_size http wikipedia org wiki nearest_neighbor_algorithm examples sklearn neighbors import radiusneighborsclassifier neigh radiusneighborsclassifier radius neigh fit radiusneighborsclassifier reference scikit learn user guide release print neigh predict methods fit get_params deep predict radius_neighbors radius return_distance radius_neighbors_graph radius mode score set_params params fit model using training data target values get parameters estimator predict class labels provided data finds neighbors point within given radius computes weighted graph neighbors points returns mean accuracy given test data labels set parameters estimator 
3327: __init__ radius weights uniform algorithm auto leaf_size outlier_label none fit fit model using training data target values parameters array like sparse matrix balltree ckdtree training data array matrix shape n_samples n_features array like sparse matrix shape n_samples target values array integer values 
3328: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3329: predict predict class labels provided data parameters array array representing test points 
3330: returns labels array list class labels one data sample 
3331: radius_neighbors radius none return_distance true finds neighbors point within given radius returns distance parameters array like last dimension data new point 
3332: radius oat limiting distance neighbors return default value passed constructor 
3333: return_distance boolean optional defaults true 
3334: false distances returned chapter user guide scikit learn user guide release returns dist array array representing lengths point present return_distance true ind array indices nearest points population matrix 
3335: examples following example construnct neighborsclassier class array representing data set ask whos closest point samples sklearn neighbors import nearestneighbors neigh nearestneighbors radius neigh fit samples nearestneighbors algorithm auto leaf_size print neigh radius_neighbors array array rst array returned contains distances points closer second array returned contains indices general multiple points queried time number neighbors point necessarily equal radius_neighbors returns array objects object array indices 
3336: radius_neighbors_graph radius none mode connectivity computes weighted graph neighbors points neighborhoods restricted points distance lower radius 
3337: parameters array like shape n_samples n_features sample data radius oat radius neighborhoods default value passed constructor 
3338: mode connectivity distance optional type returned matrix connectivity return connectivity matrix ones zeros distance edges euclidean distance points 
3339: returns sparse matrix csr format shape n_samples n_samples assigned weight edge connects 
3340: see also kneighbors_graph examples sklearn neighbors import nearestneighbors neigh nearestneighbors radius neigh fit nearestneighbors algorithm auto leaf_size neigh radius_neighbors_graph todense reference scikit learn user guide release matrix score returns mean accuracy given test data labels 
3341: parameters array like shape n_samples n_features training set 
3342: array like shape n_samples labels 
3343: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn neighbors kneighborsregressor class sklearn neighbors kneighborsregressor n_neighbors algorithm auto warn_on_equidistant true weights uniform leaf_size regression based nearest neighbors target predicted local interpolation targets associated nearest neighbors training set 
3344: parameters n_neighbors int optional default number neighbors use default k_neighbors queries 
3345: weights str callable weight function used prediction possible values uniform uniform weights points neighborhood weighted equally distance weight points inverse distance case closer neigh bors query point greater inuence neighbors away callable user dened function accepts array distances returns array shape containing weights 
3346: uniform weights used default 
3347: algorithm auto ball_tree kd_tree brute optional algorithm used compute nearest neighbors ball_tree use balltree kd_tree use scipy spatial ckdtree brute use brute force search 
3348: chapter user guide scikit learn user guide release auto attempt decide appropriate algorithm based values passed fit method 
3349: note tting sparse input override setting parameter using brute force 
3350: leaf_size int optional default leaf size passed balltree ckdtree affect speed construction query well memory required store tree optimal value depends nature problem 
3351: warn_on_equidistant boolean optional defaults true 
3352: generate warning equidistant neighbors discarded classication regres sion based neighbors neighbor neighbor identical distances different labels result dependent ordering training data method kd_tree warnings generated 
3353: integer optional default parameter minkowski metric sklearn metrics pairwise pairwise_distances equivalent using manhattan_distance euclidean_distance arbitrary minkowski_distance l_p used 
3354: see also nearestneighbors radiusneighborsclassifier radiusneighborsregressor kneighborsclassifier notes see nearest neighbors online documentation discussion choice algorithm leaf_size http wikipedia org wiki nearest_neighbor_algorithm examples sklearn neighbors import kneighborsregressor neigh kneighborsregressor n_neighbors neigh fit kneighborsregressor print neigh predict methods fit get_params deep kneighbors n_neighbors return_distance kneighbors_graph n_neighbors mode predict fit model using training data target values get parameters estimator finds neighbors point computes weighted graph neighbors points predict target provided data reference continued next page scikit learn user guide release score set_params params returns coefcient determination prediction set parameters estimator 
3355: table continued previous page __init__ n_neighbors warn_on_equidistant true weights uniform algorithm auto leaf_size fit fit model using training data target values parameters array like sparse matrix balltree ckdtree training data array matrix shape n_samples n_features array like sparse matrix shape n_samples target values array oat values 
3356: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3357: kneighbors n_neighbors none return_distance true finds neighbors point returns distance parameters array like last dimension data new point n_neighbors int number neighbors get default value passed constructor 
3358: return_distance boolean optional defaults true 
3359: false distances returned returns dist array array representing lengths point present return_distance true ind array indices nearest points population matrix 
3360: examples following example construct neighborsclassier class array representing data set ask whos closest point samples sklearn neighbors import nearestneighbors neigh nearestneighbors n_neighbors neigh fit samples nearestneighbors algorithm auto leaf_size print neigh kneighbors array array chapter user guide scikit learn user guide release see returns means element distance third element samples indexes start also query multiple points neigh kneighbors return_distance false array kneighbors_graph n_neighbors none mode connectivity computes weighted graph neighbors points parameters array like shape n_samples n_features sample data n_neighbors int number neighbors sample default value passed constructor 
3361: mode connectivity distance optional type returned matrix connectivity return connectivity matrix ones zeros distance edges euclidean distance points returns sparse matrix csr format shape n_samples n_samples_t n_samples_t number samples tted data assigned weight edge connects 
3362: see also nearestneighbors radius_neighbors_graph examples sklearn neighbors import nearestneighbors neigh nearestneighbors n_neighbors neigh fit nearestneighbors algorithm auto leaf_size neigh kneighbors_graph todense matrix predict predict target provided data parameters array array representing test data 
3363: returns array list target values one data sample 
3364: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
3365: reference scikit learn user guide release parameters array like shape n_samples n_features training set 
3366: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn neighbors radiusneighborsregressor class sklearn neighbors radiusneighborsregressor radius weights uniform algo regression based neighbors within xed radius target predicted local interpolation targets associated nearest neighbors training set 
3367: rithm auto leaf_size parameters radius oat optional default range parameter space use default methradius_neighbors queries 
3368: weights str callable weight function used prediction possible values uniform uniform weights points neighborhood weighted equally distance weight points inverse distance case closer neigh bors query point greater inuence neighbors away callable user dened function accepts array distances returns array shape containing weights 
3369: uniform weights used default 
3370: algorithm auto ball_tree kd_tree brute optional algorithm used compute nearest neighbors ball_tree use balltree kd_tree use scipy spatial ckdtree brute use brute force search auto attempt decide appropriate algorithm based values passed fit method 
3371: note tting sparse input override setting parameter using brute force 
3372: leaf_size int optional default leaf size passed balltree ckdtree affect speed construction query well memory required store tree optimal value depends nature problem 
3373: integer optional default chapter user guide scikit learn user guide release parameter minkowski metric sklearn metrics pairwise pairwise_distances equivalent using manhattan_distance euclidean_distance arbitrary minkowski_distance l_p used 
3374: see also nearestneighbors radiusneighborsclassifier notes kneighborsregressor kneighborsclassifier see nearest neighbors online documentation discussion choice algorithm leaf_size http wikipedia org wiki nearest_neighbor_algorithm examples sklearn neighbors import radiusneighborsregressor neigh radiusneighborsregressor radius neigh fit radiusneighborsregressor print neigh predict methods fit get_params deep predict radius_neighbors radius return_distance radius_neighbors_graph radius mode score set_params params fit model using training data target values get parameters estimator predict target provided data finds neighbors point within given radius computes weighted graph neighbors points returns coefcient determination prediction set parameters estimator 
3375: __init__ radius weights uniform algorithm auto leaf_size fit fit model using training data target values parameters array like sparse matrix balltree ckdtree training data array matrix shape n_samples n_features array like sparse matrix shape n_samples target values array oat values 
3376: get_params deep true get parameters estimator parameters deep boolean optional reference scikit learn user guide release true return parameters estimator contained subobjects estimators 
3377: predict predict target provided data parameters array array representing test data 
3378: returns array list target values one data sample 
3379: radius_neighbors radius none return_distance true finds neighbors point within given radius returns distance parameters array like last dimension data new point 
3380: radius oat limiting distance neighbors return default value passed constructor 
3381: return_distance boolean optional defaults true 
3382: false distances returned returns dist array array representing lengths point present return_distance true ind array indices nearest points population matrix 
3383: examples following example construnct neighborsclassier class array representing data set ask whos closest point samples sklearn neighbors import nearestneighbors neigh nearestneighbors radius neigh fit samples nearestneighbors algorithm auto leaf_size print neigh radius_neighbors array array rst array returned contains distances points closer second array returned contains indices general multiple points queried time number neighbors point necessarily equal radius_neighbors returns array objects object array indices 
3384: radius_neighbors_graph radius none mode connectivity computes weighted graph neighbors points neighborhoods restricted points distance lower radius 
3385: parameters array like shape n_samples n_features sample data chapter user guide scikit learn user guide release radius oat radius neighborhoods default value passed constructor 
3386: mode connectivity distance optional type returned matrix connectivity return connectivity matrix ones zeros distance edges euclidean distance points 
3387: returns sparse matrix csr format shape n_samples n_samples assigned weight edge connects 
3388: see also kneighbors_graph examples sklearn neighbors import nearestneighbors neigh nearestneighbors radius neigh fit nearestneighbors algorithm auto leaf_size neigh radius_neighbors_graph todense matrix score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
3389: parameters array like shape n_samples n_features training set 
3390: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn neighbors balltree class sklearn neighbors balltree ball tree fast nearest neighbor searches balltree leaf_size parameters array like shape n_samples n_features reference scikit learn user guide release n_samples number points data set n_features dimension parameter space note contiguous array doubles data copied otherwise internal copy made 
3391: leaf_size positive integer default number points switch brute force changing leaf_size fect results query signicantly impact speed query memory required store built ball tree amount memory needed store tree scales oor log2 n_samples leaf_size specied leaf_size leaf node guaranteed satisfy leaf_size n_points leaf_size except case n_samples leaf_size 
3392: distance metric balltree encodes minkowski distance sum must greater equal triangle inequality hold inf distance equivalent max examples query nearest neighbors import numpy random seed random random points dimensions ball_tree balltree leaf_size dist ind ball_tree query n_neighbors print ind indices closest neighbors print dist distances closest neighbors 
3393: pickle unpickle ball tree using protocol note state tree saved pickle operation tree rebuilt pickling import numpy import pickle random seed random random points dimensions ball_tree balltree leaf_size pickle dumps ball_tree protocol ball_tree_copy pickle loads dist ind ball_tree_copy query print ind indices closest neighbors print dist distances closest neighbors 
3394: attributes chapter user guide scikit learn user guide release data warning_flag methods query return_distance query_radius query ball tree nearest neighbors query_radius self count_only false __init__ __init__ initializes see help type signature query return_distance true query ball tree nearest neighbors parameters array like last dimension self dim array points query integer default number nearest neighbors return return_distance boolean default true true return tuple false return array returns return_distance false return_distance true array doubles shape shape entry gives list distances neighbors corresponding point note distances sorted array integers shape shape entry gives list indices neighbors corresponding point note neighbors sorted examples query nearest neighbors import numpy random seed random random points dimensions ball_tree balltree leaf_size dist ind ball_tree query print ind indices closest neighbors print dist distances closest neighbors 
3395: query_radius query_radius self count_only false query ball tree neighbors within ball size reference scikit learn user guide release parameters array like last dimension self dim array points query distance within neighbors returned single value array values shape shape different radii desired point 
3396: return_distance boolean default false true return distances neighbors point false return neighbors note unlike balltree query setting return_distance true adds computation time distances need calculated explicitly return_distance false results sorted default see sort_results keyword 
3397: count_only boolean default false true return count points within distance false return indices points within distance return_distance true setting count_only true result error 
3398: sort_results boolean default false true distances indices sorted returned false results sorted return_distance false setting sort_results true result error 
3399: returns count count_only true ind count_only false return_distance false ind dist count_only false return_distance true count array integers shape shape entry gives number neighbors within distance corresponding point 
3400: ind array objects shape shape element numpy integer array listing indices neighbors correspond ing point note unlike results balltree query returned neighbors sorted distance dist array objects shape shape element numpy double array listing distances corresponding indices 
3401: examples query neighbors given radius import numpy random seed random random points dimensions ball_tree balltree leaf_size print ball_tree query_radius count_only true ind ball_tree query_radius print ind indices neighbors within distance chapter user guide scikit learn user guide release sklearn neighbors nearestcentroid class sklearn neighbors nearestcentroid metric euclidean shrink_threshold none nearest centroid classier class represented centroid test samples classied class nearest centroid 
3402: parameters metric string callable metric use calculating distance instances feature array metric string callable must one options allowed met rics pairwise pairwise_distances metric parameter 
3403: shrink_threshold oat optional threshold shrinking centroids remove features 
3404: see also sklearn neighbors kneighborsclassifiernearest neighbors classier notes used text classication tfidf vectors classier also known rocchio classier 
3405: references tibshirani hastie narasimhan chu diagnosis multiple cancer types shrunken centroids gene expression proceedings national academy sciences united states america national academy sciences 
3406: examples sklearn neighbors nearest_centroid import nearestcentroid import numpy array array clf nearestcentroid clf fit nearestcentroid metric euclidean shrink_threshold none print clf predict attributes centroids_ array like shape n_classes n_features centroid class methods fit get_params deep fit nearestcentroid model according given training data get parameters estimator continued next page reference scikit learn user guide release table continued previous page predict score set_params params perform classication array test vectors returns mean accuracy given test data labels set parameters estimator 
3407: __init__ metric euclidean shrink_threshold none fit fit nearestcentroid model according given training data 
3408: parameters array like sparse matrix shape n_samples n_features training vector n_samples number samples n_features num ber features note centroid shrinking cannot used sparse matrices 
3409: array shape n_samples target values integers get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3410: predict perform classication array test vectors predicted class sample returned 
3411: parameters array like shape n_samples n_features returns array shape n_samples notes metric constructor parameter precomputed assumed distance matrix data predicted self centroids_ 
3412: score returns mean accuracy given test data labels 
3413: parameters array like shape n_samples n_features training set 
3414: array like shape n_samples labels 
3415: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self chapter user guide scikit learn user guide release neighbors kneighbors_graph n_neighbors computes weighted graph neighbors points neighbors radius_neighbors_graph radius computes weighted graph neighbors points sklearn neighbors kneighbors_graph sklearn neighbors kneighbors_graph n_neighbors mode connectivity computes weighted graph neighbors points parameters array like balltree shape n_samples n_features sample data form numpy array precomputed balltree 
3416: n_neighbors int number neighbors sample 
3417: mode connectivity distance optional type returned matrix connectivity return connectivity matrix ones zeros distance edges euclidean distance points 
3418: returns sparse matrix csr format shape n_samples n_samples assigned weight edge connects 
3419: see also radius_neighbors_graph examples sklearn neighbors import kneighbors_graph kneighbors_graph todense matrix sklearn neighbors radius_neighbors_graph sklearn neighbors radius_neighbors_graph radius mode connectivity computes weighted graph neighbors points neighborhoods restricted points distance lower radius 
3420: parameters array like balltree shape n_samples n_features sample data form numpy array precomputed balltree 
3421: radius oat radius neighborhoods 
3422: mode connectivity distance optional type returned matrix connectivity return connectivity matrix ones zeros distance edges euclidean distance points 
3423: reference scikit learn user guide release returns sparse matrix csr format shape n_samples n_samples assigned weight edge connects 
3424: see also kneighbors_graph examples sklearn neighbors import radius_neighbors_graph radius_neighbors_graph todense matrix sklearn pls partial least squares sklearn pls module implements partial least squares pls user guide see partial least squares section details 
3425: pls plsregression n_components scale pls plscanonical n_components scale pls cca n_components scale max_iter pls plssvd n_components scale copy pls regression plscanonical implements blocks canonical pls original wold cca canonical correlation analysis cca inherits pls partial least square svd sklearn pls plsregression class sklearn pls plsregression n_components scale true max_iter tol copy true pls regression plsregression implements pls blocks regression known pls2 pls1 case one dimensional response class inherits _pls mode deation_mode regression norm_y_weights false algorithm nipals 
3426: parameters array like predictors shape n_samples training vectors n_samples number samples number predictors 
3427: array like response shape n_samples training vectors n_samples number samples number response variables 
3428: n_components int default number components keep 
3429: scale boolean default true whether scale data max_iter integer default chapter user guide scikit learn user guide release maximum number iterations nipals inner loop used algo rithm nipals tol non negative real tolerance used iterative algorithm default 
3430: copy boolean default true whether deation done copy let default value true unless dont care side effect notes component weights optimizes max corr var var note maximizes correlations scores intra block variances residual matrix block obtained deation current score x_score residual matrix block obtained deation current score performs pls regression known pls2 mode prediction oriented implementation provides results pls packages provided language project mixomics function pls mode regression plspm function plsreg2 pls function oscorespls references jacob wegelin survey partial least squares pls methods emphasis two block case technical report department statistics university washington seattle french still reference tenenhaus regression pls theorie pratique paris editions technic 
3431: examples sklearn pls import plscanonical plsregression cca pls2 plsregression n_components pls2 fit plsregression copy true max_iter n_components scale true tol y_pred pls2 predict reference scikit learn user guide release attributes x_weights_ y_weights_ x_loadings_ y_loadings_ x_scores_ y_scores_ x_rotations_ y_rotations_ coefs array methods array n_components array n_components array n_components array n_components array n_samples n_components scores array n_samples n_components scores array n_components array n_components block weights vectors block weights vectors block loadings vectors block loadings vectors 
3432: block latents rotations block latents rotations coecients linear model coefs err fit get_params deep predict copy set_params params transform copy apply dimension reduction learned train data 
3433: get parameters estimator apply dimension reduction learned train data set parameters estimator 
3434: __init__ n_components scale true max_iter tol copy true get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators predict copy true apply dimension reduction learned train data 
3435: parameters array like predictors shape n_samples training vectors n_samples number samples number predictors 
3436: copy boolean whether copy perform place normalization 
3437: notes call require estimation matrix may issue high dimensional space 
3438: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self chapter user guide scikit learn user guide release transform none copy true apply dimension reduction learned train data 
3439: parameters array like predictors shape n_samples training vectors n_samples number samples number predictors 
3440: array like response shape n_samples optional training vectors n_samples number samples number response variables 
3441: copy boolean whether copy perform place normalization 
3442: returns x_scores given x_scores y_scores otherwise sklearn pls plscanonical class sklearn pls plscanonical n_components scale true algorithm nipals max_iter tol copy true plscanonical implements blocks canonical pls original wold algorithm tenenhaus refered pls c2a wegelin class inherits pls mode deation_mode canonical norm_y_weights true gorithm nipals svd provide similar results numerical errors 
3443: parameters array like predictors shape n_samples training vectors n_samples number samples number predictors 
3444: array like response shape n_samples training vectors n_samples number samples number response variables 
3445: n_components int number components keep default scale boolean scale data default true algorithm string nipals svd algorithm used estimate weights called n_components times iteration outer loop 
3446: max_iter integer default maximum number iterations nipals inner loop used algo rithm nipals tol non negative real default tolerance used iterative algorithm copy boolean default true whether deation done copy let default value true unless dont care side effect see also cca plssvd reference scikit learn user guide release notes component weights optimize max corr var var note maximizes correlations scores intra block variances residual matrix block obtained deation current score x_score residual matrix block obtained deation current score performs canonical symetric version pls regression slightly different cca mode mostly used modeling implementation provides results plspm package provided language project using function plsca results equal colinear function pls mode canonical mixomics package difference relies fact mixomics implmentation exactly implement wold algorithm since normalize y_weights one 
3447: references jacob wegelin survey partial least squares pls methods emphasis two block case technical report department statistics university washington seattle tenenhaus regression pls theorie pratique paris editions technic 
3448: examples sklearn pls import plscanonical plsregression cca plsca plscanonical n_components plsca fit plscanonical algorithm nipals copy true max_iter n_components x_c y_c plsca transform scale true tol attributes x_weights_ y_weights_ x_loadings_ y_loadings_ x_scores_ y_scores_ x_rotations_ y_rotations_ methods array shape n_components array shape n_components array shape n_components array shape n_components array shape n_samples n_components scores array shape n_samples n_components scores array shape n_components array shape n_components block weights vectors block weights vectors block loadings vectors block loadings vectors 
3449: block latents rotations block latents rotations 
3450: fit continued next page chapter user guide scikit learn user guide release table continued previous page get_params deep predict copy set_params params transform copy apply dimension reduction learned train data 
3451: get parameters estimator apply dimension reduction learned train data set parameters estimator 
3452: __init__ n_components scale true algorithm nipals max_iter tol copy true get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators predict copy true apply dimension reduction learned train data 
3453: parameters array like predictors shape n_samples training vectors n_samples number samples number predictors 
3454: copy boolean whether copy perform place normalization 
3455: notes call require estimation matrix may issue high dimensional space 
3456: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform none copy true apply dimension reduction learned train data 
3457: parameters array like predictors shape n_samples training vectors n_samples number samples number predictors 
3458: array like response shape n_samples optional training vectors n_samples number samples number response variables 
3459: copy boolean whether copy perform place normalization 
3460: returns x_scores given x_scores y_scores otherwise reference scikit learn user guide release sklearn pls cca class sklearn pls cca n_components scale true max_iter tol copy true pls mode dea cca canonical correlation analysis tion_mode canonical 
3461: cca inherits parameters array like predictors shape n_samples training vectors n_samples number samples number predictors 
3462: array like response shape n_samples training vectors n_samples number samples number response variables 
3463: n_components int default 
3464: number components keep 
3465: scale boolean default true whether scale data max_iter integer default maximum number iterations nipals inner loop used algo rithm nipals tol non negative real default 
3466: tolerance used iterative algorithm copy boolean whether deation done copy let default value true unless dont care side effects see also plscanonical plssvd notes component weights maximizes max corr note maximizes correlations scores residual matrix block obtained deation current score x_score residual matrix block obtained deation current score 
3467: references jacob wegelin survey partial least squares pls methods emphasis two block case technical report department statistics university washington seattle french still reference tenenhaus regression pls theorie pratique paris editions technic 
3468: chapter user guide scikit learn user guide release examples sklearn pls import plscanonical plsregression cca cca cca n_components cca fit cca copy true max_iter n_components scale true tol x_c y_c cca transform attributes x_weights_ y_weights_ x_loadings_ y_loadings_ x_scores_ y_scores_ x_rotations_ y_rotations_ methods array n_components array n_components array n_components array n_components array n_samples n_components scores array n_samples n_components scores array n_components array n_components block weights vectors block weights vectors block loadings vectors block loadings vectors 
3469: block latents rotations block latents rotations 
3470: fit get_params deep predict copy set_params params transform copy apply dimension reduction learned train data 
3471: get parameters estimator apply dimension reduction learned train data set parameters estimator 
3472: __init__ n_components scale true max_iter tol copy true get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators predict copy true apply dimension reduction learned train data 
3473: parameters array like predictors shape n_samples training vectors n_samples number samples number predictors 
3474: copy boolean whether copy perform place normalization 
3475: reference scikit learn user guide release notes call require estimation matrix may issue high dimensional space 
3476: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform none copy true apply dimension reduction learned train data 
3477: parameters array like predictors shape n_samples training vectors n_samples number samples number predictors 
3478: array like response shape n_samples optional training vectors n_samples number samples number response variables 
3479: copy boolean whether copy perform place normalization 
3480: returns x_scores given x_scores y_scores otherwise sklearn pls plssvd class sklearn pls plssvd n_components scale true copy true partial least square svd simply perform svd crosscovariance matrix iterative deation 
3481: parameters array like predictors shape n_samples training vector n_samples number samples number predictors centered analysis 
3482: array like response shape n_samples training vector n_samples number samples number response variables centered analysis 
3483: n_components int default 
3484: number components keep 
3485: scale boolean default true scale see also plscanonical cca chapter user guide scikit learn user guide release attributes x_weights_ y_weights_ x_scores_ y_scores_ array n_components array n_components array n_samples n_components scores array n_samples n_components scores 
3486: block weights vectors block weights vectors 
3487: methods fit get_params deep set_params params transform get parameters estimator set parameters estimator apply dimension reduction learned train data 
3488: __init__ n_components scale true copy true get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform none apply dimension reduction learned train data 
3489: sklearn pipeline pipeline sklearn pipeline module implements utilites build composite estimator chain transforms estimators 
3490: pipeline pipeline steps pipeline transforms nal estimator 
3491: sklearn pipeline pipeline class sklearn pipeline pipeline steps pipeline transforms nal estimator sequentially apply list transforms nal estimator intermediate steps pipeline must trans forms must implements transform methods nal estimator needs implements purpose pipeline assemble several steps cross validated together setting differ ent parameters enables setting parameters various steps using names parameter reference scikit learn user guide release name separated example 
3492: parameters steps list list name transform tuples implementing transform chained order chained last object estimator 
3493: examples sklearn import svm sklearn datasets import samples_generator sklearn feature_selection import selectkbest sklearn feature_selection import f_regression sklearn pipeline import pipeline generate data play samples_generator make_classification 
3494: n_informative n_redundant random_state anova svm anova_filter selectkbest f_regression clf svm svc kernel linear anova_svm pipeline anova anova_filter svc clf set parameters using names issued instance fit using selectkbest parameter svn anova_svm set_params anova__k svc__c fit pipeline steps prediction anova_svm predict anova_svm score attributes steps list name object list named object compose pipeline order applied data 
3495: methods decision_function applies transforms data decision_function method nal estimator fit fit_transform get_params deep inverse_transform predict predict_log_proba predict_proba score applies transforms data predict_proba method nal estimator applies transforms data score method nal estimator 
3496: fit transforms one transform fit transforms one transform data use t_transform transformed data using nal estimator 
3497: applies transforms data predict method nal estimator 
3498: chapter user guide scikit learn user guide release set_params params transform set parameters estimator applies transforms data transform method nal estimator 
3499: table continued previous page __init__ steps decision_function applies transforms data decision_function method nal estimator valid nal estimator implements decision_function 
3500: fit none t_params fit transforms one transform data transformed data using nal estimator 
3501: fit_transform none t_params fit transforms one transform data use t_transform transformed data using nal estimator valid nal estimator implements t_transform 
3502: predict applies transforms data predict method nal estimator valid nal estimator implements predict predict_proba applies transforms data predict_proba method nal estimator valid nal estimator implements predict_proba 
3503: score none applies transforms data score method nal estimator valid nal estimator implements score 
3504: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform applies transforms data transform method nal estimator valid nal estimator implements transform 
3505: sklearn preprocessing preprocessing normalization user guide see preprocessing data section details 
3506: preprocessing scaler copy with_mean with_std preprocessing normalizer norm copy preprocessing binarizer threshold copy preprocessing labelbinarizer neg_label preprocessing kernelcenterer standardize features removing mean scaling unit variance normalize samples individually unit norm binarize data set feature values according threshold binarize labels one fashion center kernel matrix reference scikit learn user guide release sklearn preprocessing scaler class sklearn preprocessing scaler copy true with_mean true with_std true standardize features removing mean scaling unit variance centering scaling happen indepently feature computing relevant statistics samples training set mean standard deviation stored used later data using transform method standardization dataset common requirement many machine learning estimators might behave badly individual feature less look like standard normally distributed data gaussian mean unit variance instance many elements used objective function learning algorithm rbf kernel support vector machines regularizers linear models assume features centered around variance order feature variance orders magnitude larger others might dominate objective function make estimator unable learn features correctly expected 
3507: parameters with_mean boolean true default true center data scaling 
3508: with_std boolean true default true scale data unit variance equivalently unit standard deviation 
3509: copy boolean optional default true set false perform inplace row normalization avoid copy input already numpy array scipy sparse csr matrix axis 
3510: see also sklearn preprocessing scale scaling sklearn decomposition randomizedpca attributes mean_ std_ array oats shape n_features array oats shape n_features mean value feature training set standard deviation feature training set 
3511: methods fit fit_transform get_params deep inverse_transform copy set_params params transform copy compute mean std used later scaling fit data transform get parameters estimator scale back data original representation set parameters estimator perform standardization centering scaling __init__ copy true with_mean true with_std true fit none compute mean std used later scaling parameters array like csr matrix shape n_samples n_features chapter user guide scikit learn user guide release data used compute mean standard deviation used later scaling along features axis 
3512: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
3513: parameters numpy array shape n_samples n_features training set 
3514: numpy array shape n_samples target values 
3515: returns x_new numpy array shape n_samples n_features_new transformed array 
3516: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
3517: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3518: inverse_transform copy none scale back data original representation parameters array like shape n_samples n_features data used scale along features axis 
3519: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform none copy none perform standardization centering scaling parameters array like shape n_samples n_features data used scale along features axis 
3520: sklearn preprocessing normalizer class sklearn preprocessing normalizer norm copy true normalize samples individually unit norm sample row data matrix least one non zero component rescaled independently samples norm equals one 
3521: reference scikit learn user guide release transformer able work dense numpy arrays scipy sparse matrix use csr format want avoid burden copy conversion scaling inputs unit norms common operation text classication clustering instance instance dot product two normalized idf vectors cosine similarity vectors base similarity metric vector space model commonly used information retrieval community 
3522: parameters norm optional default norm use normalize non zero sample 
3523: copy boolean optional default true set false perform inplace row normalization avoid copy input already numpy array scipy sparse csr matrix 
3524: see also sklearn preprocessing normalize without notes estimator stateless besides constructor parameters method nothing useful used pipeline 
3525: methods fit fit_transform get_params deep set_params params transform copy nothing return estimator unchanged fit data transform get parameters estimator set parameters estimator scale non zero row unit norm __init__ norm copy true fit none nothing return estimator unchanged method implement usual api hence work pipelines 
3526: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
3527: parameters numpy array shape n_samples n_features training set 
3528: numpy array shape n_samples target values 
3529: returns x_new numpy array shape n_samples n_features_new transformed array 
3530: chapter user guide scikit learn user guide release notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
3531: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform none copy none scale non zero row unit norm parameters array scipy sparse matrix shape n_samples n_features data normalize row row scipy sparse matrices csr format avoid necessary copy 
3532: sklearn preprocessing binarizer class sklearn preprocessing binarizer threshold copy true binarize data set feature values according threshold default threshold non zero values set zeros left untouched binarization common operation text count data analyst decide consider presence absence feature rather quantied number occurences instance also used pre processing step estimators consider boolean random variables modeled using bernoulli distribution bayesian setting 
3533: parameters threshold oat optional default lower bound triggers feature values replaced 
3534: copy boolean optional default true set false perform inplace binarization avoid copy input already numpy array scipy sparse csr matrix 
3535: notes input sparse matrix non zero values subject update binarizer class estimator stateless besides constructor parameters method nothing useful used pipeline 
3536: reference scikit learn user guide release methods fit fit_transform get_params deep set_params params transform copy binarize element nothing return estimator unchanged fit data transform get parameters estimator set parameters estimator 
3537: __init__ threshold copy true fit none nothing return estimator unchanged method implement usual api hence work pipelines 
3538: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
3539: parameters numpy array shape n_samples n_features training set 
3540: numpy array shape n_samples target values 
3541: returns x_new numpy array shape n_samples n_features_new transformed array 
3542: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
3543: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform none copy none binarize element parameters array scipy sparse matrix shape n_samples n_features data binarize element element scipy sparse matrices csr format avoid necessary copy 
3544: chapter user guide scikit learn user guide release sklearn preprocessing labelbinarizer class sklearn preprocessing labelbinarizer neg_label pos_label binarize labels one fashion several regression binary classication algorithms available scikit simple way extend algorithms multi class classication case use called one scheme learning time simply consists learning one regressor binary classier per class one needs convert multi class labels binary labels belong belong class labelbinarizer makes process easy transform method prediction time one assigns class corresponding model gave greatest condence belbinarizer makes easy inverse_transform method 
3545: parameters neg_label int default value negative labels must encoded 
3546: pos_label int default value positive labels must encoded 
3547: examples sklearn import preprocessing clf preprocessing labelbinarizer clf fit labelbinarizer neg_label pos_label clf classes_ array clf transform array clf fit_transform array clf classes_ array attributes classes_ array shape n_class holds label class 
3548: methods fit fit_transform get_params deep inverse_transform threshold transform binary labels back multi class labels set_params params transform fit label binarizer fit data transform get parameters estimator set parameters estimator transform multi class labels binary labels reference scikit learn user guide release __init__ neg_label pos_label fit fit label binarizer parameters numpy array shape n_samples sequence sequences target values multilabel case nested sequences variable lengths 
3549: returns self returns instance self 
3550: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
3551: parameters numpy array shape n_samples n_features training set 
3552: numpy array shape n_samples target values 
3553: returns x_new numpy array shape n_samples n_features_new transformed array 
3554: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
3555: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3556: inverse_transform threshold none transform binary labels back multi class labels parameters numpy array shape n_samples n_classes target values 
3557: threshold oat none threshold used binary multi label cases use contains output decision_function classier use contains output predict_proba none threshold assumed half way neg_label pos_label 
3558: returns numpy array shape n_samples sequence sequences target values multilabel case nested sequences variable lengths 
3559: chapter user guide scikit learn user guide release notes case binary labels fractional probabilistic inverse_transform chooses class greatest value typically allows use output linear models decision_function method directly input inverse_transform 
3560: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform transform multi class labels binary labels output transform sometimes referred authors coding scheme 
3561: parameters numpy array shape n_samples sequence sequences target values multilabel case nested sequences variable lengths 
3562: returns numpy array shape n_samples n_classes sklearn preprocessing kernelcenterer class sklearn preprocessing kernelcenterer center kernel matrix equivalent centering phi sklearn preprocessing scaler with_std false 
3563: methods fit fit_transform get_params deep set_params params transform copy fit kernelcenterer fit data transform get parameters estimator set parameters estimator center kernel __init__ __init__ initializes see help type signature fit fit kernelcenterer parameters numpy array shape n_samples n_samples kernel matrix 
3564: returns self returns instance self 
3565: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
3566: parameters numpy array shape n_samples n_features reference scikit learn user guide release training set 
3567: numpy array shape n_samples target values 
3568: returns x_new numpy array shape n_samples n_features_new transformed array 
3569: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
3570: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform copy true center kernel parameters numpy array shape n_samples1 n_samples2 kernel matrix 
3571: returns k_new numpy array shape n_samples1 n_samples2 preprocessing scale axis with_mean standardize dataset along axis preprocessing normalize norm axis copy normalize dataset along axis preprocessing binarize threshold copy boolean thresholding array like scipy sparse matrix sklearn preprocessing scale sklearn preprocessing scale axis with_mean true with_std true copy true standardize dataset along axis center mean component wise scale unit variance 
3572: parameters array like csr matrix 
3573: data center scale 
3574: axis int default axis used compute means standard deviations along standardize feature otherwise standardize sample 
3575: independently with_mean boolean true default chapter user guide scikit learn user guide release true center data scaling 
3576: with_std boolean true default true scale data unit variance equivalently unit standard deviation 
3577: copy boolean optional default true set false perform inplace row normalization avoid copy input already numpy array scipy sparse csr matrix axis 
3578: see also sklearn preprocessing scaler scaling sklearn pipeline pipeline notes implementation refuse center scipy sparse matrices since would make non sparse would potentially crash program memory exhaustion problems instead caller expected either set explicitly with_mean false case variance scaling performed features csr matrix call toarray expects materialized dense array memory avoid memory copy caller pass csr matrix 
3579: sklearn preprocessing normalize sklearn preprocessing normalize norm axis copy true normalize dataset along axis parameters array scipy sparse matrix shape n_samples n_features data normalize element element scipy sparse matrices csr format avoid necessary copy 
3580: norm optional default norm use normalize non zero sample non zero feature axis 
3581: axis optional default axis used normalize data along independently normalize sample oth erwise normalize feature copy boolean optional default true set false perform inplace row normalization avoid copy input already numpy array scipy sparse csr matrix axis 
3582: see also sklearn preprocessing normalizer using sklearn pipeline pipeline sklearn preprocessing binarize sklearn preprocessing binarize threshold copy true boolean thresholding array like scipy sparse matrix parameters array scipy sparse matrix shape n_samples n_features reference scikit learn user guide release data binarize element element scipy sparse matrices csr format avoid necessary copy 
3583: threshold oat optional default lower bound triggers feature values replaced 
3584: copy boolean optional default true set false perform inplace binarization avoid copy input already numpy array scipy sparse csr matrix axis 
3585: see also sklearn preprocessing binarizer using sklearn pipeline pipeline sklearn qda quadratic discriminant analysis quadratic discriminant analysis user guide see linear quadratic discriminant analysis section details 
3586: qda qda priors quadratic discriminant analysis qda sklearn qda qda class sklearn qda qda priors none quadratic discriminant analysis qda classier quadratic decision boundary generated tting class conditional densities data using bayes rule model gaussian density class 
3587: parameters priors array optional shape n_classes priors classes see also sklearn lda ldalinear discriminant analysis examples sklearn qda import qda import numpy array array clf qda clf fit qda priors none print clf predict chapter user guide scikit learn user guide release attributes means_ priors_ covariances_ array like shape n_classes n_features array like shape n_classes list array like shape n_features n_features covariance matrices class class means class priors sum methods decision_function fit store_covariances tol get_params deep predict predict_log_proba predict_proba score set_params params apply decision function array samples fit qda model according given training data parameters get parameters estimator perform classication array test vectors return posterior probabilities classication return posterior probabilities classication returns mean accuracy given test data labels set parameters estimator 
3588: __init__ priors none decision_function apply decision function array samples 
3589: parameters array like shape n_samples n_features array samples test vectors 
3590: returns array shape n_samples n_classes decision function values related class per sample 
3591: fit store_covariances false tol fit qda model according given training data parameters 
3592: parameters array like shape n_samples n_features training vector n_samples number samples n_features num ber features 
3593: array shape n_samples target values integers store_covariances boolean true covariance matrices computed stored self covariances_ tribute 
3594: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3595: predict perform classication array test vectors predicted class sample returned 
3596: reference scikit learn user guide release parameters array like shape n_samples n_features returns array shape n_samples predict_log_proba return posterior probabilities classication 
3597: parameters array like shape n_samples n_features array samples test vectors 
3598: returns array shape n_samples n_classes posterior log probabilities classication per class 
3599: predict_proba return posterior probabilities classication 
3600: parameters array like shape n_samples n_features array samples test vectors 
3601: returns array shape n_samples n_classes posterior probabilities classication per class 
3602: score returns mean accuracy given test data labels 
3603: parameters array like shape n_samples n_features training set 
3604: array like shape n_samples labels 
3605: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn svm support vector machines sklearn svm module includes support vector machine algorithms user guide see support vector machines section details 
3606: estimators svm svc kernel degree gamma coef0 svm linearsvc penalty loss dual tol svm nusvc kernel degree gamma svm svr kernel degree gamma coef0 tol svm nusvr kernel degree gamma support vector classication linear support vector classication support vector classication epsilon support vector regression support vector regression 
3607: continued next page chapter user guide scikit learn user guide release svm oneclasssvm kernel degree gamma unsupervised outliers detection 
3608: table continued previous page sklearn svm svc class sklearn svm svc kernel rbf degree gamma coef0 shrinking true proba bility false tol cache_size class_weight none verbose false support vector classication implementations based libsvm time complexity quadratic number samples makes hard scale dataset couple samples multiclass support handled according one one scheme details precise mathematical formulation provided kernel functions gamma coef0 degree affect see corresponding section narrative documentation kernel functions 
3609: parameters oat none optional default none penalty parameter error term none set n_samples 
3610: kernel string optional default rbf species kernel type used algorithm must one linear poly rbf sigmoid precomputed none given rbf used 
3611: degree int optional default degree kernel function signicant poly sigmoid 
3612: gamma oat optional default kernel coefcient rbf poly gamma n_features used instead 
3613: coef0 oat optional default independent term kernel function signicant poly sigmoid 
3614: probability boolean optional default false whether enable probability estimates must enabled prior calling pre dict_proba 
3615: shrinking boolean optional default true whether use shrinking heuristic 
3616: tol oat optional default tolerance stopping criterion 
3617: cache_size oat optional specify size kernel cache class_weight dict auto optional set parameter class class_weight svc given classes supposed weight one auto mode uses values automatically adjust weights inversely proportional class frequencies 
3618: verbose bool default false enable verbose output note setting takes advantage per process runtime setting libsvm enabled may work properly multithreaded context 
3619: reference scikit learn user guide release see also svrsupport vector machine regression implemented using libsvm linearsvcscalable linear support vector machine classifcation implemented using liblinear check see also section linearsvc comparison element 
3620: examples import numpy array array sklearn svm import svc clf svc clf fit svc cache_size class_weight none coef0 degree gamma kernel rbf probability false shrinking true tol verbose false print clf predict attributes index support vectors 
3621: support vectors 
3622: number support vector class 
3623: coefcients support vector decision function multiclass coefcient classiers layout coefcients multiclass case somewhat non trivial see section multi class classication svm section user guide details weights asigned features coefcients primal problem available case linear kernel coef_ readonly property derived dual_coef_ support_vectors_ constants decision function 
3624: sup port_ sup port_vectors_ array like shape n_sv array like shape n_sv n_features n_support_array like dtype int32 shape n_class dual_coef_array shape n_class n_sv coef_ inter cept_ array shape n_class n_features array shape n_class n_class methods decision_function fit class_weight sample_weight get_params deep predict predict_log_proba predict_proba distance samples separating hyperplane fit svm model according given training data get parameters estimator perform classication regression samples compute log likehoods possible outcomes samples compute likehoods possible outcomes samples 
3625: continued next page chapter user guide scikit learn user guide release score set_params params returns mean accuracy given test data labels set parameters estimator 
3626: table continued previous page __init__ kernel rbf degree gamma coef0 shrinking true probabil ity false tol cache_size class_weight none verbose false decision_function distance samples separating hyperplane 
3627: parameters array like shape n_samples n_features returns array like shape n_samples n_class n_class returns decision function sample class model 
3628: fit class_weight none sample_weight none fit svm model according given training data 
3629: parameters array like sparse matrix shape n_samples n_features training vectors n_samples number samples n_features num ber features 
3630: array like shape n_samples target values integers classication real numbers regression sample_weight array like shape n_samples optional weights applied individual samples unweighted 
3631: returns self object returns self 
3632: notes ordered contiguous arrays oat64 scipy sparse csr_matrix may copied dense array methods support sparse matrices input 
3633: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3634: predict perform classication regression samples classication model predicted class sample returned regression model function value calculated returned one class model returned 
3635: parameters array like sparse matrix shape n_samples n_features returns array shape n_samples reference scikit learn user guide release predict_log_proba compute log likehoods possible outcomes samples model need probability information computed training time attribute probability set true 
3636: parameters array like shape n_samples n_features returns array like shape n_samples n_classes returns log probabilities sample class model classes ordered arithmetical order 
3637: notes probability model created using cross validation results slightly different obtained predict also meaningless results small datasets 
3638: predict_proba compute likehoods possible outcomes samples model need probability information computed training time attribute probability set true 
3639: parameters array like shape n_samples n_features returns array like shape n_samples n_classes returns probability sample class model classes ordered arithmetical order 
3640: notes probability model created using cross validation results slightly different obtained predict also meaningless results small datasets 
3641: score returns mean accuracy given test data labels 
3642: parameters array like shape n_samples n_features training set 
3643: array like shape n_samples labels 
3644: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self chapter user guide sklearn svm linearsvc scikit learn user guide release class sklearn svm linearsvc penalty multi_class ovr class_weight none verbose loss dual true t_intercept true tol intercept_scaling linear support vector classication similar svc parameter kernel linear implemented terms liblinear rather libsvm exibility choice penalties loss functions scale better large numbers samples class supports dense sparse input multiclass support handled according one rest scheme 
3645: parameters oat none optional default none penalty parameter error term none set n_samples 
3646: loss string default species loss function squared hinge loss 
3647: hinge loss standard svm penalty string default species norm used penalization penalty standard used svc leads coef_ vectors sparse 
3648: dual bool default true select algorithm either solve dual primal optimization problem prefer dual false n_samples n_features 
3649: tol oat optional default tolerance stopping criteria multi_class string ovr crammer_singer default ovr determines multi class strategy contains two classes ovr trains n_classes one rest classiers crammer_singer optimizes joint objective classes crammer_singer interesting theoretical perspective consistent seldom used practice rarely leads better accuracy expensive compute crammer_singer choosen options loss penalty dual ignored 
3650: t_intercept boolean optional default true whether calculate intercept model set false intercept used calculations data expected already centered 
3651: intercept_scaling oat optional default self t_intercept true instance vector becomes self intercept_scaling synthetic feature constant value equals intercept_scaling appended instance vector intercept becomes intercept_scaling synthetic feature weight note synthetic feature weight subject regularization features lessen effect regularization synthetic feature weight therefore intercept intercept_scaling increased class_weight dict auto optional reference scikit learn user guide release set parameter class class_weight svc given classes supposed weight one auto mode uses values automatically adjust weights inversely proportional class frequencies 
3652: verbose int default enable verbose output note setting takes advantage per process runtime setting liblinear enabled may work properly multithreaded context 
3653: see also svcimplementation support vector machine classier using libsvm kernel non linear smo algorithm scale large number samples linearsvc furthermore svc multi class mode implemented using one one scheme linearsvc uses one rest possible implement one rest svc using sklearn multiclass onevsrestclassifier wrapper finally svc dense data without memory copy input contiguous sparse data still incur memory copy though 
3654: sklearn linear_model sgdclassifiersgdclassier optimize cost function lin earsvc adjusting penalty loss parameters furthermore sgdclassier scalable large number samples uses stochastic gradient descent optimizer finally sgdclassier dense sparse data without memory copy input contiguous csr 
3655: notes underlying implementation uses random number generator select features tting model thus uncommon slightly different results input data happens try smaller tol parameter underlying implementation liblinear uses sparse internal representation data incur memory copy references liblinear library large linear classication attributes coef_ array shape n_features n_classes else n_classes n_features ter cept_ array shape n_classes else n_classes methods weights asigned features coefcients primal problem available case linear kernel coef_ readonly property derived raw_coef_ follows internal memory layout liblinear constants decision function 
3656: decision_function decision function value according trained model fit class_weight fit_transform get_params deep predict score fit model according given training data fit data transform get parameters estimator predict target values according tted model returns mean accuracy given test data labels continued next page chapter user guide scikit learn user guide release table continued previous page set_params params transform threshold reduce important features 
3657: set parameters estimator 
3658: __init__ penalty loss t_intercept true intercept_scaling class_weight none verbose dual true tol multi_class ovr decision_function decision function value according trained model 
3659: parameters array like shape n_samples n_features returns array like shape n_samples n_class returns decision function sample class model 
3660: fit class_weight none fit model according given training data 
3661: parameters array like sparse matrix shape n_samples n_features training vector n_samples number samples n_features num ber features 
3662: array like shape n_samples target vector relative class_weight dict auto optional weights associated classes given classes supposed weight one 
3663: returns self object returns self 
3664: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
3665: parameters numpy array shape n_samples n_features training set 
3666: numpy array shape n_samples target values 
3667: returns x_new numpy array shape n_samples n_features_new transformed array 
3668: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
3669: get_params deep true get parameters estimator parameters deep boolean optional reference scikit learn user guide release true return parameters estimator contained subobjects estimators 
3670: predict predict target values according tted model 
3671: parameters array like sparse matrix shape n_samples n_features returns array shape n_samples score returns mean accuracy given test data labels 
3672: parameters array like shape n_samples n_features training set 
3673: array like shape n_samples labels 
3674: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform threshold none reduce important features 
3675: parameters array scipy sparse matrix shape n_samples n_features input samples 
3676: threshold string oat none optional default none threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor mean may also used none available object attribute threshold used otherwise mean used default 
3677: returns x_r array shape n_samples n_selected_features input samples selected features 
3678: sklearn svm nusvc class sklearn svm nusvc kernel rbf degree gamma coef0 shrinking true probability false tol cache_size verbose false support vector classication similar svc uses parameter control number support vectors implementation based libsvm 
3679: parameters oat optional default upper bound fraction training errors lower bound fraction support vectors interval 
3680: chapter user guide scikit learn user guide release kernel string optional default rbf species kernel type used algorithm one linear poly rbf sigmoid precomputed none given rbf used 
3681: degree int optional default degree kernel function signicant poly rbf sigmoid gamma oat optional default kernel coefcient rbf poly gamma n_features taken 
3682: coef0 oat optional default independent term kernel function signicant poly sigmoid 
3683: probability boolean optional default false whether enable probability estimates must enabled prior calling pre dict_proba 
3684: shrinking boolean optional default true whether use shrinking heuristic 
3685: tol oat optional default tolerance stopping criterion 
3686: cache_size oat optional specify size kernel cache class_weight dict auto optional set parameter class class_weight svc given classes supposed weight one auto mode uses values automatically adjust weights inversely proportional class frequencies 
3687: verbose bool default false enable verbose output note setting takes advantage per process runtime setting libsvm enabled may work properly multithreaded context 
3688: see also svcsupport vector machine classication using libsvm linearsvcscalable linear support vector machine classication using liblinear 
3689: examples import numpy array array sklearn svm import nusvc clf nusvc clf fit nusvc cache_size coef0 degree gamma kernel rbf probability false shrinking true tol verbose false print clf predict reference scikit learn user guide release attributes index support vectors 
3690: support vectors 
3691: number support vector class 
3692: coefcients support vector decision function multiclass coefcient classiers layout coefcients multiclass case somewhat non trivial see section multi class classication svm section user guide details weights asigned features coefcients primal problem available case linear kernel coef_ readonly property derived dual_coef_ support_vectors_ constants decision function 
3693: sup port_ sup port_vectors_ array like shape n_sv array like shape n_sv n_features n_support_array like dtype int32 shape n_class dual_coef_array shape n_class n_sv coef_ inter cept_ array shape n_class n_features array shape n_class n_class methods decision_function fit class_weight sample_weight get_params deep predict predict_log_proba predict_proba score set_params params distance samples separating hyperplane fit svm model according given training data get parameters estimator perform classication regression samples compute log likehoods possible outcomes samples compute likehoods possible outcomes samples returns mean accuracy given test data labels set parameters estimator 
3694: __init__ kernel rbf degree gamma coef0 shrinking true probabil ity false tol cache_size verbose false decision_function distance samples separating hyperplane 
3695: parameters array like shape n_samples n_features returns array like shape n_samples n_class n_class returns decision function sample class model 
3696: fit class_weight none sample_weight none fit svm model according given training data 
3697: parameters array like sparse matrix shape n_samples n_features training vectors n_samples number samples n_features num ber features 
3698: array like shape n_samples target values integers classication real numbers regression chapter user guide scikit learn user guide release sample_weight array like shape n_samples optional weights applied individual samples unweighted 
3699: returns self object returns self 
3700: notes ordered contiguous arrays oat64 scipy sparse csr_matrix may copied dense array methods support sparse matrices input 
3701: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3702: predict perform classication regression samples classication model predicted class sample returned regression model function value calculated returned one class model returned 
3703: parameters array like sparse matrix shape n_samples n_features returns array shape n_samples predict_log_proba compute log likehoods possible outcomes samples model need probability information computed training time attribute probability set true 
3704: parameters array like shape n_samples n_features returns array like shape n_samples n_classes returns log probabilities sample class model classes ordered arithmetical order 
3705: notes probability model created using cross validation results slightly different obtained predict also meaningless results small datasets 
3706: predict_proba compute likehoods possible outcomes samples model need probability information computed training time attribute probability set true 
3707: parameters array like shape n_samples n_features returns array like shape n_samples n_classes reference scikit learn user guide release returns probability sample class model classes ordered arithmetical order 
3708: notes probability model created using cross validation results slightly different obtained predict also meaningless results small datasets 
3709: score returns mean accuracy given test data labels 
3710: parameters array like shape n_samples n_features training set 
3711: array like shape n_samples labels 
3712: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn svm svr class sklearn svm svr kernel rbf degree gamma coef0 tol epsilon shrinking true probability false cache_size verbose false epsilon support vector regression free parameters model epsilon implementations based libsvm 
3713: parameters oat none optional default none penalty parameter error term none set n_samples 
3714: epsilon oat optional default epsilon epsilon svr model species epsilon tube within penalty associated training loss function points predicted within distance epsilon actual value 
3715: kernel string optional default rbf species kernel type used algorithm one linear poly rbf sigmoid precomputed none given rbf used 
3716: degree int optional default degree kernel function signicant poly rbf sigmoid gamma oat optional default kernel coefcient rbf poly gamma n_features taken 
3717: chapter user guide scikit learn user guide release coef0 oat optional default independent term kernel function signicant poly sigmoid 
3718: probability boolean optional default false whether enable probability estimates must enabled prior calling pre dict_proba 
3719: shrinking boolean optional default true whether use shrinking heuristic 
3720: tol oat optional default tolerance stopping criterion 
3721: cache_size oat optional specify size kernel cache verbose bool default false enable verbose output note setting takes advantage per process runtime setting libsvm enabled may work properly multithreaded context 
3722: see also nusvrsupport vector machine regression implemented using libsvm using parameter control num ber support vectors 
3723: examples sklearn svm import svr import numpy n_samples n_features random seed random randn n_samples random randn n_samples n_features clf svr epsilon clf fit svr cache_size coef0 degree epsilon gamma kernel rbf probability false shrinking true tol verbose false reference scikit learn user guide release attributes sup port_ sup port_vectors_ array like shape n_sv array like shape nsv n_features index support vectors 
3724: support vectors 
3725: coef_ dual_coef_array shape n_classes n_sv array shape n_classes n_features array shape n_class n_class inter cept_ coefcients support vector decision function 
3726: weights asigned features coefcients primal problem available case linear kernel coef_ readonly property derived dual_coef_ support_vectors_ constants decision function 
3727: methods decision_function fit class_weight sample_weight get_params deep predict predict_log_proba predict_proba score set_params params distance samples separating hyperplane fit svm model according given training data get parameters estimator perform classication regression samples compute log likehoods possible outcomes samples compute likehoods possible outcomes samples returns coefcient determination prediction set parameters estimator 
3728: __init__ kernel rbf degree gamma coef0 tol epsilon shrink ing true probability false cache_size verbose false decision_function distance samples separating hyperplane 
3729: parameters array like shape n_samples n_features returns array like shape n_samples n_class n_class returns decision function sample class model 
3730: fit class_weight none sample_weight none fit svm model according given training data 
3731: parameters array like sparse matrix shape n_samples n_features training vectors n_samples number samples n_features num ber features 
3732: array like shape n_samples target values integers classication real numbers regression sample_weight array like shape n_samples optional weights applied individual samples unweighted 
3733: returns self object chapter user guide scikit learn user guide release returns self 
3734: notes ordered contiguous arrays oat64 scipy sparse csr_matrix may copied dense array methods support sparse matrices input 
3735: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3736: predict perform classication regression samples classication model predicted class sample returned regression model function value calculated returned one class model returned 
3737: parameters array like sparse matrix shape n_samples n_features returns array shape n_samples predict_log_proba compute log likehoods possible outcomes samples model need probability information computed training time attribute probability set true 
3738: parameters array like shape n_samples n_features returns array like shape n_samples n_classes returns log probabilities sample class model classes ordered arithmetical order 
3739: notes probability model created using cross validation results slightly different obtained predict also meaningless results small datasets 
3740: predict_proba compute likehoods possible outcomes samples model need probability information computed training time attribute probability set true 
3741: parameters array like shape n_samples n_features returns array like shape n_samples n_classes returns probability sample class model classes ordered arithmetical order 
3742: reference scikit learn user guide release notes probability model created using cross validation results slightly different obtained predict also meaningless results small datasets 
3743: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
3744: parameters array like shape n_samples n_features training set 
3745: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn svm nusvr class sklearn svm nusvr kernel rbf degree gamma coef0 shrink ing true probability false tol cache_size verbose false support vector regression similar nusvc regression uses parameter control number support vectors however unlike nusvc replaces replaces parameter epsilon svr implementations based libsvm 
3746: parameters oat none optional default none penalty parameter error term none set n_samples 
3747: oat optional upper bound fraction training errors lower bound fraction support vectors interval default taken available impl nu_svc 
3748: kernel string optional default rbf species kernel type used algorithm one linear poly rbf sigmoid precomputed none given rbf used 
3749: degree int optional default degree kernel function signicant poly rbf sigmoid gamma oat optional default kernel coefcient rbf poly gamma n_features taken 
3750: coef0 oat optional default chapter user guide scikit learn user guide release independent term kernel function signicant poly sigmoid 
3751: probability boolean optional default false whether enable probability estimates must enabled prior calling pre dict_proba 
3752: shrinking boolean optional default true whether use shrinking heuristic 
3753: tol oat optional default tolerance stopping criterion 
3754: cache_size oat optional specify size kernel cache verbose bool default false enable verbose output note setting takes advantage per process runtime setting libsvm enabled may work properly multithreaded context 
3755: see also nusvcsupport vector machine classication implemented libsvm parameter control number support vectors 
3756: svrepsilon support vector machine regression implemented libsvm 
3757: examples sklearn svm import nusvr import numpy n_samples n_features random seed random randn n_samples random randn n_samples n_features clf nusvr clf fit nusvr cache_size coef0 degree gamma kernel rbf probability false shrinking true tol verbose false reference scikit learn user guide release attributes sup port_ sup port_vectors_ array like shape n_sv array like shape nsv n_features index support vectors 
3758: support vectors 
3759: coef_ dual_coef_array shape n_classes n_sv array shape n_classes n_features array shape n_class n_class inter cept_ coefcients support vector decision function 
3760: weights asigned features coefcients primal problem available case linear kernel coef_ readonly property derived dual_coef_ support_vectors_ constants decision function 
3761: methods decision_function fit class_weight sample_weight get_params deep predict predict_log_proba predict_proba score set_params params distance samples separating hyperplane fit svm model according given training data get parameters estimator perform classication regression samples compute log likehoods possible outcomes samples compute likehoods possible outcomes samples returns coefcient determination prediction set parameters estimator 
3762: __init__ kernel rbf degree gamma coef0 shrinking true probabil ity false tol cache_size verbose false decision_function distance samples separating hyperplane 
3763: parameters array like shape n_samples n_features returns array like shape n_samples n_class n_class returns decision function sample class model 
3764: fit class_weight none sample_weight none fit svm model according given training data 
3765: parameters array like sparse matrix shape n_samples n_features training vectors n_samples number samples n_features num ber features 
3766: array like shape n_samples target values integers classication real numbers regression sample_weight array like shape n_samples optional weights applied individual samples unweighted 
3767: returns self object chapter user guide scikit learn user guide release returns self 
3768: notes ordered contiguous arrays oat64 scipy sparse csr_matrix may copied dense array methods support sparse matrices input 
3769: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3770: predict perform classication regression samples classication model predicted class sample returned regression model function value calculated returned one class model returned 
3771: parameters array like sparse matrix shape n_samples n_features returns array shape n_samples predict_log_proba compute log likehoods possible outcomes samples model need probability information computed training time attribute probability set true 
3772: parameters array like shape n_samples n_features returns array like shape n_samples n_classes returns log probabilities sample class model classes ordered arithmetical order 
3773: notes probability model created using cross validation results slightly different obtained predict also meaningless results small datasets 
3774: predict_proba compute likehoods possible outcomes samples model need probability information computed training time attribute probability set true 
3775: parameters array like shape n_samples n_features returns array like shape n_samples n_classes returns probability sample class model classes ordered arithmetical order 
3776: reference scikit learn user guide release notes probability model created using cross validation results slightly different obtained predict also meaningless results small datasets 
3777: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
3778: parameters array like shape n_samples n_features training set 
3779: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self sklearn svm oneclasssvm class sklearn svm oneclasssvm kernel rbf degree gamma coef0 tol shrinking true cache_size verbose false unsupervised outliers detection estimate support high dimensional distribution implementation based libsvm 
3780: parameters kernel string optional species kernel type used algorithm one linear poly rbf sigmoid precomputed none given rbf used 
3781: oat optional upper bound fraction training errors lower bound fraction support vectors interval default taken 
3782: degree int optional degree kernel function signicant poly rbf sigmoid 
3783: gamma oat optional default kernel coefcient rbf poly gamma n_features taken 
3784: coef0 oat optional independent term kernel function signicant poly sigmoid 
3785: tol oat optional tolerance stopping criterion 
3786: chapter user guide scikit learn user guide release shrinking boolean optional whether use shrinking heuristic 
3787: cache_size oat optional specify size kernel cache verbose bool default false enable verbose output note setting takes advantage per process runtime setting libsvm enabled may work properly multithreaded context 
3788: attributes sup port_ sup port_vectors_ array like shape n_sv array like shape nsv n_features index support vectors 
3789: support vectors 
3790: coef_ dual_coef_array shape n_classes n_sv array shape n_classes n_features array shape n_classes inter cept_ coefcient support vector decision function 
3791: weights asigned features coefcients primal problem available case linear kernel coef_ readonly property derived dual_coef_ support_vectors_ constants decision function 
3792: methods decision_function distance samples separating hyperplane fit sample_weight get_params deep predict predict_log_proba compute log likehoods possible outcomes samples predict_proba set_params params detects soft boundary set samples get parameters estimator perform classication regression samples 
3793: compute likehoods possible outcomes samples set parameters estimator 
3794: __init__ kernel rbf degree gamma coef0 cache_size verbose false decision_function distance samples separating hyperplane 
3795: tol shrinking true parameters array like shape n_samples n_features returns array like shape n_samples n_class n_class returns decision function sample class model 
3796: fit sample_weight none params detects soft boundary set samples 
3797: parameters array like sparse matrix shape n_samples n_features reference scikit learn user guide release set samples n_samples number samples n_features number features 
3798: returns self object returns self 
3799: notes ordered contiguous array copied 
3800: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3801: predict perform classication regression samples classication model predicted class sample returned regression model function value calculated returned one class model returned 
3802: parameters array like sparse matrix shape n_samples n_features returns array shape n_samples predict_log_proba compute log likehoods possible outcomes samples model need probability information computed training time attribute probability set true 
3803: parameters array like shape n_samples n_features returns array like shape n_samples n_classes returns log probabilities sample class model classes ordered arithmetical order 
3804: notes probability model created using cross validation results slightly different obtained predict also meaningless results small datasets 
3805: predict_proba compute likehoods possible outcomes samples model need probability information computed training time attribute probability set true 
3806: parameters array like shape n_samples n_features returns array like shape n_samples n_classes returns probability sample class model classes ordered arithmetical order 
3807: chapter user guide scikit learn user guide release notes probability model created using cross validation results slightly different obtained predict also meaningless results small datasets 
3808: set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self svm l1_min_c loss t_intercept return lowest bound l1_min_c innity sklearn svm l1_min_c sklearn svm l1_min_c loss t_intercept true intercept_scaling return lowest bound l1_min_c empty ear_model logisticregression penalty value valid class_weight parameter set 
3809: guaranteed applies penalized classiers linearsvc penalty lin innity model parameters array like sparse matrix shape n_samples n_features training vector n_samples number samples n_features num ber features 
3810: array shape n_samples target vector relative loss log default species loss function loss squared hinge loss log loss logistic regression models 
3811: t_intercept bool default true species intercept tted model must match method paramenter 
3812: intercept_scaling oat default t_intercept true instance vector becomes intercept_scaling syn thetic feature constant value equals intercept_scaling appended stance vector must match method parameter 
3813: returns l1_min_c oat minimum value low level methods svm libsvm fit svm libsvm decision_function train model using libsvm low level method predict margin libsvm name predict_values reference continued next page scikit learn user guide release svm libsvm predict svm libsvm predict_proba svm libsvm cross_validation predict target values given model low level method predict probabilities svm_model stores parameters needed predict given value binding cross validation routine low level routine table continued previous page sklearn svm libsvm sklearn svm libsvm fit train model using libsvm low level method parameters array like dtype oat64 size n_samples n_features array dtype oat64 size n_samples target vector svm_type type svm c_svc nusvc oneclasssvm epsilonsvr nusvr respectevely 
3814: kernel linear rbf poly sigmoid precomputed kernel use model linear polynomial rbf sigmoid precomputed 
3815: degree int32 degree polynomial kernel relevant kernel set polynomial gamma oat64 gamma parameter rbf kernel relevant kernel set rbf coef0 oat64 independent parameter poly sigmoid kernel 
3816: tol oat64 stopping criteria 
3817: oat64 parameter support vector classication oat64 cache_size oat64 returns support array shape n_support index support vectors support_vectors array shape n_support n_features support vectors equivalent support return empty array case precomputed kernel 
3818: n_class_sv array number support vectors class 
3819: sv_coef array coefcients support vectors decision function 
3820: intercept array chapter user guide scikit learn user guide release intercept decision function label labels different classes relevant classication proba probb array probability estimates empty array probability false sklearn svm libsvm decision_function sklearn svm libsvm decision_function predict margin libsvm name predict_values reconstruct model parameters make sure stay sync python object 
3821: sklearn svm libsvm predict sklearn svm libsvm predict predict target values given model low level method parameters array like dtype oat size n_samples n_features svm_type type svm svc svc one class epsilon svr svr kernel linear rbf poly sigmoid precomputed kernel use model linear polynomial rbf sigmoid precomputed 
3822: degree int degree polynomial kernel relevant kernel set polynomial gamma oat gamma parameter rbf kernel relevant kernel set rbf coef0 oat independent parameter poly sigmoid kernel 
3823: eps oat stopping criteria 
3824: oat parameter support vector classication returns dec_values array predicted values 
3825: todo probably theres point setting parameters like cache_size weights reference scikit learn user guide release sklearn svm libsvm predict_proba sklearn svm libsvm predict_proba predict probabilities svm_model stores parameters needed predict given value speed real work done level function copy_predict libsvm_helper reconstruct model parameters make sure stay sync python object see sklearn svm predict complete list parameters 
3826: parameters array like dtype oat array target vector kernel linear rbf poly sigmoid precomputed returns dec_values array predicted values 
3827: sklearn svm libsvm cross_validation sklearn svm libsvm cross_validation binding cross validation routine low level routine parameters array like dtype oat size n_samples n_features array dtype oat size n_samples target vector svm_type type svm svc svc one class epsilon svr svr kernel linear rbf poly sigmoid precomputed kernel use model linear polynomial rbf sigmoid precomputed 
3828: degree int degree polynomial kernel relevant kernel set polynomial gamma oat gamma parameter rbf kernel relevant kernel set rbf coef0 oat independent parameter poly sigmoid kernel 
3829: tol oat stopping criteria 
3830: oat parameter support vector classication oat cache_size oat chapter user guide scikit learn user guide release returns target array oat sklearn tree decision trees sklearn tree module includes decision tree based models classication regression user guide see decision trees section details 
3831: tree decisiontreeclassifier criterion decision tree classier tree decisiontreeregressor criterion tree extratreeclassifier criterion tree extratreeregressor criterion tree regressor extremely randomized tree classier extremely randomized tree regressor 
3832: sklearn tree decisiontreeclassier class sklearn tree decisiontreeclassifier criterion gini min_samples_split min_density pute_importances false random_state none max_features none max_depth none min_samples_leaf com decision tree classier 
3833: parameters criterion string optional default gini function measure quality split supported criteria gini gini impurity entropy information gain 
3834: max_depth integer none optional default none maximum depth tree none nodes expanded leaves pure leaves contain less min_samples_split samples 
3835: min_samples_split integer optional default minimum number samples required split internal node 
3836: min_samples_leaf integer optional default minimum number samples required leaf node 
3837: min_density oat optional default parameter controls trade optimization heuristic controls minimum density sample_mask fraction samples mask density falls threshold mask recomputed input data packed results min_density equals one partitions always represented data copying copies original data otherwise partitions represented bit masks aka sample masks 
3838: max_features int string none optional default none number features consider looking best split auto max_features sqrt n_features classication tasks max_features n_features regression problems log2 max_features log2 n_features none max_features n_features 
3839: sqrt max_features sqrt n_features 
3840: compute_importances boolean optional default true whether computed feature_importances_ attribute calling 
3841: importances feature stored reference scikit learn user guide release random_state int randomstate instance none optional default none int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used random 
3842: see also decisiontreeregressor references r76 r77 r78 r79 examples sklearn datasets import load_iris sklearn cross_validation import cross_val_score sklearn tree import decisiontreeclassifier clf decisiontreeclassifier random_state iris load_iris cross_val_score clf iris data iris target array 
3843:  
3844:  
3845: attributes tree_ fea ture_importances_ tree object array shape n_features underlying tree object feature mportances higher important feature importance feature computed normalized total reduction error brought feature also known gini importance r79 
3846: methods fit sample_mask x_argsorted build decision tree training set fit_transform get_params deep predict predict_log_proba predict_proba score set_params params transform threshold fit data transform get parameters estimator predict class regression target predict class log probabilities input samples predict class probabilities input samples returns mean accuracy given test data labels set parameters estimator reduce important features 
3847: __init__ criterion gini min_samples_leaf min_density max_features none compute_importances false random_state none min_samples_split max_depth none chapter user guide scikit learn user guide release fit sample_mask none x_argsorted none build decision tree training set 
3848: parameters array like shape n_samples n_features training input samples 
3849: array like shape n_samples target values integers correspond classes classication real numbers regression 
3850: returns self object returns self 
3851: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
3852: parameters numpy array shape n_samples n_features training set 
3853: numpy array shape n_samples target values 
3854: returns x_new numpy array shape n_samples n_features_new transformed array 
3855: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
3856: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3857: predict predict class regression target classication model predicted class sample returned regression model predicted value based returned 
3858: parameters array like shape n_samples n_features input samples 
3859: returns array shape n_samples predicted classes predict values 
3860: predict_log_proba predict class log probabilities input samples 
3861: parameters array like shape n_samples n_features input samples 
3862: reference scikit learn user guide release returns array shape n_samples n_classes class log probabilities input samples classes ordered arithmetical order predict_proba predict class probabilities input samples 
3863: parameters array like shape n_samples n_features input samples 
3864: returns array shape n_samples n_classes class probabilities input samples classes ordered arithmetical order 
3865: score returns mean accuracy given test data labels 
3866: parameters array like shape n_samples n_features training set 
3867: array like shape n_samples labels 
3868: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform threshold none reduce important features 
3869: parameters array scipy sparse matrix shape n_samples n_features input samples 
3870: threshold string oat none optional default none threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor mean may also used none available object attribute threshold used otherwise mean used default 
3871: returns x_r array shape n_samples n_selected_features input samples selected features 
3872: sklearn tree decisiontreeregressor class sklearn tree decisiontreeregressor criterion mse tree regressor 
3873: min_samples_split min_density pute_importances false random_state none max_features none max_depth none min_samples_leaf com chapter user guide scikit learn user guide release parameters criterion string optional default mse function measure quality split supported criterion mse mean squared error 
3874: max_depth integer none optional default none maximum depth tree none nodes expanded leaves pure leaves contain less min_samples_split samples 
3875: min_samples_split integer optional default minimum number samples required split internal node 
3876: min_samples_leaf integer optional default minimum number samples required leaf node 
3877: min_density oat optional default parameter controls trade optimization heuristic controls minimum density sample_mask fraction samples mask density falls threshold mask recomputed input data packed results min_density equals one partitions always represented data copying copies original data otherwise partitions represented bit masks aka sample masks 
3878: max_features int string none optional default none number features consider looking best split auto max_features sqrt n_features classication tasks max_features n_features regression problems log2 max_features log2 n_features none max_features n_features 
3879: sqrt max_features sqrt n_features 
3880: compute_importances boolean optional default true whether computed feature_importances_ attribute calling 
3881: importances feature stored random_state int randomstate instance none optional default none int random_state seed used random number generator randomstate instance random_state random number generator none random number generator randomstate instance used random 
3882: see also decisiontreeclassifier references r80 r81 r82 r83 examples sklearn datasets import load_boston sklearn cross_validation import cross_val_score sklearn tree import decisiontreeregressor boston load_boston regressor decisiontreeregressor random_state reference scikit learn user guide release scores coefcient determination folds cross_val_score regressor boston data boston target array attributes tree_ fea ture_importances_ tree object array shape n_features underlying tree object feature mportances higher important feature importance feature computed normalized total reduction error brought feature also known gini importance r83 
3883: methods fit sample_mask x_argsorted build decision tree training set fit_transform get_params deep predict score set_params params transform threshold fit data transform get parameters estimator predict class regression target returns coefcient determination prediction set parameters estimator reduce important features 
3884: __init__ criterion mse max_depth none min_samples_leaf min_density max_features none compute_importances false random_state none min_samples_split fit sample_mask none x_argsorted none build decision tree training set 
3885: parameters array like shape n_samples n_features training input samples 
3886: array like shape n_samples target values integers correspond classes classication real numbers regression 
3887: returns self object returns self 
3888: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
3889: parameters numpy array shape n_samples n_features training set 
3890: numpy array shape n_samples target values 
3891: returns x_new numpy array shape n_samples n_features_new chapter user guide scikit learn user guide release transformed array 
3892: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
3893: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3894: predict predict class regression target classication model predicted class sample returned regression model predicted value based returned 
3895: parameters array like shape n_samples n_features input samples 
3896: returns array shape n_samples predicted classes predict values 
3897: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
3898: parameters array like shape n_samples n_features training set 
3899: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform threshold none reduce important features 
3900: parameters array scipy sparse matrix shape n_samples n_features input samples 
3901: threshold string oat none optional default none reference scikit learn user guide release threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor mean may also used none available object attribute threshold used otherwise mean used default 
3902: returns x_r array shape n_samples n_selected_features input samples selected features 
3903: sklearn tree extratreeclassier class sklearn tree extratreeclassifier criterion gini min_samples_split min_density pute_importances false random_state none max_features auto max_depth none min_samples_leaf com extremely randomized tree classier extra trees differ classic decision trees way built looking best split separate samples node two groups random splits drawn max_features randomly selected features best split among chosen max_features set amounts building totally random decision tree warning extra trees used within ensemble methods see also extratreeregressor extratreesclassifier extratreesregressor references r84 methods fit sample_mask x_argsorted build decision tree training set fit_transform get_params deep predict predict_log_proba predict_proba score set_params params transform threshold fit data transform get parameters estimator predict class regression target predict class log probabilities input samples predict class probabilities input samples returns mean accuracy given test data labels set parameters estimator reduce important features 
3904: __init__ criterion gini min_density dom_state none max_depth none min_samples_split max_features auto compute_importances false min_samples_leaf ran fit sample_mask none x_argsorted none build decision tree training set 
3905: parameters array like shape n_samples n_features training input samples 
3906: chapter user guide scikit learn user guide release array like shape n_samples target values integers correspond classes classication real numbers regression 
3907: returns self object returns self 
3908: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
3909: parameters numpy array shape n_samples n_features training set 
3910: numpy array shape n_samples target values 
3911: returns x_new numpy array shape n_samples n_features_new transformed array 
3912: notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
3913: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3914: predict predict class regression target classication model predicted class sample returned regression model predicted value based returned 
3915: parameters array like shape n_samples n_features input samples 
3916: returns array shape n_samples predicted classes predict values 
3917: predict_log_proba predict class log probabilities input samples 
3918: parameters array like shape n_samples n_features input samples 
3919: returns array shape n_samples n_classes class log probabilities input samples classes ordered arithmetical order 
3920: reference scikit learn user guide release predict_proba predict class probabilities input samples 
3921: parameters array like shape n_samples n_features input samples 
3922: returns array shape n_samples n_classes class probabilities input samples classes ordered arithmetical order 
3923: score returns mean accuracy given test data labels 
3924: parameters array like shape n_samples n_features training set 
3925: array like shape n_samples labels 
3926: returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform threshold none reduce important features 
3927: parameters array scipy sparse matrix shape n_samples n_features input samples 
3928: threshold string oat none optional default none threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor mean may also used none available object attribute threshold used otherwise mean used default 
3929: returns x_r array shape n_samples n_selected_features input samples selected features 
3930: sklearn tree extratreeregressor class sklearn tree extratreeregressor criterion mse min_samples_split min_density pute_importances false random_state none max_features auto max_depth none min_samples_leaf com extremely randomized tree regressor extra trees differ classic decision trees way built looking best split separate samples node two groups random splits drawn max_features randomly selected features best split among chosen max_features set amounts building totally random decision tree 
3931: chapter user guide scikit learn user guide release warning extra trees used within ensemble methods see also extratreeclassifiera classier base extremely randomized trees sklearn ensemble extratreesclassifieran ensemble extra trees classication sklearn ensemble extratreesregressoran ensemble extra trees regression references r85 methods fit sample_mask x_argsorted build decision tree training set fit_transform get_params deep predict score set_params params transform threshold fit data transform get parameters estimator predict class regression target returns coefcient determination prediction set parameters estimator reduce important features 
3932: __init__ criterion mse min_density dom_state none max_depth none min_samples_split min_samples_leaf ran max_features auto compute_importances false fit sample_mask none x_argsorted none build decision tree training set 
3933: parameters array like shape n_samples n_features training input samples 
3934: array like shape n_samples target values integers correspond classes classication real numbers regression 
3935: returns self object returns self 
3936: fit_transform none t_params fit data transform fits transformer optional parameters t_params returns transformed version 
3937: parameters numpy array shape n_samples n_features training set 
3938: numpy array shape n_samples target values 
3939: returns x_new numpy array shape n_samples n_features_new transformed array 
3940: reference scikit learn user guide release notes method calls transform consecutively optimized implementation t_transform unlike transformers pca 
3941: get_params deep true get parameters estimator parameters deep boolean optional true return parameters estimator contained subobjects estimators 
3942: predict predict class regression target classication model predicted class sample returned regression model predicted value based returned 
3943: parameters array like shape n_samples n_features input samples 
3944: returns array shape n_samples predicted classes predict values 
3945: score returns coefcient determination prediction coefcient dened regression sum squares y_pred sum residual sum squares y_true y_true mean sum best possible score lower values worse 
3946: parameters array like shape n_samples n_features training set 
3947: array like shape n_samples returns oat set_params params set parameters estimator method works simple estimators well nested objects pipelines former parameters form component parameter possible update component nested object returns self transform threshold none reduce important features 
3948: parameters array scipy sparse matrix shape n_samples n_features input samples 
3949: threshold string oat none optional default none threshold value use feature selection features whose importance greater equal kept others discarded median resp mean threshold value median resp mean feature importances scaling factor mean may also used none available object attribute threshold used otherwise mean used default 
3950: chapter user guide scikit learn user guide release returns x_r array shape n_samples n_selected_features input samples selected features 
3951: tree export_graphviz decision_tree export decision tree dot format 
3952: sklearn tree export_graphviz sklearn tree export_graphviz decision_tree out_le none feature_names none export decision tree dot format function generates graphviz representation decision tree written out_le exported graphical renderings generated using example dot tps tree dot tree dot tpng tree dot tree png postscript format png format parameters decision_tree decision tree classier decision tree exported graphviz object string optional default none handle name output 
3953: feature_names list strings optional default none names features 
3954: returns out_le object object tree exported user expected close object done 
3955: examples sklearn datasets import load_iris sklearn import tree clf tree decisiontreeclassifier iris load_iris clf clf fit iris data iris target import tempfile out_file tree export_graphviz clf out_file tempfile temporaryfile out_file close sklearn utils utilities sklearn utils module includes various utilites developer guide see utilities developers page details 
3956: utils check_random_state seed turn seed random randomstate instance utils resample arrays options utils shuffle arrays options resample arrays sparse matrices consistent way shufe arrays sparse matrices consistent way reference scikit learn user guide release sklearn utils check_random_state sklearn utils check_random_state seed turn seed random randomstate instance seed none return randomstate singleton used random seed int return new ran domstate instance seeded seed seed already randomstate instance return otherwise raise valueerror 
3957: sklearn utils resample sklearn utils resample arrays options resample arrays sparse matrices consistent way default strategy implements one step bootstrapping procedure 
3958: parameters arrays sequence arrays scipy sparse matrices shape replace boolean true default implements resampling replacement false implement sliced random permutations 
3959: n_samples int none default number samples generate dimension arrays 
3960: left none automatically set rst random_state int randomstate instance control shufing reproducible behavior 
3961: returns sequence resampled views collections original arrays impacted see also sklearn cross_validation bootstrap sklearn utils shuffle examples possible mix sparse dense arrays run array scipy sparse import coo_matrix x_sparse coo_matrix sklearn utils import resample x_sparse resample x_sparse random_state array x_sparse 3x2 sparse matrix type type numpy float64 stored elements compressed sparse row format chapter user guide scikit learn user guide release x_sparse toarray array array resample n_samples random_state array sklearn utils shufe sklearn utils shuffle arrays options shufe arrays sparse matrices consistent way convenience alias resample arrays replace false random permutations collections 
3962: parameters arrays sequence arrays scipy sparse matrices shape random_state int randomstate instance control shufing reproducible behavior 
3963: n_samples int none default number samples generate dimension arrays 
3964: left none automatically set rst returns sequence shufed views collections original arrays impacted see also sklearn utils resample examples possible mix sparse dense arrays run array scipy sparse import coo_matrix x_sparse coo_matrix sklearn utils import shuffle x_sparse shuffle x_sparse random_state array x_sparse 3x2 sparse matrix type type numpy float64 stored elements compressed sparse row format reference scikit learn user guide release x_sparse toarray array array shuffle n_samples random_state array chapter user guide chapter two example gallery examples general examples general purpose introductory examples scikit 
3965: figure plot classication probability plot classication probability plot classication probability different classiers use class dataset classify support vector classier well penalized logistic regression logistic regression multiclass classier box result identify rst class 
3966: scikit learn user guide release script output classif_rate linear svc classif_rate logistic classif_rate logistic python source code plot_classification_probability print __doc__ author alexandre gramfort alexandre gramfort inria license bsd style 
3967: import pylab import numpy sklearn linear_model import logisticregression sklearn svm import svc sklearn import datasets chapter example gallery scikit learn user guide release iris datasets load_iris iris data take first two features visualization iris target n_features shape create different classifiers logistic regression cannot multiclass box classifiers logistic logisticregression penalty logistic logisticregression penalty linear svc svc kernel linear probability true n_classifiers len classifiers figure figsize n_classifiers subplots_adjust bottom top index name classifier enumerate classifiers iteritems classifier fit y_pred classifier predict classif_rate mean y_pred ravel ravel print classif_rate name classif_rate view probabilities linspace linspace meshgrid xfull ravel ravel probas classifier predict_proba xfull n_classes unique y_pred size range n_classes subplot n_classifiers n_classes index n_classes title class ylabel name imshow_handle imshow probas reshape extent origin lower xticks yticks idx y_pred idx scatter idx idx marker axes title probability colorbar imshow_handle cax orientation horizontal show examples scikit learn user guide release figure confusion matrix confusion matrix example confusion matrix usage evaluate quality output classier 
3968: script output chapter example gallery scikit learn user guide release python source code plot_confusion_matrix print __doc__ import random import pylab sklearn import svm datasets sklearn metrics import confusion_matrix import data play iris datasets load_iris iris data iris target n_samples n_features shape range n_samples random seed random shuffle half int n_samples run classifier classifier svm svc kernel linear classifier fit half half predict half compute confusion matrix confusion_matrix half print show confusion matrix matshow title confusion matrix colorbar show figure recognizing hand written digits recognizing hand written digits example showing scikit learn used recognize images hand written digits example commented tutorial section user manual 
3969: examples scikit learn user guide release script output classification report classifier svc cache_size class_weight none coef0 degree gamma kernel rbf probability false shrinking true tol verbose false precision recall score support avg total confusion matrix chapter example gallery scikit learn user guide release python source code plot_digits_classification print __doc__ author gael varoquaux gael dot varoquaux normalesup dot org license simplified bsd standard scientific python imports import pylab import datasets classifiers performance metrics sklearn import datasets svm metrics digits dataset digits datasets load_digits data interested made 8x8 images digits lets look first images stored images attribute dataset working image files could load using pylab imread images know digit represent given target dataset index image label enumerate zip digits images digits target subplot index axis imshow image cmap gray_r interpolation nearest title training label apply classifier data need flatten image turn data samples feature matrix n_samples len digits images data digits images reshape n_samples create classifier support vector classifier classifier svm svc gamma learn digits first half digits classifier fit data n_samples digits target n_samples predict value digit second half expected digits target n_samples predicted classifier predict data n_samples print classification report classifier classifier metrics classification_report expected predicted print confusion matrix metrics confusion_matrix expected predicted index image prediction enumerate zip digits images n_samples predicted subplot index axis imshow image cmap gray_r interpolation nearest examples scikit learn user guide release title prediction prediction show figure pipelining chaining pca logistic regression pipelining chaining pca logistic regression pca unsupervised dimensionality reduction logistic regression prediction use gridsearchcv set dimensionality pca python source code plot_digits_pipe print __doc__ code source gael varoqueux modified documentation merge jaques grobler license bsd import numpy import pylab sklearn import linear_model decomposition datasets cross_validation logistic linear_model logisticregression pca decomposition pca sklearn pipeline import pipeline pipe pipeline steps pca pca logistic logistic chapter example gallery scikit learn user guide release digits datasets load_digits x_digits digits data y_digits digits target plot pca spectrum pca fit x_digits figure figsize clf axes plot pca explained_variance_ linewidth axis tight xlabel n_components ylabel explained_variance_ prediction sklearn grid_search import gridsearchcv n_components logspace parameters pipelines set using separated parameter names estimator gridsearchcv pipe estimator fit x_digits y_digits dict pca__n_components n_components logistic__c axvline estimator best_estimator_ named_steps pca n_components linestyle label n_components chosen legend prop dict size show figure univariate feature selection univariate feature selection example showing univariate feature selection noisy non informative features added iris data univariate feature selection applied feature plot values univariate feature selection corresponding weights svm see univariate feature selection selects informative features larger svm weights total set features rst ones signicant see highest score univariate feature selection svm attributes small weights features weight non zero 
3970: examples scikit learn user guide release applying univariate feature selection svm increases svm weight attributed signicant features thus improve classication 
3971: python source code plot_feature_selection print __doc__ import numpy import pylab sklearn import datasets svm sklearn feature_selection import selectpercentile f_classif import data play iris dataset iris datasets load_iris noisy data correlated random normal size len iris data add noisy data informative features hstack iris data iris target chapter example gallery scikit learn user guide release figure clf x_indices arange shape univariate feature selection test feature scoring use default selection function significant features selector selectpercentile f_classif percentile selector fit scores log10 selector scores_ scores scores max bar x_indices scores width label runivariate score log value color compare weights svm clf svm svc kernel linear clf fit svm_weights clf coef_ sum axis svm_weights svm_weights max bar x_indices svm_weights width label svm weight color title comparing feature selection xlabel feature number yticks axis tight legend loc upper right show figure demonstration sampling hmm demonstration sampling hmm script shows sample points hiden markov model hmm use components specied mean covariance plot show sequence observations generated transitions see specied transition matrix transition component 
3972: examples scikit learn user guide release python source code plot_hmm_sampling import numpy import matplotlib pyplot plt sklearn import hmm prepare parameters components hmm initial population probability start_prob array transition matrix note transitions possible component trans_mat array means component means array covariance component covars tile identity chapter example gallery scikit learn user guide release build hmm instance set parameters model hmm gaussianhmm full start_prob trans_mat random_state instead fitting data directly set estimated parameters means covariance components model means_ means model covars_ covars generate samples model sample plot sampled data plt plot label observations mfc orange alpha indicate component numbers enumerate means plt text component size horizontalalignment center bbox dict alpha facecolor plt legend loc best plt show figure gaussian hmm stock data gaussian hmm stock data script shows use gaussian hmm uses stock price data obtained yahoo nance information get stock prices matplotlib please refer date_demo1 matplotlib 
3973: examples scikit learn user guide release script output fitting hmm decoding done transition matrix 82570383e 00013693e 07843572e 22066524e 97052623e 97052623e 08017604e 08227185e 57251401e 86343408e 45954846e 74235836e 42822313e 87982473e 85368960e 79686205e 97087223e 97761575e 23842520e 14011406e 08075306e 89198240e 08407805e 98929510e 09198515e means vars hidden state 0th hidden state mean var 27509477e 90812491e 1th hidden state mean var 75072729e 05990139e 2th hidden state chapter example gallery mean var scikit learn user guide release 3th hidden state mean var 49727960e 48267818e 4th hidden state mean var 13046223e 37575708e python source code plot_hmm_stock_analysis print __doc__ import datetime import numpy import pylab matplotlib finance import quotes_historical_yahoo matplotlib dates import yearlocator monthlocator dateformatter sklearn hmm import gaussianhmm downloading data date1 datetime date start date date2 datetime date end date get quotes yahoo finance quotes quotes_historical_yahoo intc date1 date2 len quotes raise systemexit unpack quotes dates array quotes dtype int close_v array quotes volume array quotes take diff close value makes len diff len close_t therefore others quantity also need shifted diff close_v close_v dates dates close_v close_v pack diff volume training column_stack diff volume run gaussian hmm print fitting hmm decoding n_components make hmm instance execute fit model gaussianhmm n_components diag model fit n_iter predict optimal sequence internal hidden state hidden_states model predict examples scikit learn user guide release print done print trained parameters plot print transition matrix print model transmat_ print print means vars hidden state xrange n_components print dth hidden state print mean model means_ print var diag model covars_ print every year years yearlocator months monthlocator every month yearsfmt dateformatter fig figure fig add_subplot xrange n_components use fancy indexing plot data state idx hidden_states plot_date dates idx close_v idx label dth hidden state legend format ticks xaxis set_major_locator years xaxis set_major_formatter yearsfmt xaxis set_minor_locator months autoscale_view format coords message box fmt_xdata dateformatter fmt_ydata lambda grid true fig autofmt_xdate show figure classiers comparison classiers comparison comparison nearest neighbours logistic regression linear svc classifying iris dataset 
3974: chapter example gallery scikit learn user guide release python source code plot_iris_classifiers print __doc__ code source gael varoqueux modified documentation merge jaques grobler license bsd import numpy import pylab sklearn import neighbors datasets linear_model svm import data play iris datasets load_iris iris data take first two features iris target step size mesh classifiers dict knn neighbors kneighborsclassifier logistic linear_model logisticregression 1e5 svm svm linearsvc 1e5 loss fignum create instance neighbours classifier fit data 
3975: examples scikit learn user guide release name clf classifiers iteritems clf fit plot decision boundary asign color point mesh x_min m_max y_min y_max x_min x_max min max y_min y_max min max meshgrid arange x_min x_max arange y_min y_max clf predict ravel ravel put result color plot reshape shape figure fignum figsize pcolormesh cmap paired plot also training points scatter cmap paired xlabel sepal length ylabel sepal width xlim min max ylim min max xticks yticks fignum show figure explicit feature map approximation rbf kernels explicit feature map approximation rbf kernels example shows use rbfsampler appoximate feature map rbf kernel classication svm digits dataset results using linear svm original space linear svm using approximate mapping using kernelized svm compared timings accuracy varying amounts monte carlo samplings approximate mapping shown sampling dimensions clearly leads better classication results comes greater cost means tradeoff runtime accuracy given parameter n_components note solving linear svm also approximate kernel svm could greatly accelerated using stochastic gradient descent via sklearn linear_model sgdclassifier easily possible case kernelized svm second plot visualized decision surfaces rbf kernel svm linear svm approximate kernel map plot shows decision surfaces classiers projected onto rst two principal components data visualization taken grain salt since interesting slice decision surface dimensions particular note datapoint represented dot necessarily classied chapter example gallery region lying since lie plane rst two principal components span usage rbfsampler described detail kernel approximation 
3976: scikit learn user guide release python source code plot_kernel_approximation print __doc__ author gael varoquaux gael dot varoquaux normalesup dot org license simplified bsd modified andreas mueller standard scientific python imports import pylab import numpy time import time import datasets classifiers performance metrics sklearn import datasets svm pipeline sklearn kernel_approximation import rbfsampler sklearn decomposition import pca digits dataset digits datasets load_digits n_class apply classifier data need flatten image turn data samples feature matrix n_samples len digits data data digits data data data mean axis learn digits first half digits data_train targets_train data n_samples digits target n_samples predict value digit second half examples scikit learn user guide release data_test targets_test data n_samples digits target n_samples data_test scaler transform data_test create classifier support vector classifier kernel_svm svm svc gamma linear_svm svm linearsvc create pipeline kernel approximation linear svm feature_map rbfsampler gamma random_state approx_kernel_svm pipeline pipeline feature_map feature_map svm svm linearsvc fit predict using linear kernel svm kernel_svm_time time kernel_svm fit data_train targets_train kernel_svm_score kernel_svm score data_test targets_test kernel_svm_time time kernel_svm_time linear_svm_time time linear_svm fit data_train targets_train linear_svm_score linear_svm score data_test targets_test linear_svm_time time linear_svm_time sample_sizes arange approx_kernel_scores approx_kernel_times sample_sizes approx_kernel_svm set_params feature_map__n_components approx_kernel_timing time approx_kernel_svm fit data_train targets_train approx_kernel_times append time approx_kernel_timing score approx_kernel_svm score data_test targets_test approx_kernel_scores append score plot results accuracy subplot second axis timeings timescale subplot accuracy plot sample_sizes approx_kernel_scores label approx kernel timescale plot sample_sizes approx_kernel_times label approx kernel horizontal lines exact rbf linear kernels accuracy plot sample_sizes sample_sizes linear_svm_score linear_svm_score label linear svm timescale plot sample_sizes sample_sizes linear_svm_time linear_svm_time label linear svm accuracy plot sample_sizes sample_sizes kernel_svm_score kernel_svm_score label rbf svm timescale plot sample_sizes sample_sizes kernel_svm_time kernel_svm_time label rbf svm vertical line dataset dimensionality chapter example gallery scikit learn user guide release accuracy plot label n_features legends labels accuracy set_title classification accuracy timescale set_title training times accuracy set_xlim sample_sizes sample_sizes accuracy set_xticks accuracy set_ylim min approx_kernel_scores timescale set_xlabel sampling steps transformed feature dimension accuracy set_ylabel classification accuracy timescale set_ylabel training time seconds accuracy legend loc best timescale legend loc best visualize decision surface projected first two principal components dataset pca pca n_components fit data_train pca transform data_train gemerate grid along first two principal components multiples arange steps along first component first multiples newaxis pca components_ steps along second component second multiples newaxis pca components_ combine grid first newaxis second newaxis flat_grid grid reshape data shape title plots titles svc rbf kernel svc linear kernel rbf feature map n_components figure figsize predict plot clf enumerate kernel_svm approx_kernel_svm plot decision boundary asign color point mesh x_min m_max y_min y_max subplot clf predict flat_grid put result color plot reshape grid shape contourf multiples multiples cmap paired axis plot also training points scatter targets_train cmap paired title titles show linear quadratic discriminant analysis condence ellipsoid plot condence ellipsoids class decision boundary examples scikit learn user guide release figure linear quadratic discriminant analysis condence ellipsoid python source code plot_lda_qda print __doc__ scipy import linalg import numpy import pylab import matplotlib mpl matplotlib import colors sklearn lda import lda sklearn qda import qda chapter example gallery scikit learn user guide release colormap cmap colors linearsegmentedcolormap red_blue_classes red green blue register_cmap cmap cmap generate datasets def dataset_fixed_cov generate gaussians samples covariance matrix dim random seed array dot random randn dim dot random randn dim array hstack zeros ones return def dataset_cov generate gaussians samples different covariance matrices dim random seed array dot random randn dim dot random randn dim array hstack zeros ones return plot functions def plot_data lda y_pred fig_index splot subplot fig_index fig_index title linear discriminant analysis ylabel data fixed covariance elif fig_index title quadratic discriminant analysis elif fig_index ylabel data varying covariances true positive y_pred tp0 tp1 x0_tp x0_fp tp0 tp0 true x1_tp x1_fp tp1 tp1 true xmin xmax min max ymin ymax min max class dots plot x0_tp x0_tp color red plot x0_fp x0_fp color dark red class dots plot x1_tp x1_tp color blue examples scikit learn user guide release plot x1_fp x1_fp color dark blue class areas x_min x_max xlim y_min y_max ylim meshgrid linspace x_min x_max linspace y_min y_max lda predict_proba ravel ravel reshape shape pcolormesh cmap red_blue_classes norm colors normalize contour linewidths colors means plot lda means_ lda means_ color black markersize plot lda means_ lda means_ color black markersize return splot def plot_ellipse splot mean cov color linalg eigh cov linalg norm angle arctan angle angle convert degrees filled gaussian standard deviation ell mpl patches ellipse mean angle color color ell set_clip_box splot bbox ell set_alpha splot add_artist ell splot set_xticks splot set_yticks def plot_lda_cov lda splot plot_ellipse splot lda means_ lda covariance_ red plot_ellipse splot lda means_ lda covariance_ blue def plot_qda_cov qda splot plot_ellipse splot qda means_ qda covariances_ red plot_ellipse splot qda means_ qda covariances_ blue enumerate dataset_fixed_cov dataset_cov lda lda lda y_pred lda fit store_covariance true predict splot plot_data lda y_pred fig_index plot_lda_cov lda splot axis tight qda qda qda chapter example gallery scikit learn user guide release y_pred qda fit store_covariances true predict splot plot_data qda y_pred fig_index plot_qda_cov qda splot axis tight suptitle lda qda show figure multilabel classication multilabel classication example simulates multi label document classication problem dataset generated randomly based following process pick number labels poisson n_labels times choose class multinomial theta pick document length poisson length times choose word multinomial theta_c process rejection sampling used make sure document length never zero likewise reject classes already chosen documents assigned classes plotted surrounded two colored circles classication performed projecting rst two principal components found pca cca visual isation purposes followed using sklearn multiclass onevsrestclassifier metaclassier using two svcs linear kernels learn discriminative model class note pca used perform unsupervised dimensionality reduction cca used perform supervised one 
3977: examples scikit learn user guide release python source code plot_multilabel print __doc__ import numpy import matplotlib pylab sklearn datasets import make_multilabel_classification sklearn multiclass import onevsrestclassifier sklearn svm import svc sklearn preprocessing import labelbinarizer sklearn decomposition import pca sklearn pls import cca def plot_hyperplane clf min_x max_x linestyle label get separating hyperplane clf coef_ linspace min_x max_x make sure line long enough clf intercept_ plot linestyle label label def plot_subfigure subplot title transform transform pca chapter example gallery scikit learn user guide release pca n_components fit_transform elif transform cca convert list tuples class indicator matrix first y_indicator labelbinarizer fit transform cca n_components fit y_indicator transform else raise valueerror min_x min max_x max classif onevsrestclassifier svc kernel linear classif fit subplot subplot title title zero_class one_class scatter gray scatter zero_class zero_class edgecolors facecolors none linewidths label class scatter one_class one_class edgecolors orange facecolors none linewidths label class axis tight plot_hyperplane classif estimators_ min_x max_x boundary nfor class plot_hyperplane classif estimators_ min_x max_x boundary nfor class xticks yticks subplot xlim min_x max_x xlabel first principal component ylabel second principal component legend loc upper left figure figsize make_multilabel_classification n_classes n_labels allow_unlabeled true random_state plot_subfigure unlabeled samples cca cca plot_subfigure unlabeled samples pca pca make_multilabel_classification n_classes n_labels allow_unlabeled false random_state plot_subfigure without unlabeled samples cca cca plot_subfigure without unlabeled samples pca pca subplots_adjust show examples scikit learn user guide release figure test permutations signicance classication score test permutations signicance classication score order test classication score signicative technique repeating classication procedure ran domizing permuting labels value given percentage runs score obtained greater classication score obtained rst place 
3978: script output classification score pvalue python source code plot_permutation_test_for_classification author license bsd alexandre gramfort alexandre gramfort inria chapter example gallery scikit learn user guide release print __doc__ import numpy import pylab sklearn svm import svc sklearn cross_validation import stratifiedkfold permutation_test_score sklearn import datasets sklearn metrics import zero_one_score loading dataset iris datasets load_iris iris data iris target n_classes unique size noisy data correlated random random randomstate seed random normal size len add noisy data informative features make task harder svm svc kernel linear stratifiedkfold score permutation_scores pvalue permutation_test_score svm zero_one_score n_permutations n_jobs print classification score pvalue score pvalue view histogram permutation scores hist permutation_scores label permutation scores ylim ylim bug vlines linestyle fails older versions matplotlib vlines score ylim ylim linestyle vlines n_classes ylim ylim linestyle plot score ylim linewidth color linewidth label classification score pvalue pvalue color linewidth label luck label classification score pvalue pvalue plot n_classes ylim linewidth label luck ylim ylim legend xlabel score show examples scikit learn user guide release figure pls partial least squares pls partial least squares simple usage various pls avor plscanonical plsregression multivariate response pls2 plsregression univariate response pls1 cca given multivariate covarying two dimensional datasets pls extracts directions covariance components datasets explain shared variance datasets apparent scatterplot matrix display components dataset dataset maximaly correlated points lie around rst diagonal also true components dataset however correlation across datasets different components weak point cloud spherical 
3979: script output chapter example gallery scikit learn user guide release corr 
3980: corr 
3981:  
3982: true err estimated 
3983:  
3984: estimated betas python source code plot_pls print __doc__ import numpy import pylab sklearn pls import plscanonical plsregression cca dataset based latent variables model latents vars random normal size examples scikit learn user guide release random normal size latents array latents random normal size reshape latents random normal size reshape x_train y_train x_test y_test print corr print round corrcoef print corr print round corrcoef canonical symetric pls transform data plsca plscanonical n_components plsca fit x_train y_train x_train_r y_train_r plsca transform x_train y_train x_test_r y_test_r plsca transform x_test y_test scatter plot scores diagonal plot scores components subplot plot x_train_r y_train_r label train plot x_test_r y_test_r label test xlabel scores ylabel scores title comp test corr corrcoef x_test_r y_test_r legend subplot plot x_train_r y_train_r label train plot x_test_r y_test_r label test xlabel scores ylabel scores title comp test corr corrcoef x_test_r y_test_r legend diagonal plot components subplot plot x_train_r x_train_r label train plot x_test_r x_test_r label test xlabel comp ylabel comp title comp comp test corr corrcoef x_test_r x_test_r legend subplot chapter example gallery scikit learn user guide release plot y_train_r y_train_r label train plot y_test_r y_test_r label test xlabel comp ylabel comp title comp comp test corr corrcoef y_test_r y_test_r legend show pls regression multivariate response pls2 random normal size reshape array noize dot random normal size reshape pls2 plsregression n_components pls2 fit print true err print compare pls2 coefs print estimated print round pls2 coefs pls2 predict pls regression univariate response pls1 random normal size reshape random normal size pls1 plsregression n_components pls1 fit note number compements exceeds dimension print estimated betas print round pls1 coefs cca pls mode symetric deflation cca cca n_components cca fit x_train y_train x_train_r y_train_r plsca transform x_train y_train x_test_r y_test_r plsca transform x_test y_test precision recall example precision recall metric evaluate quality output classier 
3985: examples scikit learn user guide release figure precision recall script output area curve python source code plot_precision_recall print __doc__ import random import pylab import numpy sklearn import svm datasets sklearn metrics import precision_recall_curve sklearn metrics import auc chapter example gallery scikit learn user guide release import data play iris datasets load_iris iris data iris target keep also classes n_samples n_features shape range n_samples shuffle samples random seed random shuffle half int n_samples add noisy features random seed random randn n_samples n_features run classifier classifier svm svc kernel linear probability true probas_ classifier fit half half predict_proba half compute precision recall plot curve precision recall thresholds precision_recall_curve half probas_ area auc recall precision print area curve area clf plot recall precision label precision recall curve xlabel recall ylabel precision ylim xlim title precision recall example auc area legend loc lower left show figure recursive feature elimination recursive feature elimination recursive feature elimination example showing relevance pixels digit classication task 
3986: examples scikit learn user guide release python source code plot_rfe_digits print __doc__ sklearn svm import svc sklearn datasets import load_digits sklearn feature_selection import rfe load digits dataset digits load_digits digits images reshape len digits images digits target create rfe object rank pixel svc svc kernel linear rfe rfe estimator svc n_features_to_select step rfe fit ranking rfe ranking_ reshape digits images shape chapter example gallery scikit learn user guide release plot pixel ranking import pylab matshow ranking colorbar title ranking pixels rfe show figure recursive feature elimination cross validation recursive feature elimination cross validation recursive feature elimination example automatic tuning number features selected cross validation 
3987: script output examples scikit learn user guide release optimal number features python source code plot_rfe_with_cross_validation print __doc__ sklearn svm import svc sklearn cross_validation import stratifiedkfold sklearn feature_selection import rfecv sklearn datasets import make_classification sklearn metrics import zero_one build classification task using informative features make_classification n_samples n_features n_informative n_redundant n_repeated n_classes n_clusters_per_class random_state create rfe object compute cross validated score svc svc kernel linear rfecv rfecv estimator svc step stratifiedkfold loss_func zero_one rfecv fit print optimal number features rfecv n_features_ plot number features cross validation scores import pylab figure xlabel number features selected ylabel cross validation score misclassifications plot xrange len rfecv cv_scores_ rfecv cv_scores_ show figure receiver operating characteristic roc receiver operating characteristic roc example receiver operating characteristic roc metric evaluate quality output classier 
3988: chapter example gallery scikit learn user guide release script output area roc curve python source code plot_roc print __doc__ import numpy import pylab sklearn import svm datasets sklearn utils import shuffle sklearn metrics import roc_curve auc random_state random randomstate import data play iris datasets load_iris iris data iris target make binary classification problem removing third class n_samples n_features shape add noisy features make problem harder random_state randn n_samples n_features examples scikit learn user guide release shuffle split training test sets shuffle random_state random_state half int n_samples x_train x_test half half y_train y_test half half run classifier classifier svm svc kernel linear probability true probas_ classifier fit x_train y_train predict_proba x_test compute roc curve area curve fpr tpr thresholds roc_curve y_test probas_ roc_auc auc fpr tpr print area roc curve roc_auc plot roc curve clf plot fpr tpr label roc curve area roc_auc plot xlim ylim xlabel false positive rate ylabel true positive rate title receiver operating characteristic example legend loc lower right show figure receiver operating characteristic roc cross validation receiver operating characteristic roc cross validation example receiver operating characteristic roc metric evaluate quality output classier using cross validation 
3989: chapter example gallery scikit learn user guide release python source code plot_roc_crossval print __doc__ import numpy scipy import interp import pylab sklearn import svm datasets sklearn metrics import roc_curve auc sklearn cross_validation import stratifiedkfold data generation import data play iris datasets load_iris iris data iris target n_samples n_features shape add noisy features random randn n_samples n_features examples scikit learn user guide release classification roc analysis run classifier crossvalidation plot roc curves stratifiedkfold classifier svm svc kernel linear probability true mean_tpr mean_fpr linspace all_tpr train test enumerate probas_ classifier fit train train predict_proba test compute roc curve area curve fpr tpr thresholds roc_curve test probas_ mean_tpr interp mean_fpr fpr tpr mean_tpr roc_auc auc fpr tpr plot fpr tpr label roc fold area roc_auc plot color label luck mean_tpr len mean_tpr mean_auc auc mean_fpr mean_tpr plot mean_fpr mean_tpr label mean roc area mean_auc xlim ylim xlabel false positive rate ylabel true positive rate title receiver operating characteristic example legend loc lower right show figure train error test error train error test error illustration performance estimator unseen data test data performance training data regularization increases performance train decreases performance test optimal within range values regularization parameter example elastic net regression model performance measured using explained variance 
3990: chapter example gallery scikit learn user guide release script output optimal regularization parameter python source code plot_train_error_vs_test_error print __doc__ author alexandre gramfort alexandre gramfort inria license bsd style 
3991: import numpy sklearn import linear_model generate sample data n_samples_train n_samples_test n_features random seed coef random randn n_features coef top features impacting model random randn n_samples_train n_samples_test n_features dot coef split train test data x_train x_test n_samples_train n_samples_train y_train y_test n_samples_train n_samples_train examples scikit learn user guide release compute train test errors alphas logspace enet linear_model elasticnet rho train_errors list test_errors list alpha alphas enet set_params alpha alpha enet fit x_train y_train train_errors append enet score x_train y_train test_errors append enet score x_test y_test i_alpha_optim argmax test_errors alpha_optim alphas i_alpha_optim print optimal regularization parameter alpha_optim estimate coef_ full data optimal regularization parameter enet set_params alpha alpha_optim coef_ enet fit coef_ plot results functions import pylab subplot semilogx alphas train_errors label train semilogx alphas test_errors label test vlines alpha_optim ylim max test_errors color linewidth label optimum test legend loc lower left ylim xlabel regularization parameter ylabel performance show estimated coef_ true coef subplot plot coef label true coef plot coef_ label estimated coef legend subplots_adjust show figure classication text documents using sparse features chapter example gallery scikit learn user guide release classication text documents using sparse features example showing scikit learn used classify documents topics using bag words approach example uses scipy sparse matrix store features instead standard numpy arrays demos various classiers efciently handle sparse matrices dataset used example newsgroups dataset automatically downloaded cached adjust number categories giving names dataset loader setting none get python source code document_classification_20newsgroups author peter prettenhofer peter prettenhofer gmail com license simplified bsd olivier grisel olivier grisel ensta org mathieu blondel mathieu mblondel org lars buitinck buitinck uva import logging import numpy optparse import optionparser import sys time import time import pylab sklearn datasets import fetch_20newsgroups sklearn feature_extraction text import tfidfvectorizer sklearn feature_selection import selectkbest chi2 sklearn linear_model import ridgeclassifier sklearn svm import linearsvc sklearn linear_model import sgdclassifier sklearn linear_model import perceptron sklearn naive_bayes import bernoullinb multinomialnb sklearn neighbors import kneighborsclassifier sklearn neighbors import nearestcentroid sklearn utils extmath import density sklearn import metrics display progress logs stdout logging basicconfig level logging info format asctime levelname message parse commandline arguments optionparser add_option report action store_true dest print_report help print detailed classification report add_option chi2_select action store type int dest select_chi2 help select number features using chi squared test add_option confusion_matrix action store_true dest print_cm help print confusion matrix add_option top10 action store_true dest print_top10 examples scikit learn user guide release help print ten discriminative terms per class every classifier opts args parse_args len args error script takes arguments sys exit print __doc__ print_help print load categories training set categories alt atheism talk religion misc comp graphics sci space uncomment following analysis categories categories none print loading newsgroups dataset categories print categories categories else data_train fetch_20newsgroups subset train categories categories shuffle true random_state data_test fetch_20newsgroups subset test categories categories shuffle true random_state print data loaded categories data_train target_names case categories none print documents training set len data_train data print documents testing set len data_test data print categories len categories print split training set test set y_train y_test data_train target data_test target print extracting features training dataset using sparse vectorizer time vectorizer tfidfvectorizer sublinear_tf true max_df stop_words english x_train vectorizer fit_transform data_train data print done time print n_samples n_features x_train shape print print extracting features test dataset using vectorizer time x_test vectorizer transform data_test data print done time print n_samples n_features x_test shape chapter example gallery scikit learn user guide release print opts select_chi2 print extracting best features chi squared test opts select_chi2 time ch2 selectkbest chi2 opts select_chi2 x_train ch2 fit_transform x_train y_train x_test ch2 transform x_test print done time print def trim trim string fit terminal assuming column display return len else mapping integer feature name original token string feature_names vectorizer get_feature_names benchmark classifiers def benchmark clf print print training print clf time clf fit x_train y_train train_time time print train time 3fs train_time time pred clf predict x_test test_time time print test time 3fs test_time score metrics f1_score y_test pred print score score hasattr clf coef_ print dimensionality clf coef_ shape print density density clf coef_ opts print_top10 print top keywords per class category enumerate categories top10 argsort clf coef_ print trim category join feature_names top10 print opts print_report print classification report print metrics classification_report y_test pred target_names categories examples scikit learn user guide release opts print_cm print confusion matrix print metrics confusion_matrix y_test pred print clf_descr str clf split return clf_descr score train_time test_time results clf name ridgeclassifier tol ridge classifier perceptron n_iter perceptron kneighborsclassifier n_neighbors knn print print name results append benchmark clf penalty print print penalty penalty upper train liblinear model results append benchmark linearsvc loss penalty penalty dual false tol train sgd model results append benchmark sgdclassifier alpha n_iter penalty penalty train sgd elastic net penalty print print elastic net penalty results append benchmark sgdclassifier alpha n_iter penalty elasticnet train nearestcentroid without threshold print print nearestcentroid aka rocchio classifier results append benchmark nearestcentroid train sparse naive bayes classifiers print print naive bayes results append benchmark multinomialnb alpha results append benchmark bernoullinb alpha class l1linearsvc linearsvc def fit self smaller stronger regularization regularization sparsity self transformer_ linearsvc penalty dual false tol self transformer_ fit_transform return linearsvc fit self def predict self self transformer_ transform chapter example gallery scikit learn user guide release return linearsvc predict self print print linearsvc based feature selection results append benchmark l1linearsvc make plots indices arange len results results results xrange clf_names score training_time test_time results title score barh indices score label score color barh indices training_time label training time color barh indices test_time label test time color yticks legend loc best subplots_adjust left zip indices clf_names text show figure clustering text documents using means clustering text documents using means example showing scikit learn used cluster documents topics using bag words approach example uses scipy sparse matrix store features instead standard numpy arrays two algorithms demoed ordinary means faster cousin minibatch means python source code document_clustering author peter prettenhofer peter prettenhofer gmail com license simplified bsd lars buitinck buitinck uva sklearn datasets import fetch_20newsgroups sklearn feature_extraction text import tfidfvectorizer sklearn import metrics examples scikit learn user guide release sklearn cluster import kmeans minibatchkmeans import logging optparse import optionparser import sys time import time import numpy display progress logs stdout logging basicconfig level logging info format asctime levelname message parse commandline arguments optionparser add_option minibatch action store_false dest minibatch default true help use ordinary means algorithm print __doc__ print_help opts args parse_args len args error script takes arguments sys exit load categories training set categories alt atheism talk religion misc comp graphics sci space uncomment following analysis categories categories none print loading newsgroups dataset categories print categories dataset fetch_20newsgroups subset categories categories shuffle true random_state print documents len dataset data print categories len dataset target_names print labels dataset target true_k unique labels shape print extracting features training dataset using sparse vectorizer time vectorizer tfidfvectorizer max_df max_features vectorizer fit_transform dataset data stop_words english chapter example gallery scikit learn user guide release print done time print n_samples n_features shape print actual clustering opts minibatch minibatchkmeans true_k init means n_init init_size batch_size verbose else kmeans true_k init random max_iter n_init verbose print clustering sparse data time fit print done 3fs time print print homogeneity metrics homogeneity_score labels labels_ print completeness metrics completeness_score labels labels_ print measure metrics v_measure_score labels labels_ print adjusted rand index metrics adjusted_rand_score labels labels_ print silhouette coefficient metrics silhouette_score labels sample_size print figure pipeline anova svm pipeline anova svm simple usage pipeline runs successively univariate feature selection anova svm selected features python source code feature_selection_pipeline print __doc__ sklearn import svm sklearn datasets import samples_generator sklearn feature_selection import selectkbest f_regression sklearn pipeline import pipeline examples scikit learn user guide release import data play samples_generator make_classification n_features n_informative n_redundant n_classes n_clusters_per_class anova svm anova filter take best ranked features anova_filter selectkbest f_regression svm clf svm svc kernel linear anova_svm pipeline anova anova_filter svm clf anova_svm fit anova_svm predict figure parameter estimation using grid search nested cross validation parameter estimation using grid search nested cross validation classier optimized nested cross validation using sklearn grid_search gridsearchcv ject development set comprises half available labeled data performance selected hyper parameters trained model measured dedicated evaluation set used model selection step details tools available model selection found sections cross validation evaluating estimator performance grid search setting estimator parameters python source code grid_search_digits print __doc__ sklearn import datasets sklearn cross_validation import train_test_split sklearn grid_search import gridsearchcv sklearn metrics import classification_report sklearn metrics import precision_score sklearn metrics import recall_score sklearn svm import svc loading digits dataset digits datasets load_digits apply classifier data need flatten image turn data samples feature matrix n_samples len digits images digits images reshape n_samples chapter example gallery scikit learn user guide release digits target split dataset two equal parts x_train x_test y_train y_test train_test_split test_fraction random_state set parameters cross validation tuned_parameters kernel rbf gamma kernel linear scores precision precision_score recall recall_score score_name score_func scores print tuning hyper parameters score_name print clf gridsearchcv svc tuned_parameters score_func score_func clf fit x_train y_train print best parameters set found development set print print clf best_estimator_ print print grid scores development set print params mean_score scores clf grid_scores_ print 03f mean_score scores std params print print detailed classification report print print model trained full development set print scores computed full evaluation set print y_true y_pred y_test clf predict x_test print classification_report y_true y_pred print note problem easy hyperparameter plateau flat output model precision recall ties quality 
3992: figure sample pipeline text feature extraction evaluation examples scikit learn user guide release sample pipeline text feature extraction evaluation dataset used example newsgroups dataset automatically downloaded cached reused document classication example adjust number categories giving name dataset loader setting none get sample output run quad core machine loading newsgroups dataset categories alt atheism talk religion misc documents categories performing grid search pipeline vect tfidf clf parameters clf__alpha 0000000000000001e 9999999999999995e clf__n_iter clf__penalty elasticnet tfidf__use_idf true false vect__max_n vect__max_df vect__max_features none done 030s best score best parameters set clf__alpha 9999999999999995e clf__n_iter clf__penalty elasticnet tfidf__use_idf true vect__max_n vect__max_df vect__max_features python source code grid_search_text_feature_extraction print __doc__ author olivier grisel olivier grisel ensta org license simplified bsd peter prettenhofer peter prettenhofer gmail com mathieu blondel mathieu mblondel org pprint import pprint time import time import logging sklearn datasets import fetch_20newsgroups sklearn feature_extraction text import countvectorizer sklearn feature_extraction text import tfidftransformer sklearn linear_model import sgdclassifier sklearn grid_search import gridsearchcv sklearn pipeline import pipeline display progress logs stdout logging basicconfig level logging info chapter example gallery scikit learn user guide release format asctime levelname message load categories training set categories alt atheism talk religion misc uncomment following analysis categories categories none print loading newsgroups dataset categories print categories data fetch_20newsgroups subset train categories categories print documents len data filenames print categories len data target_names print define pipeline combining text feature extractor simple classifier pipeline pipeline vect countvectorizer tfidf tfidftransformer clf sgdclassifier parameters uncommenting parameters give better exploring power increase processing time combinatorial way vect__max_df vect__max_features none vect__max_n words bigrams tfidf__use_idf true false tfidf__norm clf__alpha clf__penalty elasticnet clf__n_iter __name__ __main__ multiprocessing requires fork happen __main__ protected block find best parameters feature extraction classifier grid_search gridsearchcv pipeline parameters n_jobs verbose print performing grid search print pipeline name name pipeline steps print parameters pprint parameters time grid_search fit data data data target print done 3fs time print examples scikit learn user guide release print best score grid_search best_score print best parameters set best_parameters grid_search best_estimator get_params param_name sorted parameters keys print param_name best_parameters param_name figure classication text documents using mlcomp dataset classication text documents using mlcomp dataset example showing scikit learn used classify documents topics using bag words approach example uses scipy sparse matrix store features instead standard numpy arrays dataset used example newsgroups dataset downloaded http mlcomp org free registration required http mlcomp org datasets downloaded unzip archive somewhere lesystem instance mkdir data mlcomp unzip path dataset 20news 18828_xxxxx zip data mlcomp get folder data mlcomp named metadata subfolders raw train test holding text documents organized newsgroups set mlcomp_datasets_home environment variable pointing root folder holding uncompressed archive export mlcomp_datasets_home data mlcomp ready run example using favorite python shell ipython examples mlcomp_sparse_document_classification python source code mlcomp_sparse_document_classification print __doc__ author olivier grisel olivier grisel ensta org license simplified bsd time import time import sys import import numpy import scipy sparse chapter example gallery scikit learn user guide release import pylab sklearn datasets import load_mlcomp sklearn feature_extraction text import tfidfvectorizer sklearn linear_model import sgdclassifier sklearn metrics import confusion_matrix sklearn metrics import classification_report sklearn naive_bayes import multinomialnb mlcomp_datasets_home environ print mlcomp_datasets_home set please follow instructions sys exit load training set print loading newsgroups training set news_train load_mlcomp 20news train print news_train descr print documents len news_train filenames print categories len news_train target_names print extracting features dataset using sparse vectorizer time vectorizer tfidfvectorizer charset latin1 x_train vectorizer fit_transform open read news_train filenames print done time print n_samples n_features x_train shape assert issparse x_train y_train news_train target print loading newsgroups test set news_test load_mlcomp 20news test time print done time print predicting labels test set print documents len news_test filenames print categories len news_test target_names print extracting features dataset using vectorizer time x_test vectorizer transform open read news_test filenames y_test news_test target print done time print n_samples n_features x_test shape benchmark classifiers def benchmark clf_class params name print parameters params time clf clf_class params fit x_train y_train print done time hasattr clf coef_ print percentage non zeros coef examples scikit learn user guide release mean clf coef_ print predicting outcomes testing set time pred clf predict x_test print done time print classification report test set classifier print clf print print classification_report y_test pred target_names news_test target_names confusion_matrix y_test pred print confusion matrix print show confusion matrix matshow title confusion matrix classifier name colorbar print testbenching linear classifier parameters loss hinge penalty n_iter alpha fit_intercept true benchmark sgdclassifier parameters sgd print testbenching multinomialnb classifier parameters alpha benchmark multinomialnb parameters multinomialnb show examples based real world datasets applications real world problems medium sized datasets interactive user interface 
3993: figure outlier detection real data set chapter example gallery scikit learn user guide release outlier detection real data set example illustrates need robust covariance estimation real data set detection better understanding data structure selected two sets two variables boston housing data set illustration kind analysis done several outlier detection tools purpose vizualisation working two dimensional examples one aware things trivial high dimension pointed examples main result empirical covariance estimate non robust one highly inuenced heterogeneous structure observations although robust covariance estimate able focus main mode data distribution sticks assumption data gaussian distributed yielding biased estimation data structure yet accurate extent one class svm algorithm useful outlier first example rst example illustrates robust covariance estimation help concentrating relevant cluster one exists many observations confounded one break empirical covariance estima tion course screening tools would pointed presence two clusters support vector machines gaussian mixture models univariate outlier detection high dimensional example none could applied easily 
3994: second example second example shows ability minimum covariance determinant robust estimator covariance concentrate main mode data distribution location seems well estimated although covariance hard estimate due banana shaped distribution anyway get rid outlying observations one class svm able capture real data structure difculty adjust kernel bandwith parameter obtain good compromise shape data scatter matrix risk tting data 
3995: examples scikit learn user guide release python source code plot_outlier_detection_housing print __doc__ author virgile fritsch virgile fritsch inria license bsd import numpy sklearn covariance import ellipticenvelope sklearn svm import oneclasssvm import matplotlib pyplot plt import matplotlib font_manager sklearn datasets import load_boston get data load_boston data two clusters load_boston data banana shaped define classifiers used classifiers empirical covariance ellipticenvelope support_fraction contamination robust covariance minimum covariance determinant ellipticenvelope contamination ocsvm oneclasssvm gamma colors legend1 legend2 learn frontier outlier detection several classifiers xx1 yy1 meshgrid linspace linspace xx2 yy2 meshgrid linspace linspace clf_name clf enumerate classifiers iteritems plt figure clf fit clf decision_function xx1 ravel yy1 ravel reshape xx1 shape legend1 clf_name plt contour xx1 yy1 levels linewidths colors colors plt figure clf fit clf decision_function xx2 ravel yy2 ravel reshape xx2 shape legend2 clf_name plt contour chapter example gallery scikit learn user guide release xx2 yy2 levels linewidths colors colors plot results shape data points cloud plt figure two clusters plt title outlier detection real data set boston housing plt scatter color black bbox_args dict boxstyle round arrow_args dict arrowstyle plt annotate several confounded points xycoords data textcoords data xytext bbox bbox_args arrowprops arrow_args plt xlim xx1 min xx1 max plt ylim yy1 min yy1 max plt legend legend1 values collections legend1 values collections legend1 values collections legend1 keys legend1 keys legend1 keys loc upper center prop matplotlib font_manager fontproperties size plt ylabel accessibility radial highways plt xlabel pupil teatcher ratio town plt figure banana shape plt title outlier detection real data set boston housing plt scatter color black plt xlim xx2 min xx2 max plt ylim yy2 min yy2 max plt legend legend2 values collections legend2 values collections legend2 values collections legend2 keys legend2 keys legend2 keys loc upper center prop matplotlib font_manager fontproperties size plt ylabel lower status population plt xlabel average number rooms per dwelling plt show figure species distribution modeling species distribution modeling modeling species geographic distributions important problem conservation biology example model geographic distribution two south american mammals given past observations environmental variables since positive examples unsuccessful observations cast problem density estimation problem use oneclasssvm provided package sklearn svm modeling tool dataset provided phillips available example uses basemap plot coast lines national examples scikit learn user guide release boundaries south america two species bradypus variegatus brown throated sloth microryzomys minutus also known forest small rice rat rodent lives peru colombia ecuador peru venezuela 
3996: references maximum entropy modeling species geographic distributions phillips anderson schapire ecological modelling 
3997: script output ________________________________________________________________________________ modeling distribution species bradypus variegatus fit oneclasssvm plot coastlines coverage predict species distribution done 
3998: area roc curve ________________________________________________________________________________ modeling distribution species microryzomys minutus fit oneclasssvm 
3999: done 
4000: chapter example gallery scikit learn user guide release plot coastlines coverage predict species distribution area roc curve time elapsed 32s python source code plot_species_distribution_modeling authors peter prettenhoer peter prettenhofer gmail com license bsd style 
4001: jake vanderplas vanderplas astro washington edu time import time import numpy import pylab sklearn datasets base import bunch sklearn datasets import fetch_species_distributions sklearn datasets species_distributions import construct_grids sklearn import svm metrics basemap available well use otherwise well improvise later try mpl_toolkits basemap import basemap basemap true except importerror basemap false print __doc__ def create_species_bunch species_name train test coverages xgrid ygrid create bunch information particular organism use test train record arrays extract data specific given species name bunch bunch name join species_name split points dict test test train train label pts points iteritems choose points associated desired species pts pts pts species species_name bunch pts_ label pts determine coverage values training testing points searchsorted xgrid pts long searchsorted ygrid pts lat bunch cov_ label coverages examples scikit learn user guide release return bunch def plot_species_distribution species bradypus_variegatus_0 microryzomys_minutus_0 plot species distribution len species print note two species provided first two used time load compressed data data fetch_species_distributions set data grid xgrid ygrid construct_grids data grid coordinates meshgrid xgrid ygrid create bunch species bv_bunch create_species_bunch species mm_bunch create_species_bunch species data train data test data coverages xgrid ygrid data train data test data coverages xgrid ygrid background points grid coordinates evaluation random seed background_points random randint low high data size random randint low high data size well make use fact coverages measurements land points land_reference data coverages help decide land water 
4002: fit predict plot species species enumerate bv_bunch mm_bunch print print modeling distribution species species name standardize features mean species cov_train mean axis std species cov_train std axis train_cover_std species cov_train mean std fit oneclasssvm print fit oneclasssvm clf svm oneclasssvm kernel rbf gamma clf fit train_cover_std print done chapter example gallery scikit learn user guide release plot map south america subplot basemap print plot coastlines using basemap basemap projection cyl llcrnrlat min urcrnrlat max llcrnrlon min urcrnrlon max resolution drawcoastlines drawcountries else print plot coastlines coverage contour land_reference levels colors linestyles solid xticks yticks print predict species distribution predict species distribution using training data ones data data dtype float64 well predict land points idx land_reference coverages_land data coverages idx idx pred clf decision_function coverages_land mean std pred min idx idx pred levels linspace min max land_reference plot contours prediction contourf levels levels cmap reds colorbar format scatter training testing points scatter species pts_train long species pts_train lat black marker label train scatter species pts_test long species pts_test lat black marker label test legend title species name axis equal compute auc background points pred_background background_points background_points pred_test clf decision_function species cov_test mean std scores pred_test pred_background ones pred_test shape zeros pred_background shape fpr tpr thresholds metrics roc_curve scores roc_auc metrics auc fpr tpr text auc roc_auc right print area roc curve roc_auc examples scikit learn user guide release print ntime elapsed 2fs time plot_species_distribution show figure visualizing stock market structure visualizing stock market structure example employs several unsupervised learning techniques extract stock market structure variations historical quotes quantity use daily variation quote price quotes linked tend couctuate day 
4003: learning graph structure use sparse inverse covariance estimation quotes correlated conditionally others speci cally sparse inverse covariance gives graph list connection symbol symbols connected useful expain uctuations 
4004: clustering use clustering group together quotes behave similarly amongst various clustering techniques available scikit learn use afnity propagation enforce equal size clusters choose automatically number clusters data note gives different indication graph graph reects conditional relations variables clustering reects marginal properties variables clustered together considered similar impact level full stock market 
4005: embedding space visualization purposes need lay different symbols canvas use manifold learning techniques retrieve embedding 
4006: visualization output models combined graph nodes represents stocks edges chapter example gallery scikit learn user guide release cluster labels used dene color nodes sparse covariance model used display strength edges embedding used position nodes plan example fair amount visualization related code visualization crucial display graph one challenge position labels minimizing overlap use heuristic based direction nearest neighbor along axis 
4007: script output cluster pepsi coca cola kellogg cluster apple amazon yahoo cluster glaxosmithkline novartis sanofi aventis cluster comcast time warner cablevision cluster conocophillips chevron total valero energy exxon cluster walgreen cvs cluster kraft foods cluster navistar sony marriott caterpillar canon toyota honda mitsubishi xerox unilever cluster kimberly clark colgate palmolive procter gamble cluster american express ryder goldman sachs wal mart general electrics pfizer wells fargo dupont nemours bank america aig home depot news corp ford jpmorgan chase donalds cluster microsoft sap ibm texas instruments dell cisco cluster raytheon boeing lookheed martin general dynamics northrop grumman examples scikit learn user guide release python source code plot_stock_market print __doc__ author gael varoquaux gael varoquaux normalesup org license bsd import datetime import numpy import pylab matplotlib import finance matplotlib collections import linecollection sklearn import cluster covariance manifold retrieve data internet choose time period reasonnably calm long ago get high tech firms crash datetime datetime datetime datetime symbol_dict tot total xom exxon cvx chevron cop conocophillips vlo valero energy msft microsoft ibm ibm twx time warner cmcsa comcast cvc cablevision yhoo yahoo dell dell hpq amzn amazon toyota caj canon mtu mitsubishi sne sony ford hmc honda nav navistar noc northrop grumman boeing coca cola mmm mcd donalds pep pepsi kft kraft foods kellogg unilever mar marriott procter gamble colgate palmolive nws news corp chapter example gallery scikit learn user guide release general electrics wfc wells fargo jpm jpmorgan chase aig aig axp american express bac bank america goldman sachs aapl apple sap sap csco cisco txn texas instruments xrx xerox lmt lookheed martin wmt wal mart wag walgreen home depot gsk glaxosmithkline pfe pfizer sny sanofi aventis nvs novartis kmb kimberly clark ryder general dynamics rtn raytheon cvs cvs cat caterpillar dupont nemours symbols names array symbol_dict items quotes finance quotes_historical_yahoo symbol asobject true symbol symbols open array open quotes astype float close array close quotes astype float daily variations quotes carry information variation close open learn graphical structure correlations edge_model covariance graphlassocv standardize time series using correlations rather covariance efficient structure recovery variation copy std axis edge_model fit cluster using affinity propagation labels cluster affinity_propagation edge_model covariance_ n_labels labels max range n_labels print cluster join names labels examples scikit learn user guide release find low dimension embedding visualization find best position nodes stocks plane use dense eigen_solver achieve reproducibility arpack initiated random vectors dont control addition use large number neighbors capture large scale structure node_position_model manifold locallylinearembedding n_components eigen_solver dense n_neighbors embedding node_position_model fit_transform visualization figure facecolor figsize clf axes axis display graph partial correlations partial_correlations edge_model precision_ copy sqrt diag partial_correlations partial_correlations partial_correlations newaxis non_zero abs triu partial_correlations plot nodes using coordinates embedding scatter embedding embedding labels cmap spectral plot edges start_idx end_idx non_zero sequence line0 line1 line2 linen segments embedding start embedding stop start stop zip start_idx end_idx values abs partial_correlations non_zero linecollection segments zorder cmap hot_r norm normalize values max set_array values set_linewidths values add_collection add label node challenge want position labels avoid overlap labels index name label enumerate zip names labels embedding embedding index embedding index this_dx argmin abs this_dy argmin abs this_dx horizontalalignment left chapter example gallery scikit learn user guide release else horizontalalignment right this_dy verticalalignment bottom else verticalalignment top text name size horizontalalignment horizontalalignment verticalalignment verticalalignment bbox dict facecolor edgecolor spectral label float n_labels alpha xlim embedding min embedding ptp embedding max embedding ptp ylim embedding min embedding ptp embedding max embedding ptp show figure compressive sensing tomography reconstruction prior lasso compressive sensing tomography reconstruction prior lasso example shows reconstruction image set parallel projections acquired along different angles dataset acquired computed tomography without prior information sample number projections required reconstruct image order linear size image pixels simplicity consider sparse image pixels boundary objects non zero value data could correspond example cellular material note however images sparse different basis haar wavelets projections acquired therefore necessary use prior information available sample sparsity example compressive sensing tomography projection operation linear transformation addition data delity term corresponding linear regression penalize norm image account sparsity resulting optimization problem called lasso use class sklearn linear_model sparse lasso uses coordinate descent algorithm importantly implementation computationally efcient sparse matrix projection operator used reconstruction penalization gives result zero error pixels successfully labeled even noise added projections comparison penalization sklearn linear_model ridge produces large number labeling errors pixels important artifacts observed reconstructed image contrary penalization note particular circular artifact separating pixels corners contributed fewer projections central disk 
4008: examples scikit learn user guide release python source code plot_tomography_l1_reconstruction print __doc__ author emmanuelle gouillart emmanuelle gouillart nsup org license simplified bsd import numpy scipy import sparse scipy import ndimage sklearn linear_model sparse import lasso sklearn linear_model import ridge import matplotlib pyplot plt def _weights orig ravel floor_x floor orig alpha orig floor_x return hstack floor_x floor_x hstack alpha alpha def _generate_center_coordinates l_x l_x float l_x mgrid l_x l_x center l_x center center return def build_projection_operator l_x n_dir compute tomography design matrix 
4009: parameters l_x int linear size image array chapter example gallery scikit learn user guide release n_dir int number angles projections acquired 
4010: returns sparse matrix shape n_dir l_x l_x _generate_center_coordinates l_x angles linspace n_dir endpoint false data_inds weights camera_inds data_unravel_indices arange l_x data_unravel_indices hstack data_unravel_indices data_unravel_indices angle enumerate angles xrot cos angle sin angle inds _weights xrot orig min mask logical_and inds inds l_x weights list mask camera_inds list inds mask l_x data_inds list data_unravel_indices mask proj_operator sparse coo_matrix weights camera_inds data_inds return proj_operator def generate_synthetic_data synthetic binary data random randomstate n_pts ogrid mask_outer mask zeros points rand n_pts mask points astype int points astype int mask ndimage gaussian_filter mask sigma n_pts res logical_and mask mask mean mask_outer return res ndimage binary_erosion res generate synthetic images projections proj_operator build_projection_operator data generate_synthetic_data proj proj_operator data ravel newaxis proj random randn proj shape reconstruction ridge penalization rgr_ridge ridge alpha rgr_ridge fit proj_operator proj ravel rec_l2 rgr_ridge coef_ reshape reconstruction lasso penalization best value alpha determined using cross validation lassocv rgr_lasso lasso alpha rgr_lasso fit proj_operator proj ravel rec_l1 rgr_lasso coef_ reshape plt figure figsize examples scikit learn user guide release plt subplot plt imshow data cmap plt gray interpolation nearest plt axis plt title original image plt subplot plt imshow rec_l2 cmap plt gray interpolation nearest plt title penalization plt axis plt subplot plt imshow rec_l1 cmap plt gray interpolation nearest plt title penalization plt axis plt subplots_adjust hspace wspace top bottom left right plt show figure faces recognition example using eigenfaces svms faces recognition example using eigenfaces svms dataset used example preprocessed excerpt labeled faces wild aka lfw http vis www umass edu lfw lfw funneled tgz 233mb expected results top represented people dataset precision recall score support gerhard_schroeder donald_rumsfeld tony_blair colin_powell george_w_bush avg total python source code face_recognition print __doc__ time import time import logging import pylab sklearn cross_validation import train_test_split chapter example gallery scikit learn user guide release sklearn datasets import fetch_lfw_people sklearn grid_search import gridsearchcv sklearn metrics import classification_report sklearn metrics import confusion_matrix sklearn decomposition import randomizedpca sklearn svm import svc display progress logs stdout logging basicconfig level logging info format asctime message download data already disk load numpy arrays lfw_people fetch_lfw_people min_faces_per_person resize introspect images arrays find shapes plotting n_samples lfw_people images shape fot machine learning use data directly relative pixel positions info ignored model lfw_people data n_features shape label predict person lfw_people target target_names lfw_people target_names n_classes target_names shape print total dataset size print n_samples n_samples print n_features n_features print n_classes n_classes split training set test set using stratified fold split training testing set x_train x_test y_train y_test train_test_split test_fraction compute pca eigenfaces face dataset treated unlabeled dataset unsupervised feature extraction dimensionality reduction n_components print extracting top eigenfaces faces n_components x_train shape time pca randomizedpca n_components n_components whiten true fit x_train print done 3fs time eigenfaces pca components_ reshape n_components print projecting input data eigenfaces orthonormal basis time examples scikit learn user guide release x_train_pca pca transform x_train x_test_pca pca transform x_test print done 3fs time train svm classification model print fitting classifier training set time param_grid 1e3 5e3 1e4 5e4 1e5 gamma clf gridsearchcv svc kernel rbf class_weight auto param_grid clf clf fit x_train_pca y_train print done 3fs time print best estimator found grid search print clf best_estimator_ quantitative evaluation model quality test set print predicting people names testing set time y_pred clf predict x_test_pca print done 3fs time print classification_report y_test y_pred target_names target_names print confusion_matrix y_test y_pred labels range n_classes qualitative evaluation predictions using matplotlib def plot_gallery images titles n_row n_col helper function plot gallery portraits figure figsize n_col n_row subplots_adjust bottom left right top hspace range n_row n_col subplot n_row n_col imshow images reshape cmap gray title titles size xticks yticks plot result prediction portion test set def title y_pred y_test target_names pred_name target_names y_pred rsplit true_name target_names y_test rsplit return predicted ntrue pred_name true_name prediction_titles title y_pred y_test target_names range y_pred shape chapter example gallery scikit learn user guide release plot_gallery x_test prediction_titles plot gallery significative eigenfaces eigenface_titles eigenface range eigenfaces shape plot_gallery eigenfaces eigenface_titles show figure libsvm gui libsvm gui simple graphical frontend libsvm mainly intended didactic purposes create data points point click visualize decision region induced different kernels parameter settings create positive examples click left mouse button create negative examples click right button examples class uses one class svm python source code svm_gui __future__ import division print __doc__ author peter prettenhoer peter prettenhofer gmail com license bsd style 
4011: import matplotlib matplotlib use tkagg matplotlib backends backend_tkagg import figurecanvastkagg matplotlib backends backend_tkagg import navigationtoolbar2tkagg matplotlib figure import figure matplotlib contour import contourset import tkinter import sys import numpy sklearn import svm sklearn datasets import dump_svmlight_file y_min y_max x_min x_max examples scikit learn user guide release class model object model hold data implements observable observer pattern notifies registered observers change event def __init__ self self observers self surface none self data self cls none self surface_type def changed self event notify observers observer self observers observer update event self def add_observer self observer register observer self observers append observer def set_surface self surface self surface surface def dump_svmlight_file self file data array self data data data dump_svmlight_file file class controller object def __init__ self model self model model self kernel intvar self surface_type intvar whether model fitted self fitted false def fit self print fit model train array self model data train train float self complexity get gamma float self gamma get coef0 float self coef0 get degree int self degree get kernel_map linear rbf poly len unique clf svm oneclasssvm kernel kernel_map self kernel get gamma gamma coef0 coef0 degree degree clf fit else clf svm svc kernel kernel_map self kernel get chapter example gallery scikit learn user guide release gamma gamma coef0 coef0 degree degree clf fit hasattr clf score print accuracy clf score self decision_surface clf self model clf clf self model set_surface self model surface_type self surface_type get self fitted true self model changed surface def decision_surface self cls delta arange x_min x_max delta delta arange y_min y_max delta delta meshgrid cls decision_function ravel ravel reshape shape return def clear_data self self model data self fitted false self model changed clear def add_example self label self model data append label self model changed example_added update decision surface already fitted self refit def refit self refit model already fitted self fitted self fit class view object test docstring def __init__ self root controller figure add_subplot set_xticks set_yticks set_xlim x_min x_max set_ylim y_min y_max canvas figurecanvastkagg master root canvas show canvas get_tk_widget pack side top fill expand canvas _tkcanvas pack side top fill expand canvas mpl_connect button_press_event self onclick toolbar navigationtoolbar2tkagg canvas root toolbar update self controllbar controllbar root controller self self self canvas canvas examples scikit learn user guide release self controller controller self contours self c_labels none self plot_kernels def plot_kernels self self text linear self text rbf exp gamma self text poly gamma def onclick self event event xdata event ydata event button self controller add_example event xdata event ydata elif event button self controller add_example event xdata event ydata def update_example self model idx model data idx color elif color self plot color scalex scaley def update self event model event examples_loaded xrange len model data self update_example model event example_added self update_example model event clear self clear self set_xticks self set_yticks self contours self c_labels none self plot_kernels event surface self remove_surface self plot_support_vectors model clf support_vectors_ self plot_decision_surface model surface model surface_type self canvas draw def remove_surface self remove old decision surface len self contours contour self contours isinstance contour contourset lineset contour collections lineset remove else contour remove self contours chapter example gallery scikit learn user guide release def plot_support_vectors self support_vectors plot support vectors placing circles corresponding data points adds circle collection contours list self scatter support_vectors support_vectors edgecolors facecolors none self contours append def plot_decision_surface self surface type surface type levels linestyles dashed solid dashed colors self contours append self contour levels elif type self contours append self contourf colors colors linestyles linestyles cmap matplotlib bone origin lower alpha self contours append self contour colors linestyles solid else raise valueerror surface type unknown class controllbar object def __init__ self root controller frame root kernel_group frame radiobutton kernel_group text linear variable controller kernel radiobutton kernel_group text rbf variable controller kernel value command controller refit pack anchor value command controller refit pack anchor radiobutton kernel_group text poly variable controller kernel value command controller refit pack anchor kernel_group pack side left valbox frame controller complexity stringvar controller complexity set frame valbox label text anchor width pack side left entry width textvariable controller complexity pack side left pack controller gamma stringvar controller gamma set frame valbox label text gamma anchor width pack side left entry width textvariable controller gamma pack side left pack controller degree stringvar examples scikit learn user guide release controller degree set frame valbox label text degree anchor width pack side left entry width textvariable controller degree pack side left pack controller coef0 stringvar controller coef0 set frame valbox label text coef0 anchor width pack side left entry width textvariable controller coef0 pack side left pack valbox pack side left cmap_group frame radiobutton cmap_group text hyperplanes variable controller surface_type value command controller refit pack anchor radiobutton cmap_group text surface variable controller surface_type value command controller refit pack anchor cmap_group pack side left train_button button text fit width command controller fit train_button pack pack side left button text clear width command controller clear_data pack side left def get_parser optparse import optionparser optionparser add_option output action store type str dest output help path dump data return def main argv get_parser opts args parse_args argv root model model controller controller model root wm_title scikit learn libsvm gui view view root controller model add_observer view mainloop opts output model dump_svmlight_file opts output __name__ __main__ main sys argv chapter example gallery scikit learn user guide release figure topics extraction non negative matrix factorization topics extraction non negative matrix factorization proof concept application non negative matrix factorization term frequency matrix corpus documents extract additive model topic structure corpus default parameters n_samples n_features n_topics make example runnable couple tens seconds try increase dimensions problem ware time complexity polynomial sample extracted topics look quite good topic god people bible israel jesus christian true moral think christians believe say human israeli church life children jewish topic drive windows card drivers video scsi software thanks vga graphics help disk uni dos ide controller work topic game team nhl games hockey players buffalo edu year play university teams baseball columbia league player toronto topic window manager application mit motif size display widget program xlib windows user color event informa tion use events x11r5 values topic pitt gordon banks science pittsburgh univ computer soon disease edu reply pain health david article medical medicine python source code topics_extraction_with_nmf author olivier grisel olivier grisel ensta org license simplified bsd time import time sklearn feature_extraction import text sklearn import decomposition sklearn import datasets n_samples n_features n_topics n_top_words load newsgroups dataset vectorize using common word frequency idf weighting without top stop words time print loading dataset extracting idf features dataset datasets fetch_20newsgroups shuffle true random_state vectorizer text countvectorizer max_df max_features n_features examples scikit learn user guide release counts vectorizer fit_transform dataset data n_samples tfidf text tfidftransformer fit_transform counts print done 3fs time fit nmf model print fitting nmf model n_samples n_features n_samples n_features nmf decomposition nmf n_components n_topics fit tfidf print done 3fs time inverse vectorizer vocabulary able feature_names vectorizer get_feature_names topic_idx topic enumerate nmf components_ print topic topic_idx print join feature_names print topic argsort n_top_words figure wikipedia principal eigenvector wikipedia principal eigenvector classical way assert relative importance vertices graph compute principal eigenvector adjacency matrix assign vertex values components rst eigenvector centrality score http wikipedia org wiki eigenvector_centrality graph webpages links values called pagerank scores google goal example analyze graph links inside wikipedia articles rank articles relative importance according eigenvector centrality traditional way compute principal eigenvector use power iteration method http wikipedia org wiki power_iteration computation achieved thanks martinssons randomized svd algoritm implemented scikit graph data fetched dbpedia dumps dbpedia extraction latent structured data wikipedia content python source code wikipedia_principal_eigenvector print __doc__ author olivier grisel olivier grisel ensta org license simplified bsd chapter example gallery scikit learn user guide release bz2 import bz2file import datetime import datetime pprint import pprint time import time import numpy scipy import sparse sklearn utils extmath import randomized_svd sklearn externals joblib import memory download data already disk redirects_url http downloads dbpedia org redirects_en bz2 redirects_filename redirects_url rsplit page_links_url http downloads dbpedia org page_links_en bz2 page_links_filename page_links_url rsplit resources redirects_url redirects_filename page_links_url page_links_filename url filename resources path exists filename import urllib print downloading data please wait url opener urllib urlopen url open filename write opener read print loading redirect files memory memory cachedir def index redirects index_map find index article name redirect resolution redirects get return index_map setdefault len index_map dbpedia_resource_prefix_len len http dbpedia org resource shortname_slice slice dbpedia_resource_prefix_len def short_name nt_uri remove uri markers common uri prefix return nt_uri shortname_slice def get_redirects redirects_filename examples scikit learn user guide release parse redirections build transitively closed map redirects print parsing redirect file line enumerate bz2file redirects_filename split line split len split print ignoring malformed line line continue redirects short_name split short_name split print line 08d datetime isoformat compute transitive closure print computing transitive closure redirect relation source enumerate redirects keys transitive_target none target redirects source seen set source true transitive_target target target redirects get target target none target seen break seen add target redirects source transitive_target print line 08d datetime isoformat return redirects disabling joblib pickling large dicts seems much slow memory cache def get_adjacency_matrix redirects_filename page_links_filename limit none extract adjacency graph scipy sparse matrix redirects resolved first 
4012: returns scipy sparse adjacency matrix redirects python dict article names article names index_map python dict article names python int article indexes print computing redirect map redirects get_redirects redirects_filename print computing integer index map index_map dict links list line enumerate bz2file page_links_filename split line split len split print ignoring malformed line line continue index redirects index_map short_name split index redirects index_map short_name split links append chapter example gallery scikit learn user guide release print line 08d datetime isoformat limit none limit break print computing adjacency matrix sparse lil_matrix len index_map len index_map dtype float32 links del links print converting csr representation tocsr print csr conversion done return redirects index_map stop links make possible work ram redirects index_map get_adjacency_matrix redirects_filename page_links_filename limit names dict name name index_map iteritems print computing principal singular vectors using randomized_svd time randomized_svd n_iterations print done 3fs time print names wikipedia related strongest compenents principal singular vector similar highest eigenvector print top wikipedia pages according principal singular vectors pprint names abs argsort pprint names abs argsort def centrality_scores alpha max_iter tol power iteration computation principal eigenvector method also known google pagerank implementation based one networkx project bsd licensed copyrights aric hagberg hagberg lanl gov dan schult dschult colgate edu pieter swart swart lanl gov shape copy incoming_counts asarray sum axis ravel print normalizing graph incoming_counts nonzero data indptr indptr incoming_counts dangle asarray sum axis ravel scores ones dtype float32 initial guess range max_iter print power iteration prev_scores scores scores alpha scores dot dangle prev_scores examples scikit learn user guide release alpha prev_scores sum check convergence normalized l_inf norm scores_max abs scores max scores_max scores_max err abs scores prev_scores max scores_max print error err err tol return scores return scores print computing principal eigenvector score using power iteration method time scores centrality_scores max_iter tol print done 3fs time pprint names abs scores argsort clustering examples concerning sklearn cluster package 
4013: figure adjustment chance clustering performance evaluation adjustment chance clustering performance evaluation following plots demonstrate impact number clusters number samples various clustering performance evaluation metrics non adjusted measures measure show dependency number clusters number samples mean measure random labeling increases signicantly number clusters closer total number samples used compute measure adjusted chance measure ari display random variations centered around mean score number samples clusters adjusted measures hence safely used consensus index evaluate average stability clustering algorithms given value various overlapping sub samples dataset 
4014: chapter example gallery scikit learn user guide release script output computing adjusted_rand_score values n_clusters n_samples done 253s computing v_measure_score values n_clusters n_samples done 110s computing adjusted_mutual_info_score values n_clusters n_samples done 992s computing mutual_info_score values n_clusters n_samples done 033s computing adjusted_rand_score values n_clusters n_samples done 432s computing v_measure_score values n_clusters n_samples done 932s computing adjusted_mutual_info_score values n_clusters n_samples done 824s computing mutual_info_score values n_clusters n_samples done 155s python source code plot_adjusted_for_chance_measures print __doc__ author olivier grisel olivier grisel ensta org license simplified bsd import numpy import pylab time import time examples scikit learn user guide release sklearn import metrics def uniform_labelings_scores score_func n_samples n_clusters_range fixed_n_classes none n_runs seed compute score random uniform cluster labelings 
4015: random labelings number clusters value possible value n_clusters_range 
4016: fixed_n_classes none first labeling considered ground truth class assignement fixed number classes random_labels random randomstate seed random_integers scores zeros len n_clusters_range n_runs fixed_n_classes none labels_a random_labels low high fixed_n_classes size n_samples enumerate n_clusters_range range n_runs fixed_n_classes none labels_a random_labels low high size n_samples labels_b random_labels low high size n_samples scores score_func labels_a labels_b return scores score_funcs metrics adjusted_rand_score metrics v_measure_score metrics adjusted_mutual_info_score metrics mutual_info_score independent random clusterings equal cluster number n_samples n_clusters_range linspace n_samples astype int figure plots names score_func score_funcs print computing values n_clusters n_samples score_func __name__ len n_clusters_range n_samples time scores uniform_labelings_scores score_func n_samples n_clusters_range print done 3fs time plots append errorbar n_clusters_range median scores axis scores std axis names append score_func __name__ title clustering measures random uniform labelings equal number clusters xlabel number clusters number samples fixed n_samples chapter example gallery scikit learn user guide release ylabel score value legend plots names ylim ymin ymax random labeling varying n_clusters ground class labels fixed number clusters n_samples n_clusters_range linspace astype int n_classes figure plots names score_func score_funcs print computing values n_clusters n_samples score_func __name__ len n_clusters_range n_samples time scores uniform_labelings_scores score_func n_samples n_clusters_range fixed_n_classes n_classes print done 3fs time plots append errorbar n_clusters_range scores mean axis scores std axis names append score_func __name__ title clustering measures random uniform labeling reference assignement classes n_classes xlabel number clusters number samples fixed n_samples ylabel score value ylim ymin ymax legend plots names show figure demo afnity propagation clustering algorithm demo afnity propagation clustering algorithm reference brendan frey delbert dueck clustering passing messages data points science feb examples scikit learn user guide release script output estimated number clusters homogeneity completeness measure adjusted rand index adjusted mutual information silhouette coefficient python source code plot_affinity_propagation print __doc__ import numpy sklearn cluster import affinitypropagation sklearn import metrics sklearn datasets samples_generator import make_blobs generate sample data centers labels_true make_blobs n_samples centers centers cluster_std compute similarities x_norms sum axis chapter example gallery scikit learn user guide release x_norms newaxis x_norms newaxis dot median compute affinity propagation affinitypropagation fit cluster_centers_indices cluster_centers_indices_ labels labels_ n_clusters_ len cluster_centers_indices print estimated number clusters n_clusters_ print homogeneity metrics homogeneity_score labels_true labels print completeness metrics completeness_score labels_true labels print measure metrics v_measure_score labels_true labels print adjusted rand index metrics adjusted_rand_score labels_true labels print adjusted mutual information metrics adjusted_mutual_info_score labels_true labels min print silhouette coefficient metrics silhouette_score labels metric precomputed plot result import pylab itertools import cycle close figure clf colors cycle bgrcmykbgrcmykbgrcmykbgrcmyk col zip range n_clusters_ colors class_members labels cluster_center cluster_centers_indices plot class_members class_members col plot cluster_center cluster_center markerfacecolor col markeredgecolor markersize class_members plot cluster_center cluster_center col title estimated number clusters n_clusters_ show comparing different clustering algorithms toy datasets example aims showing characteristics different clustering algorithms datasets interesting still last dataset example null situation clustering data homogeneous good clustering examples give intuition algorithms intuition might apply high dimensional data results could improved tweaking parameters clustering strategy instance setting number clusters methods needs parameter specied note afnity propagation tendency create many clusters thus example two parameters damping per point preference set mitigate examples scikit learn user guide release figure comparing different clustering algorithms toy datasets behavior 
4017: python source code plot_cluster_comparison print __doc__ import time import numpy import pylab sklearn import cluster datasets sklearn metrics import euclidean_distances chapter example gallery scikit learn user guide release sklearn neighbors import kneighbors_graph sklearn preprocessing import scaler random seed generate datasets choose size big enough see scalability algorithms big avoid long running times n_samples noisy_circles datasets make_circles n_samples n_samples factor noisy_moons datasets make_moons n_samples n_samples noise blobs datasets make_blobs n_samples n_samples random_state no_structure random rand n_samples none noise colors array bgrcmykbgrcmykbgrcmykbgrcmyk colors hstack colors figure figsize subplots_adjust left right bottom top wspace hspace plot_num i_dataset dataset enumerate noisy_circles noisy_moons blobs no_structure dataset normalize dataset easier parameter selection scaler fit_transform estimate bandwidth mean shift bandwidth cluster estimate_bandwidth quantile connectivity matrix structured ward connectivity kneighbors_graph n_neighbors make connectivity symmetric connectivity connectivity connectivity compute distances distances euclidean_distances create clustering estimators cluster meanshift bandwidth bandwidth bin_seeding true two_means cluster minibatchkmeans ward_five cluster ward n_clusters connectivity connectivity spectral cluster spectralclustering mode arpack dbscan cluster dbscan eps affinity_propagation cluster affinitypropagation damping algorithm two_means affinity_propagation spectral ward_five dbscan predict cluster memberships time time algorithm spectral algorithm fit connectivity elif algorithm affinity_propagation set low preference avoid creating many clusters parameter hard set practice algorithm fit distances distances max else examples scikit learn user guide release algorithm fit time time hasattr algorithm labels_ y_pred algorithm labels_ astype int else y_pred algorithm predict plot subplot plot_num i_dataset title str algorithm split size scatter color colors y_pred tolist hasattr algorithm cluster_centers_ centers algorithm cluster_centers_ center_colors colors len centers scatter centers centers center_colors xlim ylim xticks yticks text 2fs lstrip transform gca transaxes size horizontalalignment right plot_num show figure means clustering means clustering plots display rstly means algorithm would yield using three clusters shown effect bad initialization classication process setting n_init default amount times algorithm run different centroid seeds reduced next plot displays using eight clusters would deliver nally ground truth 
4018: chapter example gallery scikit learn user guide release python source code plot_cluster_iris print __doc__ code source gael varoqueux modified documentation merge jaques grobler license bsd import numpy import pylab mpl_toolkits mplot3d import axes3d sklearn cluster import kmeans sklearn import datasets random seed centers iris datasets load_iris iris data iris target estimators k_means_iris_3 kmeans k_means_iris_8 kmeans k_means_iris_bad_init kmeans n_init init random examples scikit learn user guide release fignum name est estimators iteritems fig figure fignum figsize clf axes3d fig rect elev azim cla est fit labels est labels_ scatter labels astype float w_xaxis set_ticklabels w_yaxis set_ticklabels w_zaxis set_ticklabels set_xlabel petal width set_ylabel sepal length set_zlabel petal length fignum fignum plot ground truth fig figure fignum figsize clf axes3d fig rect elev azim cla name label setosa versicolour virginica text3d label mean label mean label mean name horizontalalignment center bbox dict alpha edgecolor facecolor reorder labels colors matching cluster results choose astype float scatter w_xaxis set_ticklabels w_yaxis set_ticklabels w_zaxis set_ticklabels set_xlabel petal width set_ylabel sepal length set_zlabel petal length show color quantization using means performs pixel wise vector quantization image summer palace china reducing number colors required show image unique colors preserving overall appearance quality example pixels represented space means used color clusters image processing literature codebook obtained means cluster centers called color palette using single byte colors addressed whereas rgb encoding requires bytes per pixel gif chapter example gallery scikit learn user guide release figure color quantization using means format example uses palette comparison quantized image using random codebook colors picked randomly also shown 
4019: examples scikit learn user guide release script output fitting estimator small sub sample data done 301s predicting color indices full image means done 353s predicting color indices full image random done 830s 
4020: python source code plot_color_quantization authors robert layton robertlayton gmail com license bsd olivier grisel olivier grisel ensta org mathieu blondel mathieu mblondel org print __doc__ import numpy import pylab sklearn cluster import kmeans sklearn metrics import euclidean_distances sklearn datasets import load_sample_image sklearn utils import shuffle time import time n_colors load summer palace photo china load_sample_image china jpg convert floats instead default bits integer coding dividing important imshow behaves works well foat data need range china array china dtype float64 load image transform numpy array original_shape tuple china shape assert image_array reshape china print fitting estimator small sub sample data time image_array_sample shuffle image_array random_state chapter example gallery scikit learn user guide release kmeans kmeans n_colors random_state fit image_array_sample print done 3fs time get labels points print predicting color indices full image means time labels kmeans predict image_array print done 3fs time codebook_random shuffle image_array random_state n_colors print predicting color indices full image random time dist euclidean_distances codebook_random image_array squared true labels_random dist argmin axis print done 3fs time def recreate_image codebook labels recreate compressed image code book labels codebook shape image zeros label_idx range range image codebook labels label_idx label_idx return image display results alongside original image figure clf axes axis title original image colors imshow china figure clf axes axis title quantized image colors means imshow recreate_image kmeans cluster_centers_ labels figure clf axes axis title quantized image colors random imshow recreate_image codebook_random labels_random show demo dbscan clustering algorithm finds core samples high density expands clusters 
4021: examples scikit learn user guide release figure demo dbscan clustering algorithm script output estimated number clusters homogeneity completeness measure adjusted rand index adjusted mutual information silhouette coefficient python source code plot_dbscan print __doc__ import numpy chapter example gallery scikit learn user guide release scipy spatial import distance sklearn cluster import dbscan sklearn import metrics sklearn datasets samples_generator import make_blobs generate sample data centers labels_true make_blobs n_samples centers centers cluster_std compute similarities distance squareform distance pdist max compute dbscan dbscan fit eps min_samples core_samples core_sample_indices_ labels labels_ number clusters labels ignoring noise present n_clusters_ len set labels labels else print estimated number clusters n_clusters_ print homogeneity metrics homogeneity_score labels_true labels print completeness metrics completeness_score labels_true labels print measure metrics v_measure_score labels_true labels print adjusted rand index metrics adjusted_rand_score labels_true labels print adjusted mutual information metrics adjusted_mutual_info_score labels_true labels print silhouette coefficient metrics silhouette_score labels metric precomputed plot result import pylab itertools import cycle close figure clf black removed used noise instead colors cycle bgrcmybgrcmybgrcmybgrcmy col zip set labels colors black used noise col markersize class_members index index argwhere labels cluster_core_samples index index core_samples labels index index class_members index index core_samples examples scikit learn user guide release markersize else markersize plot markerfacecolor col markeredgecolor markersize markersize title estimated number clusters n_clusters_ show figure feature agglomeration feature agglomeration images similiar features merged together using feature agglomeration 
4022: python source code plot_digits_agglomeration print __doc__ code source gael varoqueux modified documentation merge jaques grobler license bsd import numpy import pylab chapter example gallery scikit learn user guide release sklearn import datasets cluster sklearn feature_extraction image import grid_to_graph digits datasets load_digits images digits images reshape images len images connectivity grid_to_graph images shape agglo cluster wardagglomeration connectivity connectivity n_clusters agglo fit x_reduced agglo transform x_restored agglo inverse_transform x_reduced images_restored reshape x_restored images shape figure figsize clf subplots_adjust left right bottom top range subplot imshow images cmap gray vmax interpolation nearest xticks yticks title original data subplot imshow images_restored cmap gray vmax interpolation nearest title agglomerated data xticks yticks subplot imshow reshape agglo labels_ images shape interpolation nearest cmap spectral xticks yticks title labels figure feature agglomeration univariate selection feature agglomeration univariate selection example compares dimensionality reduction strategies univariate feature selection anova feature agglomeration ward hierarchical clustering methods compared regression problem using bayesianridge supervised estimator 
4023: examples scikit learn user guide release script output ________________________________________________________________________________ memory calling sklearn cluster hierarchical ward_tree ward_tree array 1600x1600 sparse matrix type type numpy int32 stored elements coordinate format copy true n_components ________________________________________________________ward_tree 0min ________________________________________________________________________________ memory calling sklearn cluster hierarchical ward_tree ward_tree array 1600x1600 sparse matrix type type numpy int32 stored elements coordinate format copy true n_components ________________________________________________________ward_tree 0min ________________________________________________________________________________ memory calling sklearn cluster hierarchical ward_tree ward_tree array 1600x1600 sparse matrix type type numpy int32 stored elements coordinate format copy true n_components ________________________________________________________ward_tree 0min ________________________________________________________________________________ memory calling sklearn feature_selection univariate_selection f_regression f_regression array array _____________________________________________________f_regression 0min ________________________________________________________________________________ memory calling sklearn feature_selection univariate_selection f_regression f_regression array array _____________________________________________________f_regression 0min ________________________________________________________________________________ memory calling sklearn feature_selection univariate_selection f_regression f_regression array chapter example gallery scikit learn user guide release array _____________________________________________________f_regression 0min python source code plot_feature_agglomeration_vs_univariate_selection author alexandre gramfort alexandre gramfort inria license bsd style 
4024: print __doc__ import shutil import tempfile import numpy import pylab scipy import linalg ndimage sklearn feature_extraction image import grid_to_graph sklearn import feature_selection sklearn cluster import wardagglomeration sklearn linear_model import bayesianridge sklearn pipeline import pipeline sklearn grid_search import gridsearchcv sklearn externals joblib import memory sklearn cross_validation import kfold generate data n_samples size image size roi_size snr random seed mask ones size size dtype bool coef zeros size size coef roi_size roi_size coef roi_size roi_size 
4025: random randn n_samples size smooth data ndimage gaussian_filter reshape size size sigma ravel mean axis std axis dot coef ravel noise random randn shape noise_coef linalg norm exp snr linalg norm noise noise_coef noise add noise compute coefs bayesian ridge gridsearch kfold len cross validation generator model selection ridge bayesianridge cachedir tempfile mkdtemp mem memory cachedir cachedir verbose examples scikit learn user guide release ward agglomeration followed bayesianridge grid_to_graph n_x size n_y size ward wardagglomeration n_clusters connectivity memory mem n_components clf pipeline ward ward ridge ridge select optimal number parcels grid search clf gridsearchcv clf ward__n_clusters n_jobs clf fit set best parameters coef_ clf best_estimator_ steps coef_ coef_ clf best_estimator_ steps inverse_transform coef_ coef_agglomeration_ coef_ reshape size size anova univariate feature selection followed bayesianridge f_regression mem cache feature_selection f_regression caching function anova feature_selection selectpercentile f_regression clf pipeline anova anova ridge ridge select optimal percentage features grid search clf gridsearchcv clf anova__percentile clf fit set best parameters coef_ clf best_estimator_ steps coef_ coef_ clf best_estimator_ steps inverse_transform coef_ coef_selection_ coef_ reshape size size inverse transformation plot results image close figure figsize subplot imshow coef interpolation nearest cmap rdbu_r title true weights subplot imshow coef_selection_ interpolation nearest cmap rdbu_r title feature selection subplot imshow coef_agglomeration_ interpolation nearest cmap rdbu_r title feature agglomeration subplots_adjust show attempt remove temporary cachedir dont worry fails shutil rmtree cachedir ignore_errors true figure demo means clustering handwritten digits data demo means clustering handwritten digits data example compare various initialization strategies means terms runtime quality results 
4026: chapter example gallery scikit learn user guide release ground truth known also apply different cluster quality metrics judge goodness cluster labels ground truth cluster quality metrics evaluated see clustering performance evaluation denitions discussions met rics shorthand homo compl meas ari ami silhouette full name homogeneity score completeness score measure adjusted rand index adjusted mutual information silhouette coefcient script output n_samples n_features time inertia homo compl meas ari ami silhouette n_digits _______________________________________________________________________________ init means random pca based _______________________________________________________________________________ 15s 05s 18s python source code plot_kmeans_digits examples scikit learn user guide release print __doc__ time import time import numpy import pylab sklearn import metrics sklearn cluster import kmeans sklearn datasets import load_digits sklearn decomposition import pca sklearn preprocessing import scale random seed digits load_digits data scale digits data n_samples n_features data shape n_digits len unique digits target labels digits target sample_size print n_digits n_samples n_features n_digits n_samples n_features print print init time inertia homo compl meas ari ami silhouette def bench_k_means estimator name data time estimator fit data print 2fs name time estimator inertia_ metrics homogeneity_score labels estimator labels_ metrics completeness_score labels estimator labels_ metrics v_measure_score labels estimator labels_ metrics adjusted_rand_score labels estimator labels_ metrics adjusted_mutual_info_score labels metrics silhouette_score data estimator labels_ estimator labels_ metric euclidean sample_size sample_size bench_k_means kmeans init means n_digits n_init name means data data bench_k_means kmeans init random n_digits n_init name random data data case seeding centers deterministic hence run kmeans algorithm n_init pca pca n_components n_digits fit data bench_k_means kmeans init pca components_ n_digits n_init name pca based chapter example gallery scikit learn user guide release print data data visualize results pca reduced data reduced_data pca n_components fit_transform data kmeans kmeans init means n_digits n_init fit reduced_data step size mesh decrease increase quality point mesh x_min m_max y_min y_max 
4027: plot decision boundary asign color x_min x_max reduced_data min reduced_data max y_min y_max reduced_data min reduced_data max meshgrid arange x_min x_max arange y_min y_max obtain labels point mesh use last trained model kmeans predict ravel ravel put result color plot reshape shape figure clf imshow interpolation nearest extent min max min max cmap paired aspect auto origin lower plot reduced_data reduced_data markersize plot centroids white centroids kmeans cluster_centers_ scatter centroids centroids marker linewidths color zorder title means clustering digits dataset pca reduced data centroids marked white cross xlim x_min x_max ylim y_min y_max xticks yticks show figure empirical evaluation impact means initialization examples scikit learn user guide release empirical evaluation impact means initialization evaluate ability means initializations strategies make algorithm convergence robust measured relative standard deviation inertia clustering sum distances nearest cluster center rst plot shows best inertia reached combination model kmeans minibatchkmeans init method init random init kmeans increasing values n_init parameter controls number initializations second plot demonstrate one single run minibatchkmeans estimator using init random n_init run leads bad convergence local optimum estimated centers stucked ground truth clusters dataset used evaluation grid isotropic gaussian clusters widely spaced 
4028: script output evaluation kmeans means init evaluation kmeans random init evaluation minibatchkmeans means init evaluation minibatchkmeans random init python source code plot_kmeans_stability_low_dim_dense print __doc__ author olivier grisel olivier grisel ensta org license simplified bsd import numpy chapter example gallery scikit learn user guide release import pylab import matplotlib sklearn utils import shuffle sklearn utils import check_random_state sklearn cluster import minibatchkmeans sklearn cluster import kmeans random_state random randomstate number run randomly generated dataset strategy able compute estimate standard deviation n_runs means models several random inits able trade cpu time convergence robustness n_init_range array datasets generation parameters n_samples_per_center grid_size scale n_clusters grid_size def make_data random_state n_samples_per_center grid_size scale random_state check_random_state random_state centers array range grid_size range grid_size n_clusters_true n_featues centers shape noise random_state normal scale scale size n_samples_per_center centers shape concatenate noise centers concatenate n_samples_per_center range n_clusters_true return shuffle random_state random_state part quantitative evaluation various init methods fig figure plots legends cases kmeans means kmeans random minibatchkmeans means max_no_improvement minibatchkmeans random max_no_improvement init_size factory init params cases print evaluation init factory __name__ init inertia empty len n_init_range n_runs run_id range n_runs examples scikit learn user guide release make_data run_id n_samples_per_center grid_size scale n_init enumerate n_init_range factory n_clusters init init random_state run_id n_init n_init params fit inertia run_id inertia_ errorbar n_init_range inertia mean axis inertia std axis plots append legends append init factory __name__ init xlabel n_init ylabel inertia legend plots legends title mean inertia various means init across runs n_runs part qualitative visual inspection convergence make_data random_state n_samples_per_center grid_size scale minibatchkmeans n_clusters init random n_init random_state random_state fit fig figure range n_clusters my_members labels_ color spectral float n_clusters plot my_members my_members marker color cluster_center cluster_centers_ plot cluster_center cluster_center markerfacecolor color markeredgecolor markersize title example cluster allocation single random init minibatchkmeans show figure vector quantization example vector quantization example classic image processing example lena bit grayscale bit depth sized image used illustrate means used vector quantization 
4029: chapter example gallery scikit learn user guide release python source code plot_lena_compress print __doc__ code source gael varoqueux modified documentation merge jaques grobler license bsd import numpy import scipy import pylab sklearn import cluster n_clusters random seed try lena lena except attributeerror newer versions scipy lena misc scipy import misc lena misc lena lena reshape need n_sample n_feature array k_means cluster kmeans n_clusters n_init k_means fit values k_means cluster_centers_ squeeze labels k_means labels_ create array labels values lena_compressed choose labels values lena_compressed shape lena shape vmin lena min vmax lena max examples scikit learn user guide release original lena figure figsize imshow lena cmap gray vmin vmin vmax compressed lena figure figsize imshow lena_compressed cmap gray vmin vmin vmax vmax equal bins lena regular_values linspace n_clusters regular_labels searchsorted regular_values lena regular_values regular_values regular_values mean regular_lena choose regular_labels ravel regular_values regular_lena shape lena shape figure figsize imshow regular_lena cmap gray vmin vmin vmax vmax histogram figure figsize clf axes hist bins color edgecolor yticks xticks regular_values values sort values center_1 center_2 zip values values axvline center_1 center_2 color center_1 center_2 zip regular_values regular_values axvline center_1 center_2 color linestyle show figure segmenting picture lena regions segmenting picture lena regions example uses spectral clustering graph created voxel voxel difference image break image multiple partly homogenous regions procedure spectral clustering image efcient approximate solution nding normalized graph cuts 
4030: chapter example gallery scikit learn user guide release python source code plot_lena_segmentation print __doc__ author gael varoquaux gael varoquaux normalesup org license bsd import numpy import scipy import pylab sklearn feature_extraction import image sklearn cluster import spectral_clustering lena misc lena downsample image factor lena lena lena lena lena lena lena lena lena lena convert image graph value gradient edges graph image img_to_graph lena take decreasing function gradient exponential smaller beta independent segmentation examples scikit learn user guide release actual image beta segmentation close voronoi beta eps graph data exp beta graph data lena std eps apply spectral clustering step goes much faster pyamg installed n_regions labels spectral_clustering graph n_regions labels labels reshape lena shape visualize resulting regions figure figsize imshow lena range n_regions cmap gray contour labels contours colors spectral float n_regions xticks yticks show figure demo structured ward hierarchical clustering lena image demo structured ward hierarchical clustering lena image compute segmentation image ward hierarchical clustering clustering spatially constrained order segmented region one piece 
4031: chapter example gallery scikit learn user guide release script output compute structured hierarchical clustering elaspsed time number pixels number clusters python source code plot_lena_ward_segmentation author vincent michel license bsd style 
4032: alexandre gramfort print __doc__ import time time import numpy import scipy import pylab sklearn feature_extraction image import grid_to_graph sklearn cluster import ward generate data lena misc lena downsample image factor examples scikit learn user guide release lena lena lena lena lena reshape lena define structure data pixels connected neighbors connectivity grid_to_graph lena shape compute clustering print compute structured hierarchical clustering time time n_clusters number regions ward ward n_clusters n_clusters connectivity connectivity fit label reshape ward labels_ lena shape print elaspsed time time time print number pixels label size print number clusters unique label size plot results image figure figsize imshow lena cmap gray range n_clusters contour label contours colors spectral float n_clusters xticks yticks show figure demo mean shift clustering algorithm demo mean shift clustering algorithm reference dorin comaniciu peter meer mean shift robust approach toward feature space analysis ieee transactions pattern analysis machine intelligence 
4033: chapter example gallery scikit learn user guide release script output number estimated clusters python source code plot_mean_shift print __doc__ import numpy sklearn cluster import meanshift estimate_bandwidth sklearn datasets samples_generator import make_blobs generate sample data centers make_blobs n_samples centers centers cluster_std compute clustering meanshift following bandwidth automatically detected using bandwidth estimate_bandwidth quantile n_samples meanshift bandwidth bandwidth bin_seeding true fit labels labels_ cluster_centers cluster_centers_ examples scikit learn user guide release labels_unique unique labels n_clusters_ len labels_unique print number estimated clusters n_clusters_ plot result import pylab itertools import cycle figure clf colors cycle bgrcmykbgrcmykbgrcmykbgrcmyk col zip range n_clusters_ colors my_members labels cluster_center cluster_centers plot my_members my_members col plot cluster_center cluster_center markerfacecolor col markeredgecolor markersize title estimated number clusters n_clusters_ show figure demo means clustering algorithm demo means clustering algorithm want compare performance minibatchkmeans kmeans minibatchkmeans faster gives slightly different results see mini batch means cluster set data rst kmeans minibatchkmeans plot results also plot points labelled differently two algorithms 
4034: python source code plot_mini_batch_kmeans chapter example gallery scikit learn user guide release print __doc__ import time import numpy import pylab sklearn cluster import minibatchkmeans kmeans sklearn metrics pairwise import euclidean_distances sklearn datasets samples_generator import make_blobs generate sample data random seed batch_size centers n_clusters len centers labels_true make_blobs n_samples centers centers cluster_std compute clustering means k_means kmeans init means n_init time time k_means fit t_batch time time k_means_labels k_means labels_ k_means_cluster_centers k_means cluster_centers_ k_means_labels_unique unique k_means_labels compute clustering minibatchkmeans mbk minibatchkmeans init means batch_size batch_size n_init max_no_improvement verbose time time mbk fit t_mini_batch time time mbk_means_labels mbk labels_ mbk_means_cluster_centers mbk cluster_centers_ mbk_means_labels_unique unique mbk_means_labels plot result fig figure figsize fig subplots_adjust left right bottom top colors 4eacc5 ff9c34 4e9a06 want colors cluster minibatchkmeans kmeans algorithm lets pair cluster centers per closest one 
4035: distance euclidean_distances k_means_cluster_centers mbk_means_cluster_centers squared true order distance argmin axis examples scikit learn user guide release kmeans fig add_subplot col zip range n_clusters colors my_members k_means_labels cluster_center k_means_cluster_centers plot my_members my_members markerfacecolor col marker plot cluster_center cluster_center markerfacecolor col markeredgecolor markersize set_title kmeans set_xticks set_yticks text train time 2fs ninertia t_batch k_means inertia_ minibatchkmeans fig add_subplot col zip range n_clusters colors my_members mbk_means_labels order cluster_center mbk_means_cluster_centers order plot my_members my_members markerfacecolor col marker plot cluster_center cluster_center markerfacecolor col markeredgecolor markersize set_title minibatchkmeans set_xticks set_yticks text train time 2fs ninertia t_mini_batch mbk inertia_ initialise different array false different mbk_means_labels fig add_subplot range n_clusters different k_means_labels mbk_means_labels order identic logical_not different plot identic identic markerfacecolor bbbbbb marker plot different different markerfacecolor marker set_title difference set_xticks set_yticks show spectral clustering image segmentation example image connected circles generated spectral clustering used separate circles settings spectral clustering approach solves problem know normalized graph cuts image seen graph connected voxels spectral clustering algorithm amounts choosing graph cuts dening regions minimizing ratio gradient along cut volume region algorithm tries balance volume balance region sizes take circles different sizes chapter example gallery scikit learn user guide release figure spectral clustering image segmentation segmentation fails addition useful information intensity image gradient choose perform spectral clustering graph weakly informed gradient close performing voronoi partition graph addition use mask objects restrict graph outline objects example interested separating objects one background 
4036: examples scikit learn user guide release python source code plot_segmentation_toy print __doc__ authors license bsd emmanuelle gouillart emmanuelle gouillart normalesup org gael varoquaux gael varoquaux normalesup org import numpy import pylab sklearn feature_extraction import image sklearn cluster import spectral_clustering indices center1 center2 chapter example gallery scikit learn user guide release center3 center4 radius1 radius2 radius3 radius4 circle1 center1 center1 radius1 circle2 center2 center2 radius2 circle3 center3 center3 radius3 circle4 center4 center4 radius4 circles img circle1 circle2 circle3 circle4 mask img astype bool img img astype float img random randn img shape convert image graph value gradient edges graph image img_to_graph img mask mask take decreasing function gradient take weakly dependant gradient segmentation close voronoi graph data exp graph data graph data std force solver arpack since amg numerically unstable example labels spectral_clustering graph mode arpack label_im ones mask shape label_im mask labels matshow img matshow label_im circles img circle1 circle2 mask img astype bool img img astype float img random randn img shape graph image img_to_graph img mask mask graph data exp graph data graph data std labels spectral_clustering graph mode arpack label_im ones mask shape label_im mask labels matshow img matshow label_im show examples scikit learn user guide release figure hierarchical clustering structured unstructured ward hierarchical clustering structured unstructured ward example builds swiss roll dataset runs hierarchical clustering position rst step hierarchical clustering without connectivity constraints structure solely based distance whereas second step clustering restricted nearest neighbors graph hierarchical clustering structure prior clusters learned without connectivity constraints respect structure swiss roll extend across different folds manifolds opposite opposing connectivity constraints clusters form nice parcellation swiss roll 
4037: script output compute unstructured hierarchical clustering elapsed time number points chapter example gallery scikit learn user guide release compute structured hierarchical clustering elapsed time number points python source code plot_ward_structured_vs_unstructured authors vincent michel license bsd alexandre gramfort gael varoquaux print __doc__ import time time import numpy import pylab import mpl_toolkits mplot3d axes3d sklearn cluster import ward sklearn datasets samples_generator import make_swiss_roll generate data swiss roll dataset n_samples noise make_swiss_roll n_samples noise make thinner compute clustering print compute unstructured hierarchical clustering time time ward ward n_clusters fit label ward labels_ print elapsed time time time print number points label size plot result fig figure axes3d fig view_init unique label plot3d label label label color jet float max label title without connectivity constraints define structure data nearest neighbors sklearn neighbors import kneighbors_graph connectivity kneighbors_graph n_neighbors compute clustering print compute structured hierarchical clustering time time ward ward n_clusters connectivity connectivity fit examples scikit learn user guide release label ward labels_ print elapsed time time time print number points label size plot result fig figure axes3d fig view_init unique label plot3d label label label color jet float max label title connectivity constraints show covariance estimation examples concerning sklearn covariance package 
4038: figure ledoit wolf covariance simple estimation ledoit wolf covariance simple estimation usual covariance maximum likelihood estimate regularized using shrinkage ledoit wolf proposed close formula compute asymptotical optimal shrinkage parameter minimizing mse criterion yielding ledoit wolf covariance estimate chen proposed improvement ledoit wolf shrinkage parameter oas coefcient whose convergence signicantly better assumption data gaussian example compute likelihood unseen data different values shrinkage parameter highlighting oas estimates ledoit wolf estimate stays close likelihood criterion optimal value artifact method since asymptotic working small number observations oas estimate deviates likelihood criterion optimal value better approximate mse optimal value especially small number observations 
4039: chapter example gallery scikit learn user guide release python source code plot_covariance_estimation print __doc__ import numpy import pylab scipy import linalg generate sample data n_features n_samples base_x_train random normal size n_samples n_features base_x_test random normal size n_samples n_features color samples coloring_matrix random normal size n_features n_features x_train dot base_x_train coloring_matrix x_test dot base_x_test coloring_matrix compute ledoit wolf covariances grid shrinkages sklearn covariance import ledoitwolf oas shrunkcovariance log_likelihood empirical_covariance ledoit wolf optimal shrinkage coefficient estimate examples scikit learn user guide release ledoitwolf loglik_lw fit x_train assume_centered true score x_test assume_centered true oas coefficient estimate oas loglik_oa fit x_train assume_centered true score x_test assume_centered true spanning range possible shrinkage coefficient values shrinkages logspace negative_logliks shrunkcovariance shrinkage fit x_train assume_centered true score x_test assume_centered true shrinkages getting likelihood real model real_cov dot coloring_matrix coloring_matrix emp_cov empirical_covariance x_train loglik_real log_likelihood emp_cov linalg inv real_cov plot results figure title regularized covariance likelihood shrinkage coefficient xlabel shrinkage ylabel negative log likelihood range shrinkage curve loglog shrinkages negative_logliks real likelihood reference bug hlines linestyle breaks older versions matplotlib hlines loglik_real xlim xlim color red plot xlim loglik_real label real covariance likelihood label real covariance likelihood linestyle adjust view lik_max amax negative_logliks lik_min amin negative_logliks ylim0 lik_min log ylim ylim ylim1 lik_max log lik_max lik_min likelihood vlines shrinkage_ ylim0 loglik_lw color linewidth label ledoit wolf estimate oas likelihood vlines shrinkage_ ylim0 loglik_oa color orange linewidth label oas estimate ylim ylim0 ylim1 xlim shrinkages shrinkages legend show ledoit wolf oas estimation usual covariance maximum likelihood estimate regularized using shrinkage ledoit wolf proposed close formula compute asymptotical optimal shrinkage parameter minimizing mse criterion yielding chapter example gallery scikit learn user guide release figure ledoit wolf oas estimation ledoit wolf covariance estimate chen proposed improvement ledoit wolf shrinkage parameter oas coefcient whose convergence signicantly better assumption data gaussian example inspired chens publication shows comparison estimated mse oas methods using gaussian distributed data shrinkage algorithms mmse covariance estimation chen ieee trans sign proc volume issue october 
4040: python source code plot_lw_vs_oas print __doc__ examples scikit learn user guide release import numpy import pylab scipy linalg import toeplitz cholesky sklearn covariance import ledoitwolf oas n_features simulation covariance matrix process real_cov toeplitz arange n_features coloring_matrix cholesky real_cov n_samples_range arange repeat lw_mse zeros n_samples_range size repeat oa_mse zeros n_samples_range size repeat lw_shrinkage zeros n_samples_range size repeat oa_shrinkage zeros n_samples_range size repeat n_samples enumerate n_samples_range range repeat dot random normal size n_samples n_features coloring_matrix ledoitwolf store_precision false fit assume_centered true lw_mse error_norm real_cov scaling false lw_shrinkage shrinkage_ oas store_precision false fit assume_centered true oa_mse error_norm real_cov scaling false oa_shrinkage shrinkage_ plot mse subplot errorbar n_samples_range lw_mse mean yerr lw_mse std label ledoit wolf color errorbar n_samples_range oa_mse mean yerr oa_mse std label oas color ylabel squared error legend loc upper right title comparison covariance estimators xlim plot shrinkage coefficient subplot errorbar n_samples_range lw_shrinkage mean yerr lw_shrinkage std label ledoit wolf color errorbar n_samples_range oa_shrinkage mean yerr oa_shrinkage std label oas color xlabel n_samples ylabel shrinkage legend loc lower right ylim ylim ylim ylim xlim show chapter example gallery scikit learn user guide release figure robust covariance estimation mahalanobis distances relevance robust covariance estimation mahalanobis distances relevance gaussian ditributed data distance observation mode distribution computed using mahalanobis distance cid location covariance underlying gaussian distribution practice replaced estimates usual covariance maximum likelihood estimate sensitive presence outliers data set therefor corresponding mahalanobis distances one would better use robust estimator covariance garanty estimation resistant errorneous observations data set associated mahalanobis distances accurately reect true organisation observations minimum covariance determinant estimator robust high breakdown point used estimate covariance matrix highly contaminated datasets math rac n_samples n_features outliers estimator covariance idea math rac n_samples n_features observations whose empirical covariance smallest determinant yielding pure subset observations compute standards estimates location covariance minimum covariance determinant estimator mcd introduced rousseuw example illustrates mahalanobis distances affected outlying data observations drawn contaminating distribution distinguishable observations comming real gaussian distribution one may want work using mcd based mahalanobis distances two populations become distinguish able associated applications outliers detection observations ranking clustering vizualisation purpose cubique root mahalanobis distances represented boxplot wilson hilferty suggest rousseeuw least median squares regression stat ass wilson hilferty distribution chi square proceedings national academy sciences united states america 
4041: examples scikit learn user guide release python source code plot_mahalanobis_distances print __doc__ import numpy import pylab sklearn covariance import empiricalcovariance mincovdet n_samples n_outliers n_features generate data gen_cov eye n_features gen_cov dot random randn n_samples n_features gen_cov add outliers outliers_cov eye n_features outliers_cov arange n_features arange n_features n_outliers dot random randn n_outliers n_features outliers_cov fit minimum covariance determinant mcd robust estimator data robust_cov mincovdet fit compare estimators learnt full data set true parameters chapter example gallery scikit learn user guide release emp_cov empiricalcovariance fit display results fig figure subplots_adjust hspace wspace top bottom show data set subfig1 subplot inlier_plot subfig1 scatter color black label inliers outlier_plot subfig1 scatter n_outliers n_outliers subfig1 set_xlim subfig1 get_xlim subfig1 set_title mahalanobis distances contaminated data set color red label outliers show contours distance functions meshgrid linspace xlim xlim linspace ylim ylim ravel ravel mahal_emp_cov emp_cov mahalanobis mahal_emp_cov mahal_emp_cov reshape shape emp_cov_contour subfig1 contour sqrt mahal_emp_cov cmap pubu_r linestyles dashed mahal_robust_cov robust_cov mahalanobis mahal_robust_cov mahal_robust_cov reshape shape robust_contour subfig1 contour sqrt mahal_robust_cov cmap ylorbr_r linestyles dotted subfig1 legend emp_cov_contour collections robust_contour collections inlier_plot outlier_plot mle dist robust dist inliers outliers loc upper right borderaxespad xticks yticks plot scores point emp_mahal emp_cov mahalanobis mean subfig2 subplot subfig2 boxplot emp_mahal n_outliers emp_mahal n_outliers widths subfig2 plot ones n_samples n_outliers emp_mahal n_outliers markeredgewidth subfig2 plot ones n_outliers emp_mahal n_outliers markeredgewidth subfig2 axes set_xticklabels inliers outliers size subfig2 set_ylabel sqrt mahal dist size subfig2 set_title non robust estimates maximum likelihood yticks robust_mahal robust_cov mahalanobis robust_cov location_ subfig3 subplot subfig3 boxplot robust_mahal n_outliers robust_mahal n_outliers subfig3 plot ones n_samples n_outliers widths examples scikit learn user guide release robust_mahal n_outliers markeredgewidth subfig3 plot ones n_outliers robust_mahal n_outliers markeredgewidth subfig3 axes set_xticklabels inliers outliers size subfig3 set_ylabel sqrt mahal dist size subfig3 set_title robust estimates minimum covariance determinant yticks show figure outlier detection several methods 
4042: outlier detection several methods 
4043: example illustrates two ways performing novelty outlier detection amount contamination known based robust estimator covariance assuming data gaussian distributed performs better one class svm case 
4044: using one class svm ability capture shape data set hence performing better data strongly non gaussian two well separated clusters ground truth inliers outliers given points colors orange lled area indicates points reported outliers method assume know fraction outliers datasets thus rather using predict method objects set threshold decision_function separate corresponding fraction 
4045: chapter example gallery scikit learn user guide release python source code plot_outlier_detection print __doc__ import numpy import pylab import matplotlib font_manager scipy import stats sklearn import svm sklearn covariance import ellipticenvelope example settings n_samples outliers_fraction clusters_separation define two outlier detection tools compared classifiers one class svm svm oneclasssvm outliers_fraction kernel rbf gamma robust covariance estimator ellipticenvelope contamination compare given classifiers given settings meshgrid linspace linspace n_inliers int outliers_fraction n_samples n_outliers int outliers_fraction n_samples ground_truth ones n_samples dtype int ground_truth n_outliers fit problem varying cluster separation offset enumerate clusters_separation random seed data generation random randn n_inliers offset random randn n_inliers offset add outliers random uniform low high size n_outliers fit model one class svm figure figsize clf_name clf enumerate classifiers iteritems fit data tag outliers clf fit y_pred clf decision_function ravel threshold stats scoreatpercentile y_pred examples scikit learn user guide release outliers_fraction y_pred y_pred threshold n_errors y_pred ground_truth sum plot levels lines points clf decision_function ravel ravel reshape shape subplot subplot subplot set_title outlier detection subplot contourf levels linspace min threshold cmap blues_r subplot contour levels threshold linewidths colors red subplot contourf levels threshold max colors orange subplot scatter n_outliers n_outliers white subplot scatter n_outliers n_outliers black subplot axis tight subplot legend collections learned decision function true inliers true outliers prop matplotlib font_manager fontproperties size subplot set_xlabel errors clf_name n_errors subplot set_xlim subplot set_ylim subplots_adjust show figure robust empirical covariance estimate robust empirical covariance estimate usual covariance maximum likelihood estimate sensitive presence outliers data set case one would better use robust estimator covariance garanty estimation resistant errorneous observations data set minimum covariance determinant estimator robust high breakdown point used estimate covariance matrix highly contaminated datasets math rac n_samples n_features outliers estimator covariance idea math rac n_samples n_features observations whose empirical covariance smallest determinant yielding pure subset observations compute standards estimates location covariance correction step aiming compensating fact estimates learnt portion initial data end robust estimates data set location covariance minimum covariance determinant estimator mcd introduced rousseuw example compare estimation errors made using three types location covariance estimates contaminated gaussian distributed data sets chapter example gallery scikit learn user guide release mean empirical covariance full dataset break soon outliers data set robust mcd low error provided n_samples n_features mean empirical covariance observations known good ones consid ered perfect mcd estimation one trust implementation comparing case 
4046: rousseeuw least median squares regression stat ass johanna hardin david rocke journal computational graphical statistics december 
4047: python source code plot_robust_vs_empirical_covariance print __doc__ import numpy import pylab import matplotlib font_manager sklearn covariance import empiricalcovariance mincovdet example settings n_samples n_features repeat examples scikit learn user guide release range_n_outliers concatenate linspace n_samples linspace n_samples n_samples definition arrays store results err_loc_mcd zeros range_n_outliers size repeat err_cov_mcd zeros range_n_outliers size repeat err_loc_emp_full zeros range_n_outliers size repeat err_cov_emp_full zeros range_n_outliers size repeat err_loc_emp_pure zeros range_n_outliers size repeat err_cov_emp_pure zeros range_n_outliers size repeat computation n_outliers enumerate range_n_outliers range repeat generate data random randn n_samples n_features add outliers outliers_index random permutation n_samples n_outliers outliers_offset random randint size n_outliers n_features outliers_index outliers_offset inliers_mask ones n_samples astype bool inliers_mask outliers_index false fit minimum covariance determinant mcd robust estimator data mincovdet fit compare raw robust estimates true location covariance err_loc_mcd sum location_ err_cov_mcd error_norm eye n_features compare estimators learnt full data set true parameters err_loc_emp_full sum mean err_cov_emp_full empiricalcovariance fit error_norm eye n_features compare empirical covariance learnt pure data set perfect mcd pure_x inliers_mask pure_location pure_x mean pure_emp_cov empiricalcovariance fit pure_x err_loc_emp_pure sum pure_location err_cov_emp_pure pure_emp_cov error_norm eye n_features display results font_prop matplotlib font_manager fontproperties size subplot errorbar range_n_outliers err_loc_mcd mean yerr err_loc_mcd std sqrt repeat label robust location color errorbar range_n_outliers err_loc_emp_full mean yerr err_loc_emp_full std sqrt repeat label full data set mean color green errorbar range_n_outliers err_loc_emp_pure mean yerr err_loc_emp_pure std sqrt repeat label pure data set mean color black title influence outliers location estimation ylabel error hat legend loc upper left prop font_prop chapter example gallery scikit learn user guide release subplot x_size range_n_outliers size errorbar range_n_outliers err_cov_mcd mean yerr err_cov_mcd std label robust covariance mcd color errorbar range_n_outliers x_size err_cov_emp_full mean x_size yerr err_cov_emp_full std x_size label full data set empirical covariance color green plot range_n_outliers x_size x_size err_cov_emp_full mean x_size x_size color green errorbar range_n_outliers err_cov_emp_pure mean yerr err_cov_emp_pure std label pure data set empirical covariance color black title influence outliers covariance estimation xlabel amount contamination ylabel rmse legend loc upper center prop font_prop show figure sparse inverse covariance estimation sparse inverse covariance estimation using graphlasso estimator learn covariance sparse precision small number samples estimate probabilistic model gaussian model estimating precision matrix inverse covari ance matrix important estimating covariance matrix indeed gaussian model parametrized precision matrix favorable recovery conditions sample data model sparse inverse covariance matrix addition ensure data much correlated limiting largest coefcient precision matrix small coefcients precision matrix cannot recovered addition small number observations easier recover correlation matrix rather covariance thus scale time series number samples slightly larger number dimensions thus empirical covariance still invertible however observations strongly correlated empirical covariance matrix ill conditioned result inverse empirical precision matrix far ground truth use shrinkage ledoit wolf estimator number samples small need shrink lot result ledoit wolf precision fairly close ground truth precision far diagonal diagonal structure lost penalized estimator recover part diagonal structure able recover exact sparsity pattern detects many non zero coefcients however highest non zero coefcients estimated correspond non zero coefcients ground truth finally coefcients learns sparse precision 
4048: examples scikit learn user guide release precision estimate biased toward zero penalty smaller corresponding ground truth value seen gure note color range precision matrices tweeked improve readibility gure full range values empirical precision displayed alpha parameter graphlasso setting sparsity model set internal cross validation graphlassocv seen gure grid compute cross validation score iteratively rened neighborhood maximum 
4049: python source code plot_sparse_cov print __doc__ author gael varoquaux gael varoquaux inria license bsd style copyright inria import numpy scipy import linalg sklearn datasets import make_sparse_spd_matrix sklearn covariance import graphlassocv ledoit_wolf import pylab generate data n_samples n_features prng random randomstate prec make_sparse_spd_matrix n_features alpha smallest_coef largest_coef random_state prng cov linalg inv prec sqrt diag cov cov cov newaxis chapter example gallery scikit learn user guide release prec prec newaxis prng multivariate_normal zeros n_features cov size n_samples mean axis std axis estimate covariance emp_cov dot n_samples model graphlassocv model fit cov_ model covariance_ prec_ model precision_ lw_cov_ ledoit_wolf lw_prec_ linalg inv lw_cov_ plot results figure figsize subplots_adjust left right plot covariances covs empirical emp_cov ledoit wolf lw_cov_ graphlasso cov_ true cov vmax cov_ max name this_cov enumerate covs subplot imshow this_cov interpolation nearest vmin vmax vmax vmax cmap rdbu_r xticks yticks title covariance name plot precisions precs empirical linalg inv emp_cov ledoit wolf lw_prec_ graphlasso prec_ true prec vmax prec_ max name this_prec enumerate precs subplot imshow masked_equal this_prec interpolation nearest vmin vmax vmax vmax cmap rdbu_r xticks yticks title precision name set_axis_bgcolor plot model selection metric figure figsize axes plot model cv_alphas_ mean model cv_scores axis axvline model alpha_ color title model selection ylabel cross validation score xlabel alpha examples scikit learn user guide release show dataset examples examples concerning sklearn datasets package 
4050: figure digit dataset digit dataset dataset made 8x8 images image like one shown hand written digit order ultilise 8x8 gure like wed rst transform feature vector lengh see information dataset 
4051: python source code plot_digits_last_image print __doc__ code source gael varoqueux modified documentation merge jaques grobler license bsd sklearn import datasets import pylab load digits dataset chapter example gallery scikit learn user guide release digits datasets load_digits display first digit figure figsize imshow digits images cmap gray_r interpolation nearest show figure iris dataset iris dataset data sets consists different types irises setosa versicolour virginica petal sepal length stored 150x4 numpy ndarray rows samples columns sepal length sepal width petal length petal width plot uses rst two features see information dataset 
4052: python source code plot_iris_dataset print __doc__ code source gael varoqueux modified documentation merge jaques grobler license bsd import pylab sklearn import datasets import data play iris datasets load_iris examples scikit learn user guide release iris data take first two features iris target x_min x_max min max y_min y_max min max figure figsize clf plot also training points scatter cmap paired xlabel sepal length ylabel sepal width xlim x_min x_max ylim y_min y_max xticks yticks show figure plot randomly generated classication dataset plot randomly generated classication dataset plot several randomly generated classication datasets example illustrates datasets make_classication function three binary two multi class classication datasets generated different numbers informative features clusters per class 
4053: chapter example gallery scikit learn user guide release python source code plot_random_dataset print __doc__ import pylab sklearn datasets import make_classification figure figsize subplots_adjust bottom top left right subplot title one informative feature one cluster fontsize small make_classification n_features n_redundant n_informative n_clusters_per_class scatter marker subplot title two informative features one cluster fontsize small make_classification n_features n_redundant n_informative n_clusters_per_class scatter marker subplot title two informative features two clusters fontsize small make_classification n_features n_redundant n_informative examples scikit learn user guide release scatter marker subplot title multi class two informative features one cluster fontsize small make_classification n_features n_redundant n_informative n_clusters_per_class n_classes scatter marker show decomposition examples concerning sklearn decomposition package 
4054: figure faces dataset decompositions faces dataset decompositions example applies olivetti faces dataset different unsupervised matrix decomposition dimension reduction methods module sklearn decomposition see documentation chapter decomposing signals components matrix factorization problems 
4055: chapter example gallery scikit learn user guide release examples scikit learn user guide release script output dataset consists faces extracting top eigenfaces randomizedpca done 395s extracting top non negative components nmf done 598s extracting top independent components fastica done 912s extracting top sparse comp minibatchsparsepca done 705s extracting top minibatchdictionarylearning done 368s extracting top cluster centers minibatchkmeans done 447s python source code plot_faces_decomposition print __doc__ authors vlad niculae alexandre gramfort license bsd import logging time import time numpy random import randomstate import pylab sklearn datasets import fetch_olivetti_faces chapter example gallery scikit learn user guide release sklearn cluster import minibatchkmeans sklearn import decomposition display progress logs stdout logging basicconfig level logging info format asctime levelname message n_row n_col n_components n_row n_col image_shape rng randomstate load faces data dataset fetch_olivetti_faces shuffle true random_state rng faces dataset data n_samples n_features faces shape global centering faces_centered faces faces mean axis local centering faces_centered faces_centered mean axis reshape n_samples print dataset consists faces n_samples def plot_gallery title images figure figsize n_col n_row suptitle title size comp enumerate images subplot n_row n_col vmax max comp max comp min imshow comp reshape image_shape cmap gray interpolation nearest vmin vmax vmax vmax xticks yticks subplots_adjust list different estimators whether center transpose problem whether transformer uses clustering api estimators eigenfaces randomizedpca decomposition randomizedpca n_components n_components whiten true true false non negative components nmf decomposition nmf n_components n_components init nndsvda beta tol sparseness components false false independent components fastica decomposition fastica n_components n_components whiten true true true max_iter examples scikit learn user guide release sparse comp minibatchsparsepca decomposition minibatchsparsepca n_components n_components alpha n_iter chunk_size random_state rng true false minibatchdictionarylearning decomposition minibatchdictionarylearning n_atoms alpha n_iter chunk_size random_state rng true false cluster centers minibatchkmeans minibatchkmeans n_components tol batch_size max_iter random_state rng true false plot sample input data plot_gallery first centered olivetti faces faces_centered n_components estimation plot name estimator center transpose estimators print extracting top n_components name time data faces center data faces_centered transpose data data estimator fit data train_time time print done 3fs train_time hasattr estimator cluster_centers_ components_ estimator cluster_centers_ else components_ estimator components_ transpose components_ components_ plot_gallery train time 1fs name train_time components_ n_components show blind source separation using fastica independent component analysis ica used estimate sources given noisy measurements imagine instruments playing simultaneously microphones recording mixed signals ica used recover sources played instrument 
4056: chapter example gallery scikit learn user guide release figure blind source separation using fastica python source code plot_ica_blind_source_separation print __doc__ import numpy import pylab sklearn decomposition import fastica generate sample data random seed n_samples time linspace n_samples sin time signal sinusoidal signal examples scikit learn user guide release sign sin time signal square signal random normal size shape add noise std axis standardize data mix data array mixing matrix dot generate observations compute ica ica fastica ica fit transform get estimated sources ica get_mixing_matrix get estimated mixing matrix assert allclose dot plot results figure subplot plot title true sources subplot plot title observations mixed signal subplot plot title ica estimated sources subplots_adjust show figure fastica point clouds fastica point clouds illustrate visually results independent component analysis ica principal component analysis pca feature space representing ica feature space gives view geometric ica ica algorithm nds directions feature space corresponding projections high non gaussianity directions need orthogonal original feature space orthogonal whitened feature space directions correspond variance pca hand nds orthogonal directions raw feature space correspond directions accounting maximum variance simulate independent sources using highly non gaussian process student low number degrees freedom top left gure mix create observations top right gure raw observation space rections identied pca represented green vectors represent signal pca space whitening chapter example gallery variance corresponding pca vectors lower left running ica corresponds nding rotation space identify directions largest non gaussianity lower right 
4057: scikit learn user guide release python source code plot_ica_vs_pca print __doc__ authors alexandre gramfort gael varoquaux license bsd import numpy import pylab sklearn decomposition import pca fastica generate sample data rng random randomstate rng standard_t size 
4058: mix data array mixing matrix dot generate observations examples scikit learn user guide release pca pca s_pca_ pca fit transform ica fastica random_state rng s_ica_ ica fit transform estimate sources s_ica_ s_ica_ std axis plot results def plot_samples axis_list none scatter marker linewidths zorder axis_list none colors color axis zip colors axis_list axis axis std x_axis y_axis axis trick get legend work plot x_axis y_axis linewidth color color quiver x_axis y_axis x_axis y_axis zorder width quiver x_axis y_axis zorder width scale color color hlines vlines xlim ylim xlabel ylabel subplot plot_samples std title true independent sources axis_list pca components_ ica get_mixing_matrix subplot plot_samples std axis_list axis_list legend pca ica loc upper left title observations subplot plot_samples s_pca_ std s_pca_ axis title pca scores subplot plot_samples s_ica_ std s_ica_ title ica estimated sources subplots_adjust show chapter example gallery scikit learn user guide release figure image denoising using dictionary learning image denoising using dictionary learning example comparing effect reconstructing noisy fragments lena using online dictionary learning various transform methods dictionary tted non distorted left half image subsequently used reconstruct right half common practice evaluating results image denoising looking difference recon struction original image reconstruction perfect look like gaussian noise seen plots results orthogonal matching pursuit omp two non zero coefcients bit less biased keeping one edges look less prominent addition closer ground truth frobenius norm result least angle regression much strongly biased difference reminiscent local intensity value original image thresholding clearly useful denoising show produce suggestive output high speed thus useful tasks object classication performance necessarily related visualisation 
4059: examples scikit learn user guide release script output distorting image extracting clean patches done 27s learning dictionary done 67s extracting noisy patches done 19s orthogonal matching pursuit atom done 15s orthogonal matching pursuit chapter example gallery scikit learn user guide release atoms done 46s least angle regression atoms done 56s thresholding alpha done 97s 
4060: python source code plot_image_denoising print __doc__ time import time import pylab import numpy scipy misc import lena sklearn decomposition import minibatchdictionarylearning sklearn feature_extraction image import extract_patches_2d sklearn feature_extraction image import reconstruct_from_patches_2d load lena image extract patches lena lena downsample higher speed lena lena lena lena lena lena height width lena shape distort right half image print distorting image distorted lena copy distorted height random randn width height extract clean patches left half image print extracting clean patches time patch_size data extract_patches_2d distorted height patch_size data data reshape data shape data mean data axis data std data axis print done 2fs time learn dictionary clean patches print learning dictionary time dico minibatchdictionarylearning n_atoms alpha n_iter dico fit data components_ time print done 2fs examples scikit learn user guide release figure figsize comp enumerate subplot imshow comp reshape patch_size cmap gray_r interpolation nearest xticks yticks suptitle dictionary learned lena patches train time 1fs patches len data fontsize subplots_adjust display distorted image def show_with_diff image reference title helper function display denoising figure figsize subplot title image imshow image vmin vmax cmap gray interpolation nearest xticks yticks subplot difference image reference title difference norm sqrt sum difference imshow difference vmin vmax cmap puor interpolation nearest xticks yticks suptitle title size subplots_adjust show_with_diff distorted lena distorted image extract noisy patches reconstruct using dictionary print extracting noisy patches time data extract_patches_2d distorted height patch_size data data reshape data shape intercept mean data axis data intercept print done 2fs time transform_algorithms orthogonal matching pursuit atom omp transform_n_nonzero_coefs orthogonal matching pursuit atoms omp transform_n_nonzero_coefs least angle regression atoms lars thresholding alpha threshold transform_alpha transform_n_nonzero_coefs reconstructions chapter example gallery scikit learn user guide release title transform_algorithm kwargs transform_algorithms print title reconstructions title lena copy time dico set_params transform_algorithm transform_algorithm kwargs code dico transform data patches dot code transform_algorithm threshold patches patches min patches patches max patches intercept patches patches reshape len data patch_size transform_algorithm threshold patches patches min patches patches max reconstructions title height reconstruct_from_patches_2d patches width height time print done 2fs show_with_diff reconstructions title lena title time 1fs show figure kernel pca kernel pca example shows kernel pca able projection data makes data linearly separable 
4061: examples scikit learn user guide release python source code plot_kernel_pca print __doc__ authors mathieu blondel andreas mueller license bsd import numpy import pylab sklearn decomposition import pca kernelpca sklearn datasets import make_circles random seed make_circles n_samples factor noise kpca kernelpca kernel rbf fit_inverse_transform true gamma x_kpca kpca fit_transform x_back kpca inverse_transform x_kpca pca pca x_pca pca fit_transform plot results chapter example gallery scikit learn user guide release figure subplot aspect equal title original space reds blues plot reds reds plot blues blues xlabel x_1 ylabel x_2 meshgrid linspace linspace x_grid array ravel ravel projection first principal component phi space z_grid kpca transform x_grid reshape shape contour z_grid colors grey linewidths origin lower subplot aspect equal plot x_pca reds x_pca reds plot x_pca blues x_pca blues title projection pca xlabel 1st principal component ylabel 2nd component subplot aspect equal plot x_kpca reds x_kpca reds plot x_kpca blues x_kpca blues title projection kpca xlabel 1st principal component space induced phi ylabel 2nd component subplot aspect equal plot x_back reds x_back reds plot x_back blues x_back blues title original space inverse transform xlabel x_1 ylabel x_2 subplots_adjust show figure principal component analysis principal component analysis gures aid illustrating point cloud one direction pca would come choose direction 
4062: examples scikit learn user guide release python source code plot_pca_3d print __doc__ code source gael varoqueux modified documentation merge jaques grobler license bsd import pylab import numpy scipy import stats mpl_toolkits mplot3d import axes3d exp random seed def pdf return stats norm scale pdf stats norm scale pdf random normal scale size random normal scale size random normal scale size len density pdf pdf pdf_z pdf density pdf_z norm sqrt var var norm norm chapter example gallery scikit learn user guide release plot figures def plot_figs fig_num elev azim fig figure fig_num figsize clf axes3d fig rect elev elev azim azim scatter density marker alpha pca_score linalg svd full_matrices false x_pca_axis y_pca_axis z_pca_axis pca_score pca_score min x_pca_axis y_pca_axis z_pca_axis x_pca_plane x_pca_axis x_pca_axis y_pca_plane y_pca_axis y_pca_axis z_pca_plane z_pca_axis z_pca_axis x_pca_plane shape y_pca_plane shape z_pca_plane shape plot_surface x_pca_plane y_pca_plane z_pca_plane w_xaxis set_ticklabels w_yaxis set_ticklabels w_zaxis set_ticklabels elev azim plot_figs elev azim elev azim plot_figs elev azim show figure pca example iris data set examples scikit learn user guide release pca example iris data set python source code plot_pca_iris print __doc__ code source gael varoqueux license bsd import numpy import pylab mpl_toolkits mplot3d import axes3d sklearn import decomposition sklearn import datasets random seed centers iris datasets load_iris iris data iris target fig figure figsize clf axes3d fig rect elev azim cla pca decomposition pca n_components pca fit pca transform name label setosa versicolour virginica text3d label mean label mean label mean name chapter example gallery scikit learn user guide release horizontalalignment center bbox dict alpha edgecolor facecolor reorder labels colors matching cluster results choose astype float scatter cmap spectral x_surf min max min max y_surf max max min min x_surf array x_surf y_surf array y_surf pca transform pca components_ pca transform pca components_ w_xaxis set_ticklabels w_yaxis set_ticklabels w_zaxis set_ticklabels show figure comparison lda pca projection iris dataset comparison lda pca projection iris dataset iris dataset represents kind iris owers setosa versicolour virginica attributes sepal length sepal width petal length petal width principal component analysis pca applied data identies combination attributes principal compo nents directions feature space account variance data plot different samples rst principal components linear discriminant analysis lda tries identify attributes account variance classes particular lda constrast pca supervised method using known class labels 
4063: examples scikit learn user guide release script output explained variance ratio first two components python source code plot_pca_vs_lda print __doc__ import pylab sklearn import datasets sklearn decomposition import pca sklearn lda import lda iris datasets load_iris iris data iris target target_names iris target_names pca pca n_components x_r pca fit transform lda lda n_components x_r2 lda fit transform percentage variance explained components print explained variance ratio first two components pca explained_variance_ratio_ chapter example gallery scikit learn user guide release figure target_name zip rgb target_names scatter x_r x_r label target_name legend title pca iris dataset figure target_name zip rgb target_names scatter x_r2 x_r2 label target_name legend title lda iris dataset show figure sparse coding precomputed dictionary sparse coding precomputed dictionary transform signal sparse combination ricker wavelets example visually compares different sparse coding methods using sklearn decomposition sparsecoder estimator ricker also known mexican hat second derivative gaussian particularily good kernel represent piecewise constant signals like one therefore seen much adding different widths atoms matters therefore motivates learning dictionary best type signals richer dictionary right larger size heavier subsampling performed order stay order magnitude 
4064: examples scikit learn user guide release python source code plot_sparse_coding print __doc__ import numpy import matplotlib pylab sklearn decomposition import sparsecoder def ricker_function resolution center width discrete sub sampled ricker mexican hat wavelet linspace resolution resolution sqrt width center width exp center width return def ricker_matrix width resolution n_atoms dictionary ricker mexican hat wavelets centers linspace resolution n_atoms empty n_atoms resolution center enumerate centers ricker_function resolution center width sqrt sum axis newaxis return resolution subsampling subsampling factor width n_atoms resolution subsampling compute wavelet dictionary d_fixed ricker_matrix width width resolution resolution n_atoms n_atoms d_multi tuple ricker_matrix width resolution resolution n_atoms floor n_atoms generate signal linspace resolution resolution first_quarter resolution first_quarter logical_not first_quarter 
4065: list different sparse coding methods following format title transform_algorithm transform_alpha transform_n_nozero_coefs estimators omp omp none lasso lasso_cd none figure figsize subplot title enumerate zip d_fixed d_multi fixed width multiple widths subplot subplot title sparse coding dictionary title plot dotted label original signal wavelet approximation chapter example gallery scikit learn user guide release title algo alpha n_nonzero estimators coder sparsecoder dictionary transform_n_nonzero_coefs n_nonzero transform_alpha alpha transform_algorithm algo coder transform density len flatnonzero ravel dot squared_error sum plot label nonzero coefs error title density squared_error soft thresholding debiasing coder sparsecoder dictionary transform_algorithm threshold transform_alpha coder transform idx idx linalg lstsq idx ravel dot squared_error sum plot label thresholding debiasing nonzero coefs error len idx squared_error axis tight legend subplots_adjust show ensemble methods examples concerning sklearn ensemble package 
4066: figure feature importances forests trees feature importances forests trees examples shows use forests trees evaluate importance features artical classication task red plots feature importances individual tree blue plot feature importance whole forest expected knee blue plot suggests features informative remaining 
4067: examples scikit learn user guide release script output feature ranking feature feature feature feature feature feature feature feature feature feature python source code plot_forest_importances print __doc__ import numpy sklearn datasets import make_classification sklearn ensemble import extratreesclassifier build classification task using informative features make_classification n_samples n_features chapter example gallery scikit learn user guide release n_informative n_redundant n_repeated n_classes random_state shuffle false build forest compute feature importances forest extratreesclassifier n_estimators compute_importances true random_state forest fit importances forest feature_importances_ indices argsort importances print feature ranking print feature ranking xrange print feature indices importances indices plot feature importances trees forest import pylab figure title feature importances tree forest estimators_ plot xrange tree feature_importances_ indices plot xrange importances indices show figure pixel importances parallel forest trees pixel importances parallel forest trees example shows use forests trees evaluate importance pixels image classication task faces hotter pixel important code also illustrates construction computation predictions parallelized within multiple jobs 
4068: examples scikit learn user guide release script output fitting extratreesclassifier faces data cores done 886s python source code plot_forest_importances_faces print __doc__ time import time import pylab sklearn datasets import fetch_olivetti_faces sklearn ensemble import extratreesclassifier number cores use perform parallel fitting forest model n_jobs loading digits dataset chapter example gallery scikit learn user guide release data fetch_olivetti_faces data images reshape len data images data target mask limit classes mask mask build forest compute pixel importances print fitting extratreesclassifier faces data cores n_jobs time forest extratreesclassifier n_estimators max_features compute_importances true n_jobs n_jobs random_state forest fit print done 3fs time importances forest feature_importances_ importances importances reshape data images shape plot pixel importances matshow importances cmap hot title pixel importances forests trees show figure plot decision surfaces ensembles trees iris dataset plot decision surfaces ensembles trees iris dataset plot decision surfaces forests randomized trees trained pairs features iris dataset plot compares decision surfaces learned decision tree classier rst column random forest classi second column extra trees classier third column rst row classiers built using sepal width sepal length features second row using petal length sepal length third row using petal width petal length 
4069: examples scikit learn user guide release python source code plot_forest_iris print __doc__ import numpy import pylab sklearn import clone sklearn datasets import load_iris sklearn ensemble import randomforestclassifier extratreesclassifier sklearn tree import decisiontreeclassifier parameters n_classes n_estimators plot_colors bry plot_step load data iris load_iris plot_idx pair model decisiontreeclassifier randomforestclassifier n_estimators n_estimators chapter example gallery scikit learn user guide release extratreesclassifier n_estimators n_estimators take two corresponding features iris data pair iris target shuffle idx arange shape random seed random shuffle idx idx idx standardize mean mean axis std std axis mean std train clf clone model clf model fit plot decision boundary subplot plot_idx x_min x_max min max y_min y_max min max meshgrid arange x_min x_max plot_step arange y_min y_max plot_step isinstance model decisiontreeclassifier model predict ravel ravel reshape shape contourf cmap paired else tree model estimators_ tree predict ravel ravel reshape shape contourf alpha cmap paired axis tight plot training points zip xrange n_classes plot_colors idx scatter idx idx label iris target_names cmap paired axis tight plot_idx suptitle decision surfaces decision tree random forest extra trees classifier show examples scikit learn user guide release figure gradient boosting regression gradient boosting regression demonstrate gradient boosting boston housing dataset example gradient boosting model least squares loss regression trees depth 
4070: script output mse python source code plot_gradient_boosting_regression print __doc__ author peter prettenhofer peter prettenhofer gmail com license bsd import numpy import pylab sklearn import ensemble sklearn import datasets sklearn utils import shuffle sklearn metrics import mean_squared_error chapter example gallery scikit learn user guide release load data boston datasets load_boston shuffle boston data boston target random_state astype float32 offset int shape x_train y_train offset offset x_test y_test offset offset fit regression model params n_estimators max_depth min_samples_split learn_rate loss clf ensemble gradientboostingregressor params clf fit x_train y_train mse mean_squared_error y_test clf predict x_test print mse mse plot training deviance compute test set deviance test_score zeros params n_estimators dtype float64 y_pred enumerate clf staged_decision_function x_test test_score clf loss_ y_test y_pred figure figsize subplot title deviance plot arange params n_estimators clf train_score_ label training set deviance plot arange params n_estimators test_score label test set deviance legend loc upper right xlabel boosting iterations ylabel deviance plot feature importance feature_importance clf feature_importances_ make importances relative max importance feature_importance feature_importance feature_importance max sorted_idx argsort feature_importance pos arange sorted_idx shape subplot barh pos feature_importance sorted_idx align center yticks pos boston feature_names sorted_idx xlabel relative importance title variable importance show gradient boosting regularization illustration effect different regularization strategies gradient boosting example taken hastie 
4071: examples scikit learn user guide release figure gradient boosting regularization loss function used binomial deviance combination shrinkage stochastic gradient boosting sample produce accurate models subsampling without shrinkage usually poorly 
4072: python source code plot_gradient_boosting_regularization print __doc__ author peter prettenhofer peter prettenhofer gmail com license bsd import numpy import pylab sklearn import ensemble chapter example gallery scikit learn user guide release sklearn import datasets datasets make_hastie_10_2 n_samples random_state astype float32 x_train x_test y_train y_test original_params n_estimators max_depth random_state min_samples_split figure label color setting shrinkage orange learn_rate subsample shrink turquoise learn_rate subsample sample blue learn_rate subsample shrink sample gray learn_rate subsample params dict original_params params update setting clf ensemble gradientboostingclassifier params clf fit x_train y_train compute test set deviance test_deviance zeros params n_estimators dtype float64 y_pred enumerate clf staged_decision_function x_test test_deviance clf loss_ y_test y_pred plot arange test_deviance shape test_deviance color color label label title deviance legend loc upper left xlabel boosting iterations ylabel test set deviance show tutorial exercices exercises tutorials figure cross validation diabetes dataset exercise examples scikit learn user guide release cross validation diabetes dataset exercise exercise used cross validated estimators part model selection choosing estimators parameters section tutorial statistical learning scientic data processing 
4073: script output python source code plot_cv_diabetes print __doc__ import numpy import pylab sklearn import cross_validation datasets linear_model diabetes datasets load_diabetes diabetes data diabetes target lasso linear_model lasso alphas logspace scores list scores_std list alpha alphas lasso alpha alpha this_scores cross_validation cross_val_score lasso n_jobs scores append mean this_scores scores_std append std this_scores figure figsize clf axes semilogx alphas scores semilogx alphas array scores array scores_std semilogx alphas array scores array scores_std yticks ylabel score xlabel alpha axhline max scores linestyle color chapter example gallery scikit learn user guide release text max scores bonus much trust selection alpha k_fold cross_validation kfold len print lasso fit train train alpha train k_fold figure cross validation digits dataset exercise cross validation digits dataset exercise exercise used cross validation generators part model selection choosing estimators parameters section tutorial statistical learning scientic data processing 
4074: python source code plot_cv_digits print __doc__ import numpy sklearn import cross_validation datasets svm digits datasets load_digits digits data digits target svc svm svc c_s logspace scores list scores_std list c_s svc this_scores cross_validation cross_val_score svc n_jobs scores append mean this_scores scores_std append std this_scores import pylab figure figsize examples scikit learn user guide release clf axes semilogx c_s scores semilogx c_s array scores array scores_std semilogx c_s array scores array scores_std yticks ylabel score xlabel parameter ylim axhline max scores linestyle color text c_s argmax scores max scores max scores verticalalignment top horizontalalignment center show figure digits classication exercise digits classication exercise exercise used classication part supervised learning predicting output variable high dimensional observations section tutorial statistical learning scientic data processing script output knn score logisticregression score python source code plot_digits_classification_exercise print __doc__ sklearn import datasets neighbors linear_model digits datasets load_digits x_digits digits data y_digits digits target n_samples len x_digits x_train x_digits n_samples y_train y_digits n_samples x_test x_digits n_samples y_test y_digits n_samples knn neighbors kneighborsclassifier logistic linear_model logisticregression print knn score chapter example gallery scikit learn user guide release knn fit x_train y_train score x_test y_test print logisticregression score logistic fit x_train y_train score x_test y_test figure svm exercise svm exercise exercise used using kernels part supervised learning predicting output variable high dimensional observations section tutorial statistical learning scientic data processing 
4075: python source code plot_iris_exercise print __doc__ import numpy import pylab examples scikit learn user guide release sklearn import datasets svm iris datasets load_iris iris data iris target n_sample len random seed order random permutation n_sample order order astype float x_train n_sample y_train n_sample x_test n_sample y_test n_sample fit model fig_num kernel enumerate linear rbf poly clf svm svc kernel kernel gamma clf fit x_train y_train figure fig_num clf scatter zorder cmap paired circle test data scatter x_test x_test facecolors none zorder axis tight x_min min x_max max y_min min y_max max mgrid x_min x_max 200j y_min y_max 200j clf decision_function ravel ravel put result color plot reshape shape pcolormesh cmap paired contour colors linestyles levels title kernel show gaussian process machine learning examples concerning sklearn gaussian_process package 
4076: chapter example gallery scikit learn user guide release figure gaussian processes classication example exploiting probabilistic output gaussian processes classication example exploiting probabilistic output two dimensional regression exercise post processing allowing probabilistic classication thanks gaussian property prediction gure illustrates probability prediction negative respect remaining uncertainty prediction red blue lines corresponds condence interval prediction zero level set 
4077: python source code plot_gp_probabilistic_classification_after_regression print __doc__ author vincent dubourg vincent dubourg gmail com license bsd style examples scikit learn user guide release import numpy scipy import stats sklearn gaussian_process import gaussianprocess matplotlib import pyplot matplotlib import standard normal distribution functions phi stats distributions norm pdf phi stats distributions norm cdf phiinv stats distributions norm ppf constants lim def function predict classification consist predicting whether return 
4078: design experiments array observations instanciate fit gaussian process model gaussianprocess theta0 dont perform mle youll get perfect prediction simple example fit evaluate real function prediction mse grid res meshgrid linspace lim lim res vstack reshape size reshape size linspace lim lim res y_true y_pred mse predict eval_mse true sigma sqrt mse y_true y_true reshape res res y_pred y_pred reshape res res sigma sigma reshape res res phiinv plot probabilistic classification iso values using gaussian property prediction fig figure fig add_subplot axes set_aspect equal chapter example gallery scikit learn user guide release xticks yticks set_xticklabels set_yticklabels xlabel x_1 ylabel x_2 cax imshow flipud phi y_pred sigma cmap gray_r alpha extent lim lim lim lim norm matplotlib colors normalize vmin vmax colorbar cax ticks norm norm set_label mathbb left widehat mathbf leq right plot markersize plot markersize contour y_true colors linestyles dashdot contour phi y_pred sigma colors clabel fontsize linestyles solid contour phi y_pred sigma colors clabel fontsize linestyles dashed contour phi y_pred sigma colors clabel fontsize linestyles solid show figure gaussian processes regression basic introductory example gaussian processes regression basic introductory example simple one dimensional regression exercise computed two different ways noise free case cubic correlation model noisy case squared euclidean correlation model cases model parameters estimated using maximum likelihood principle gures illustrate interpolating property gaussian process model well probabilistic nature form pointwise condence interval 
4079: examples scikit learn user guide release note parameter nugget applied tikhonov regularization assumed covariance training points special case squared euclidean correlation model nugget mathematically equivalent normalized variance cid cid nuggeti python source code plot_gp_regression print __doc__ author vincent dubourg vincent dubourg gmail com license bsd style jake vanderplas vanderplas astro washington edu import numpy sklearn gaussian_process import gaussianprocess matplotlib import pyplot random seed def function predict return sin chapter example gallery scikit learn user guide release first noiseless case atleast_2d observations ravel mesh input space evaluations real function prediction mse atleast_2d linspace instanciate gaussian process model gaussianprocess corr cubic theta0 thetal thetau random_start fit data using maximum likelihood estimation parameters fit make prediction meshed axis ask mse well y_pred mse predict eval_mse true sigma sqrt mse plot function prediction confidence interval based mse fig figure plot label sin plot markersize label uobservations plot y_pred label uprediction fill concatenate concatenate y_pred sigma alpha none label confidence interval y_pred sigma xlabel ylabel ylim legend loc upper left noisy case linspace atleast_2d observations noise ravel random random shape noise random normal noise mesh input space evaluations real function prediction mse atleast_2d linspace instanciate gaussian process model gaussianprocess corr squared_exponential theta0 thetal thetau nugget random_start fit data using maximum likelihood estimation parameters examples scikit learn user guide release fit make prediction meshed axis ask mse well y_pred mse predict eval_mse true sigma sqrt mse plot function prediction confidence interval based mse fig figure plot label sin errorbar ravel fmt markersize label uobservations plot y_pred label uprediction fill concatenate concatenate y_pred sigma alpha none label confidence interval y_pred sigma xlabel ylabel ylim legend loc upper left show figure gaussian processes regression goodness diabetes dataset gaussian processes regression goodness diabetes dataset example consists tting gaussian process model onto diabetes dataset correlation parameters determined means maximum likelihood estimation mle anisotropic squared exponential correlation model constant regression model assumed also used nugget order account strong noise targets compute compute cross validation estimate coefcient determination without reperforming mle using set correlation parameters found whole dataset python source code gp_diabetes_dataset print __doc__ author vincent dubourg vincent dubourg gmail com license bsd style sklearn import datasets sklearn gaussian_process import gaussianprocess sklearn cross_validation import cross_val_score kfold chapter example gallery scikit learn user guide release load dataset scikits data sets diabetes datasets load_diabetes diabetes data diabetes target instanciate model gaussianprocess regr constant corr absolute_exponential theta0 thetal thetau nugget optimizer welch fit model data performing maximum likelihood estimation fit deactivate maximum likelihood estimation cross validation loop theta0 theta given correlation parameter mle thetal thetau none none none bounds deactivate mle perform cross validation estimate coefficient determination using cross_validation module using cpus available machine cross_val_score kfold size n_jobs mean print folds estimate coefficient determination folds generalized linear models examples concerning sklearn linear_model package 
4080: figure automatic relevance determination regression ard automatic relevance determination regression ard fit regression model bayesian ridge regression compared ols ordinary least squares estimator coefcient weights slightly shifted toward zeros wich stabilises histogram estimated weights peaked sparsity inducing prior implied weights estimation model done iteratively maximizing marginal log likelihood observations 
4081: examples scikit learn user guide release python source code plot_ard print __doc__ import numpy import pylab scipy import stats sklearn linear_model import ardregression linearregression chapter example gallery scikit learn user guide release generating simulated data gaussian weigthts parameters example random seed n_samples n_features create gaussian data random randn n_samples n_features create weigts precision lambda_ lambda_ zeros n_features keep weights interest relevant_features random randint n_features relevant_features stats norm rvs loc scale sqrt lambda_ create noite precision alpha alpha_ noise stats norm rvs loc scale sqrt alpha_ size n_samples create target dot noise fit ard regression clf ardregression compute_score true clf fit ols linearregression ols fit plot true weights estimated weights histogram weights figure figsize title weights model plot clf coef_ label ard estimate plot ols coef_ label ols estimate plot label ground truth xlabel features ylabel values weights legend loc figure figsize title histogram weights hist clf coef_ bins n_features log true plot clf coef_ relevant_features ones len relevant_features label relevant features ylabel features xlabel values weights legend loc figure figsize title marginal log likelihood plot clf scores_ ylabel score xlabel iterations show examples scikit learn user guide release figure bayesian ridge regression bayesian ridge regression computes bayesian ridge regression synthetic dataset compared ols ordinary least squares estimator coefcient weights slightly shifted toward zeros wich stabilises prior weights gaussian prior histogram estimated weights gaussian estimation model done iteratively maximizing marginal log likelihood observations 
4082: chapter example gallery scikit learn user guide release python source code plot_bayesian_ridge print __doc__ import numpy import pylab scipy import stats sklearn linear_model import bayesianridge linearregression generating simulated data gaussian weigthts random seed n_samples n_features random randn n_samples n_features create gaussian data create weigts precision lambda_ lambda_ zeros n_features keep weights interest relevant_features random randint n_features relevant_features stats norm rvs loc scale sqrt lambda_ create noise precision alpha alpha_ noise stats norm rvs loc scale sqrt alpha_ size n_samples create target dot noise fit bayesian ridge regression ols comparison clf bayesianridge compute_score true clf fit ols linearregression ols fit plot true weights estimated weights histogram weights figure figsize title weights model plot clf coef_ label bayesian ridge estimate plot label ground truth plot ols coef_ label ols estimate examples scikit learn user guide release xlabel features ylabel values weights legend loc best prop dict size figure figsize title histogram weights hist clf coef_ bins n_features log true plot clf coef_ relevant_features ones len relevant_features label relevant features ylabel features xlabel values weights legend loc lower left figure figsize title marginal log likelihood plot clf scores_ ylabel score xlabel iterations show figure logistic regression class classier logistic regression class classier show logistic regression classiers decision boundaries iris dataset datapoints colored according labels 
4083: python source code plot_iris_logistic print __doc__ chapter example gallery scikit learn user guide release code source gael varoqueux modified documentation merge jaques grobler license bsd import numpy import pylab sklearn import linear_model datasets import data play iris datasets load_iris iris data take first two features iris target step size mesh logreg linear_model logisticregression 1e5 create instance neighbours classifier fit data logreg fit plot decision boundary asign color point mesh x_min m_max y_min y_max x_min x_max min max y_min y_max min max meshgrid arange x_min x_max arange y_min y_max logreg predict ravel ravel put result color plot reshape shape figure figsize pcolormesh cmap paired plot also training points scatter edgecolors cmap paired xlabel sepal length ylabel sepal width xlim min max ylim min max xticks yticks show figure lasso elastic net sparse signals examples scikit learn user guide release lasso elastic net sparse signals script output lasso alpha copy_x true fit_intercept true max_iter normalize false positive false precompute auto tol warm_start false test data elasticnet alpha copy_x true fit_intercept true max_iter normalize false positive false precompute auto rho tol warm_start false test data python source code plot_lasso_and_elasticnet print __doc__ import numpy import pylab sklearn metrics import r2_score generate sparse data play random seed chapter example gallery scikit learn user guide release n_samples n_features random randn n_samples n_features coef random randn n_features inds arange n_features random shuffle inds coef inds sparsify coef dot coef add noise random normal n_samples split data train set test set n_samples shape x_train y_train n_samples n_samples x_test y_test n_samples n_samples lasso sklearn linear_model import lasso alpha lasso lasso alpha alpha y_pred_lasso lasso fit x_train y_train predict x_test r2_score_lasso r2_score y_test y_pred_lasso print lasso print test data r2_score_lasso elasticnet sklearn linear_model import elasticnet enet elasticnet alpha alpha rho y_pred_enet enet fit x_train y_train predict x_test r2_score_enet r2_score y_test y_pred_enet print enet print test data r2_score_enet plot enet coef_ label elastic net coefficients plot lasso coef_ label lasso coefficients plot coef label original coefficients legend loc best title lasso elastic net r2_score_lasso r2_score_enet show figure lasso elastic net examples scikit learn user guide release lasso elastic net lasso elastic net penalisation implemented using coordinate descent coefcients forced positive 
4084: script output computing regularization path using lasso computing regularization path using positive lasso computing regularization path using elastic net computing regularization path using positve elastic net 
4085: python source code plot_lasso_coordinate_descent_path chapter example gallery scikit learn user guide release print __doc__ author alexandre gramfort alexandre gramfort inria license bsd style 
4086: import numpy import pylab sklearn linear_model import lasso_path enet_path sklearn import datasets diabetes datasets load_diabetes diabetes data diabetes target std standardize data easier set rho parameter compute paths eps smaller longer path print computing regularization path using lasso models lasso_path eps eps alphas_lasso array model alpha model models coefs_lasso array model coef_ model models print computing regularization path using positive lasso models lasso_path eps eps positive true alphas_positive_lasso array model alpha model models coefs_positive_lasso array model coef_ model models print computing regularization path using elastic net models enet_path eps eps rho alphas_enet array model alpha model models coefs_enet array model coef_ model models print computing regularization path using positve elastic net models enet_path eps eps rho positive true alphas_positive_enet array model alpha model models coefs_positive_enet array model coef_ model models display results figure gca set_color_cycle plot coefs_lasso plot coefs_enet linestyle xlabel log lambda ylabel weights title lasso elastic net paths legend lasso elastic net loc lower left axis tight examples scikit learn user guide release figure gca set_color_cycle plot coefs_lasso plot coefs_positive_lasso linestyle xlabel log lambda ylabel weights title lasso positive lasso legend lasso positive lasso loc lower left axis tight figure gca set_color_cycle plot coefs_enet plot coefs_positive_enet linestyle xlabel log lambda ylabel weights title elastic net positive elastic net legend elastic net positive elastic net loc lower left axis tight show figure lasso path using lars lasso path using lars computes lasso path along regularization parameter using lars algorithm diabetest dataset 
4087: chapter example gallery scikit learn user guide release script output computing regularization path using lars 
4088: python source code plot_lasso_lars print __doc__ author fabian pedregosa fabian pedregosa inria license bsd style 
4089: alexandre gramfort alexandre gramfort inria import numpy import pylab sklearn import linear_model sklearn import datasets diabetes datasets load_diabetes diabetes data diabetes target print computing regularization path using lars alphas coefs linear_model lars_path method lasso verbose true sum abs coefs axis examples scikit learn user guide release plot coefs ymin ymax ylim vlines ymin ymax linestyle dashed xlabel coef max coef ylabel coefficients title lasso path axis tight show figure lasso model selection cross validation aic bic lasso model selection cross validation aic bic use akaike information criterion aic bayes information criterion bic cross validation select optimal value regularization parameter alpha lasso estimator results obtained lassolarsic based aic bic criteria information criterion based model selection fast relies proper estimation degrees freedom derived large samples asymptotic results assume model correct data actually generated model also tend break problem badly conditioned features samples cross validation use fold algorithms compute lasso path coordinate descent implemented lassocv class lars least angle regression implemented lassolarscv class algorithms give roughly results differ regards execution speed sources numerical errors lars computes path solution kink path result efcient kinks case features samples also able compute full path without setting meta parameter opposite coordinate descent compute path points pre specied grid use default thus efcient number grid points smaller number kinks path strategy interesting number features really large enough samples select large amount terms numerical errors heavily correlated variables lars accumulate erros coordinate descent algorithm sample path grid note optimal value alpha varies fold illustrates nested cross validation necessary trying evaluate performance method parameter chosen cross validation choice parameter may optimal unseen data 
4090: chapter example gallery scikit learn user guide release script output computing regularization path using coordinate descent lasso computing regularization path using lars lasso 
4091: python source code plot_lasso_model_selection print __doc__ author olivier grisel gael varoquaux alexandre gramfort license bsd style 
4092: import time examples scikit learn user guide release import numpy import pylab sklearn linear_model import lassocv lassolarscv lassolarsic sklearn import datasets diabetes datasets load_diabetes diabetes data diabetes target rng random randomstate rng randn shape add bad features normalize data done lars allow comparison sqrt sum axis lassolarsic least angle regression bic aic criterion model_bic lassolarsic criterion bic time time model_bic fit t_bic time time alpha_bic_ model_bic alpha_ model_aic lassolarsic criterion aic model_aic fit alpha_aic_ model_aic alpha_ def plot_ic_criterion model name color alpha_ model alpha_ alphas_ model alphas_ criterion_ model criterion_ plot log10 alphas_ criterion_ color color linewidth label criterion name axvline log10 alpha_ color color linewidth label alpha estimate name xlabel log lambda ylabel criterion figure plot_ic_criterion model_aic aic plot_ic_criterion model_bic bic legend title information criterion model selection training time 3fs t_bic lassocv coordinate descent compute paths print computing regularization path using coordinate descent lasso time time model lassocv fit t_lasso_cv time time display results chapter example gallery scikit learn user guide release m_log_alphas log10 model alphas figure ymin ymax plot m_log_alphas model mse_path_ plot m_log_alphas model mse_path_ mean axis label average across folds linewidth axvline log10 model alpha linestyle color label alpha estimate legend xlabel log lambda ylabel mean square error title mean square error fold coordinate descent train time 2fs t_lasso_cv axis tight ylim ymin ymax lassolarscv least angle regression compute paths print computing regularization path using lars lasso time time model lassolarscv fit t_lasso_lars_cv time time display results m_log_alphas log10 model cv_alphas figure plot m_log_alphas model cv_mse_path_ plot m_log_alphas model cv_mse_path_ mean axis label average across folds linewidth axvline log10 model alpha linestyle color label alpha legend xlabel log lambda ylabel mean square error title mean square error fold lars train time 2fs t_lasso_lars_cv axis tight ylim ymin ymax show figure logit function examples scikit learn user guide release logit function show plot logistic regression would synthetic dataset classify values either class one two using logit curve 
4093: python source code plot_logistic print __doc__ code source gael varoqueux license bsd import numpy import pylab sklearn import linear_model test set straight line gaussian noise xmin xmax n_samples random seed random normal size n_samples astype float random normal size n_samples newaxis run classifier clf linear_model logisticregression 1e5 clf fit plot result figure figsize clf scatter ravel color black zorder x_test linspace chapter example gallery scikit learn user guide release def model return exp loss model x_test clf coef_ clf intercept_ ravel plot x_test loss color blue linewidth ols linear_model linearregression ols fit plot x_test ols coef_ x_test ols intercept_ linewidth axhline color ylabel xlabel xticks yticks ylim xlim show figure penalty sparsity logistic regression penalty sparsity logistic regression comparison sparsity percentage zero coefcients solutions penalty used different values see large values give freedom model conversely smaller values constrain model penalty case leads sparser solutions classify 8x8 images digits two classes visualization shows coefcients models varying 
4094: examples scikit learn user guide release script output sparsity penalty score penalty sparsity penalty score penalty sparsity penalty score penalty sparsity penalty score penalty sparsity penalty score penalty sparsity penalty score penalty python source code plot_logistic_l1_l2_sparsity print __doc__ authors alexandre gramfort alexandre gramfort inria license bsd style 
4095: mathieu blondel mathieu mblondel org andreas mueller amueller ais uni bonn chapter example gallery scikit learn user guide release import numpy import pylab sklearn linear_model import logisticregression sklearn import datasets sklearn preprocessing import scaler digits datasets load_digits digits data digits target scaler fit_transform classify small large digits astype int set regularization parameter enumerate arange turn tolerance short training time clf_l1_lr logisticregression penalty tol clf_l2_lr logisticregression penalty tol clf_l1_lr fit clf_l2_lr fit coef_l1_lr clf_l1_lr coef_ ravel coef_l2_lr clf_l2_lr coef_ ravel coef_l1_lr contains zeros due sparsity inducing norm sparsity_l1_lr mean coef_l1_lr sparsity_l2_lr mean coef_l2_lr print print sparsity penalty sparsity_l1_lr print score penalty clf_l1_lr score print sparsity penalty sparsity_l2_lr print score penalty clf_l2_lr score l1_plot subplot l2_plot subplot l1_plot set_title penalty l2_plot set_title penalty l1_plot imshow abs coef_l1_lr reshape interpolation nearest cmap binary vmax vmin l2_plot imshow abs coef_l2_lr reshape interpolation nearest cmap binary vmax vmin text l1_plot set_xticks l1_plot set_yticks l2_plot set_xticks l2_plot set_yticks show examples scikit learn user guide release figure path logistic regression path logistic regression computes path iris dataset 
4096: script output computing regularization path took python source code plot_logistic_path print __doc__ author alexandre gramfort alexandre gramfort inria license bsd style 
4097: chapter example gallery scikit learn user guide release datetime import datetime import numpy import pylab sklearn import linear_model sklearn import datasets sklearn svm import l1_min_c iris datasets load_iris iris data iris target mean demo path functions l1_min_c loss log logspace print computing regularization path start datetime clf linear_model logisticregression penalty tol coefs_ clf set_params clf fit coefs_ append clf coef_ ravel copy print took datetime start coefs_ array coefs_ plot log10 coefs_ ymin ymax ylim xlabel log ylabel coefficients title logistic regression path axis tight show figure linear regression example examples scikit learn user guide release linear regression example example uses rst feature diabetes dataset order illustrate two dimensional plot regression technique straight line seen plot showing linear regression attempts draw straight line best minimize residual sum squares observed responses dataset responses predicted linear approximation coefcients residual sum squares variance score also calculated 
4098: script output coefficients residual sum squares variance score python source code plot_ols print __doc__ code source jaques grobler license bsd import pylab import numpy chapter example gallery scikit learn user guide release sklearn import datasets linear_model load diabetes dataset diabetes datasets load_diabetes use one feature diabetes_x diabetes data newaxis diabetes_x_temp diabetes_x split data training testing sets diabetes_x_train diabetes_x_temp diabetes_x_test diabetes_x_temp sklearn datasets samples_generator import make_regression test set straight line gaussian noise make_regression n_samples n_features n_informative random_state noise split targets training testing sets diabetes_y_train diabetes target diabetes_y_test diabetes target create linear regression object regr linear_model linearregression train model using training sets regr fit diabetes_x_train diabetes_y_train coefficients print coefficients regr coef_ mean square error print residual sum squares mean regr predict diabetes_x_test diabetes_y_test explained variance score perfect prediction print variance score regr score diabetes_x_test diabetes_y_test plot outputs scatter diabetes_x_test diabetes_y_test plot diabetes_x_test regr predict diabetes_x_test color blue color black linewidth xticks yticks show figure sparsity example fitting features examples scikit learn user guide release sparsity example fitting features features diabetes dataset tted plotted illustrates although feature strong coefcient full model give much regarding compared feautre python source code plot_ols_3d print __doc__ code source gael varoqueux modified documentation merge jaques grobler license bsd import pylab import numpy mpl_toolkits mplot3d import axes3d sklearn import datasets linear_model diabetes datasets load_diabetes indices x_train diabetes data indices x_test diabetes data indices y_train diabetes target y_test diabetes target ols linear_model linearregression chapter example gallery scikit learn user guide release ols fit x_train y_train plot figure def plot_figs fig_num elev azim x_train clf fig figure fig_num figsize clf axes3d fig elev elev azim azim scatter x_train x_train y_train marker plot_surface array array clf predict array reshape alpha set_xlabel x_1 set_ylabel x_2 set_zlabel w_xaxis set_ticklabels w_yaxis set_ticklabels w_zaxis set_ticklabels generate three different figures different views elev azim plot_figs elev azim x_train ols elev azim plot_figs elev azim x_train ols elev azim plot_figs elev azim x_train ols show figure ordinary least squares ridge regression variance ordinary least squares ridge regression variance due points dimension straight line linear regression uses follow points well noise observations cause great variace shown rst plot every lines slope vary quite bit prediction due noise induced observations ridge regression basically minimizing penalised version least squared function penalising shrinks value regression coefcients despite data points dimension slope prediction much stable variance line greatly reduced comparison standard linear regression examples scikit learn user guide release python source code plot_ols_ridge_variance print __doc__ code source gael varoqueux modified documentation merge jaques grobler license bsd import numpy import pylab sklearn import linear_model x_train y_train x_test random seed classifiers dict ols linear_model linearregression ridge linear_model ridge alpha fignum name clf classifiers iteritems fig figure fignum figsize clf axes range this_x random normal size x_train clf fit this_x y_train plot x_test clf predict x_test color scatter this_x y_train marker zorder chapter example gallery scikit learn user guide release clf fit x_train y_train plot x_test clf predict x_test linewidth color blue scatter x_train y_train marker zorder set_xticks set_yticks set_ylim set_xlabel set_ylabel set_xlim fignum show figure orthogonal matching pursuit orthogonal matching pursuit using orthogonal matching pursuit recovering sparse signal noisy measurement encoded dictionary examples scikit learn user guide release python source code plot_omp print __doc__ import pylab import numpy sklearn linear_model import orthogonal_mp sklearn datasets import make_sparse_coded_signal n_components n_features n_atoms generate data n_atoms make_sparse_coded_signal n_samples n_components n_components n_features n_features n_nonzero_coefs n_atoms random_state idx nonzero chapter example gallery scikit learn user guide release distort clean signal y_noisy random randn len plot sparse signal subplot xlim title sparse signal stem idx idx plot noise free reconstruction x_r orthogonal_mp n_atoms idx_r x_r nonzero subplot xlim title recovered signal noise free measurements stem idx_r x_r idx_r plot noisy reconstruction x_r orthogonal_mp y_noisy n_atoms idx_r x_r nonzero subplot xlim title recovered signal noisy measurements stem idx_r x_r idx_r subplots_adjust suptitle sparse signal recovery orthogonal matching pursuit fontsize show figure polynomial interpolation polynomial interpolation example demonstrates approximate function polynomial degree n_degree using ridge regression concretely n_samples points sufces build vandermonde matrix n_samples n_degree following form x_1 x_1 x_1 x_2 x_2 x_2 intuitively matrix interpreted matrix pseudo features points raised power matrix akin different matrix induced polynomial kernel 
4099: examples scikit learn user guide release example shows non linear regression linear model manually adding non linear features kernel methods extend idea induce high even innite dimensional feature spaces 
4100: python source code plot_polynomial_interpolation print __doc__ author mathieu blondel license bsd style 
4101: import numpy import pylab sklearn linear_model import ridge def function approximate polynomial interpolation return sin generate points used plot x_plot linspace generate points keep subset linspace rng random randomstate chapter example gallery scikit learn user guide release rng shuffle sort plot x_plot x_plot label ground truth scatter label training points degree ridge ridge ridge fit vander degree plot x_plot ridge predict vander x_plot degree label degree degree legend loc lower left show figure plot ridge coefcients function regularization plot ridge coefcients function regularization shows effect collinearity coefcients ridge end path alpha tends toward zero solution tends towards ordinary least squares coefcients exhibit big oscillations 
4102: examples scikit learn user guide release python source code plot_ridge_path author fabian pedregosa fabian pedregosa inria license bsd style 
4103: print __doc__ import numpy import pylab sklearn import linear_model 10x10 hilbert matrix arange arange newaxis ones compute paths n_alphas alphas logspace n_alphas clf linear_model ridge fit_intercept false coefs alphas clf set_params alpha clf fit chapter example gallery scikit learn user guide release coefs append clf coef_ display results gca set_color_cycle plot alphas coefs set_xscale log set_xlim get_xlim reverse axis xlabel alpha ylabel weights title ridge coefficients function regularization axis tight show figure plot multi class sgd iris dataset plot multi class sgd iris dataset plot decision surface multi class sgd iris dataset hyperplanes corresponding three one versus ova classiers represented dashed lines 
4104: examples scikit learn user guide release python source code plot_sgd_iris print __doc__ import numpy import pylab sklearn import datasets sklearn linear_model import sgdclassifier import data play iris datasets load_iris iris data take first two features could avoid ugly slicing using two dim dataset iris target colors bry shuffle idx arange shape random seed random shuffle idx idx idx standardize mean mean axis std std axis chapter example gallery scikit learn user guide release mean std step size mesh clf sgdclassifier alpha n_iter fit create mesh plot x_min x_max min max y_min y_max min max meshgrid arange x_min x_max arange y_min y_max plot decision boundary asign color point mesh x_min m_max y_min y_max clf predict ravel ravel put result color plot reshape shape contourf cmap paired axis tight plot also training points color zip clf classes_ colors idx scatter idx idx color label iris target_names cmap paired title decision surface multi class sgd axis tight plot three one classifiers xmin xmax xlim ymin ymax ylim coef clf coef_ intercept clf intercept_ def plot_hyperplane color def line return coef intercept coef plot xmin xmax line xmin line xmax color color color zip clf classes_ colors plot_hyperplane color legend show figure sgd convex loss functions examples scikit learn user guide release sgd convex loss functions plot convex loss functions supported sklearn linear_model stochastic_gradient 
4105: python source code plot_sgd_loss_functions print __doc__ import numpy import pylab sklearn linear_model sgd_fast import hinge modifiedhuber squaredloss define loss funcitons xmin xmax hinge hinge log_loss lambda log2 exp modified_huber modifiedhuber squared_loss squaredloss plot loss funcitons linspace xmin xmax plot xmin xmax label zero one loss chapter example gallery scikit learn user guide release plot hinge loss label hinge loss plot log_loss label log loss plot modified_huber loss label modified huber loss label squared loss plot squared_loss loss ylim legend loc upper right xlabel cdot ylabel show figure ordinary least squares sgd ordinary least squares sgd simple ordinary least squares example stochastic gradient descent draw linear least squares solution random set points plane 
4106: examples scikit learn user guide release python source code plot_sgd_ols print __doc__ import pylab sklearn linear_model import sgdregressor sklearn datasets samples_generator import make_regression test set straight line gaussian noise make_regression n_samples n_features n_informative random_state noise run classifier clf sgdregressor alpha n_iter clf fit plot result scatter color black plot clf predict color blue linewidth show chapter example gallery scikit learn user guide release figure sgd penalties sgd penalties plot contours three penalties supported sklearn linear_model stochastic_gradient 
4107: python source code plot_sgd_penalties __future__ import division print __doc__ import numpy import pylab def examples scikit learn user guide release return array sqrt sqrt def return array sqrt def return array def cross ext plot ext ext plot ext ext linspace alpha division throuh zero cross plot label plot plot plot plot label plot plot plot plot alpha label elastic net plot alpha plot alpha plot alpha xlabel w_0 ylabel w_1 legend axis equal show sgd maximum margin separating hyperplane plot maximum margin separating hyperplane within two class separable dataset using linear support vector machines classier trained using sgd 
4108: chapter example gallery scikit learn user guide release figure sgd maximum margin separating hyperplane python source code plot_sgd_separating_hyperplane print __doc__ import numpy import pylab sklearn linear_model import sgdclassifier sklearn datasets samples_generator import make_blobs create separable points make_blobs n_samples centers random_state cluster_std fit model clf sgdclassifier loss hinge alpha n_iter fit_intercept true examples scikit learn user guide release clf fit plot line points nearest vectors plane linspace linspace meshgrid empty shape val ndenumerate val clf decision_function levels linestyles dashed solid dashed colors contour levels colors colors linestyles linestyles scatter cmap paired axis tight show figure sgd separating hyperplane weighted classes sgd separating hyperplane weighted classes fit linear svms without class weighting allows handle problems unbalanced classes 
4109: chapter example gallery scikit learn user guide release python source code plot_sgd_weighted_classes print __doc__ import numpy import pylab sklearn linear_model import sgdclassifier create separable points random seed n_samples_1 n_samples_2 random randn n_samples_1 random randn n_samples_2 array n_samples_1 n_samples_2 dtype float64 idx arange shape random shuffle idx idx idx mean mean axis std std axis mean std fit model get separating hyperplane clf sgdclassifier n_iter alpha clf fit examples scikit learn user guide release clf coef_ ravel linspace clf intercept_ get separating hyperplane using weighted classes wclf sgdclassifier n_iter alpha class_weight wclf fit wclf coef_ ravel wyy wclf intercept_ plot separating hyperplanes samples plot label weights plot wyy label weights scatter cmap paired legend axis tight show figure sgd weighted samples sgd weighted samples plot decision function weighted dataset size points proportional weight 
4110: chapter example gallery scikit learn user guide release python source code plot_sgd_weighted_samples print __doc__ import numpy import pylab sklearn import linear_model create points random seed random randn random randn sample_weight abs random randn assign bigger weight last samples sample_weight plot weighted data points meshgrid linspace linspace figure scatter sample_weight alpha cmap bone fit unweighted model clf linear_model sgdclassifier alpha n_iter clf fit clf decision_function ravel ravel examples scikit learn user guide release reshape shape no_weights contour levels linestyles solid fit weighted model clf linear_model sgdclassifier alpha n_iter clf fit sample_weight sample_weight clf decision_function ravel ravel reshape shape samples_weights contour levels linestyles dashed legend no_weights collections samples_weights collections weights weights loc lower left xticks yticks show figure sparse recovery feature selection sparse linear models sparse recovery feature selection sparse linear models given small number observations want recover features relevant explain sparse linear models outperform standard statistical tests true model sparse small fraction features relevant detailed compressive sensing notes ability based approach identify relevant variables pends sparsity ground truth number samples number features conditionning design matrix signal subspace amount noise absolute value smallest non zero coefcient wainwright2006 http statistics berkeley edu tech reports pdf keep parameters constant vary conditionning design matrix well conditionned design matrix small mutual incoherence exactly compressive sensing conditions gaussian sensing matrix recovery lasso performs well ill conditionned matrix high mutual incoherence regressors correlated lasso randomly selects one however randomized lasso recover ground truth well situation rst vary alpha parameter setting sparsity estimated model look stability scores randomized lasso analysis knowing ground truth shows optimal regime relevant features stand irrelevant ones alpha chosen small non relevant variables enter model opposite alpha selected large lasso equivalent stepwise regression thus brings advantage univariate test second time set alpha compare performance different feature selection methods using area curve auc precision recall 
4111: chapter example gallery scikit learn user guide release examples scikit learn user guide release python source code plot_sparse_recovery print __doc__ author alexandre gramfort gael varoquaux license bsd import pylab import numpy scipy import linalg sklearn linear_model import randomizedlasso lasso_stability_path sklearn feature_selection import f_regression sklearn preprocessing import scaler sklearn metrics import auc precision_recall_curve sklearn ensemble import extratreesregressor lassolarscv def mutual_incoherence x_relevant x_irelevant mutual incoherence defined formula 26a wainwright2006 projector dot dot x_irelevant x_relevant linalg pinv dot x_relevant x_relevant return max abs projector sum axis conditionning simulate regression data correlated design n_features n_relevant_features noise_level coef_min donoho tanner phase transition around n_samples completely fail recover well conditionned case n_samples block_size n_relevant_features rng random randomstate coefficients model coef zeros n_features coef n_relevant_features coef_min rng rand n_relevant_features correlation design variables correlated blocs corr zeros n_features n_features range n_features block_size corr block_size block_size conditionning corr flat n_features corr linalg cholesky corr design rng normal size n_samples n_features dot corr keep wainwright2006 26c constant n_relevant_features abs chapter example gallery scikit learn user guide release linalg svdvals n_relevant_features max scaler fit_transform copy output variable dot coef std scale added noise function average correlation design output variable noise_level rng normal size n_samples mutual_incoherence n_relevant_features n_relevant_features plot stability selection path using high eps early stopping path save computation time alpha_grid scores_path lasso_stability_path random_state eps figure plot path function alpha alpha_max power power scales path less brutally log enables see progression along path plot alpha_grid scores_path coef plot alpha_grid scores_path coef ymin ymax ylim xlabel alpha alpha_ max ylabel stability score proportion times selected title stability scores path mutual incoherence axis tight legend relevant features irrelevant features loc best plot estimated stability scores given alpha use fold cross validation rather default fold leads better choice alpha lars_cv lassolarscv fit run randomizedlasso use paths going alpha_max avoid exploring regime noisy variables enter model alphas linspace lars_cv alphas_ lars_cv alphas_ clf randomizedlasso alpha alphas random_state fit trees extratreesregressor compute_importances true fit compare score f_regression figure name score test stability selection clf scores_ lasso coefs abs lars_cv coef_ trees trees feature_importances_ precision recall thresholds precision_recall_curve coef score semilogy maximum score max score label auc name auc recall precision examples scikit learn user guide release plot coef n_relevant_features label ground truth xlabel features ylabel score plot first coefficients xlim legend loc best title feature selection scores mutual incoherence show figure lasso dense sparse data lasso dense sparse data show linear_model lasso linear_model sparse lasso provide results case sparse data linear_model sparse lasso improves speed python source code lasso_dense_vs_sparse_data print __doc__ time import time scipy import sparse scipy import linalg sklearn datasets samples_generator import make_regression sklearn linear_model sparse import lasso sparselasso sklearn linear_model import lasso denselasso two lasso implementations dense data print dense matrices make_regression n_samples n_features random_state alpha sparse_lasso sparselasso alpha alpha fit_intercept false max_iter dense_lasso denselasso alpha alpha fit_intercept false max_iter time sparse_lasso fit print sparse lasso done time chapter example gallery scikit learn user guide release time dense_lasso fit print dense lasso done time print distance coefficients linalg norm sparse_lasso coef_ dense_lasso coef_ two lasso implementations sparse data print sparse matrices copy sparse coo_matrix tocsc print matrix density nnz float size alpha sparse_lasso sparselasso alpha alpha fit_intercept false max_iter dense_lasso denselasso alpha alpha fit_intercept false max_iter time sparse_lasso fit print sparse lasso done time time dense_lasso fit todense print dense lasso done time print distance coefficients linalg norm sparse_lasso coef_ dense_lasso coef_ manifold learning examples concerning sklearn manifold package 
4112: figure comparison manifold learning methods comparison manifold learning methods illustration dimensionality reduction curve dataset various manifold learning methods 
4113: examples scikit learn user guide release discussion comparison algorithms see manifold module page script output standard sec ltsa sec hessian sec modified sec isomap sec python source code plot_compare_methods author jake vanderplas vanderplas astro washington edu print __doc__ time import time import pylab mpl_toolkits mplot3d import axes3d matplotlib ticker import nullformatter sklearn import manifold datasets next line silence pyflakes import needed axes3d n_points color datasets samples_generator make_s_curve n_points n_neighbors chapter example gallery scikit learn user guide release n_components fig figure figsize suptitle manifold learning points neighbors n_neighbors fontsize try compatibility matplotlib fig add_subplot projection scatter color cmap spectral view_init except fig add_subplot projection scatter color cmap spectral methods standard ltsa hessian modified labels lle ltsa hessian lle modified lle method enumerate methods time manifold locallylinearembedding n_neighbors n_components eigen_solver auto method method fit_transform time print sec methods fig add_subplot scatter color cmap spectral title sec labels xaxis set_major_formatter nullformatter yaxis set_major_formatter nullformatter axis tight time manifold isomap n_neighbors n_components fit_transform time print isomap sec fig add_subplot scatter color cmap spectral title isomap sec xaxis set_major_formatter nullformatter yaxis set_major_formatter nullformatter axis tight show figure manifold learning handwritten digits locally linear embedding isomap 
4114: examples scikit learn user guide release manifold learning handwritten digits locally linear embedding isomap 
4115: illustration various embeddings digits dataset 
4116: chapter example gallery scikit learn user guide release examples scikit learn user guide release script output computing random projection computing pca projection computing lda projection computing isomap embedding done computing lle embedding done reconstruction error 28548e computing modified lle embedding done reconstruction error computing hessian lle embedding done reconstruction error computing ltsa embedding done reconstruction error python source code plot_lle_digits authors fabian pedregosa fabian pedregosa inria license bsd inria olivier grisel olivier grisel ensta org mathieu blondel mathieu mblondel org print __doc__ time import time import numpy import pylab matplotlib import offsetbox chapter example gallery scikit learn user guide release sklearn utils fixes import qr_economic sklearn import manifold datasets decomposition lda digits datasets load_digits n_class digits data digits target n_samples n_features shape n_neighbors scale visualize embedding vectors def plot_embedding title none x_min x_max min max x_min x_max x_min figure subplot range digits data shape text str digits target color set1 digits target fontdict weight bold size hasattr offsetbox annotationbbox print thumbnails matplotlib shown_images array something big range digits data shape dist sum shown_images min dist dont show points close continue shown_images shown_images imagebox offsetbox annotationbbox offsetbox offsetimage digits images cmap gray_r add_artist imagebox xticks yticks title none title title plot images digits img zeros range range img reshape imshow img cmap binary xticks yticks title selection dimensional digits dataset random projection using random unitary matrix examples scikit learn user guide release print computing random projection rng random randomstate qr_economic rng normal size n_features x_projected dot plot_embedding x_projected random projection digits projection first principal components print computing pca projection time x_pca decomposition randomizedpca n_components fit_transform plot_embedding x_pca principal components projection digits time 2fs time projection first linear discriminant components print computing lda projection copy flat shape make invertible time x_lda lda lda n_components fit_transform plot_embedding x_lda linear discriminant projection digits time 2fs time isomap projection digits dataset print computing isomap embedding time x_iso manifold isomap n_neighbors n_components fit_transform print done plot_embedding x_iso isomap projection digits time 2fs time locally linear embedding digits dataset print computing lle embedding clf manifold locallylinearembedding n_neighbors n_components method standard time x_lle clf fit_transform print done reconstruction error clf reconstruction_error_ plot_embedding x_lle locally linear embedding digits time 2fs time modified locally linear embedding digits dataset print computing modified lle embedding clf manifold locallylinearembedding n_neighbors n_components chapter example gallery scikit learn user guide release method modified time x_mlle clf fit_transform print done reconstruction error clf reconstruction_error_ plot_embedding x_mlle modified locally linear embedding digits time 2fs time hlle embedding digits dataset print computing hessian lle embedding clf manifold locallylinearembedding n_neighbors n_components method hessian time x_hlle clf fit_transform print done reconstruction error clf reconstruction_error_ plot_embedding x_hlle hessian locally linear embedding digits time 2fs time ltsa embedding digits dataset print computing ltsa embedding clf manifold locallylinearembedding n_neighbors n_components method ltsa time x_ltsa clf fit_transform print done reconstruction error clf reconstruction_error_ plot_embedding x_ltsa local tangent space alignment digits time 2fs time show figure swiss roll reduction lle swiss roll reduction lle illustration swiss roll reduction locally linear embedding examples scikit learn user guide release script output computing lle embedding done reconstruction error 68564e python source code plot_swissroll author fabian pedregosa fabian pedregosa inria license bsd inria print __doc__ import pylab import needed modify way figure behaves mpl_toolkits mplot3d import axes3d axes3d locally linear embedding swiss roll sklearn import manifold datasets color datasets samples_generator make_swiss_roll n_samples print computing lle embedding x_r err manifold locally_linear_embedding n_neighbors n_components chapter example gallery scikit learn user guide release print done reconstruction error err plot result fig figure try compatibility matplotlib fig add_subplot projection scatter color cmap spectral except fig add_subplot scatter color cmap spectral set_title original data fig add_subplot scatter x_r x_r color cmap spectral axis tight xticks yticks title projected data show gaussian mixture models examples concerning sklearn mixture package 
4117: figure gaussian mixture model ellipsoids gaussian mixture model ellipsoids plot condence ellipsoids mixture two gaussians variational dirichlet process models access components data note model necessarily use components model effectively use many needed good property dirichlet process prior see model splits components arbitrarily trying many components dirichlet process model adapts number state automatically example doesnt show low dimensional space another advantage dirichlet process model full covariance matrices effectively even less examples per cluster dimensions data due regularization properties inference algorithm 
4118: examples scikit learn user guide release python source code plot_gmm import itertools import numpy scipy import linalg import pylab import matplotlib mpl sklearn import mixture number samples per component n_samples generate random sample two components random seed array dot random randn n_samples random randn n_samples array fit mixture gaussians using five components gmm mixture gmm n_components covariance_type full gmm fit fit dirichlet process mixture gaussians using five components dpgmm mixture dpgmm n_components covariance_type full chapter example gallery scikit learn user guide release dpgmm fit color_iter itertools cycle clf title enumerate gmm gmm dpgmm dirichlet process gmm splot subplot clf predict mean covar color enumerate zip clf means_ clf _get_covars color_iter linalg eigh covar linalg norm use every component access unless needs shouldnt plot redundant components continue scatter color color plot ellipse show gaussian component angle arctan angle angle convert degrees ell mpl patches ellipse mean angle color color ell set_clip_box splot bbox ell set_alpha splot add_artist ell xlim ylim xticks yticks title title show figure gmm classication gmm classication demonstration gaussian mixture models classication plots predicted labels training held test data using variety gmm classiers iris dataset compares gmms spherical diagonal full tied covariance matrices increasing order performance although one would expect full covariance perform best general prone overtting small datasets generalize well held test data plots train data shown dots test data shown crosses iris dataset four dimensional examples scikit learn user guide release rst two dimensions shown thus points separated dimensions 
4119: python source code plot_gmm_classifier print __doc__ author ron weiss ronweiss gmail com gael varoquaux license bsd style 
4120: import pylab import matplotlib mpl import numpy sklearn import datasets sklearn cross_validation import stratifiedkfold sklearn mixture import gmm chapter example gallery scikit learn user guide release def make_ellipses gmm color enumerate rgb linalg eigh gmm _get_covars linalg norm angle arctan2 angle angle convert degrees ell mpl patches ellipse gmm means_ angle color color ell set_clip_box bbox ell set_alpha add_artist ell iris datasets load_iris break dataset non overlapping training testing sets skf stratifiedkfold iris target take first fold train_index test_index skf __iter__ next x_train iris data train_index y_train iris target train_index x_test iris data test_index y_test iris target test_index n_classes len unique y_train try gmms using different types covariances classifiers dict covar_type gmm n_components n_classes covariance_type covar_type init_params n_iter covar_type spherical diag tied full n_classifiers len classifiers figure figsize n_classifiers subplots_adjust bottom top hspace wspace left right index name classifier enumerate classifiers iteritems since class labels training data initialize gmm parameters supervised manner classifier means_ array x_train y_train mean axis xrange n_classes train parameters using algorithm classifier fit x_train subplot n_classifiers index make_ellipses classifier color enumerate rgb data iris data iris target scatter data data color color plot test data crosses label iris target_names examples scikit learn user guide release color enumerate rgb data x_test y_test plot data data color color y_train_pred classifier predict x_train train_accuracy mean y_train_pred ravel y_train ravel text train accuracy train_accuracy transform transaxes y_test_pred classifier predict x_test test_accuracy mean y_test_pred ravel y_test ravel text test accuracy test_accuracy transform transaxes xticks yticks title name legend loc lower right prop dict size show figure density estimation mixture gaussians density estimation mixture gaussians plot density estimation mixture two gaussians data generated two gaussians different centers covariance matrices 
4121: chapter example gallery scikit learn user guide release python source code plot_gmm_pdf import numpy import pylab sklearn import mixture n_samples generate random sample two components random seed array x_train dot random randn n_samples random randn n_samples array clf mixture gmm n_components covariance_type full clf fit x_train linspace linspace meshgrid ravel ravel log clf eval reshape shape contour colorbar shrink extend examples scikit learn user guide release scatter x_train x_train axis tight show figure gaussian mixture model selection gaussian mixture model selection example shows model selection perfomed gaussian mixture models using information theoretic criteria bic model selection concerns covariance type number components model case aic also provides right result shown save time bic better suited problem identify right model unlike bayesian procedures inferences prior free case model components full covariance corresponds true generative model selected 
4122: chapter example gallery scikit learn user guide release python source code plot_gmm_selection print __doc__ import itertools import numpy scipy import linalg import pylab import matplotlib mpl sklearn import mixture number samples per component n_samples generate random sample two components random seed array dot random randn n_samples random randn n_samples array lowest_bic infty bic n_components_range range cv_types spherical tied diag full examples scikit learn user guide release cv_type cv_types n_components n_components_range fit mixture gaussians gmm mixture gmm n_components n_components covariance_type cv_type gmm fit bic append gmm bic bic lowest_bic lowest_bic bic best_gmm gmm bic array bic color_iter itertools cycle clf best_gmm bars plot bic scores spl subplot cv_type color enumerate zip cv_types color_iter xpos array n_components_range bars append bar xpos bic len n_components_range len n_components_range width color color xticks n_components_range ylim bic min bic max bic max title bic score per model xpos mod bic argmin len n_components_range floor bic argmin len n_components_range text xpos bic min bic max fontsize spl set_xlabel number components spl legend bars cv_types plot winner splot subplot clf predict mean covar color enumerate zip clf means_ clf covars_ color_iter linalg eigh covar continue scatter color color plot ellipse show gaussian component angle arctan2 angle angle convert degrees ell mpl patches ellipse mean angle color color ell set_clip_box splot bbox ell set_alpha splot add_artist ell xlim ylim xticks yticks title selected gmm full model components subplots_adjust hspace bottom show chapter example gallery scikit learn user guide release figure gaussian mixture model sine curve gaussian mixture model sine curve example highlights advantages dirichlet process complexity control dealing sparse data dataset formed points loosely spaced following noisy sine curve gmm class using expectation maximization algorithm mixture gaussian components nds small components little structure dirichlet process however show model either learn global structure data small alpha easily interpolate nding relevant local structure large alpha never falling problems shown gmm class 
4123: python source code plot_gmm_sin import itertools import numpy examples scikit learn user guide release scipy import linalg import pylab import matplotlib mpl sklearn import mixture number samples per component n_samples generate random sample following sine curve random seed zeros n_samples step n_samples xrange shape step random normal sin random normal color_iter itertools cycle clf title enumerate mixture gmm n_components covariance_type full n_iter expectation maximization mixture dpgmm n_components covariance_type full dirichlet process alpha alpha n_iter mixture dpgmm n_components covariance_type diag dirichlet process alpha alpha n_iter clf fit splot subplot clf predict mean covar color enumerate zip clf means_ clf _get_covars color_iter linalg eigh covar linalg norm use every component access unless needs shouldnt plot redundant components continue scatter color color plot ellipse show gaussian component angle arctan angle angle convert degrees ell mpl patches ellipse mean angle color color ell set_clip_box splot bbox ell set_alpha splot add_artist ell xlim ylim chapter example gallery scikit learn user guide release title title xticks yticks show nearest neighbors examples concerning sklearn neighbors package 
4124: figure nearest neighbors classication nearest neighbors classication sample usage nearest neighbors classication plot decision boundaries class 
4125: examples scikit learn user guide release python source code plot_classification print __doc__ import numpy import pylab matplotlib colors import listedcolormap sklearn import neighbors datasets n_neighbors import data play iris datasets load_iris iris data take first two features could avoid ugly slicing using two dim dataset iris target step size mesh create color maps cmap_light listedcolormap ffaaaa aaffaa aaaaff cmap_bold listedcolormap ff0000 00ff00 0000ff weights uniform distance create instance neighbours classifier fit data clf neighbors kneighborsclassifier n_neighbors weights weights clf fit plot decision boundary asign color point mesh x_min m_max y_min y_max x_min x_max min max y_min y_max min max meshgrid arange x_min x_max arange y_min y_max clf predict ravel ravel put result color plot reshape shape figure pcolormesh cmap cmap_light plot also training points scatter cmap cmap_bold title class classification weights n_neighbors weights axis tight show nearest centroid classication sample usage nearest centroid classication plot decision boundaries class 
4126: chapter example gallery scikit learn user guide release figure nearest centroid classication script output none python source code plot_nearest_centroid print __doc__ import numpy import pylab matplotlib colors import listedcolormap sklearn import datasets sklearn neighbors import nearestcentroid n_neighbors import data play examples scikit learn user guide release iris datasets load_iris iris data take first two features could avoid ugly slicing using two dim dataset iris target step size mesh create color maps cmap_light listedcolormap ffaaaa aaffaa aaaaff cmap_bold listedcolormap ff0000 00ff00 0000ff shrinkage none create instance neighbours classifier fit data clf nearestcentroid shrink_threshold shrinkage clf fit y_pred clf predict print shrinkage mean y_pred plot decision boundary asign color point mesh x_min m_max y_min y_max x_min x_max min max y_min y_max min max meshgrid arange x_min x_max arange y_min y_max clf predict ravel ravel put result color plot reshape shape figure pcolormesh cmap cmap_light plot also training points scatter cmap cmap_bold title class classification shrink_threshold shrinkage axis tight show figure nearest neighbors regression nearest neighbors regression demonstrate resolution regression problem using nearest neighbor interpolation target using barycenter constant weights 
4127: chapter example gallery scikit learn user guide release python source code plot_regression print __doc__ author alexandre gramfort alexandre gramfort inria license bsd inria fabian pedregosa fabian pedregosa inria generate sample data import numpy import pylab sklearn import neighbors random seed sort random rand axis linspace newaxis sin ravel add noise targets random rand fit regression model examples scikit learn user guide release n_neighbors weights enumerate uniform distance knn neighbors kneighborsregressor n_neighbors weights weights knn fit predict subplot scatter label data plot label prediction axis tight legend title kneighborsregressor weights n_neighbors weights show semi supervised classication examples concerning sklearn semi_supervised package 
4128: figure label propagation digits demonstrating performance label propagation digits demonstrating performance example demonstrates power semisupervised learning training label spreading model classify handwritten digits sets labels handwritten digit dataset total points model trained using points labeled results form confusion matrix series metrics class good end top uncertain predictions shown 
4129: chapter example gallery scikit learn user guide release script output label spreading model labeled unlabeled points total precision recall score support avg total confusion matrix examples scikit learn user guide release python source code plot_label_propagation_digits print __doc__ authors clay woolam clay woolam org licence bsd import numpy import pylab scipy import stats sklearn import datasets sklearn semi_supervised import label_propagation sklearn metrics import metrics sklearn metrics metrics import confusion_matrix digits datasets load_digits rng random randomstate indices arange len digits data rng shuffle indices digits data indices digits target indices images digits images indices n_total_samples len n_labeled_points indices arange n_total_samples unlabeled_set indices n_labeled_points shuffle everything around y_train copy y_train unlabeled_set learn labelspreading lp_model label_propagation labelspreading gamma max_iters lp_model fit y_train predicted_labels lp_model transduction_ unlabeled_set true_labels unlabeled_set confusion_matrix true_labels predicted_labels labels lp_model classes_ print label spreading model labeled unlabeled points total n_labeled_points n_total_samples n_labeled_points n_total_samples print metrics classification_report true_labels predicted_labels print confusion matrix print calculate uncertainty values transduced distribution pred_entropies stats distributions entropy lp_model label_distributions_ chapter example gallery scikit learn user guide release pick top uncertain labels uncertainty_index argsort pred_entropies plot figure figsize index image_index enumerate uncertainty_index image images image_index sub add_subplot index sub imshow image cmap gray_r xticks yticks sub set_title predict ntrue lp_model transduction_ image_index image_index suptitle learning small amount labeled data show figure label propagation digits active learning label propagation digits active learning demonstrates active learning technique learn handwritten digits using label propagation start training label propagation model labeled points select top uncertain points label next train labeled points original new ones repeat process four times model trained labeled examples plot appear showing top uncertain digits iteration training may may contain mistakes train next model true labels 
4130: examples scikit learn user guide release script output iteration ______________________________________________________________________ label spreading model labeled unlabeled total precision recall score support avg total confusion matrix chapter example gallery scikit learn user guide release iteration ______________________________________________________________________ label spreading model labeled unlabeled total precision recall score support avg total confusion matrix iteration ______________________________________________________________________ label spreading model labeled unlabeled total precision recall score support avg total confusion matrix iteration ______________________________________________________________________ examples scikit learn user guide release label spreading model labeled unlabeled total precision recall score support avg total confusion matrix iteration ______________________________________________________________________ label spreading model labeled unlabeled total precision recall score support avg total confusion matrix python source code plot_label_propagation_digits_active_learning chapter example gallery scikit learn user guide release print __doc__ authors clay woolam clay woolam org licence bsd import numpy import pylab scipy import stats sklearn import datasets sklearn semi_supervised import label_propagation sklearn metrics import classification_report confusion_matrix digits datasets load_digits rng random randomstate indices arange len digits data rng shuffle indices digits data indices digits target indices images digits images indices n_total_samples len n_labeled_points unlabeled_indices arange n_total_samples n_labeled_points figure range y_train copy y_train unlabeled_indices lp_model label_propagation labelspreading gamma max_iters lp_model fit y_train predicted_labels lp_model transduction_ unlabeled_indices true_labels unlabeled_indices confusion_matrix true_labels predicted_labels labels lp_model classes_ print iteration print label spreading model labeled unlabeled total n_labeled_points n_total_samples n_labeled_points n_total_samples print classification_report true_labels predicted_labels print confusion matrix print compute entropies transduced label distributions pred_entropies stats distributions entropy lp_model label_distributions_ select five digit examples classifier uncertain uncertainty_index uncertainty_index argsort pred_entropies keep track indicies get labels examples scikit learn user guide release delete_indices array text model nfit labels size index image_index enumerate uncertainty_index image images image_index sub add_subplot index sub imshow image cmap gray_r sub set_title predict ntrue lp_model transduction_ image_index image_index size sub axis labeling points remote labeled set delete_index unlabeled_indices image_index delete_indices concatenate delete_indices delete_index unlabeled_indices delete unlabeled_indices delete_indices n_labeled_points suptitle active learning label propagation nrows show uncertain labels learn next model subplots_adjust show figure label propagation learning complex structure label propagation learning complex structure example labelpropagation learning complex internal structure demonstrate manifold learning outer circle labeled red inner circle blue label groups lie inside distinct shape see labels propagate correctly around circle 
4131: chapter example gallery scikit learn user guide release python source code plot_label_propagation_structure print __doc__ authors clay woolam clay woolam org licence bsd andreas mueller amueller ais uni bonn import numpy import pylab sklearn semi_supervised import label_propagation sklearn datasets import make_circles generate ring inner box n_samples make_circles n_samples n_samples shuffle false outer inner labels ones n_samples labels outer labels inner learn labelspreading label_spread label_propagation labelspreading kernel knn alpha label_spread fit labels plot output labels output_labels label_spread transduction_ figure figsize subplot plot_outer_labeled plot labels outer plot_unlabeled plot labels labels plot_inner_labeled plot labels inner labels outer legend plot_outer_labeled plot_inner_labeled plot_unlabeled outer labeled inner labeled unlabeled upper left labels inner examples scikit learn user guide release numpoints shadow false title raw data classes red blue subplot output_label_array asarray output_labels outer_numbers output_label_array outer inner_numbers output_label_array inner plot_outer plot outer_numbers outer_numbers plot_inner plot inner_numbers inner_numbers legend plot_outer plot_inner outer learned inner learned upper left numpoints shadow false title labels learned label spreading knn subplots_adjust left bottom right top show figure decision boundary label propagation versus svm iris dataset decision boundary label propagation versus svm iris dataset comparison decision boundary generated iris dataset label propagation svm demonstrates label propagation learning good boundary even small amount labeled data 
4132: chapter example gallery scikit learn user guide release python source code plot_label_propagation_versus_svm_iris print __doc__ authors clay woolam clay woolam org licence bsd import numpy import pylab sklearn import datasets sklearn import svm sklearn semi_supervised import label_propagation rng random randomstate iris datasets load_iris iris data iris target step size mesh y_30 copy y_30 rng rand len y_50 copy examples scikit learn user guide release y_50 rng rand len create instance svm fit data scale data since want plot support vectors ls30 label_propagation labelspreading fit y_30 y_30 ls50 label_propagation labelspreading fit y_50 y_50 ls100 label_propagation labelspreading fit rbf_svc svm svc kernel rbf fit create mesh plot x_min x_max min max y_min y_max min max meshgrid arange x_min x_max arange y_min y_max title plots titles label spreading data label spreading data label spreading data svc rbf kernel color_map clf y_train enumerate ls30 ls50 ls100 rbf_svc plot decision boundary asign color point mesh x_min m_max y_min y_max subplot clf predict ravel ravel put result color plot reshape shape contourf cmap paired axis plot also training points colors color_map y_train scatter colors cmap paired title titles text unlabeled points colored white show support vector machines examples concerning sklearn svm package 
4133: svm custom kernel simple usage support vector machines classify sample plot decision surface support vectors 
4134: chapter example gallery scikit learn user guide release figure svm custom kernel python source code plot_custom_kernel print __doc__ import numpy import pylab sklearn import svm datasets import data play iris datasets load_iris iris data take first two features could avoid ugly slicing using two dim dataset iris target examples scikit learn user guide release def my_kernel create custom kernel array return dot dot step size mesh create instance svm fit data clf svm svc kernel my_kernel clf fit plot decision boundary asign color point mesh x_min m_max y_min y_max x_min x_max min max y_min y_max min max meshgrid arange x_min x_max arange y_min y_max clf predict ravel ravel put result color plot reshape shape pcolormesh cmap paired plot also training points scatter cmap paired title class classification using support vector machine custom kernel axis tight show figure plot different svm classiers iris dataset plot different svm classiers iris dataset comparison different linear svm classiers iris dataset plot decision surface four different svm classiers 
4135: chapter example gallery scikit learn user guide release python source code plot_iris print __doc__ import numpy import pylab sklearn import svm datasets import data play iris datasets load_iris iris data take first two features could avoid ugly slicing using two dim dataset iris target step size mesh create instance svm fit data scale data since want plot support vectors svm regularization parameter svc svm svc kernel linear fit rbf_svc svm svc kernel rbf gamma fit poly_svc svm svc kernel poly degree fit lin_svc svm linearsvc fit create mesh plot x_min x_max min max examples scikit learn user guide release y_min y_max min max meshgrid arange x_min x_max arange y_min y_max title plots titles svc linear kernel svc rbf kernel svc polynomial degree kernel linearsvc linear kernel clf enumerate svc rbf_svc poly_svc lin_svc plot decision boundary asign color point mesh x_min m_max y_min y_max subplot clf predict ravel ravel put result color plot reshape shape contourf cmap paired axis plot also training points scatter cmap paired title titles show figure one class svm non linear kernel rbf one class svm non linear kernel rbf one class svm unsupervised algorithm learns decision function novelty detection classifying new data similar different training set 
4136: chapter example gallery scikit learn user guide release python source code plot_oneclass print __doc__ import numpy import pylab import matplotlib font_manager sklearn import svm meshgrid linspace linspace generate train data random randn x_train generate regular novel observations random randn x_test generate abnormal novel observations x_outliers random uniform low high size fit model clf svm oneclasssvm kernel rbf gamma clf fit x_train y_pred_train clf predict x_train y_pred_test clf predict x_test y_pred_outliers clf predict x_outliers n_error_train y_pred_train y_pred_train size examples scikit learn user guide release n_error_test y_pred_test y_pred_test size n_error_outliers y_pred_outliers y_pred_outliers size plot line points nearest vectors plane clf decision_function ravel ravel reshape shape title novelty detection contourf levels linspace min cmap blues_r contour levels linewidths colors red contourf levels max colors orange scatter x_train x_train white scatter x_test x_test green scatter x_outliers x_outliers red axis tight xlim ylim legend collections learned frontier training observations new regular observations new abnormal observations loc upper left prop matplotlib font_manager fontproperties size xlabel error train errors novel regular errors novel abnormal n_error_train n_error_test n_error_outliers show figure rbf svm parameters rbf svm parameters example illustrates effect parameters gamma rbf kernel svm intuitively gamma parameter denes far inuence single training example reaches low values meaning far high values meaning close parameter trades misclassication training examples simplicity decision surface low makes decision surface smooth high aims classifying training examples correctly 
4137: chapter example gallery scikit learn user guide release script output best classifier svc cache_size class_weight none coef0 degree gamma kernel rbf probability false shrinking true tol verbose false python source code plot_rbf_parameters print __doc__ import numpy import pylab sklearn import svm sklearn datasets import load_iris sklearn preprocessing import scaler iris load_iris iris data take dimensions iris target scaler scaler scaler fit_transform examples scikit learn user guide release meshgrid linspace linspace random seed gamma_range 1e1 c_range 1e2 1e4 figure c_range gamma gamma_range fit model clf svm svc gamma gamma clf fit plot decision function datapoint grid clf decision_function ravel ravel reshape shape subplot title gamma gamma pcolormesh cmap jet scatter cmap jet xticks yticks axis tight subplots_adjust left right bottom top show figure svm maximum margin separating hyperplane svm maximum margin separating hyperplane plot maximum margin separating hyperplane within two class separable dataset using support vector machines classier linear kernel 
4138: chapter example gallery scikit learn user guide release python source code plot_separating_hyperplane print __doc__ import numpy import pylab sklearn import svm create separable points random seed random randn random randn fit model clf svm svc kernel linear clf fit get separating hyperplane clf coef_ linspace clf intercept_ plot parallels separating hyperplane pass support vectors clf support_vectors_ examples scikit learn user guide release yy_down clf support_vectors_ yy_up plot line points nearest vectors plane plot plot yy_down plot yy_up scatter clf support_vectors_ clf support_vectors_ facecolors none scatter cmap paired axis tight show figure svm separating hyperplane unbalanced classes svm separating hyperplane unbalanced classes find optimal separating hyperplane using svc classes unbalanced rst separating plane plain svc plot dashed separating hyperplane automatically correction unbalanced classes 
4139: chapter example gallery scikit learn user guide release python source code plot_separating_hyperplane_unbalanced print __doc__ import numpy import pylab sklearn import svm create separable points random seed n_samples_1 n_samples_2 random randn n_samples_1 random randn n_samples_2 n_samples_1 n_samples_2 fit model get separating hyperplane clf svm svc kernel linear clf fit clf coef_ linspace clf intercept_ examples scikit learn user guide release get separating hyperplane using weighted classes wclf svm svc kernel linear class_weight wclf fit wclf coef_ wyy wclf intercept_ plot separating hyperplanes samples plot label weights plot wyy label weights scatter cmap paired legend axis tight show figure svm anova svm univariate feature selection svm anova svm univariate feature selection example shows perform univariate feature running svc support vector classier improve classication scores 
4140: chapter example gallery scikit learn user guide release python source code plot_svm_anova print __doc__ import numpy import pylab sklearn import svm datasets feature_selection cross_validation sklearn pipeline import pipeline import data play digits datasets load_digits digits target throw away data curse dimension settings digits data n_samples len reshape n_samples add non informative features hstack random random n_samples create feature selection transform instance svm combine together full blown estimator transform feature_selection selectpercentile feature_selection f_classif examples scikit learn user guide release clf pipeline anova transform svc svm svc plot cross validation score function percentile features score_means list score_stds list percentiles percentile percentiles clf set_params anova__percentile percentile compute cross validation score using cpus this_scores cross_validation cross_val_score clf n_jobs score_means append this_scores mean score_stds append this_scores std errorbar percentiles score_means array score_stds title performance svm anova varying percentile features selected xlabel percentile ylabel prediction rate axis tight show figure svm svc support vector classication svm svc support vector classication classication application svm used iris dataset used example decision boundaries shown points training set 
4141: chapter example gallery scikit learn user guide release python source code plot_svm_iris print __doc__ code source gael varoqueux modified documentation merge jaques grobler license bsd import numpy import pylab sklearn import svm datasets import data play iris datasets load_iris iris data take first two features iris target step size mesh clf svm svc kernel linear create instance svm classifier fit data clf fit plot decision boundary asign color point mesh x_min m_max y_min y_max x_min x_max min max y_min y_max min max meshgrid arange x_min x_max arange y_min y_max clf predict ravel ravel put result color plot reshape shape figure figsize pcolormesh cmap paired plot also training points examples scikit learn user guide release scatter cmap paired xlabel sepal length ylabel sepal width xlim min max ylim min max xticks yticks show figure svm kernels svm kernels three different types svm kernels displayed polynomial rbf especially useful data points linearly seperable 
4142: python source code plot_svm_kernels chapter example gallery scikit learn user guide release print __doc__ code source gael varoqueux license bsd import numpy import pylab sklearn import svm dataset targets figure number fignum fit model kernel linear poly rbf clf svm svc kernel kernel gamma clf fit plot line points nearest vectors plane figure fignum figsize clf scatter clf support_vectors_ clf support_vectors_ facecolors none zorder scatter zorder cmap paired axis tight x_min x_max y_min y_max mgrid x_min x_max 200j y_min y_max 200j clf decision_function ravel ravel put result color plot examples scikit learn user guide release reshape shape figure fignum figsize pcolormesh cmap paired contour colors linestyles levels xlim x_min x_max ylim y_min y_max xticks yticks fignum fignum show figure svm margins example svm margins example plots illustrate effect parameter seperation line large value basically tells model much faith datas distrubution consider points close line seperation small value includes observations allowing margins calculated using data area 
4143: python source code plot_svm_margin print __doc__ code source gael varoqueux chapter example gallery scikit learn user guide release modified documentation merge jaques grobler license bsd import numpy import pylab sklearn import svm create separable points random seed random randn random randn figure number fignum fit model name penality unreg reg clf svm svc kernel linear penality clf fit get separating hyperplane clf coef_ linspace clf intercept_ plot parallels separating hyperplane pass support vectors margin sqrt sum clf coef_ yy_down margin yy_up margin plot line points nearest vectors plane figure fignum figsize clf plot plot yy_down plot yy_up scatter clf support_vectors_ clf support_vectors_ facecolors none zorder scatter zorder cmap paired axis tight x_min x_max y_min y_max mgrid x_min x_max 200j y_min y_max 200j clf predict ravel ravel put result color plot reshape shape figure fignum figsize pcolormesh cmap paired examples scikit learn user guide release xlim x_min x_max ylim y_min y_max xticks yticks fignum fignum show figure non linear svm non linear svm perform binary classication using non linear svc rbf kernel target predict xor inputs color map illustrates decision function learn svc 
4144: chapter example gallery scikit learn user guide release python source code plot_svm_nonlinear print __doc__ import numpy import pylab sklearn import svm meshgrid linspace linspace random seed random randn logical_xor fit model clf svm nusvc clf fit plot decision function datapoint grid clf decision_function ravel ravel reshape shape imshow interpolation nearest extent min max min max aspect auto origin lower cmap puor_r contours contour levels linewidths examples scikit learn user guide release linetypes scatter cmap paired xticks yticks axis show figure seleting hyper parameter gamma rbf kernel svm seleting hyper parameter gamma rbf kernel svm svms particular kernelized svms setting hyperparameter crucial non trivial practice usually set using hold validation set using cross validation example shows use stratied fold crossvalidation set gamma rbf kernel svm use logarithmic grid parameters 
4145: chapter example gallery scikit learn user guide release script output best classifier svc cache_size class_weight none coef0 degree gamma kernel rbf probability false shrinking true tol verbose false python source code plot_svm_parameters_selection print __doc__ import numpy import pylab sklearn svm import svc sklearn preprocessing import scaler sklearn datasets import load_iris sklearn cross_validation import stratifiedkfold sklearn grid_search import gridsearchcv iris_dataset load_iris iris_dataset data iris_dataset target usually good idea scale data svm training cheating bit example scaling data instead fitting transformation trainingset examples scikit learn user guide release applying test set 
4146: scaler scaler scaler fit_transform initial search logarithmic grid basis often helpful using basis finer tuning achieved much higher cost 
4147: c_range arange gamma_range arange param_grid dict gamma gamma_range c_range grid gridsearchcv svc param_grid param_grid stratifiedkfold grid fit print best classifier grid best_estimator_ plot scores grid grid_scores_ contains parameter settings scores score_dict grid grid_scores_ extract scores scores score_dict scores array scores reshape len c_range len gamma_range make nice figure figure figsize subplots_adjust left right bottom top imshow scores interpolation nearest cmap spectral xlabel gamma ylabel colorbar xticks arange len gamma_range gamma_range rotation yticks arange len c_range c_range show figure support vector regression svr using linear non linear kernels support vector regression svr using linear non linear kernels toy example regression using linear polynominial rbf kernels 
4148: chapter example gallery scikit learn user guide release python source code plot_svm_regression print __doc__ generate sample data import numpy sort random rand axis sin ravel add noise targets random rand fit regression model sklearn svm import svr svr_rbf svr kernel rbf 1e3 gamma svr_lin svr kernel linear 1e3 svr_poly svr kernel poly 1e3 degree y_rbf svr_rbf fit predict y_lin svr_lin fit predict y_poly svr_poly fit predict examples scikit learn user guide release look results import pylab scatter label data hold plot y_rbf label rbf model plot y_lin label linear model plot y_poly label polynomial model xlabel data ylabel target title support vector regression legend show figure svm weighted samples svm weighted samples plot decision function weighted dataset size points proportional weight 
4149: chapter example gallery scikit learn user guide release python source code plot_weighted_samples print __doc__ import numpy import pylab sklearn import svm create points random seed random randn random randn sample_weight abs random randn assign bigger weight last samples sample_weight fit model clf svm svc clf fit sample_weight sample_weight plot decision function meshgrid linspace linspace clf decision_function ravel ravel reshape shape examples scikit learn user guide release plot line points nearest vectors plane contourf alpha scatter sample_weight alpha cmap bone axis show decision trees examples concerning sklearn tree package 
4150: figure plot decision surface decision tree iris dataset plot decision surface decision tree iris dataset plot decision surface decision tree trained pairs features iris dataset pair iris features decision tree learns decision boundaries made combinations simple thresholding rules inferred training samples 
4151: chapter example gallery scikit learn user guide release python source code plot_iris print __doc__ import numpy import pylab sklearn datasets import load_iris sklearn tree import decisiontreeclassifier parameters n_classes plot_colors bry plot_step load data iris load_iris pairidx pair enumerate take two corresponding features iris data pair iris target shuffle idx arange shape examples scikit learn user guide release random seed random shuffle idx idx idx standardize mean mean axis std std axis mean std train clf decisiontreeclassifier fit plot decision boundary subplot pairidx x_min x_max min max y_min y_max min max meshgrid arange x_min x_max plot_step arange y_min y_max plot_step clf predict ravel ravel reshape shape contourf cmap paired xlabel iris feature_names pair ylabel iris feature_names pair axis tight plot training points color zip xrange n_classes plot_colors idx scatter idx idx color label iris target_names cmap paired axis tight suptitle decision surface decision tree using paired features legend show figure decision tree regression decision tree regression regression decision trees decision tree used sine curve addition noisy observation result learns local linear regressions approximating sine curve 
4152: chapter example gallery see maximum depth tree controled max_depth parameter set high decision trees learn details training data learn noise overt 
4153: scikit learn user guide release python source code plot_tree_regression print __doc__ import numpy create random dataset rng random randomstate sort rng rand axis sin ravel rng rand fit regression model sklearn tree import decisiontreeregressor clf_1 decisiontreeregressor max_depth clf_2 decisiontreeregressor max_depth clf_1 fit clf_2 fit predict x_test arange newaxis y_1 clf_1 predict x_test examples scikit learn user guide release y_2 clf_2 predict x_test plot results import pylab figure scatter label data plot x_test y_1 label max_depth linewidth plot x_test y_2 label max_depth linewidth xlabel data ylabel target title decision tree regression legend show chapter example gallery chapter three development contributing project community effort everyone welcome contribute project hosted http github com scikit learn scikit learn submitting bug report case experience issues using package hesitate submit ticket bug tracker also welcome post feature requests links pull requests 
4154: retrieving latest code use git version control github hosting main repository check latest sources command git clone git github com scikit learn scikit learn git write privileges git clone git github com scikit learn scikit learn git run development version cumbersome reinstall package time update sources thus preferred add scikit learn directory pythonpath build extension place python setup build_ext inplace unix like systems simply type make top level folder build place launch tests look makefile additional utilities 
4155: contributing code note avoid duplicating work highly advised contact developers mailing list starting work non trivial feature https lists sourceforge net lists listinfo scikit learn general scikit learn user guide release contribute preferred way contribute scikit learn fork main repository github create account github already one fork project repository click fork button near top page creates copy code account github server 
4156: clone copy local disk git clone git github com yourlogin scikit learn git work copy computer using git version control git add modified_files git commit git push origin master 
4157: changes trivial xes better directly work branch name feature working case replace step step create branch host changes publish public repo git checkout feature git add modified_files git commit git push origin feature ready pushed changes github repo web page repo click pull request send pull request send email committers might also send email mailing list order get visibility 
4158: setup origin remote repository points yourlogin scikit learn git 
4159: note wish fetch merge main repository instead forked one need add another remote use instead origin choose name upstream command git remote add upstream git github com scikit learn scikit learn git seems like magic look git documentation web recommended check contribution complies following rules submitting pull request follow coding guidelines see applicable use validation tools code sklearn utils submodule list utility routines available developers found utilities developers page 
4160: public methods informative docstrings sample usage presented doctests appropri ate 
4161: tests pass everything rebuilt scratch unix like systems check toplevel source folder make adding additional functionality provide least one example script examples folder look examples reference examples demonstrate new functionality useful practice possible compare methods available scikit learn 
4162: chapter development scikit learn user guide release least one paragraph narrative documentation links references literature pdf links possible example documentation also include expected time space complexity algorithm scalability algorithm scale large number samples scale dimensionality n_features expected lower build documentation see documentation section 
4163: also check common programming errors following tools code good unittest coverage least check pip install nose coverage nosetests coverage path tests_for_package pyakes warnings check pip install pyflakes pyflakes path module pep8 warnings check pip install pep8 pep8 path module autopep8 help easy redundant errors pip install autopep8 autopep8 path pep8 bonus points contributions include performance analysis benchmark script proling output please report mailing list github wiki also check optimize speed guide details proling cython optimizations 
4164: note current state scikit learn code base compliant guidelines expect enforcing constraints new contributions get overall code base quality right direction 
4165: easyfix issues great way start contributing scikit learn pick item list easyfix issues issue tracker resolving issues allow start contributing project without much prior knowledge assistance area greatly appreciated experienced developers helps free time concentrate issues 
4166: documentation glad accept sort documentation function docstrings restructuredtext documents like one tutorials etc restructuredtext documents live source code repository doc directory edit documentation using text editor generate html output typing make html doc directory alternatively make html noplot used quickly generate documentation without example gallery resulting html les placed _build html viewable web browser see readme doc directory information building documentation need sphinx matplotlib 
4167: contributing scikit learn user guide release writing documentation important keep good compromise mathematical algorith mic details give intuition reader algorithm best always start small paragraph hand waiving explanation method data gure coming example illustrat ing 
4168: warning sphinx version best documentation build many version sphinx possible different versions tend behave slightly differently get best results use version 
4169: developers web site information found developers wiki 
4170: ways contribute code way contribute scikit learn instance documentation also important part project often doesnt get much attention deserves typo documentation made improvements hesitate send email mailing list submit github pull request full documentation found doc directory also helps spread word reference project blog articles link website simply say use coding guidelines following guidelines new code written course special cases exceptions rules however following rules submitting new code makes review easier new code integrated less time uniformly formatted code makes easier share code ownership scikit learn project tries closely follow ofcial python guidelines detailed pep8 detail code formatted indented please read follow addition add following guidelines use underscores separate words non class names n_samples rather nsamples avoid multiple statements one line prefer line return control statement use relative imports references inside scikit learn please dont use import case considered harmful ofcial python recommendations makes code harder read origin symbols longer explicitly referenced important prevents using static analysis tool like pyakes automatically bugs scikit learn 
4171: use numpy docstring standard docstrings 
4172: good example code like found 
4173: input validation module sklearn utils contains various functions input validation conversion sometimes asarray sufces validation use asanyarray atleast_2d since let numpys matrix different api means dot product matrix hadamard product ndarray 
4174: chapter development scikit learn user guide release cases sure call safe_asarray atleast2d_or_csr as_float_array array2d array like argument passed scikit learn api function exact function use depends mainly whether scipy sparse matrices must accepted information refer utilities developers page 
4175: random numbers code depends random number generator use numpy random random similar routines ensure repeatability error checking routine accept keyword random_state use con struct numpy random randomstate object see sklearn utils check_random_state utilities developers heres simple example code using guidelines sklearn utils import array2d check_random_state def choose_random_sample random_state choose random point parameters array like shape n_samples n_features array representing data random_state randomstate int seed default random number generator instance define state random permutations generator 
4176: returns numpy array shape n_features random point selected array2d random_state check_random_state random_state random_state randint shape return apis scikit learn objects uniform api try common basic api objects addition avoid proliferation framework code try adopt simple conventions limit minimum number methods object must implement 
4177: different objects main objects scikit learn one class implement multiple interfaces estimator base object implements estimator obj fit data predictor supervised learning unsupervised problems implements contributing scikit learn user guide release prediction obj predict data transformer ltering modifying data supervised unsupervised way implements new_data obj transform data tting transforming performed much efciently together separately implements new_data obj fit_transform data model model give goodness likelihood unseen data implements higher better score obj score data estimators api one predominant object estimator estimator object model based training data capable inferring properties new data instance classier regressor estimators implement method estimator fit built estimators also set_params method sets data independent parameters overriding previ ous parameter values passed __init__ method required object estimator estimators inherit sklearn base baseestimator 
4178: instantiation concerns creation object objects __init__ method might accept constants arguments determine estimators behavior like constant svms however take actual training data argument left fit method clf2 svc clf3 svc wrong arguments accepted __init__ keyword arguments default value words user able instantiate estimator without passing arguments arguments correspond hyperparameters describing model optimisation problem estimator tries solve addition every keyword argument accepted __init__ correspond attribute instance scikit learn relies relevant attributes set estimator model selection summarize __init__ look like def __init__ self param1 param2 self param1 param1 self param2 param2 logic parameters changed corresponding logic put parameters used following wrong def __init__ self param1 param2 param3 wrong parameters modified param1 chapter development scikit learn user guide release param2 self param1 param1 wrong objects attributes exactly name argument constructor self param3 param2 scikit learn relies mechanism introspect objects set parameters cross validation 
4179: fitting next thing probably want estimate parameters model implemented fit method fit method takes training data arguments one array case unsupervised learning two arrays case supervised learning note model tted using object holds reference however exceptions case precomputed kernels data must stored use predict method 
4180: parameters kwargs array like shape number samples number features array shape number samples optional data dependent parameters 
4181: shape shape requisite met exception type valueerror raised might ignored case unsupervised learning however make possible use estimator part pipeline mix supervised unsupervised transformers even unsupervised estimators kindly asked accept none keyword argument second position ignored estimator method return object self pattern useful able implement quick one liners ipython session y_predicted svc fit x_train y_train predict x_test depending nature algorithm fit sometimes also accept additional keywords arguments however parameter value assigned prior access data __init__ keyword argument parameters restricted directly data dependent variables instance gram matrix afnity matrix precomputed data matrix data dependent tolerance stopping criterion tol directly data dependent although optimal value according scoring function probably attribute ends expected overridden call fit second time without taking previous value account idempotent 
4182: optional arguments iterative algorithms number iterations specied integer called n_iter 
4183: unresolved api issues things must still decided happen predict called fit exception raised shape arrays match fit contributing scikit learn user guide release working notes unresolved issues todos remarks ongoing work developers advised maintain notes github wiki 
4184: specic models linear models coefcients stored array called coef_ independent term stored intercept_ 
4185: optimize speed following gives practical guidelines help write efcient code scikit learn project 
4186: note always useful prole code check performance assumptions also highly recommended review literature ensure implemented algorithm state art task investing costly implementation optimization times times hours efforts invested optimizing complicated implementation details rended irrele vant late discovery simple algorithmic tricks using another algorithm altogether better suited problem section sample algorithmic trick warm restarts cross validation gives example trick 
4187: python cython general scikit learn project emphasizes readability source code make easy project users dive source code understand algorithm behaves data also ease maintanability developers implementing new algorithm thus recommended start implementing python using numpy scipy taking care avoiding looping code using vectorized idioms libraries practice means trying replace nested loops calls equivalent numpy array methods goal avoid cpu wasting time python interpreter rather crunching numbers statistical model sometimes however algorithm cannot expressed efciently simple vectorized numpy code case recommended strategy following prole python implementation main bottleneck isolate dedicated module level func tion function reimplemented compiled extension module 
4188: exists well maintained bsd mit implementation algorithm big write cython wrapper include copy source code library scikit learn source tree strategy used classes svm linearsvc svm svc linear_model logisticregression wrappers liblinear libsvm 
4189: otherwise write optimized version python function using cython directly strategy used linear_model elasticnet linear_model sgdclassifier classes instance 
4190: move python version function tests use check results compiled extension consistent gold standard easy debug python version 
4191: code optimized simple bottleneck spottable proling check whether possible coarse grained parallelism amenable multi processing using joblib parallel class 
4192: chapter development scikit learn user guide release using cython include generated source code alongside cython source code goal make possible install scikit machine python numpy scipy compiler 
4193: proling python code order prole python code recommend write script loads prepare data use ipython integrated proler interactively exploring relevant part code suppose want prole non negative matrix factorization module scikit let setup new ipython session load digits dataset recognizing hand written digits example sklearn decomposition import nmf sklearn datasets import load_digits load_digits data starting proling session engaging tentative optimization iterations important measure total execution time function want optimize without kind proler overhead save somewhere later reference timeit nmf n_components tol fit loops best per loop look overall performance prole using prun magic command prun nmf nmf n_components tol fit function calls cpu seconds ordered internal time list reduced due restriction nmf ncalls tottime percall cumtime percall filename lineno function nmf _nls_subproblem nmf _pos nmf fit_transform nmf norm nmf _initialize_nmf nmf _sparseness nmf _neg nmf __init__ nmf fit totime columns interesting gives total time spent executing code given function ignoring time spent executing sub functions real total time local code sub function calls given cumtime column note use nmf restricts output lines contains nmf string useful quick look hotspot nmf python module self ignoring anything else begining output command without nmf lter prun nmf n_components tol fit function calls cpu seconds ordered internal time ncalls tottime percall cumtime percall filename lineno function numpy core _dotblas dot optimize speed scikit learn user guide release nmf _nls_subproblem nmf _pos method sum numpy ndarray objects nmf fit_transform method flatten numpy ndarray objects method numpy ndarray objects fromnumeric sum numpy linalg lapack_lite dgesdd nmf norm 
4194: results show execution largely dominated dot products operations delegated blas hence probably huge gain expect rewriting code cython case total execution time almost spent compiled code consider optimal rewriting rest python code assuming could achieve boost portion highly unlikely given shallowness python loops would gain speed globally hence major improvements achieved algorithmic improvements particular example trying operation costly useless avoid computing rather trying optimize implementation however still interesting check whats happening inside _nls_subproblem function hotspot consider python code takes around cumulated time module order better understand prole specic function let install line prof wire ipython pip install line profiler ipython edit ipython ipy_user_conf ensure following lines present import ipython ipapi ipython ipapi get towards end dene lprun magic import line_profiler expose_magic lprun line_profiler magic_lprun ipython rst create conguration prole ipython profile create create named ipython extensions line_profiler_ext following con tent import line_profiler def load_ipython_extension define_magic lprun line_profiler magic_lprun register ipython profile_default ipython_config terminalipythonapp extensions line_profiler_ext interactiveshellapp extensions line_profiler_ext chapter development register lprun magic command ipython terminal application frontends qtconsole notebook 
4195: scikit learn user guide release restart ipython let use new toy sklearn datasets import load_digits sklearn decomposition nmf import _nls_subproblem nmf load_digits data lprun _nls_subproblem nmf n_components tol fit timer unit file sklearn decomposition nmf function _nls_subproblem line total time line time line contents per hit hits time 
4196: def _nls_subproblem h_init tol max_iter non negative least square solver h_init raise valueerror negative values h_init passed nls solver h_init wtv dot wtw dot values justified paper alpha beta n_iter xrange max_iter grad dot wtw wtv proj_gradient norm grad logical_or grad proj_gradient tol break inner_iter xrange alpha grad _pos gradd sum grad dqd sum dot wtw looking top values time column really easy pin point expensive expressions would deserve additional care 
4197: performance tips cython developer proling python code reveals python interpreter overhead larger one order magnitude cost actual numerical computation loops vector components nested evaluation conditional expression scalar arithmetics probably adequate extract hotspot portion code optimize speed scikit learn user guide release standalone function pyx add static type declarations use cython generate program suitable compiled python extension module ofcial documentation available http docs cython org contains tutorial reference guide developing module following highlight couple tricks found important practice existing cython codebase scikit learn project todo html report type declarations bound checks division zero checks memory alignement direct blas calls 
4198: http www euroscipy org vid download http conference scipy org proceedings scipy2009 paper_1 http conference scipy org proceedings scipy2009 paper_2 proling compiled extensions working compiled extensions written wrapper directly cython extension default python proler useless need dedicated tool instrospect whats happening inside compiled extension self order prole compiled python extensions one could use gprof recompiled project gcc using python dbg variant interpreter debian ubuntu however approach requires also numpy scipy recompiled rather complicated get working fortunately exist two alternative prolers dont require recompile everything 
4199: using google perftools todo https github com fabianp yep http fseoane net blog proler python extensions note google perftools provides nice line line report mode triggered lines option however seem work correctly time writing issue tracked project issue tracker 
4200: using valgrind callgrind kcachegrind todo multi core parallelism using joblib parallel todo give simple teaser example checkout ofcial joblib documentation http packages python org joblib sample algorithmic trick warm restarts cross validation todo demonstrate warm restart tricks cross validation linear regression coordinate descent 
4201: chapter development scikit learn user guide release utilities developers scikit learn contains number utilities help development located sklearn utils include tools number categories following functions classes module sklearn utils 
4202: warning utilities meant used internally within scikit learn package guaran teed stable versions scikit learn backports particular removed scikit learn dependencies evolve 
4203: validation tools tools used check validate input write function accepts arrays matrices sparse matrices arguments following used applicable 
4204: assert_all_finite throw error array contains nans infs safe_asarray convert input array sparse matrix equivalent asarray sparse matrices passed 
4205: as_float_array convert input array oats sparse matrix passed sparse matrix returned 
4206: array2d equivalent atleast_2d order dtype input maintained atleast2d_or_csr equivalent array2d sparse matrix passed convert csr format 
4207: also calls assert_all_finite 
4208: check_arrays check input arrays consistent rst dimensions work arbitrary number arrays 
4209: warn_if_not_float warn input oating point value input assumed dtype code relies random number generator never use functions like numpy random random numpy random normal instead numpy random randomstate object used built random_state argument passed class function function check_random_state used create random number generator object 
4210: approach lead repeatability issues unit tests 
4211: check_random_state create random randomstate object parameter random_state random_state none random randomly initialized randomstate object turned 
4212: random_state integer used seed new randomstate object random_state randomstate object passed 
4213: example sklearn utils import check_random_state random_state random_state check_random_state random_state random_state rand array utilities developers scikit learn user guide release efcient linear algebra array operations extmath randomized_range_finder construct orthonormal matrix whose range approximates range input used extmath randomized_svd 
4214: extmath randomized_svd compute truncated randomized svd algorithm nds exact truncated singular values decomposition using randomization speed computations particularly fast large matrices wish extract small number components 
4215: arrayfuncs cholesky_delete used sklearn linear_model least_angle lars_path remove item cholesky factorization 
4216: arrayfuncs min_pos used sklearn linear_model least_angle find minimum positive values within array 
4217: extmath norm computes euclidean vector norm directly calling blas nrm2 function stable scipy linalg norm see fabians blog post discussion 
4218: extmath fast_logdet efciently compute log determinant matrix extmath density efciently compute density sparse vector extmath safe_sparse_dot dot product correctly handle scipy sparse inputs inputs dense equivalent numpy dot 
4219: extmath logsumexp compute sum assuming log domain equivalent calling log sum exp robust overow underow errors note similar functionality logaddexp reduce pairwise nature routine slower large arrays scipy similar routine scipy misc logsumexp scipy versions found scipy maxentropy logsumexp scipy version accept axis keyword 
4220: extmath weighted_mode extension scipy stats mode allows item real valued weight 
4221: resample resample arrays sparse matrices consistent way used shuffle shuffle shufe arrays sparse matrices consistent way used sklearn cluster k_means 
4222: efcient routines sparse matrices sklearn utils sparsefuncs cython module hosts compiled extensions efciently process scipy sparse data 
4223: sparsefuncs mean_variance_axis0 compute means variances along axis csr matrix 
4224: used normalizing tolerance stopping criterion sklearn cluster k_means_ kmeans 
4225: sparsefuncs inplace_csr_row_normalize_l1 sparsefuncs inplace_csr_row_normalize_l2 unit norm done used normalize sklearn preprocessing normalizer 
4226: individual sparse samples sparsefuncs inplace_csr_column_scale used multiply columns csr trix constant scale one scale per column used scaling features unit standard deviation sklearn preprocessing scaler 
4227: graph routines graph single_source_shortest_path_length currently used scikit learn return shortest path single source connected nodes graph code adapted networkx 
4228: chapter development scikit learn user guide release ever needed would far faster use single iteration dijkstras algorithm graph_shortest_path 
4229: graph graph_laplacian used sklearn cluster spectral spectral_embedding turn laplacian given graph specialized code dense sparse connectivity matrices graph_shortest_path graph_shortest_path used class sklearn manifold isomap return shortest path pairs connected points directed undirected graph floyd warshall algorithm dijkstras algorithm available algorithm efcient connectivity matrix scipy sparse csr_matrix 
4230: backports fixes counter partial backport collections counter python used sklearn feature_extraction text 
4231: fixes unique backport unique numpy find unique entries array numpy versions unique less exible used sklearn cross_validation 
4232: fixes copysign backport copysign numpy change sign element wise 
4233: fixes in1d backport in1d numpy 
4234: element used sklearn datasets twenty_newsgroups test whether array sklearn feature_extraction image 
4235: second array 
4236: fixes savemat backport scipy savemat scipy save array matlab format 
4237: earlier versions keyword oned_as available 
4238: fixes count_nonzero backport count_nonzero numpy count nonzero ele ments matrix used tests sklearn linear_model 
4239: arrayfuncs solve_triangular sklearn linear_model omp sklearn gaussian_process 
4240: back ported independent back ports used scipy sklearn mixture gmm sparsetools cs_graph_components backported scipy sparse cs_graph_components sklearn cluster hierarchical well tests scipy sklearn feature_extraction 
4241: used 
4242: arpack arpack eigs backported scipy sparse linalg eigs scipy sparse non symmetric eigenvalue decomposition using arnoldi method limited version eigs available earlier scipy versions 
4243: arpack eigsh backported scipy sparse linalg eigsh scipy sparse non symmetric eigenvalue decomposition using arnoldi method limited version eigsh available earlier scipy versions 
4244: arpack svds backported scipy sparse linalg svds scipy sparse non symmetric eigenvalue decomposition using arnoldi method limited version svds available earlier scipy versions 
4245: utilities developers scikit learn user guide release benchmarking bench total_seconds back ported timedelta total_seconds python used benchmarks bench_glm 
4246: testing functions testing assert_in testing assert_not_in assertions container membership designed forward compatibility nose 
4247: mock_urllib2 object mocks urllib2 module fake requests mldata used tests sklearn datasets 
4248: helper functions gen_even_slices generator create packs slices going 
4249: sklearn decomposition dict_learning sklearn cluster k_means 
4250: used arraybuilder arraybuilder helper class incrementally build numpy ndarray currently used sklearn datasets _svmlight_format pyx 
4251: safe_mask helper function convert mask format expected numpy array scipy sparse matrix use sparse matrices support integer indices numpy arrays support boolean masks integer indices 
4252: hash functions murmurhash3_32 provides python wrapper murmurhash3_x86_32 non cryptographic hash function hash function suitable implementing lookup tables bloom lters count min sketch feature hashing implicitly dened sparse random projections sklearn utils import murmurhash3_32 murmurhash3_32 feature seed murmurhash3_32 feature seed positive true 3910350737l sklearn utils murmurhash module also cimported cython modules benet high performance murmurhash skipping overhead python interpreter 
4253: warnings exceptions deprecated decorator mark function class deprecated convergencewarning custom warning catch sklearn covariance graph_lasso 
4254: convergence problems 
4255: used chapter development scikit learn user guide release developers tips debugging memory errors debugging cython valgrind python numpys built memory management relatively robust lead performance penalties routines reason much high performance code scikit learn written cython performance gain comes tradeoff however easy memory bugs crop cython code especially situations code relies heavily pointer arithmetic memory errors manifest number ways easiest ones debug often segmentation faults related glibc errors uninitialized variables lead unexpected behavior difcult track useful tool debugging sorts errors valgrind valgrind command line tool trace memory errors variety code follow steps install valgrind system download python valgrind suppression valgrind python supp follow directions readme valgrind customize python suppressions dont spurious output coming related python interpreter instead code 
4256: run valgrind follows valgrind suppressions valgrind python supp python my_test_script result list memory related errors reference lines code generated cython pyx examine referenced lines see comments indicate corresponding location pyx source hopefully output give clues source memory error information valgrind array options see tutorials documentation valgrind web site 
4257: community effort many people contributed years 
4258: history project started google summer code project david cournapeau later year matthieu brucher started work project part thesis fabian pedregosa gael varoquaux alexandre gramfort vincent michel inria took leadership project made rst public release february 1st since several releases appeard following month cycle striving international community leading development 
4259: people david cournapeau fred mailhot david cooke david huard dave morrill developers tips debugging scikit learn user guide release schoeld eric jones jarrod millman matthieu brucher travis oliphant pearu peterson fabian pedregosa maintainer gael varoquaux jake vanderplas alexandre gramfort olivier grisel bertrand thirion vincent michel chris filo gorgolewski angel soler gollonet yaroslav halchenko ron weiss virgile fritsch mathieu blondel peter prettenhofer vincent dubourg alexandre passos vlad niculae edouard duchesnay thouis ray jones lars buitinck paolo losi nelle varoquaux brian holt robert layton gilles louppe andreas mller satra ghosh forgot anyone hesitate send email fabian pedregosa inria ill include list 
4260: citing scikit learn use scikit learn scientic publication would appreciate citations following paper scikit learn machine learning python pedregosa jmlr bibtex entry article scikit learn title scikit learn machine learning python author pedregosa varoquaux gramfort michel 
4261: thirion grisel blondel prettenhofer weiss dubourg vanderplas passos cournapeau brucher perrot duchesnay journal journal machine learning research volume pages year chapter development scikit learn user guide release funding inria actively supports project provided funding fabian pedregosa work project full time period also hosts coding sprints events 
4262: google sponsored david cournapeau summer code scholarship summer vlad niculae would like participate next google summer code program please see page neurodebian project providing debian packaging contributions supported james haxby dart mouth college 
4263: support several ways get touch developers 
4264: mailing list main mailing list scikit learn general also commit list scikit learn commits updates main repository get notied 
4265: bug tracker think youve encoutered bug please report issue tracker https github com scikit learn scikit learn issues irc developers like hang channel scikit learn irc freenode net irc client behind rewall web client works http webchat freenode net documentation resources documentation relative documentation versions found development version printable pdf documentation versions found 
4266: support scikit learn user guide release changelog highlights hofer scott white dict based simple gradient boosted regression trees gradient tree boosting classication regression peter pretten feature_extraction dictvectorizer lars buitinck 
4267: feature loader support categorical variables added matthews correlation coefcient metrics matthews_corrcoef added macro micro erage options metrics precision_score metrics recall_score metrics f1_score satrajit ghosh 
4268: bag estimates generalization error ensemble methods andreas mller randomized sparse models randomized sparse linear models feature selection alexandre gramfort gael varoquaux label propagation semi supervised learning clay woolam note semi supervised api still work progress may change 
4269: added bic aic model selection classical gaussian mixture models unied api remainder scikit learn bertrand thirion added sklearn cross_validation stratifiedshufflesplit sklearn cross_validation shufflesplit balanced splits yannick schwartz 
4270: sklearn neighbors nearestcentroid classier added along shrink_threshold param eter implements shrunken centroid classication robert layton 
4271: changes merged dense sparse implementations stochastic gradient descent module exposed utility extension types sequential datasets seq_dataset weight vectors weight_vector peter prettenhofer 
4272: added partial_t support online minibatch learning warm_start stochastic gradient descent module mathieu blondel 
4273: dense sparse implementations support vector machines classes linear_model logisticregression merged lars buitinck 
4274: regressors used base estimator multiclass multilabel algorithms module mathieu blondel 
4275: added n_jobs option metrics pairwise pairwise_kernels parallel computation mathieu blondel 
4276: metrics pairwise pairwise_distances means run parallel using n_jobs argument either means kmeans robert layton improved cross validation evaluating estimator performance grid search setting estimator parame ters documentation introduced new cross_validation train_test_split helper function olivier grisel svm svc members coef_ intercept_ changed sign consistency decision_function kernel linear coef_ xed one one case andreas mller 
4277: chapter development scikit learn user guide release performance improvements efcient leave one cross validated ridge regression esp n_samples n_features case linear_model ridgecv reuben fletcher costin 
4278: refactoring simplication text feature extraction api xed bug caused possible negative idf olivier grisel 
4279: beam pruning option _basehmm module removed since difcult cythonize interested contributing cython version use python version git history reference 
4280: classes nearest neighbors support arbitrary minkowski metric nearest neighbors searches metric specied argument 
4281: api changes summary covariance ellipticenvelop deprecated please use covariance ellipticenvelope instead 
4282: neighborsclassier neighborsregressor gone module nearest neighbors 
4283: use classes kneighborsclassifier radiusneighborsclassifier kneighborsregressor radiusneighborsregressor instead 
4284: sparse classes stochastic gradient descent module deprecated mixture gmm mixture dpgmm mixture vbgmm parameters must passed object initialising fit fit accept data input parameter 
4285: methods rvs decode gmm module deprecated sample score predict used instead attribute _scores _pvalues univariate feature selection objects deprecated scores_ pvalues_ used instead 
4286: logisticregression linearsvc svc nusvc class_weight parameter initializa tion parameter parameter makes grid searches parameter possible 
4287: lfw data always shape n_samples n_features consistent olivetti faces dataset use images pairs attribute access natural images shapes instead 
4288: svm linearsvc meaning multi_class parameter changed options ovr cram mer_singer ovr default change default behavior hopefully less confusing 
4289: classs feature_selection text vectorizer feature_selection text tfidfvectorizer 
4290: deprecated replaced preprocessor analyzer nested structure text feature extraction removed features directly passed constructor arguments feature_selection text tfidfvectorizer feature_selection text countvectorizer particular following parameters used analyzer word char switch default analysis scheme use specic python callable previously 
4291: tokenizer preprocessor introduced make still possible customize steps new api 
4292: input explicitly control interpret sequence passed fit predict lenames objects direct byte unicode strings 
4293: charset decoding explicit strict default vocabulary tted stored vocabulary_ attribute consistent project conventions 
4294: scikit learn user guide release class feature_selection text tfidfvectorizer feature_selection text countvectorizer make grid search trivial 
4295: derives directly methods rvs _basehmm module deprecated sample used instead beam pruning option _basehmm module removed since difcult cythonized inter ested look history codes git 
4296: svmlight format loader supports les zero based one based column indices since occur wild 
4297: arguments class shufflesplit consistent stratifiedshufflesplit arguments test_fraction train_fraction deprecated renamed test_size train_size accept float int 
4298: arguments class bootstrap consistent stratifiedshufflesplit arguments n_test n_train deprecated renamed test_size train_size accept float int 
4299: argument added classes nearest neighbors specify arbitrary minkowski metric nearest neigh bors searches 
4300: people andreas mller peter prettenhofer gael varoquaux olivier grisel mathieu blondel clay woolam lars buitinck jaques grobler alexandre gramfort bertrand thirion robert layton yingimmidev jake vanderplas shiqiao satrajit ghosh david marek gilles louppe vlad niculae yannick schwartz fabian pedregosa fcostin nick wilson chapter development scikit learn user guide release adrien gaidon nicolas pinto david warde farley nelle varoquaux emmanuelle gouillart joonas sillanp paolo losi charles mccarthy roy hyunjin han scott white ibayer brandyn white carlos scheidegger claire revillet conrad lee edouard duchesnay jan hendrik metzen meng xinfan rob zinkov shiqiao udi weinsberg virgile fritsch xinfan meng yaroslav halchenko jansoe leon palafox changelog python compatibility dropped minimum python version needed use scikit learn sparse inverse covariance estimation using graph lasso associated cross validated estimator gael varoquaux new tree module brian holt peter prettenhofer satrajit ghosh gilles louppe module comes complete documentation examples 
4301: fixed bug rfe module gilles louppe issue 
4302: scikit learn user guide release fixed memory leak support vector machines module brian holt issue faster tests fabian pedregosa others silhouette coefcient cluster analysis evaluation metric added sklearn metrics silhouette_score robert layton 
4303: fixed bug means handling n_init parameter clustering algorithm used run n_init times last solution retained instead best solution olivier grisel 
4304: minor refactoring stochastic gradient descent module consolidated dense sparse predict methods hanced test time performance converting model paramters fortran style arrays tting multi class 
4305: adjusted mutual information metric added sklearn metrics adjusted_mutual_info_score robert layton 
4306: models like svc svr linearsvc logisticregression libsvm liblinear support scaling regular ization parameter number samples alexandre gramfort 
4307: new ensemble methods module gilles louppe brian holt module comes random forest algorithm extra trees method along documentation examples novelty outlier detection outlier novelty detection virgile fritsch kernel approximation transform implementing kernel approximation fast sgd non linear kernels andreas mller 
4308: fixed bug due atom swapping orthogonal matching pursuit omp vlad niculae sparse coding precomputed dictionary vlad niculae mini batch means performance improvements olivier grisel means support sparse matrices mathieu blondel improved documentation developers sklearn utils module jake vanderplas vectorized 20newsgroups dataset loader sklearn datasets fetch_20newsgroups_vectorized mathieu blondel 
4309: multiclass multilabel algorithms lars buitinck utilities fast computation mean variance sparse matrices mathieu blondel make sklearn preprocessing scale sklearn preprocessing scaler work sparse matrices olivier grisel feature importances using decision trees forest trees gilles louppe parallel implementation forests randomized trees gilles louppe sklearn cross_validation shufflesplit subsample train sets well test sets olivier grisel 
4310: errors build documentation xed andreas mller 
4311: api changes summary code migration instructions updgrading scikit learn version estimators may overwrite inputs save memory previously overwrite_ parameters replaced copy_ parameters exactly opposite meaning 
4312: chapter development scikit learn user guide release particularly affects estimators linear_model default behavior still copy everything passed 
4313: svmlight dataset loader sklearn datasets load_svmlight_file longer supports loading two les use load_svmlight_files instead also unused buffer_mb parameter gone sparse estimators stochastic gradient descent module use dense parameter vector coef_ instead sparse_coef_ signicantly improves test time performance 
4314: covariance estimation module robust estimator covariance minimum covariance deter minant estimator 
4315: cluster evaluation metrics metrics cluster refactored changes back moved metrics cluster supervised along wards compatible metrics cluster unsupervised contains silhouette coefcient 
4316: permutation_test_score function behaves way cross_val_score uses mean score across folds cross validation generators use integer indices indices true default instead boolean masks 
4317: make intuitive use sparse matrix data 
4318: functions used sparse coding sparse_encode sparse_encode_parallel com bined sklearn decomposition sparse_encode shapes arrays trans posed consistency matrix factorization setting opposed regression setting 
4319: fixed one error svmlight libsvm format handling les generated using sklearn datasets dump_svmlight_file generated continue work accidentally one extra column zeros prepended basedictionarylearning class replaced sparsecodingmixin sklearn utils extmath fast_svd renamed sklearn utils extmath randomized_svd default oversampling xed additional random vectors instead doubling number components extract new behavior follows reference paper 
4320: people following people contributed scikit learn since last release andreas mller olivier grisel gilles louppe brian holt gael varoquaux lars buitinck vlad niculae peter prettenhofer fabian pedregosa robert layton mathieu blondel jake vanderplas noel dawe scikit learn user guide release alexandre gramfort virgile fritsch satrajit ghosh jan hendrik metzen kenneth arnold shiqiao tim sheerman chase yaroslav halchenko bala subrahmanyam varanasi draxus michael eickenberg bogdan trach flix antoine fortin juan manuel caicedo carvajal nelle varoquaux nicolas pinto tiziano zito xinfan meng scikit learn released september three months release includes new modules manifold learning dirichlet process well several new algorithms documentation improvements release also includes dictionary learning work developed vlad niculae part google summer code program 
4321: changelog new manifold learning module jake vanderplas fabian pedregosa new dirichlet process gaussian mixture model alexandre passos chapter development scikit learn user guide release nearest neighbors module refactoring jake vanderplas general refactoring support sparse matrices input speed documentation improvements see next section full list api changes 
4322: improvements feature selection module gilles louppe refactoring rfe classes documenta tion rewrite increased efciency minor api changes 
4323: sparse principal components analysis sparsepca minibatchsparsepca vlad niculae gael varo quaux alexandre gramfort printing estimator behaves independently architectures python version thanks jean kossai loader libsvm svmlight format mathieu blondel lars buitinck documentation improvements thumbnails example gallery fabian pedregosa important bugxes support vector machines module segfaults bad performance fabian pedregosa added multinomial naive bayes bernoulli naive bayes lars buitinck text feature extraction optimizations lars buitinck chi square feature selection feature_selection univariate_selection chi2 lars buit inck 
4324: sample generators module refactoring gilles louppe multiclass multilabel algorithms mathieu blondel ball tree rewrite jake vanderplas implementation dbscan algorithm robert layton kmeans predict transform robert layton preprocessing module refactoring olivier grisel faster mean shift conrad lee new bootstrapping cross validation random permutations cross validation shufe split various improvements cross validation schemes olivier grisel gael varoquaux adjusted rand index measure clustering evaluation metrics olivier grisel added orthogonal matching pursuit vlad niculae added patch extractor utilites feature extraction module vlad niculae implementation linear_model lassolarscv cross validated lasso solver using lars algorithm linear_model lassolarsic bic aic model selection lars gael varoquaux alexandre gramfort scalability improvements metrics roc_curve olivier hervieu distance functions metrics pairwise pairwise_kernels robert layton helper metrics pairwise pairwise_distances mini batch means nelle varoquaux peter prettenhofer downloading datasets mldata org repository utilities pietro berkes olivetti faces dataset david warde farley 
4325: api changes summary code migration instructions updgrading scikit learn version scikit learn user guide release scikits learn package renamed sklearn still scikits learn package alias backward compatibility third party projects dependency scikit learn upgrade codebase instance linux macosx run make backup rst find name xargs sed bscikits learn sklearn estimators longer accept model parameters fit arguments instead parameters must passed constructor arguments using public set_params method inhereted base baseestimator estimators still accept keyword arguments fit restricted data dependent values gram matrix afnity matrix precomputed data matrix 
4326: cross_val package renamed cross_validation although also cross_val package alias place backward compatibility third party projects dependency scikit learn upgrade codebase instance linux macosx run make backup rst find name xargs sed bcross_val cross_validation score_func argument sklearn cross_validation cross_val_score function expected accept y_test y_predicted arguments classication regression tasks x_test unsupervised estimators 
4327: gamma parameter support vector machine algorithms set n_features default instead n_samples 
4328: sklearn hmm marked orphaned removed scikit learn version unless someone steps contribute documentation examples lurking numerical stability issues 
4329: sklearn neighbors made submodule 
4330: two previously available estimators neighborsclassifier neighborsregressor marked deprecated function ality divided among new classes nearestneighbors unsupervised neighbors searches kneighborsclassifier radiusneighborsclassifier supervised classication problems kneighborsregressor radiusneighborsregressor supervised regression problems 
4331: sklearn ball_tree balltree moved sklearn neighbors balltree using former generate warning 
4332: sklearn linear_model lars related classes lassolars lassolarscv etc named sklearn linear_model lars 
4333: distance metrics kernels sklearn metrics pairwise parameter default none given result distance kernel similarity sample given result pairwise distance kernel similarity samples 
4334: sklearn metrics pairwise l1_distance called manhattan_distance default returns pairwise distance component wise distance set parameter sum_over_features false 
4335: backward compatibilty package aliases deprecated classes functions removed version 
4336: people people contributed release 
4337: vlad niculae chapter development scikit learn user guide release olivier grisel lars buitinck gael varoquaux fabian pedregosa inria parietal team jake vanderplas mathieu blondel alexandre passos alexandre gramfort peter prettenhofer gilles louppe robert layton nelle varoquaux jean kossai conrad lee pietro berkes andy david warde farley brian holt robert amit aides virgile fritsch yaroslav halchenko salvatore masecchia paolo losi vincent schut alexis metaireau bryan silverthorn andreas mller minwoo jake lee emmanuelle gouillart keith goodman lucas wiman nicolas pinto thouis ray jones tim sheerman chase scikit learn user guide release scikit learn released may one month rst international scikit learn coding sprint marked inclusion important modules hierarchical clustering partial least squares non negative matrix factorization nmf nnmf initial support python important enhacements bug xes 
4338: changelog several new modules introduced release new hierarchical clustering module vincent michel bertrand thirion alexandre gramfort gael varo quaux 
4339: kernel pca implementation mathieu blondel labeled faces wild face recognition dataset olivier grisel new partial least squares module edouard duchesnay non negative matrix factorization nmf nnmf module vlad niculae implementation oracle approximating shrinkage algorithm virgile fritsch covariance estima tion module 
4340: modules beneted signicant improvements cleanups 
4341: initial support python builds imports cleanly modules usable others failing tests fabian pedregosa 
4342: decomposition pca usable pipeline object olivier grisel guide optimize speed olivier grisel fixes memory leaks libsvm bindings bit safer balltree lars buitinck bug style xing means algorithm jan schlter add attribute coverged gaussian mixture models vincent schut implement transform predict_log_proba lda lda mathieu blondel refactoring support vector machines module bug xes fabian pedregosa gael varoquaux amit aides 
4343: refactored sgd module removed code duplication better variable naming added interface sample weight peter prettenhofer 
4344: wrapped balltree cython thouis ray jones added function svm l1_min_c paolo losi typos doc style etc yaroslav halchenko gael varoquaux olivier grisel yann malet nicolas pinto lars buitinck fabian pedregosa 
4345: people people made release possible preceeded number commits olivier grisel gael varoquaux vlad niculae chapter development scikit learn user guide release fabian pedregosa alexandre gramfort paolo losi edouard duchesnay mathieu blondel peter prettenhofer nicolas pinto virgile fritsch lars buitinck vincent michel bertrand thirion thouis ray jones vincent schut jan schlter julien miotte matthieu perrot yann malet yaroslav halchenko amit aides andreas mller feth arezki meng xinfan scikit learn released march roughly three months release release marked speed improvements existing algorithms like nearest neighbors means algorithm inclusion efcient algorithm computing ridge generalized cross validation solution unlike preceding release new modules added release 
4346: changelog performance improvements gaussian mixture model sampling jan schlter implementation efcient leave one cross validated ridge linear_model ridgecv mathieu blondel better handling collinearity early stopping linear_model lars_path alexandre gramfort fabian pedregosa 
4347: fixes liblinear ordering labels sign coefcients dan yamins paolo losi mathieu blondel fabian pedregosa 
4348: scikit learn user guide release performance improvements nearest neighbors algorithm high dimensional spaces fabian pedregosa performance improvements cluster kmeans gael varoquaux james bergstra sanity checks svm based classes mathieu blondel refactoring neighbors neighborsclassifier neighbors kneighbors_graph added different algorithms nearest neighbor search implemented stable algorithm nding barycenter weigths also added developer documentation module see notes_neighbors information fabian pedregosa 
4349: documentation improvements added pca randomizedpca linear_model logisticregression class reference also added references matrices used clustering xes gael varoquaux fabian pedregosa mathieu blondel olivier grisel virgile fritsch emmanuelle gouillart binded decision_function classes make use liblinear dense sparse variants svm linearsvc linear_model logisticregression fabian pedregosa 
4350: like performance pca randomizedpca james bergstra 
4351: api improvements metrics euclidean_distances fix compilation issues netbsd kamel ibn hassen derouiche allow input sequences different lengths hmm gaussianhmm ron weiss fix bug afnity propagation caused incorrect indexing xinfan meng people people made release possible preceeded number commits fabian pedregosa mathieu blondel alexandre gramfort james bergstra dan yamins olivier grisel gael varoquaux edouard duchesnay ron weiss satrajit ghosh vincent dubourg emmanuelle gouillart kamel ibn hassen derouiche paolo losi virgilefritsch yaroslav halchenko xinfan meng chapter development scikit learn user guide release scikit learn released december marked inclusion several new modules general renaming old ones also marked inclusion new example including applications real world datasets 
4352: changelog new stochastic gradient descent module peter prettenhofer module comes complete documentation examples 
4353: improved svm module memory consumption reduced heuristic automatically set class weights possibility assign weights samples see svm weighted samples example 
4354: new gaussian processes module vincent dubourg module also great documentation neat examples see gaussian processes regression basic introductory example gaussian processes classication example exploiting probabilistic output taste done 
4355: possible use liblinears multi class svc option multi_class svm linearsvc new features performance improvements text feature extraction improved sparse matrix support main classes grid_search gridsearchcv modules sklearn svm sparse sklearn linear_model sparse 
4356: lots cool new examples new section uses real world datasets created include faces recognition example using eigenfaces svms species distribution modeling libsvm gui wikipedia princi pal eigenvector others 
4357: faster least angle regression algorithm faster version worst case 10x times faster cases 
4358: faster full linear_model lasso_path 200x times faster 
4359: coordinate algorithm 
4360: descent particular path version lasso possible get probability estimates linear_model logisticregression model module renaming glm module renamed linear_model gmm module included general mixture model sgd module included linear_model 
4361: lots bug xes documentation improvements 
4362: people people made release possible preceeded number commits olivier grisel fabian pedregosa peter prettenhofer alexandre gramfort mathieu blondel gael varoquaux vincent dubourg ron weiss bertrand thirion scikit learn user guide release alexandre passos anne laure fouque ronan amicel christian osendorfer changelog new classes support sparse matrices classiers modules svm linear_model see svm sparse svc linear_model sparse lasso svm sparse linearsvc svm sparse svr linear_model sparse elasticnet new pipeline pipeline object compose different estimators recursive feature elimination routines module feature selection addition capable classes various cross validation linear_model module linear_model lassocv linear_model elasticnetcv etc 
4363: new efcient lars algorithm implementation lasso variant algorithm also implemented 
4364: see linear_model lars_path linear_model lars linear_model lassolars 
4365: new hidden markov models module see classes hmm gaussianhmm hmm multinomialhmm hmm gmmhmm new module feature_extraction see class reference new fastica algorithm module sklearn fastica documentation improved documentation many modules separating narrative documentation class reference 
4366: example see documentation svm module complete class reference 
4367: fixes api changes adhere variable names pep give meaningful names fixes svm module run shared memory context multiprocessing possible generate latex thus pdf sphinx docs 
4368: examples new examples using mlcomp datasets classication text documents using mlcomp dataset classication text documents using sparse features many examaples see full list examples 
4369: chapter development scikit learn user guide release external dependencies joblib dependencie package although shipped sklearn externals joblib 
4370: removed modules module ann articial neural networks removed distribution users wanting sort algorithms take look pybrain 
4371: misc new sphinx theme web page 
4372: authors following list authors release preceeded number commits fabian pedregosa gael varoquaux alexandre gramfort olivier grisel vincent michel ron weiss matthieu perrot bertrand thirion yaroslav halchenko virgilefritsch edouard duchesnay mathieu blondel ariel rokem matthieu brucher changelog major changes release include coordinate descent algorithm lasso elasticnet refactoring speed improvements roughly 100x times faster 
4373: coordinate descent refactoring bug xing consistency package glmnet new metrics module 
4374: scikit learn user guide release new gmm module contributed ron weiss implementation lars algorithm without lasso variant feature_selection module redesign migration git content management system removal obsolete attrselect module rename private compiled extensions aded underscore removal legacy unmaintained code documentation improvements docstring rst improvement build system optionally link mkl also provide lite blas implementation case system wide blas found 
4375: lots new examples many many bug xes 
4376: authors committer list release following preceded number commits fabian pedregosa alexandre gramfort olivier grisel gael varoquaux yaroslav halchenko vincent michel chris filo gorgolewski presentations tutorials scikit learn written tutorials see tutorial section documentation 
4377: videos introduction scikit learn gael varoquaux icml three minute video early stage scikit explaining basic idea approach following 
4378: introduction statistical learning scikit learn gael varoquaux scipy extensive tutorial consisting four sessions one hour tutorial covers basics machine learning many algorithms apply using scikit learn material corresponding scikit learn documentation section tutorial statistical learning scientic data processing 
4379: statistical learning text classication scikit learn nltk slides olivier grisel pycon chapter development scikit learn user guide release thirty minute introduction text classication explains use nltk scikit learn solve real world text classication tasks compares cloud based solutions 
4380: introduction interactive predictive analytics python scikit learn olivier grisel pycon hours long introduction prediction tasks using scikit learn 
4381: scikit learn machine learning python jake vanderplas pydata workshop google interactive demonstration scikit learn features minutes 
4382: presentations tutorials scikit learn scikit learn user guide release chapter development bibliography b2001 leo breiman random forests machine learning b1998 leo breiman arcing classiers annals statistics gew2006 pierre geurts damien ernst louis wehenkel extremely randomized trees machine learning 
4383: f2001 friedman greedy function approximation gradient boosting machine annals statistics vol 
4384: ridgeway generalized boosted models guide gbm package f1999 friedman stochastic gradient boosting htf2009 hastie tibshirani friedman elements statistical learning springer r2007 mrl09 online dictionary learning sparse coding mairal bach ponce sapiro jen09 structured sparse principal component analysis jenatton obozinski bach rd1999 rousseeuw van driessen fast algorithm minimum covariance determinant estimator technometrics r59 breiman random forests machine learning r60 breiman random forests machine learning r57 geurts ernst wehenkel extremely randomized trees machine learning r58 geurts ernst wehenkel extremely randomized trees machine learning rr2007 random features large scale kernel machines rahimi recht advances neural infor mation processing ls2010 random fourier approximations skewed multiplicative histogram kernels random fourier approxi mations skewed multiplicative histogram kernels lecture notes computer sciencd dagm vz2010 efcient additive kernels via explicit feature maps vedaldi zisserman computer vision pattern recognition vvz2010 generalized rbf feature maps efcient detection vempati vedaldi zisserman 
4385: jawahar rouseeuw1984 rousseeuw least median squares regression stat ass rouseeuw1999 fast algorithm minimum covariance determinant estimator american statistical association american society quality technometrics scikit learn user guide release butler1993 butler davies jhun asymptotics minimum covariance determinant esti mator annals statistics vol r48 guyon design experiments nips variable selection benchmark r49 friedman multivariate adaptive regression splines annals statistics pages r50 breiman bagging predictors machine learning pages r51 friedman multivariate adaptive regression splines annals statistics pages r52 breiman bagging predictors machine learning pages r53 friedman multivariate adaptive regression splines annals statistics pages r54 breiman bagging predictors machine learning pages r55 celeux anbari marin robert regularization regression comparing bayesian frequentist methods poorly informative situation 
4386: r56 marsland machine learning algorithmic perpsective chapter http www ist massey smarsland code lle halko2009 finding structure randomness stochastic algorithms constructing approximate matrix decom positions halko arxiv mrt randomized algorithm decomposition matrices per gunnar martinsson vladimir rokhlin mark tygert r59 breiman random forests machine learning r60 breiman random forests machine learning r57 geurts ernst wehenkel extremely randomized trees machine learning r58 geurts ernst wehenkel extremely randomized trees machine learning yates2011 baeza yates ribeiro neto modern information retrieval addison wesley msr2008 manning schtze raghavan introduction information retrieval cambridge university press 
4387: r61 guyon weston barnhill vapnik gene selection cancer classication using support vector machines mach learn 
4388: r62 guyon weston barnhill vapnik gene selection cancer classication using support vector machines mach learn 
4389: nlns2002 nielsen lophaven nielsen sondergaard dace matlab kriging toolbox 
4390: http www2 imm dtu hbn dace dace pdf wbswm1992 welch buck sacks wynn mitchell morris screening pre dicting computer experiments technometrics http www jstor org pss r63 roweis saul nonlinear dimensionality reduction locally linear embedding science 
4391: r64 donoho grimes hessian eigenmaps locally linear embedding techniques high dimensional data 
4392: proc natl acad sci 
4393: r65 zhang wang mlle modied locally linear embedding using multiple weights 
4394: http citeseerx ist psu edu viewdoc summary doi r66 zhang zha principal manifolds nonlinear dimensionality reduction via tangent space alignment 
4395: journal shanghai univ bibliography scikit learn user guide release r67 roweis saul nonlinear dimensionality reduction locally linear embedding science 
4396: r68 donoho grimes hessian eigenmaps locally linear embedding techniques high dimensional data 
4397: proc natl acad sci 
4398: r69 zhang wang mlle modied locally linear embedding using multiple weights 
4399: http citeseerx ist psu edu viewdoc summary doi r70 zhang zha principal manifolds nonlinear dimensionality reduction via tangent space alignment 
4400: journal shanghai univ hubert1985 hubert arabie comparing partitions journal classication http www springerlink com content x64124718341j1j0 http wikipedia org wiki rand_index adjusted_rand_index r72 andrew rosenberg julia hirschberg measure conditional entropy based external cluster evaluation measure http acl ldc upenn edu d07 d07 pdf r71 andrew rosenberg julia hirschberg measure conditional entropy based external cluster evaluation measure http acl ldc upenn edu d07 d07 pdf rosenberg2007 measure conditional entropy based external cluster evaluation measure andrew rosenberg julia hirschberg http acl ldc upenn edu d07 d07 pdf r73 solving multiclass learning problems via error correcting output codes dietterich bakiri journal articial intelligence research 
4401: r74 error coding method picts james hastie journal computational graphical statistics 
4402: r75 elements statistical learning hastie tibshirani friedman page second edition r76 http wikipedia org wiki decision_tree_learning r77 breiman friedman olshen stone classication regression trees wadsworth belmont 
4403: r78 hastie tibshirani friedman elements statistical learning springer r79 breiman cutler random forests http www stat berkeley edu breiman randomforests cc_home htm r80 http wikipedia org wiki decision_tree_learning r81 breiman friedman olshen stone classication regression trees wadsworth belmont 
4404: r82 hastie tibshirani friedman elements statistical learning springer r83 breiman cutler random forests http www stat berkeley edu breiman randomforests cc_home htm r84 geurts ernst wehenkel extremely randomized trees machine learning r85 geurts ernst wehenkel extremely randomized trees machine learning 
4405: bibliography scikit learn user guide release bibliography python module index sklearn cluster sklearn covariance sklearn cross_validation sklearn datasets sklearn decomposition sklearn ensemble sklearn feature_extraction sklearn feature_extraction image sklearn feature_extraction text sklearn feature_selection sklearn gaussian_process sklearn grid_search sklearn hmm sklearn kernel_approximation sklearn lda sklearn linear_model sklearn linear_model sparse sklearn manifold sklearn metrics sklearn metrics cluster sklearn metrics pairwise sklearn mixture sklearn multiclass sklearn naive_bayes sklearn neighbors sklearn pipeline sklearn pls sklearn preprocessing sklearn qda sklearn semi_supervised sklearn svm sklearn tree sklearn utils scikit learn user guide release python module index python module index sklearn cluster sklearn covariance sklearn cross_validation sklearn datasets sklearn decomposition sklearn ensemble sklearn feature_extraction sklearn feature_extraction image sklearn feature_extraction text sklearn feature_selection sklearn gaussian_process sklearn grid_search sklearn hmm sklearn kernel_approximation sklearn lda sklearn linear_model sklearn linear_model sparse sklearn manifold sklearn metrics sklearn metrics cluster sklearn metrics pairwise sklearn mixture sklearn multiclass sklearn naive_bayes sklearn neighbors sklearn pipeline sklearn pls sklearn preprocessing sklearn qda sklearn semi_supervised sklearn svm sklearn tree sklearn utils scikit learn user guide release python module index index symbols __init__ sklearn cluster afnitypropagation method __init__ sklearn cluster dbscan method __init__ sklearn cluster kmeans method __init__ sklearn cluster meanshift method __init__ sklearn cluster minibatchkmeans method __init__ sklearn cluster spectralclustering method __init__ sklearn cluster ward method __init__ sklearn covariance ellipticenvelope method __init__ sklearn decomposition dictionarylearning __init__ sklearn decomposition fastica method __init__ sklearn decomposition kernelpca method __init__ sklearn decomposition minibatchdictionarylearning __init__ sklearn decomposition minibatchsparsepca __init__ sklearn decomposition nmf method __init__ sklearn decomposition pca method __init__ sklearn decomposition probabilisticpca __init__ sklearn covariance empiricalcovariance __init__ sklearn decomposition projectedgradientnmf method method method method method method method __init__ sklearn covariance graphlasso method __init__ sklearn covariance graphlassocv method __init__ sklearn covariance ledoitwolf method __init__ sklearn covariance mincovdet method __init__ sklearn covariance oas method __init__ sklearn covariance shrunkcovariance method __init__ sklearn cross_validation bootstrap method __init__ sklearn cross_validation kfold method sklearn cross_validation leaveonelabelout __init__ __init__ __init__ method method method sklearn cross_validation leaveoneout sklearn cross_validation leaveplabelout __init__ sklearn cross_validation leavepout method __init__ sklearn cross_validation shufesplit method __init__ sklearn cross_validation stratiedkfold __init__ sklearn cross_validation stratiedshufesplit method method __init__ sklearn decomposition randomizedpca __init__ sklearn decomposition sparsecoder method __init__ sklearn decomposition sparsepca method __init__ __init__ sklearn ensemble extratreesclassier sklearn ensemble extratreesregressor __init__ sklearn ensemble gradientboostingclassier __init__ sklearn ensemble gradientboostingregressor __init__ __init__ __init__ sklearn ensemble randomforestclassier sklearn ensemble randomforestregressor sklearn feature_extraction dictvectorizer method method method method method method method method method method __init__ sklearn feature_extraction text tdftransformer __init__ sklearn feature_extraction image patchextractor __init__ sklearn feature_extraction text countvectorizer scikit learn user guide release __init__ sklearn feature_extraction text tdfvectorizer sklearn linear_model perceptron method __init__ sklearn feature_selection rfe method __init__ sklearn feature_selection rfecv method method sklearn linear_model randomizedlasso __init__ sklearn linear_model randomizedlogisticregression __init__ sklearn feature_selection selectfdr method method __init__ sklearn feature_selection selectfpr method __init__ sklearn linear_model ridge method __init__ sklearn linear_model ridgecv method __init__ __init__ __init__ sklearn feature_selection selectfwe method sklearn linear_model ridgeclassier sklearn feature_selection selectkbest sklearn linear_model ridgeclassiercv sklearn feature_selection selectpercentile __init__ sklearn linear_model sgdclassier method sklearn gaussian_process gaussianprocess __init__ sklearn linear_model sgdregressor method __init__ __init__ method method method __init__ __init__ __init__ method method method __init__ sklearn grid_search gridsearchcv method __init__ sklearn linear_model sparse elasticnet __init__ sklearn kernel_approximation rbfsampler method __init__ sklearn kernel_approximation skewedchi2sampler __init__ sklearn grid_search itergrid method __init__ sklearn hmm gmmhmm method __init__ sklearn hmm multinomialhmm method __init__ sklearn kernel_approximation additivechi2sampler method method method method method __init__ sklearn lda lda method __init__ sklearn linear_model ardregression __init__ sklearn linear_model bayesianridge __init__ sklearn linear_model elasticnet method __init__ sklearn linear_model elasticnetcv method __init__ sklearn linear_model lars method __init__ sklearn linear_model larscv method __init__ sklearn linear_model lasso method __init__ sklearn linear_model lassocv method __init__ sklearn linear_model lassolars method __init__ sklearn linear_model lassolarscv method method method __init__ sklearn linear_model sparse lasso method __init__ sklearn linear_model sparse sgdclassier __init__ sklearn linear_model sparse sgdregressor __init__ sklearn manifold isomap method __init__ sklearn manifold locallylinearembedding method __init__ sklearn mixture dpgmm method __init__ sklearn mixture gmm method __init__ sklearn mixture vbgmm method __init__ sklearn multiclass onevsoneclassier __init__ __init__ __init__ __init__ method method method sklearn multiclass onevsrestclassier sklearn multiclass outputcodeclassier sklearn naive_bayes bernoullinb method sklearn naive_bayes gaussiannb method __init__ sklearn naive_bayes multinomialnb method __init__ sklearn neighbors balltree method __init__ sklearn neighbors kneighborsclassier __init__ sklearn neighbors kneighborsregressor __init__ sklearn neighbors nearestcentroid method __init__ sklearn neighbors nearestneighbors method __init__ sklearn neighbors radiusneighborsclassier method __init__ sklearn linear_model lassolarsic method method sklearn linear_model linearregression method __init__ __init__ method method method sklearn linear_model logisticregression __init__ sklearn linear_model orthogonalmatchingpursuit index scikit learn user guide release __init__ sklearn neighbors radiusneighborsregressor method __init__ sklearn pipeline pipeline method __init__ sklearn pls cca method __init__ sklearn pls plscanonical method __init__ sklearn pls plsregression method __init__ sklearn pls plssvd method __init__ sklearn preprocessing binarizer method __init__ sklearn preprocessing kernelcenterer method method __init__ __init__ sklearn preprocessing labelbinarizer sklearn preprocessing normalizer method __init__ sklearn preprocessing scaler method __init__ sklearn qda qda method __init__ sklearn semi_supervised labelpropagation auc module sklearn metrics balltree class sklearn neighbors bayesianridge class sklearn linear_model bernoullinb class sklearn naive_bayes best_estimator sklearn grid_search gridsearchcv best_score sklearn grid_search gridsearchcv attribute tribute bic sklearn mixture dpgmm method bic sklearn mixture gmm method bic sklearn mixture vbgmm method binarize module sklearn preprocessing binarizer class sklearn preprocessing bootstrap class sklearn cross_validation build_analyzer sklearn feature_extraction text countvectorizer __init__ sklearn semi_supervised labelspreading build_analyzer sklearn feature_extraction text tdfvectorizer method method method method method method method method build_preprocessor sklearn feature_extraction text countvectorizer build_preprocessor sklearn feature_extraction text tdfvectorizer build_tokenizer sklearn feature_extraction text countvectorizer build_tokenizer sklearn feature_extraction text tdfvectorizer cca class sklearn pls check_cv module sklearn cross_validation check_random_state module sklearn utils chi2 module sklearn feature_selection class_prior sklearn naive_bayes gaussiannb attribute __init__ sklearn svm linearsvc method __init__ sklearn svm nusvc method __init__ sklearn svm nusvr method __init__ sklearn svm oneclasssvm method __init__ sklearn svm svc method __init__ sklearn svm svr method __init__ sklearn tree decisiontreeclassier method __init__ sklearn tree decisiontreeregressor method __init__ sklearn tree extratreeclassier method __init__ sklearn tree extratreeregressor method absolute_exponential module sklearn gaussian_process correlation_models additivechi2sampler sklearn kernel_approximation class adjusted_mutual_info_score sklearn metrics adjusted_rand_score module sklearn metrics afnity_propagation module sklearn cluster afnitypropagation class sklearn cluster aic sklearn mixture dpgmm method aic sklearn mixture gmm method aic sklearn mixture vbgmm method algorithm sklearn hmm gmmhmm attribute algorithm sklearn hmm multinomialhmm attribute ardregression class sklearn linear_model arg_max_reduced_likelihood_function sklearn gaussian_process gaussianprocess method classes sklearn linear_model perceptron attribute classes sklearn linear_model sgdclassier attribute module classes sklearn linear_model sparse sgdclassier tribute classication_report module sklearn metrics completeness_score module sklearn metrics confusion_matrix module sklearn metrics constant module sklearn gaussian_process regression_models correct_covariance sklearn covariance ellipticenvelope method method correct_covariance sklearn covariance mincovdet countvectorizer class sklearn feature_extraction text covariance_type sklearn hmm gmmhmm attribute index scikit learn user guide release cross_val_score module sklearn cross_validation decision_function sklearn linear_model sparse lasso cross_validation module sklearn svm libsvm cubic module sklearn gaussian_process correlation_models decision_function sklearn linear_model sparse sgdclassier decision_function sklearn linear_model sparse sgdregressor method method method method dbscan class sklearn cluster dbscan module sklearn cluster decision_function module sklearn svm libsvm decision_function sklearn covariance ellipticenvelope decision_function sklearn lda lda method decision_function sklearn linear_model ardregression decision_function sklearn linear_model bayesianridge decision_function sklearn linear_model elasticnet decision_function sklearn linear_model elasticnetcv method decision_function sklearn linear_model larscv decision_function sklearn linear_model lasso decision_function sklearn linear_model lassocv decision_function sklearn linear_model lassolars decision_function sklearn linear_model lassolarscv decision_function sklearn linear_model lassolarsic method method method method method method method method method method method decision_function sklearn pipeline pipeline method decision_function sklearn qda qda method decision_function sklearn svm linearsvc method decision_function sklearn svm nusvc method decision_function sklearn svm nusvr method decision_function sklearn svm oneclasssvm method decision_function sklearn svm svc method decision_function sklearn svm svr method decisiontreeclassier class sklearn tree decisiontreeregressor class sklearn tree decode sklearn feature_extraction text countvectorizer decode sklearn hmm gmmhmm method decode sklearn hmm multinomialhmm method decode sklearn mixture dpgmm method decode sklearn mixture gmm method decode sklearn mixture vbgmm method dict_learning module sklearn decomposition dict_learning_online module sklearn decomposition dictionarylearning class sklearn decomposition dictvectorizer class sklearn feature_extraction distance_metrics module sklearn metrics pairwise decision_function sklearn linear_model lars method decode sklearn feature_extraction text tdfvectorizer method dpgmm class sklearn mixture decision_function sklearn linear_model linearregression decision_function sklearn linear_model logisticregression elasticnet class sklearn linear_model elasticnet class sklearn linear_model sparse elasticnetcv class sklearn linear_model ellipticenvelope class sklearn covariance emissionprob_ decision_function sklearn linear_model orthogonalmatchingpursuit sklearn linear_model perceptron sklearn hmm multinomialhmm decision_function method method method decision_function sklearn linear_model ridge tribute decision_function sklearn linear_model ridgecv decision_function sklearn linear_model sgdclassier empirical_covariance module sklearn covariance empiricalcovariance class sklearn covariance error_norm sklearn covariance ellipticenvelope decision_function sklearn linear_model sgdregressor error_norm sklearn covariance empiricalcovariance decision_function sklearn linear_model sparse elasticnet sklearn covariance graphlasso method method method method method method method method error_norm index scikit learn user guide release error_norm sklearn covariance graphlassocv sklearn decomposition dictionarylearning method method sklearn covariance ledoitwolf method module error_norm error_norm sklearn covariance mincovdet method error_norm sklearn covariance oas method error_norm sklearn covariance shrunkcovariance method estimate_bandwidth module sklearn cluster euclidean_distances module sklearn metrics pairwise eval sklearn hmm gmmhmm method eval sklearn hmm multinomialhmm method eval sklearn mixture dpgmm method eval sklearn mixture gmm method eval sklearn mixture vbgmm method export_graphviz module sklearn tree extract_patches_2d sklearn feature_extraction image extratreeclassier class sklearn tree extratreeregressor class sklearn tree extratreesclassier class sklearn ensemble extratreesregressor class sklearn ensemble f1_score module sklearn metrics f_classif module sklearn feature_selection f_regression module sklearn feature_selection fastica class sklearn decomposition fastica module sklearn decomposition fbeta_score module sklearn metrics fetch_20newsgroups module sklearn datasets fetch_20newsgroups_vectorized module sklearn datasets fetch_lfw_pairs module sklearn datasets fetch_lfw_people module sklearn datasets fetch_olivetti_faces module sklearn datasets module sklearn svm libsvm sklearn cluster afnitypropagation method sklearn cluster dbscan method sklearn cluster kmeans method sklearn cluster meanshift method sklearn cluster minibatchkmeans method sklearn cluster spectralclustering method sklearn cluster ward method sklearn covariance ellipticenvelope method sklearn covariance empiricalcovariance method sklearn covariance ledoitwolf method sklearn covariance mincovdet method sklearn covariance oas method sklearn covariance shrunkcovariance method sklearn decomposition kernelpca method sklearn decomposition minibatchdictionarylearning method sklearn decomposition minibatchsparsepca method sklearn decomposition nmf method sklearn decomposition pca method sklearn decomposition probabilisticpca method sklearn decomposition projectedgradientnmf method sklearn decomposition randomizedpca method sklearn decomposition sparsecoder method sklearn decomposition sparsepca method sklearn ensemble extratreesclassier method sklearn ensemble extratreesregressor method sklearn ensemble gradientboostingclassier method sklearn ensemble gradientboostingregressor method sklearn ensemble randomforestclassier method sklearn ensemble randomforestregressor method sklearn feature_extraction dictvectorizer method sklearn feature_extraction image patchextractor method sklearn feature_extraction text countvectorizer method sklearn feature_extraction text tdftransformer method sklearn feature_extraction text tdfvectorizer method sklearn feature_selection rfe method sklearn feature_selection rfecv method sklearn feature_selection selectfdr method sklearn feature_selection selectfpr method sklearn feature_selection selectfwe method sklearn feature_selection selectkbest method sklearn feature_selection selectpercentile method sklearn gaussian_process gaussianprocess method sklearn grid_search gridsearchcv method sklearn hmm gmmhmm method sklearn hmm multinomialhmm method sklearn kernel_approximation additivechi2sampler method index scikit learn user guide release sklearn kernel_approximation rbfsampler sklearn multiclass outputcodeclassier method method method sklearn kernel_approximation skewedchi2sampler sklearn lda lda method sklearn linear_model ardregression method sklearn linear_model bayesianridge method sklearn linear_model elasticnet method sklearn linear_model elasticnetcv method sklearn linear_model lars method sklearn linear_model larscv method sklearn linear_model lasso method sklearn linear_model lassocv method sklearn linear_model lassolars method sklearn linear_model lassolarscv method sklearn linear_model lassolarsic method sklearn linear_model linearregression method sklearn linear_model logisticregression method method sklearn linear_model orthogonalmatchingpursuit sklearn linear_model perceptron method sklearn linear_model randomizedlasso method sklearn linear_model randomizedlogisticregression method sklearn linear_model ridge method sklearn linear_model ridgeclassier method sklearn linear_model ridgeclassiercv method sklearn linear_model ridgecv method sklearn linear_model sgdclassier method sklearn linear_model sgdregressor method sklearn linear_model sparse elasticnet method sklearn linear_model sparse lasso method sklearn linear_model sparse sgdclassier method sklearn naive_bayes bernoullinb method sklearn naive_bayes gaussiannb method sklearn naive_bayes multinomialnb method sklearn neighbors kneighborsclassier method sklearn neighbors kneighborsregressor method sklearn neighbors nearestcentroid method sklearn neighbors nearestneighbors method sklearn neighbors radiusneighborsclassier method sklearn neighbors radiusneighborsregressor method sklearn pipeline pipeline method sklearn preprocessing binarizer method sklearn preprocessing kernelcenterer method sklearn preprocessing labelbinarizer method sklearn preprocessing normalizer method sklearn preprocessing scaler method sklearn qda qda method sklearn semi_supervised labelpropagation method sklearn semi_supervised labelspreading method sklearn svm linearsvc method sklearn svm nusvc method sklearn svm nusvr method sklearn svm oneclasssvm method sklearn svm svc method sklearn svm svr method sklearn tree decisiontreeclassier method sklearn tree decisiontreeregressor method sklearn tree extratreeclassier method sklearn tree extratreeregressor method t_ecoc module sklearn multiclass t_ovo module sklearn multiclass t_ovr module sklearn multiclass t_predict sklearn cluster kmeans method t_predict sklearn cluster minibatchkmeans method sklearn linear_model sparse sgdregressor t_stage sklearn ensemble gradientboostingclassier method method sklearn manifold isomap method sklearn manifold locallylinearembedding sklearn mixture dpgmm method sklearn mixture gmm method sklearn mixture vbgmm method sklearn multiclass onevsoneclassier method t_stage sklearn ensemble gradientboostingregressor t_transform sklearn decomposition dictionarylearning t_transform sklearn decomposition kernelpca t_transform sklearn decomposition minibatchdictionarylearning sklearn multiclass onevsrestclassier method t_transform sklearn decomposition minibatchsparsepca method method method method method method index scikit learn user guide release method method method method method t_transform t_transform sklearn decomposition nmf method sklearn decomposition pca method t_transform sklearn linear_model randomizedlogisticregression t_transform sklearn linear_model sgdclassier t_transform sklearn decomposition probabilisticpca t_transform sklearn linear_model sgdregressor method method method method method method method method method t_transform sklearn decomposition projectedgradientnmf t_transform sklearn linear_model sparse sgdclassier t_transform sklearn decomposition randomizedpca t_transform sklearn linear_model sparse sgdregressor t_transform sklearn decomposition sparsecoder t_transform sklearn decomposition sparsepca method t_transform sklearn ensemble extratreesclassier t_transform sklearn manifold isomap method t_transform sklearn manifold locallylinearembedding t_transform sklearn pipeline pipeline method t_transform sklearn preprocessing binarizer method t_transform method method method t_transform t_transform sklearn ensemble extratreesregressor sklearn preprocessing kernelcenterer t_transform sklearn ensemble randomforestclassier t_transform sklearn preprocessing labelbinarizer t_transform sklearn ensemble randomforestregressor t_transform sklearn preprocessing normalizer t_transform sklearn feature_extraction dictvectorizer sklearn preprocessing scaler method t_transform sklearn feature_extraction text countvectorizer t_transform sklearn svm linearsvc method t_transform sklearn tree decisiontreeclassier t_transform sklearn feature_extraction text tdftransformer method t_transform sklearn feature_extraction text tdfvectorizer method t_transform sklearn feature_selection selectfdr t_transform sklearn tree decisiontreeregressor t_transform sklearn tree extratreeclassier method t_transform sklearn tree extratreeregressor t_transform sklearn feature_selection selectfpr method method method method method method method method method method method method method t_transform sklearn feature_selection selectfwe t_transform sklearn feature_selection selectkbest t_transform sklearn feature_selection selectpercentile t_transform sklearn kernel_approximation additivechi2sampler t_transform sklearn kernel_approximation rbfsampler t_transform sklearn kernel_approximation skewedchi2sampler t_transform sklearn lda lda method t_transform sklearn linear_model logisticregression gaussianhmm class sklearn hmm gaussiannb class sklearn naive_bayes gaussianprocess class sklearn gaussian_process generalized_exponential module sklearn gaussian_process correlation_models get_feature_names sklearn feature_extraction dictvectorizer get_feature_names sklearn feature_extraction text countvectorizer get_feature_names sklearn feature_extraction text tdfvectorizer get_mixing_matrix sklearn decomposition fastica t_transform sklearn linear_model perceptron get_params sklearn cluster afnitypropagation method method method method method method method method t_transform sklearn linear_model randomizedlasso get_params sklearn cluster dbscan method get_params sklearn cluster kmeans method get_params sklearn cluster meanshift method index scikit learn user guide release get_params sklearn cluster minibatchkmeans get_params sklearn feature_extraction dictvectorizer get_params sklearn cluster spectralclustering get_params sklearn feature_extraction image patchextractor method method method get_params sklearn feature_extraction text countvectorizer get_params sklearn feature_extraction text tdftransformer get_params sklearn feature_extraction text tdfvectorizer get_params sklearn cluster ward method get_params sklearn covariance ellipticenvelope method method method method method method method method method method method method method method method get_params sklearn covariance empiricalcovariance method get_params sklearn covariance graphlasso method method sklearn covariance graphlassocv sklearn covariance ledoitwolf method method sklearn covariance mincovdet method method get_params get_params sklearn feature_selection rfe method sklearn feature_selection rfecv get_params sklearn feature_selection selectfdr get_params sklearn feature_selection selectfpr get_params sklearn feature_selection selectfwe get_params sklearn covariance oas method get_params sklearn covariance shrunkcovariance get_params sklearn decomposition dictionarylearning get_params sklearn feature_selection selectkbest get_params sklearn decomposition fastica method get_params sklearn feature_selection selectpercentile get_params get_params get_params get_params sklearn decomposition kernelpca get_params sklearn gaussian_process gaussianprocess get_params sklearn decomposition minibatchdictionarylearning get_params sklearn grid_search gridsearchcv get_params sklearn decomposition minibatchsparsepca get_params sklearn hmm gmmhmm method get_params sklearn hmm multinomialhmm method method method method method method method method get_params sklearn decomposition nmf method get_params sklearn decomposition pca method get_params sklearn decomposition probabilisticpca get_params sklearn decomposition projectedgradientnmf method get_params sklearn decomposition randomizedpca method get_params sklearn decomposition sparsecoder get_params sklearn lda lda method get_params sklearn linear_model ardregression get_params sklearn decomposition sparsepca get_params sklearn linear_model bayesianridge get_params sklearn ensemble extratreesclassier get_params sklearn linear_model elasticnet method method method get_params get_params sklearn ensemble extratreesregressor sklearn linear_model elasticnetcv get_params sklearn ensemble gradientboostingclassier get_params sklearn ensemble gradientboostingregressor get_params sklearn ensemble randomforestclassier method get_params sklearn linear_model lars method get_params sklearn linear_model larscv method get_params sklearn linear_model lasso method get_params sklearn linear_model lassocv method get_params sklearn ensemble randomforestregressor get_params sklearn linear_model lassolars method method method method method method method index get_params sklearn kernel_approximation additivechi2sampler get_params sklearn kernel_approximation rbfsampler get_params sklearn kernel_approximation skewedchi2sampler method method method method method method method method method method method method method method method method method method method method get_params sklearn linear_model randomizedlasso get_params sklearn linear_model randomizedlogisticregression get_params sklearn linear_model ridge method get_params sklearn linear_model ridgeclassier get_params sklearn linear_model ridgeclassiercv method get_params sklearn linear_model ridgecv method method get_params sklearn linear_model sgdclassier method get_params sklearn linear_model sgdregressor get_params sklearn linear_model sparse elasticnet method get_params sklearn linear_model sparse lasso method get_params sklearn linear_model sparse sgdclassier get_params sklearn linear_model sparse sgdregressor get_params sklearn manifold isomap method get_params sklearn manifold locallylinearembedding get_params sklearn mixture dpgmm method get_params sklearn mixture gmm method get_params sklearn mixture vbgmm method get_params sklearn multiclass onevsoneclassier get_params sklearn multiclass onevsrestclassier scikit learn user guide release get_params sklearn linear_model lassolarscv get_params sklearn neighbors kneighborsclassier get_params sklearn linear_model lassolarsic get_params sklearn neighbors kneighborsregressor get_params sklearn linear_model linearregression get_params sklearn neighbors nearestcentroid get_params sklearn linear_model logisticregression get_params sklearn neighbors nearestneighbors get_params sklearn linear_model orthogonalmatchingpursuit get_params sklearn neighbors radiusneighborsclassier get_params sklearn linear_model perceptron method get_params sklearn neighbors radiusneighborsregressor method method method method method method get_params sklearn pls cca method get_params sklearn pls plscanonical method get_params sklearn pls plsregression method get_params sklearn pls plssvd method get_params sklearn preprocessing binarizer method get_params sklearn preprocessing kernelcenterer get_params sklearn preprocessing labelbinarizer get_params sklearn preprocessing normalizer get_params sklearn preprocessing scaler method get_params sklearn qda qda method get_params sklearn semi_supervised labelpropagation get_params sklearn semi_supervised labelspreading get_params sklearn svm linearsvc method get_params sklearn svm nusvc method get_params sklearn svm nusvr method get_params sklearn svm oneclasssvm method get_params sklearn svm svc method get_params sklearn svm svr method get_params sklearn tree decisiontreeclassier get_params sklearn tree decisiontreeregressor get_params sklearn tree extratreeclassier method get_params sklearn tree extratreeregressor method method method get_stop_words sklearn feature_extraction text countvectorizer get_stop_words sklearn feature_extraction text tdfvectorizer get_params sklearn multiclass outputcodeclassier method get_params sklearn naive_bayes bernoullinb method get_params sklearn naive_bayes gaussiannb method method sklearn naive_bayes multinomialnb method get_params method get_support sklearn feature_selection selectfdr get_support sklearn feature_selection selectfpr get_support sklearn feature_selection selectfwe method index get_support sklearn feature_selection selectkbest inverse_transform sklearn linear_model randomizedlasso get_support sklearn feature_selection selectpercentile inverse_transform sklearn linear_model randomizedlogisticregression get_support sklearn linear_model randomizedlasso inverse_transform sklearn preprocessing labelbinarizer get_support sklearn linear_model randomizedlogisticregression inverse_transform sklearn preprocessing scaler scikit learn user guide release method method method method gmm class sklearn mixture gmmhmm class sklearn hmm gradientboostingclassier class sklearn ensemble gradientboostingregressor class sklearn ensemble graph_lasso module sklearn covariance graphlasso class sklearn covariance graphlassocv class sklearn covariance grid_to_graph sklearn feature_extraction image gridsearchcv class sklearn grid_search hinge_loss module sklearn metrics homogeneity_completeness_v_measure sklearn metrics homogeneity_score module sklearn metrics img_to_graph module sklearn feature_extraction image inverse_transform sklearn decomposition kernelpca method method method method method method method method method method method isomap class sklearn manifold itergrid class sklearn grid_search k_means module sklearn cluster kernel_metrics module sklearn metrics pairwise module kernelcenterer class sklearn preprocessing kernelpca class sklearn decomposition kfold class sklearn cross_validation kmeans class sklearn cluster kneighbors sklearn neighbors kneighborsclassier kneighbors sklearn neighbors kneighborsregressor module kneighbors sklearn neighbors nearestneighbors kneighbors_graph module sklearn neighbors kneighbors_graph sklearn neighbors kneighborsclassier kneighbors_graph sklearn neighbors kneighborsregressor kneighbors_graph sklearn neighbors nearestneighbors method method method inverse_transform sklearn decomposition pca inverse_transform sklearn feature_extraction dictvectorizer kneighborsclassier class sklearn neighbors kneighborsregressor class sklearn neighbors inverse_transform sklearn decomposition probabilisticpca inverse_transform sklearn decomposition randomizedpca l1_min_c module sklearn svm labelbinarizer class sklearn preprocessing labelpropagation class sklearn semi_supervised labelspreading class sklearn semi_supervised lars class sklearn linear_model lars_path module sklearn linear_model larscv class sklearn linear_model lasso class sklearn linear_model lasso class sklearn linear_model sparse lasso_path module sklearn linear_model lasso_stability_path module sklearn linear_model inverse_transform sklearn feature_extraction text countvectorizer inverse_transform sklearn feature_extraction text tdfvectorizer sklearn feature_selection selectfdr sklearn feature_selection selectfpr inverse_transform inverse_transform method method method method inverse_transform sklearn feature_selection selectfwe inverse_transform sklearn feature_selection selectkbest inverse_transform sklearn feature_selection selectpercentile method method method method lassocv class sklearn linear_model lassolars class sklearn linear_model lassolarscv class sklearn linear_model lassolarsic class sklearn linear_model lda class sklearn lda index leaveonelabelout class sklearn cross_validation leaveoneout class sklearn cross_validation leaveplabelout class sklearn cross_validation leavepout class sklearn cross_validation ledoit_wolf module sklearn covariance ledoitwolf class sklearn covariance linear module sklearn gaussian_process correlation_models linear module sklearn gaussian_process regression_models scikit learn user guide release make_friedman2 module sklearn datasets make_friedman3 module sklearn datasets make_hastie_10_2 module sklearn datasets make_low_rank_matrix module sklearn datasets make_moons module sklearn datasets make_multilabel_classication sklearn datasets module make_regression module sklearn datasets make_s_curve module sklearn datasets make_sparse_coded_signal module sklearn datasets make_sparse_spd_matrix module sklearn datasets make_sparse_uncorrelated module sklearn datasets make_spd_matrix module sklearn datasets make_swiss_roll module sklearn datasets manhattan_distances module sklearn metrics pairwise mean_shift module sklearn cluster mean_squared_error module sklearn metrics meanshift class sklearn cluster mincovdet class sklearn covariance minibatchdictionarylearning class sklearn decomposition minibatchkmeans class sklearn cluster minibatchsparsepca class sklearn decomposition multilabel_ sklearn multiclass onevsrestclassier tribute multinomialhmm class sklearn hmm multinomialnb class sklearn naive_bayes nearestcentroid class sklearn neighbors nearestneighbors class sklearn neighbors nmf class sklearn decomposition normalize module sklearn preprocessing normalizer class sklearn preprocessing nusvc class sklearn svm nusvr class sklearn svm oas class sklearn covariance oas module sklearn covariance oneclasssvm class sklearn svm onevsoneclassier class sklearn multiclass onevsrestclassier class sklearn multiclass orthogonal_mp module sklearn linear_model orthogonal_mp_gram module sklearn linear_model orthogonalmatchingpursuit sklearn linear_model class linear_kernel module sklearn metrics pairwise linearregression class sklearn linear_model linearsvc class sklearn svm load_20newsgroups module sklearn datasets load_boston module sklearn datasets load_diabetes module sklearn datasets load_digits module sklearn datasets load_les module sklearn datasets load_iris module sklearn datasets load_lfw_pairs module sklearn datasets load_lfw_people module sklearn datasets load_linnerud module sklearn datasets load_sample_image module sklearn datasets load_sample_images module sklearn datasets load_svmlight_le module sklearn datasets locally_linear_embedding module sklearn manifold locallylinearembedding class sklearn manifold logisticregression class sklearn linear_model lower_bound sklearn mixture dpgmm method lower_bound sklearn mixture vbgmm method mahalanobis sklearn covariance ellipticenvelope mahalanobis sklearn covariance empiricalcovariance mahalanobis sklearn covariance graphlasso method method method mahalanobis method sklearn covariance graphlassocv mahalanobis sklearn covariance ledoitwolf method mahalanobis sklearn covariance mincovdet method mahalanobis sklearn covariance oas method mahalanobis sklearn covariance shrunkcovariance method make_blobs module sklearn datasets make_circles module sklearn datasets make_classication module sklearn datasets make_friedman1 module sklearn datasets index scikit learn user guide release outputcodeclassier class sklearn multiclass pairwise_distances module sklearn metrics pairwise pairwise_kernels module sklearn metrics pairwise predict predict predict sklearn ensemble gradientboostingregressor method sklearn ensemble randomforestclassier method method sklearn ensemble randomforestregressor partial_t sklearn cluster minibatchkmeans method partial_t sklearn decomposition minibatchdictionarylearning method partial_t sklearn linear_model perceptron method partial_t sklearn linear_model sgdclassier predict sklearn feature_selection rfe method predict sklearn feature_selection rfecv method predict sklearn gaussian_process gaussianprocess predict sklearn hmm gmmhmm method predict sklearn hmm multinomialhmm method predict sklearn lda lda method predict sklearn linear_model ardregression method method method method method method index partial_t sklearn linear_model sgdregressor predict sklearn linear_model bayesianridge method partial_t sklearn linear_model sparse sgdclassier partial_t sklearn linear_model sparse sgdregressor patchextractor class sklearn feature_extraction image sklearn linear_model elasticnetcv method sklearn linear_model lassocv static method path path static pca class sklearn decomposition perceptron class sklearn linear_model permutation_test_score sklearn cross_validation pipeline class sklearn pipeline plscanonical class sklearn pls plsregression class sklearn pls plssvd class sklearn pls polynomial_kernel sklearn metrics pairwise predict sklearn linear_model elasticnet method sklearn linear_model elasticnetcv method predict predict sklearn linear_model lars method predict sklearn linear_model larscv method predict sklearn linear_model lasso method predict sklearn linear_model lassocv method predict sklearn linear_model lassolars method predict sklearn linear_model lassolarscv method sklearn linear_model lassolarsic method sklearn linear_model linearregression sklearn linear_model logisticregression module predict predict predict module predict sklearn linear_model orthogonalmatchingpursuit method method method precision_recall_curve module sklearn metrics precision_recall_fscore_support module sklearn metrics precision_score module sklearn metrics predict module sklearn svm libsvm predict sklearn cluster kmeans method predict sklearn cluster minibatchkmeans method predict sklearn covariance ellipticenvelope method predict sklearn linear_model perceptron method predict sklearn linear_model ridge method predict sklearn linear_model ridgeclassier method predict sklearn linear_model ridgeclassiercv method predict sklearn linear_model ridgecv method predict sklearn linear_model sgdclassier method sklearn ensemble extratreesclassier predict sklearn linear_model sgdregressor method method sklearn ensemble extratreesregressor method sklearn ensemble gradientboostingclassier method predict predict sklearn linear_model sparse elasticnet method sklearn linear_model sparse lasso method predict predict predict predict predict method method sklearn linear_model sparse sgdclassier predict_log_proba sklearn linear_model logisticregression sklearn linear_model sparse sgdregressor predict_log_proba sklearn naive_bayes bernoullinb predict sklearn naive_bayes bernoullinb method predict sklearn naive_bayes gaussiannb method predict sklearn naive_bayes multinomialnb method predict_log_proba sklearn svm svc method predict_log_proba sklearn svm svr method predict_log_proba sklearn tree decisiontreeclassier sklearn neighbors kneighborsclassier predict_log_proba sklearn tree extratreeclassier predict sklearn mixture dpgmm method predict sklearn mixture gmm method predict sklearn mixture vbgmm method predict sklearn multiclass onevsoneclassier predict predict method method method sklearn multiclass onevsrestclassier sklearn multiclass outputcodeclassier method predict predict predict predict predict sklearn neighbors kneighborsregressor method sklearn neighbors nearestcentroid method sklearn neighbors radiusneighborsclassier method sklearn neighbors radiusneighborsregressor method predict sklearn pipeline pipeline method predict sklearn pls cca method predict sklearn pls plscanonical method predict sklearn pls plsregression method predict sklearn qda qda method predict sklearn semi_supervised labelpropagation scikit learn user guide release method method method method predict_log_proba sklearn naive_bayes gaussiannb predict_log_proba sklearn naive_bayes multinomialnb predict_log_proba sklearn qda qda method predict_log_proba sklearn svm nusvc method predict_log_proba sklearn svm nusvr method predict_log_proba sklearn svm oneclasssvm method method method method method method method predict_ovo module sklearn multiclass predict_ovr module sklearn multiclass predict_proba module sklearn svm libsvm predict_proba sklearn ensemble extratreesclassier predict_proba sklearn ensemble gradientboostingclassier predict_proba sklearn ensemble randomforestclassier predict_proba sklearn hmm gmmhmm method predict_proba sklearn hmm multinomialhmm predict_proba sklearn lda lda method predict_proba sklearn linear_model logisticregression method method method predict sklearn semi_supervised labelspreading predict_proba sklearn linear_model perceptron predict sklearn svm linearsvc method predict sklearn svm nusvc method predict sklearn svm nusvr method predict sklearn svm oneclasssvm method predict sklearn svm svc method predict sklearn svm svr method predict sklearn tree decisiontreeclassier method sklearn tree decisiontreeregressor method predict predict sklearn tree extratreeclassier method predict sklearn tree extratreeregressor method predict_ecoc module sklearn multiclass predict_log_proba sklearn ensemble extratreesclassier predict_log_proba sklearn ensemble randomforestclassier method method predict_log_proba sklearn lda lda method predict_proba sklearn linear_model sgdclassier predict_proba sklearn linear_model sparse sgdclassier predict_proba sklearn mixture dpgmm method predict_proba sklearn mixture gmm method predict_proba sklearn mixture vbgmm method sklearn naive_bayes bernoullinb predict_proba predict_proba sklearn naive_bayes gaussiannb predict_proba sklearn naive_bayes multinomialnb predict_proba sklearn pipeline pipeline method predict_proba sklearn qda qda method predict_proba sklearn semi_supervised labelpropagation method method method method method method method index scikit learn user guide release predict_proba sklearn semi_supervised labelspreading method predict_proba sklearn svm nusvc method predict_proba sklearn svm nusvr method predict_proba sklearn svm oneclasssvm method predict_proba sklearn svm svc method predict_proba sklearn svm svr method predict_proba sklearn tree decisiontreeclassier predict_proba sklearn tree extratreeclassier method method probabilisticpca class sklearn decomposition projectedgradientnmf class sklearn decomposition pure_nugget module sklearn gaussian_process correlation_models qda class sklearn qda quadratic module sklearn gaussian_process regression_models query sklearn neighbors balltree method query_radius sklearn neighbors balltree method r2_score module sklearn metrics radius_neighbors sklearn neighbors nearestneighbors radius_neighbors sklearn neighbors radiusneighborsclassier radius_neighbors sklearn neighbors radiusneighborsregressor method method method radius_neighbors_graph module sklearn neighbors radius_neighbors_graph sklearn neighbors nearestneighbors method radius_neighbors_graph sklearn neighbors radiusneighborsclassier method radius_neighbors_graph sklearn neighbors radiusneighborsregressor method radiusneighborsclassier class sklearn neighbors randomizedlasso class sklearn linear_model randomizedlogisticregression class sklearn linear_model randomizedpca class sklearn decomposition rbf_kernel module sklearn metrics pairwise rbfsampler class sklearn kernel_approximation recall_score module sklearn metrics reconstruct_from_patches_2d sklearn feature_extraction image module sklearn manifold isomap reconstruction_error method reduced_likelihood_function sklearn gaussian_process gaussianprocess method resample module sklearn utils restrict sklearn feature_extraction dictvectorizer reweight_covariance sklearn covariance ellipticenvelope method method reweight_covariance method sklearn covariance mincovdet rfe class sklearn feature_selection rfecv class sklearn feature_selection ridge class sklearn linear_model ridgeclassier class sklearn linear_model ridgeclassiercv class sklearn linear_model ridgecv class sklearn linear_model roc_curve module sklearn metrics rvs sklearn hmm gmmhmm method rvs sklearn hmm multinomialhmm method rvs sklearn mixture dpgmm method rvs sklearn mixture gmm method rvs sklearn mixture vbgmm method sample sklearn hmm gmmhmm method sample sklearn hmm multinomialhmm method sample sklearn mixture dpgmm method sample sklearn mixture gmm method sample sklearn mixture vbgmm method scale module sklearn preprocessing scaler class sklearn preprocessing score sklearn cluster kmeans method score sklearn cluster minibatchkmeans method score sklearn covariance ellipticenvelope method method score sklearn covariance graphlasso method score sklearn covariance graphlassocv method score sklearn covariance ledoitwolf method score sklearn covariance mincovdet method radiusneighborsregressor class sklearn neighbors score sklearn covariance empiricalcovariance randomforestclassier class sklearn ensemble randomforestregressor class sklearn ensemble index scikit learn user guide release score sklearn covariance oas method score sklearn covariance shrunkcovariance method score sklearn decomposition probabilisticpca score sklearn ensemble extratreesclassier method method score sklearn linear_model ridgecv method score score sklearn linear_model sgdclassier method sklearn linear_model sgdregressor method score sklearn linear_model sparse elasticnet method score sklearn ensemble extratreesregressor method score score score score sklearn ensemble gradientboostingclassier method sklearn ensemble gradientboostingregressor method sklearn ensemble randomforestclassier method method sklearn ensemble randomforestregressor score sklearn linear_model sparse lasso method score sklearn linear_model sparse sgdclassier score sklearn linear_model sparse sgdregressor method method score sklearn mixture dpgmm method score sklearn mixture gmm method score sklearn mixture vbgmm method score sklearn multiclass onevsoneclassier method score sklearn feature_selection rfe method score sklearn feature_selection rfecv method score sklearn gaussian_process gaussianprocess method score sklearn multiclass outputcodeclassier method score sklearn hmm gmmhmm method score sklearn hmm multinomialhmm method score sklearn lda lda method score sklearn linear_model ardregression method score sklearn linear_model bayesianridge method score sklearn linear_model elasticnet method score sklearn linear_model elasticnetcv method score sklearn linear_model lars method score sklearn linear_model larscv method score sklearn linear_model lasso method score sklearn linear_model lassocv method score sklearn linear_model lassolars method score sklearn linear_model lassolarscv method score sklearn linear_model lassolarsic method score sklearn linear_model linearregression method score sklearn linear_model logisticregression method method score sklearn linear_model orthogonalmatchingpursuit score sklearn linear_model perceptron method score sklearn linear_model ridge method score sklearn linear_model ridgeclassier method sklearn linear_model ridgeclassiercv method score index score sklearn naive_bayes bernoullinb method score sklearn naive_bayes gaussiannb method score sklearn naive_bayes multinomialnb method sklearn neighbors kneighborsclassier sklearn neighbors kneighborsregressor score score method method score sklearn neighbors nearestcentroid method score sklearn neighbors radiusneighborsclassier method sklearn neighbors radiusneighborsregressor method score score sklearn pipeline pipeline method score sklearn qda qda method score sklearn semi_supervised labelpropagation score sklearn semi_supervised labelspreading method method score sklearn svm linearsvc method score sklearn svm nusvc method score sklearn svm nusvr method score sklearn svm svc method score sklearn svm svr method score sklearn tree decisiontreeclassier method sklearn tree decisiontreeregressor method score score sklearn tree extratreeclassier method score sklearn tree extratreeregressor method selectfdr class sklearn feature_selection selectfpr class sklearn feature_selection selectfwe class sklearn feature_selection selectkbest class sklearn feature_selection selectpercentile class sklearn feature_selection scikit learn user guide release set_params sklearn cluster afnitypropagation set_params sklearn ensemble gradientboostingregressor method set_params sklearn cluster dbscan method set_params sklearn cluster kmeans method set_params sklearn cluster meanshift method set_params sklearn cluster minibatchkmeans method method method set_params sklearn ensemble randomforestclassier set_params sklearn ensemble randomforestregressor set_params sklearn feature_extraction dictvectorizer set_params sklearn cluster spectralclustering method method set_params sklearn cluster ward method set_params sklearn covariance ellipticenvelope set_params sklearn feature_extraction image patchextractor set_params sklearn feature_extraction text countvectorizer set_params sklearn covariance empiricalcovariance set_params sklearn feature_extraction text tdftransformer set_params set_params set_params set_params sklearn covariance graphlasso method set_params sklearn feature_extraction text tdfvectorizer sklearn covariance graphlassocv sklearn feature_selection rfe method sklearn covariance ledoitwolf method set_params sklearn feature_selection rfecv method sklearn covariance mincovdet method sklearn feature_selection selectfdr set_params set_params set_params sklearn covariance oas method set_params sklearn covariance shrunkcovariance method method set_params sklearn feature_selection selectfpr set_params sklearn feature_selection selectfwe set_params sklearn feature_selection selectkbest method method method method set_params sklearn decomposition dictionarylearning method set_params sklearn decomposition fastica method method set_params sklearn decomposition kernelpca method set_params sklearn feature_selection selectpercentile set_params sklearn gaussian_process gaussianprocess set_params sklearn decomposition minibatchdictionarylearning method set_params sklearn decomposition minibatchsparsepca method set_params sklearn decomposition nmf method set_params sklearn decomposition pca method set_params sklearn decomposition probabilisticpca set_params sklearn grid_search gridsearchcv set_params sklearn hmm gmmhmm method set_params sklearn hmm multinomialhmm method set_params sklearn kernel_approximation additivechi2sampler set_params sklearn decomposition projectedgradientnmf set_params sklearn kernel_approximation rbfsampler set_params sklearn decomposition randomizedpca set_params sklearn kernel_approximation skewedchi2sampler method method method method method method method method method method method method method method method method method set_params sklearn decomposition sparsecoder set_params sklearn decomposition sparsepca method set_params sklearn ensemble extratreesclassier method set_params sklearn ensemble extratreesregressor set_params sklearn lda lda method set_params sklearn linear_model ardregression set_params sklearn linear_model bayesianridge set_params sklearn linear_model elasticnet method set_params sklearn linear_model elasticnetcv set_params sklearn linear_model lars method method method method set_params sklearn ensemble gradientboostingclassier method index scikit learn user guide release set_params sklearn linear_model larscv method set_params sklearn naive_bayes bernoullinb method set_params sklearn linear_model lasso method set_params sklearn linear_model lassocv method set_params sklearn naive_bayes gaussiannb method set_params sklearn linear_model lassolars method method set_params sklearn linear_model lassolarscv method set_params sklearn neighbors kneighborsclassier set_params sklearn neighbors kneighborsregressor set_params sklearn linear_model lassolarsic method sklearn naive_bayes multinomialnb set_params set_params sklearn linear_model linearregression method set_params sklearn linear_model logisticregression method set_params sklearn linear_model orthogonalmatchingpursuit method set_params sklearn linear_model perceptron method method set_params sklearn linear_model randomizedlasso set_params sklearn neighbors nearestcentroid set_params sklearn neighbors nearestneighbors set_params sklearn neighbors radiusneighborsclassier set_params sklearn neighbors radiusneighborsregressor set_params sklearn linear_model randomizedlogisticregression set_params sklearn linear_model ridge method set_params sklearn linear_model ridgeclassier set_params sklearn pipeline pipeline method set_params sklearn pls cca method set_params sklearn pls plscanonical method set_params sklearn pls plsregression method set_params sklearn pls plssvd method set_params sklearn preprocessing binarizer method set_params sklearn preprocessing kernelcenterer set_params sklearn preprocessing labelbinarizer set_params sklearn preprocessing normalizer method set_params sklearn preprocessing scaler method set_params sklearn qda qda method set_params sklearn semi_supervised labelpropagation set_params sklearn semi_supervised labelspreading set_params sklearn svm linearsvc method set_params sklearn svm nusvc method set_params sklearn svm nusvr method set_params sklearn svm oneclasssvm method set_params sklearn svm svc method set_params sklearn svm svr method set_params sklearn tree decisiontreeclassier set_params sklearn tree decisiontreeregressor set_params sklearn tree extratreeclassier method set_params sklearn tree extratreeregressor method method method sgdclassier class sklearn linear_model sgdclassier class sklearn linear_model sparse sgdregressor class sklearn linear_model method method method method method method method method method method method method method method method method method method method set_params sklearn linear_model ridgeclassiercv method set_params sklearn linear_model ridgecv method method set_params sklearn linear_model sgdclassier set_params sklearn linear_model sgdregressor set_params sklearn linear_model sparse elasticnet method set_params sklearn linear_model sparse lasso method set_params sklearn linear_model sparse sgdclassier set_params sklearn linear_model sparse sgdregressor set_params sklearn manifold isomap method set_params sklearn manifold locallylinearembedding set_params sklearn mixture dpgmm method set_params sklearn mixture gmm method set_params sklearn mixture vbgmm method set_params sklearn multiclass onevsoneclassier set_params sklearn multiclass onevsrestclassier set_params sklearn multiclass outputcodeclassier index scikit learn user guide release sgdregressor class sklearn linear_model sparse staged_decision_function shrunk_covariance module sklearn covariance shrunkcovariance class sklearn covariance shufe module sklearn utils shufesplit class sklearn cross_validation sigma sklearn naive_bayes gaussiannb attribute silhouette_score module sklearn metrics skewedchi2sampler class sklearn kernel_approximation sklearn cluster module sklearn covariance module sklearn cross_validation module sklearn datasets module sklearn decomposition module sklearn ensemble module sklearn feature_extraction module sklearn feature_extraction image module sklearn feature_extraction text module sklearn feature_selection module sklearn gaussian_process module sklearn grid_search module sklearn hmm module sklearn kernel_approximation module sklearn lda module sklearn linear_model module sklearn linear_model sparse module sklearn manifold module sklearn metrics module sklearn metrics cluster module sklearn metrics pairwise module sklearn mixture module sklearn multiclass module sklearn naive_bayes module sklearn neighbors module sklearn pipeline module sklearn pls module sklearn preprocessing module sklearn qda module sklearn semi_supervised module sklearn svm module sklearn tree module sklearn utils module sparse_encode module sklearn decomposition sparsecoder class sklearn decomposition sparsepca class sklearn decomposition spectral_clustering module sklearn cluster spectralclustering class sklearn cluster squared_exponential module sklearn gaussian_process correlation_models staged_decision_function sklearn ensemble gradientboostingclassier method sklearn ensemble gradientboostingregressor method staged_predict sklearn ensemble gradientboostingregressor method startprob_ sklearn hmm gmmhmm attribute startprob_ sklearn hmm multinomialhmm attribute stratiedkfold class sklearn cross_validation stratiedshufesplit class sklearn cross_validation svc class sklearn svm svr class sklearn svm tdftransformer sklearn feature_extraction text tdfvectorizer class sklearn feature_extraction text class theta sklearn naive_bayes gaussiannb attribute train_test_split module sklearn cross_validation transform sklearn cluster kmeans method transform sklearn cluster minibatchkmeans method transform sklearn decomposition dictionarylearning method transform sklearn decomposition fastica method transform sklearn decomposition kernelpca method transform sklearn decomposition minibatchdictionarylearning method method method method method method transform sklearn decomposition minibatchsparsepca transform sklearn decomposition nmf method transform sklearn decomposition pca method transform sklearn decomposition probabilisticpca transform sklearn decomposition projectedgradientnmf transform sklearn decomposition randomizedpca transform sklearn decomposition sparsecoder transform sklearn decomposition sparsepca method transform sklearn ensemble extratreesclassier transform sklearn ensemble extratreesregressor transform sklearn ensemble randomforestclassier method method method index scikit learn user guide release transform sklearn ensemble randomforestregressor method transform sklearn feature_extraction dictvectorizer transform sklearn pls plsregression method transform sklearn pls plssvd method transform sklearn preprocessing binarizer method transform sklearn feature_extraction image patchextractor transform sklearn preprocessing kernelcenterer transform sklearn feature_extraction text countvectorizer transform sklearn preprocessing labelbinarizer method method transform sklearn feature_extraction text tdftransformer transform sklearn preprocessing normalizer method method method method method method transform sklearn feature_extraction text tdfvectorizer transform sklearn feature_selection rfe method transform sklearn feature_selection rfecv method transform sklearn feature_selection selectfdr method method transform sklearn preprocessing scaler method transform sklearn svm linearsvc method transform sklearn tree decisiontreeclassier method transform sklearn tree decisiontreeregressor transform sklearn tree extratreeclassier method transform sklearn tree extratreeregressor method transmat_ sklearn hmm gmmhmm attribute transmat_ sklearn hmm multinomialhmm attribute transform sklearn feature_selection selectfpr method transform sklearn feature_selection selectfwe transform sklearn feature_selection selectkbest method method transform sklearn feature_selection selectpercentile transform method method method sklearn kernel_approximation rbfsampler transform sklearn kernel_approximation additivechi2sampler v_measure_score module sklearn metrics vbgmm class sklearn mixture transform sklearn kernel_approximation skewedchi2sampler ward class sklearn cluster ward_tree module sklearn cluster zero_one module sklearn metrics zero_one_score module sklearn metrics transform sklearn lda lda method transform sklearn linear_model perceptron method sklearn linear_model logisticregression method method transform transform sklearn linear_model randomizedlasso transform sklearn linear_model randomizedlogisticregression method method method method method method method transform sklearn linear_model sgdclassier transform sklearn linear_model sgdregressor transform sklearn linear_model sparse sgdclassier transform sklearn linear_model sparse sgdregressor transform sklearn manifold isomap method transform sklearn manifold locallylinearembedding transform sklearn pipeline pipeline method transform sklearn pls cca method transform sklearn pls plscanonical method index 
